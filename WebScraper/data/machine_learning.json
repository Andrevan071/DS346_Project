[
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-09-12 01:06:21Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/2480650/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm aware of the gradient descent and the back-propagation algorithm. What I don't get is: when is using a bias important and how do you use it?</p>\n<p>For example, when mapping the <code>AND</code> function, when I use two inputs and one output, it does not give the correct weights. However, when I use three inputs (one of which is a bias), it gives the correct weights.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think that biases are almost always helpful.  In effect, <strong>a bias value allows you to shift the activation function to the left or right</strong>, which may be critical for successful learning.</p>\n<p>It might help to look at a simple example.  Consider this 1-input, 1-output network that has no bias:</p>\n<p><img alt=\"simple network\" src=\"https://i.sstatic.net/bI2Tm.gif\"/></p>\n<p>The output of the network is computed by multiplying the input (x) by the weight (w<sub>0</sub>) and passing the result through some kind of activation function (e.g. a sigmoid function.)</p>\n<p>Here is the function that this network computes, for various values of w<sub>0</sub>:</p>\n<p><img alt=\"network output, given different w0 weights\" src=\"https://i.sstatic.net/ddyfr.png\"/></p>\n<p>Changing the weight w<sub>0</sub> essentially changes the \"steepness\" of the sigmoid.  That's useful, but what if you wanted the network to output 0 when x is 2?  Just changing the steepness of the sigmoid won't really work -- <strong>you want to be able to shift the entire curve to the right</strong>.</p>\n<p>That's exactly what the bias allows you to do.  If we add a bias to that network, like so:</p>\n<p><img alt=\"simple network with a bias\" src=\"https://i.sstatic.net/oapHD.gif\"/></p>\n<p>...then the output of the network becomes sig(w<sub>0</sub>*x + w<sub>1</sub>*1.0).  Here is what the output of the network looks like for various values of w<sub>1</sub>:</p>\n<p><img alt=\"network output, given different w1 weights\" src=\"https://i.sstatic.net/t2mC3.png\"/></p>\n<p>Having a weight of -5 for w<sub>1</sub> shifts the curve to the right, which allows us to have a network that outputs 0 when x is 2.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A simpler way to understand what the bias is: it is somehow similar to the constant <em>b</em> of a linear function</p>\n<p><em>y = ax + b</em></p>\n<p>It allows you to move the line up and down to fit the prediction with the data better.</p>\n<p>Without <em>b</em>, the line always goes through the origin (0, 0) and you may get a poorer fit.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here are some further illustrations showing the result of a simple 2-layer feed forward neural network with and without bias units on a two-variable regression problem. Weights are initialized randomly and standard ReLU activation is used. As the answers before me concluded, without the bias the ReLU-network is not able to deviate from zero at (0,0).</p>\n<p><a href=\"https://i.sstatic.net/nsDCc.gif\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/nsDCc.gif\"/></a>\n<a href=\"https://i.sstatic.net/7rl1h.gif\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/7rl1h.gif\"/></a>\n<a href=\"https://i.sstatic.net/cd2y2.gif\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/cd2y2.gif\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 09:53:41Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/879432/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is the difference between a <strong>generative</strong> and a\n<strong>discriminative</strong> algorithm?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's say you have input data <code>x</code> and you want to classify the data into labels <code>y</code>. A generative model learns the <strong>joint</strong> probability distribution <code>p(x,y)</code> and a discriminative model learns the <strong>conditional</strong> probability distribution <code>p(y|x)</code> - which you should read as <em>\"the probability of <code>y</code> given <code>x</code>\"</em>.</p>\n<p>Here's a really simple example. Suppose you have the following data in the form <code>(x,y)</code>:</p>\n<p><code>(1,0), (1,0), (2,0), (2, 1)</code></p>\n<p><code>p(x,y)</code> is</p>\n<pre><code>      y=0   y=1\n     -----------\nx=1 | 1/2   0\nx=2 | 1/4   1/4\n</code></pre>\n<p><code>p(y|x)</code> is</p>\n<pre><code>      y=0   y=1\n     -----------\nx=1 | 1     0\nx=2 | 1/2   1/2\n</code></pre>\n<p>If you take a few minutes to stare at those two matrices, you will understand the difference between the two probability distributions.</p>\n<p>The distribution <code>p(y|x)</code> is the natural distribution for classifying a given example <code>x</code> into a class <code>y</code>, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model <code>p(x,y)</code>, which can be transformed into <code>p(y|x)</code> by applying Bayes rule and then used for classification. However, the distribution <code>p(x,y)</code> can also be used for other purposes. For example, you could use <code>p(x,y)</code> to <em>generate</em> likely <code>(x,y)</code> pairs.</p>\n<p>From the description above, you might be thinking that generative models are more generally useful and therefore better, but it's not as simple as that. <a href=\"http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf\" rel=\"noreferrer\">This paper</a> is a very popular reference on the subject of discriminative vs. generative classifiers, but it's pretty heavy going. The overall gist is that discriminative models generally outperform generative models in classification tasks.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A <strong>generative algorithm</strong> models how the data was generated in order to categorize a signal.  It asks the question: based on my generation assumptions, which category is most likely to generate this signal?</p>\n<p>A <strong>discriminative algorithm</strong> does not care about how the data was generated, it simply categorizes a given signal.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Imagine your task is to classify a speech to a language.</p>\n<p>You can do it by either:</p>\n<ol>\n<li>learning each language, and then classifying it using the knowledge you just gained</li>\n</ol>\n<p>or</p>\n<ol start=\"2\">\n<li>determining the difference in the linguistic models without learning the languages, and then classifying the speech.</li>\n</ol>\n<p>The first one is the <strong>generative</strong> approach and the second one is the <strong>discriminative</strong> approach.</p>\n<p>Check this reference for more details: <a href=\"http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf\" rel=\"noreferrer\">http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 10:21:21Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/10059594/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I am finding it hard to understand the process of Naive Bayes, and I was wondering if someone could explain it with a simple step by step process in English. I understand it takes comparisons by times occurred as a probability, but I have no idea how the training data is related to the actual dataset.</p>\n<p>Please give me an explanation of what role the training set plays. I am giving a very simple example for fruits here, like banana for example</p>\n<pre><code>training set---\nround-red\nround-orange\noblong-yellow\nround-red\n\ndataset----\nround-red\nround-orange\nround-red\nround-orange\noblong-yellow\nround-red\nround-orange\noblong-yellow\noblong-yellow\nround-red\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The accepted answer has many elements of k-NN (<em>k</em>-nearest neighbors), a different algorithm.</p>\n<p>Both k-NN and NaiveBayes are classification algorithms. Conceptually, k-NN uses the idea of \"nearness\" to classify new entities. In k-NN 'nearness' is modeled with ideas such as Euclidean Distance or Cosine Distance. By contrast, in NaiveBayes, the concept of 'probability' is used to classify new entities.</p>\n<p>Since the question is about Naive Bayes, here's how I'd describe the ideas and steps to someone. I'll try to do it with as few equations and in plain English as much as possible.</p>\n<h3>First, Conditional Probability &amp; Bayes' Rule</h3>\n<p>Before someone can understand and appreciate the nuances of Naive Bayes', they need to know a couple of related concepts first, namely, the idea of Conditional Probability, and Bayes' Rule. (If you are familiar with these concepts, skip to the section titled <strong>Getting to Naive Bayes'</strong>)</p>\n<p><strong>Conditional Probability</strong> in plain English: What is the probability that something will happen, <em>given that something else</em> has already happened.</p>\n<p>Let's say that there is some Outcome O. And some Evidence E. From the way these probabilities are defined: The Probability of having <em>both</em> the Outcome O and Evidence E is:\n(Probability of O occurring) multiplied by the (Prob of E given that O happened)</p>\n<p><em>One Example to understand Conditional Probability:</em></p>\n<p>Let say we have a collection of US Senators. Senators could be Democrats or Republicans. They are also either male or female.</p>\n<p>If we select one senator completely randomly, what is the probability that this person is a female Democrat? Conditional Probability can help us answer that.</p>\n<p>Probability of (Democrat and Female Senator)= Prob(Senator is Democrat) multiplied by Conditional Probability of Being Female given that they are a Democrat.</p>\n<pre><code>  P(Democrat &amp; Female) = P(Democrat) * P(Female | Democrat) \n</code></pre>\n<p>We could compute the exact same thing, the reverse way:</p>\n<pre><code>  P(Democrat &amp; Female) = P(Female) * P(Democrat | Female) \n</code></pre>\n<h3>Understanding Bayes Rule</h3>\n<p>Conceptually, this is a way to go from P(Evidence| Known Outcome) to P(Outcome|Known Evidence). Often, we know how frequently some particular evidence is observed, <em>given a known outcome</em>. We have to use this known fact to compute the reverse, to compute the chance of that <em>outcome happening</em>, given the evidence.</p>\n<p>P(Outcome given that we know some Evidence) = P(Evidence given that we know the Outcome) times Prob(Outcome), scaled by the P(Evidence)</p>\n<p>The classic example to understand Bayes' Rule:</p>\n<pre><code>Probability of Disease D given Test-positive = \n\n               P(Test is positive|Disease) * P(Disease)\n     _______________________________________________________________\n     (scaled by) P(Testing Positive, with or without the disease)\n</code></pre>\n<p>Now, all this was just preamble, to get to Naive Bayes.</p>\n<h2>Getting to Naive Bayes'</h2>\n<p>So far, we have talked only about one piece of evidence. In reality, we have to predict an outcome given <strong>multiple evidence.</strong> In that case, the math gets very complicated. To get around that complication, one approach is to 'uncouple' multiple pieces of evidence, and to treat each of piece of evidence as independent. This approach is why this is called <em>naive</em> Bayes.</p>\n<pre><code>P(Outcome|Multiple Evidence) = \nP(Evidence1|Outcome) * P(Evidence2|outcome) * ... * P(EvidenceN|outcome) * P(Outcome)\nscaled by P(Multiple Evidence)\n</code></pre>\n<p>Many people choose to remember this as:</p>\n<pre><code>                      P(Likelihood of Evidence) * Prior prob of outcome\nP(outcome|evidence) = _________________________________________________\n                                         P(Evidence)\n</code></pre>\n<p>Notice a few things about this equation:</p>\n<ul>\n<li>If the Prob(evidence|outcome) is 1, then we are just multiplying by 1.</li>\n<li>If the Prob(some particular evidence|outcome) is 0, then the whole prob. becomes 0. If you see contradicting evidence, we can rule out that outcome.</li>\n<li>Since we divide everything by P(Evidence), we can even get away without calculating it.</li>\n<li>The intuition behind multiplying by the <em>prior</em> is so that we give high probability to more common outcomes, and low probabilities to unlikely outcomes. These are also called <code>base rates</code> and they are a way to scale our predicted probabilities.</li>\n</ul>\n<h3>How to Apply NaiveBayes to Predict an Outcome?</h3>\n<p>Just run the formula above for each possible outcome. Since we are trying to <em>classify</em>, each outcome is called a <code>class</code> and it has a <code>class label.</code> Our job is to look at the evidence, to consider how likely it is to be this class or that class, and assign a label to each entity.\nAgain, we take a very simple approach: The class that has the highest probability is declared the \"winner\" and that class label gets assigned to that combination of evidences.</p>\n<h3>Fruit Example</h3>\n<p>Let's try it out on an example to increase our understanding: The OP asked for a 'fruit' identification example.</p>\n<p>Let's say that we have data on 1000 pieces of fruit. They happen to be <strong>Banana</strong>, <strong>Orange</strong> or some <strong>Other Fruit</strong>.\nWe know 3 characteristics about each fruit:</p>\n<ol>\n<li>Whether it is Long</li>\n<li>Whether it is Sweet and</li>\n<li>If its color is Yellow.</li>\n</ol>\n<p>This is our 'training set.' We will use this to predict the type of any <em>new</em> fruit we encounter.</p>\n<pre><code>Type           Long | Not Long || Sweet | Not Sweet || Yellow |Not Yellow|Total\n             ___________________________________________________________________\nBanana      |  400  |    100   || 350   |    150    ||  450   |  50      |  500\nOrange      |    0  |    300   || 150   |    150    ||  300   |   0      |  300\nOther Fruit |  100  |    100   || 150   |     50    ||   50   | 150      |  200\n            ____________________________________________________________________\nTotal       |  500  |    500   || 650   |    350    ||  800   | 200      | 1000\n             ___________________________________________________________________\n</code></pre>\n<p>We can pre-compute a lot of things about our fruit collection.</p>\n<p>The so-called \"Prior\" probabilities. (If we didn't know any of the fruit attributes, this would be our guess.) These are our <code>base rates.</code></p>\n<pre><code> P(Banana)      = 0.5 (500/1000)\n P(Orange)      = 0.3\n P(Other Fruit) = 0.2\n</code></pre>\n<p>Probability of \"Evidence\"</p>\n<pre><code>p(Long)   = 0.5\nP(Sweet)  = 0.65\nP(Yellow) = 0.8\n</code></pre>\n<p>Probability of \"Likelihood\"</p>\n<pre><code>P(Long|Banana) = 0.8\nP(Long|Orange) = 0  [Oranges are never long in all the fruit we have seen.]\n ....\n\nP(Yellow|Other Fruit)     =  50/200 = 0.25\nP(Not Yellow|Other Fruit) = 0.75\n</code></pre>\n<h3>Given a Fruit, how to classify it?</h3>\n<p>Let's say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?</p>\n<p>We can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and 'classify' our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set):</p>\n<pre><code>P(Banana|Long, Sweet and Yellow) \n      P(Long|Banana) * P(Sweet|Banana) * P(Yellow|Banana) * P(banana)\n    = _______________________________________________________________\n                      P(Long) * P(Sweet) * P(Yellow)\n                      \n    = 0.8 * 0.7 * 0.9 * 0.5 / P(evidence)\n\n    = 0.252 / P(evidence)\n\n\nP(Orange|Long, Sweet and Yellow) = 0\n\n\nP(Other Fruit|Long, Sweet and Yellow)\n      P(Long|Other fruit) * P(Sweet|Other fruit) * P(Yellow|Other fruit) * P(Other Fruit)\n    = ____________________________________________________________________________________\n                                          P(evidence)\n\n    = (100/200 * 150/200 * 50/200 * 200/1000) / P(evidence)\n\n    = 0.01875 / P(evidence)\n</code></pre>\n<p>By an overwhelming margin (<code>0.252 &gt;&gt; 0.01875</code>), we classify this Sweet/Long/Yellow fruit as likely to be a Banana.</p>\n<h3>Why is Bayes Classifier so popular?</h3>\n<p>Look at what it eventually comes down to. Just some counting and multiplication. We can pre-compute all these terms, and so classifying becomes easy, quick and efficient.</p>\n<p><code>Let z = 1 / P(evidence).</code> Now we quickly compute the following three quantities.</p>\n<pre><code>P(Banana|evidence) = z * Prob(Banana) * Prob(Evidence1|Banana) * Prob(Evidence2|Banana) ...\nP(Orange|Evidence) = z * Prob(Orange) * Prob(Evidence1|Orange) * Prob(Evidence2|Orange) ...\nP(Other|Evidence)  = z * Prob(Other)  * Prob(Evidence1|Other)  * Prob(Evidence2|Other)  ...\n</code></pre>\n<p>Assign the class label of whichever is the highest number, and you are done.</p>\n<p>Despite the name, Naive Bayes turns out to be excellent in certain applications. Text classification is one area where it really shines.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your question as I understand it is divided in two parts, part one being you need a better understanding of the Naive Bayes classifier &amp; part two being the confusion surrounding Training set. </p>\n<p>In general all of Machine Learning Algorithms need to be trained for supervised learning tasks like classification, prediction etc. or for unsupervised learning tasks like clustering.</p>\n<p>During the training step, the algorithms are taught with a particular input dataset (training set) so that later on we may test them for unknown inputs (which they have never seen before) for which they may classify or predict etc (in case of supervised learning) based on their learning. This is what most of the Machine Learning techniques like Neural Networks, SVM, Bayesian etc. are based upon.</p>\n<p>So in a general Machine Learning project basically you have to divide your input set to a Development Set (Training Set + Dev-Test Set) &amp; a Test Set (or Evaluation set). Remember your basic objective would be that your system learns and classifies new inputs which they have never seen before in either Dev set or test set.</p>\n<p>The test set typically has the same format as the training set. However, it is very important that the test set be distinct from the training corpus: if we simply\nreused the training set as the test set, then a model that simply memorized its input, without learning how to generalize to new examples, would receive misleadingly high scores.</p>\n<p>In general, for an example, 70% of our data can be used as training set cases. Also remember to partition the original set into the training and test sets <em>randomly</em>.</p>\n<p>Now I come to your other question about Naive Bayes.</p>\n<p>To demonstrate the concept of Naïve Bayes Classification, consider the example given below:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Eh6HI.gif\"/></p>\n<p>As indicated, the objects can be classified as either <code>GREEN</code> or <code>RED</code>. Our task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently existing objects.</p>\n<p>Since there are twice as many <code>GREEN</code> objects as <code>RED</code>, it is reasonable to believe that a new case (which hasn't been observed yet) is twice as likely to have membership <code>GREEN</code> rather than <code>RED</code>. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of <code>GREEN</code> and <code>RED</code> objects, and often used to predict outcomes before they actually happen.</p>\n<p>Thus, we can write:</p>\n<p><strong>Prior Probability of <code>GREEN</code></strong>: <code>number of GREEN objects / total number of objects</code></p>\n<p><strong>Prior Probability of <code>RED</code></strong>: <code>number of RED objects / total number of objects</code></p>\n<p>Since there is a total of <code>60</code> objects, <code>40</code> of which are <code>GREEN</code> and 20 <code>RED</code>, our prior probabilities for class membership are:</p>\n<p><strong>Prior Probability for <code>GREEN</code></strong>: <code>40 / 60</code></p>\n<p><strong>Prior Probability for <code>RED</code></strong>: <code>20 / 60</code></p>\n<p>Having formulated our prior probability, we are now ready to classify a new object (<code>WHITE</code> circle in the diagram below). Since the objects are well clustered, it is reasonable to assume that the more <code>GREEN</code> (or <code>RED</code>) objects in the vicinity of X, the more likely that the new cases belong to that particular color. To measure this likelihood, we draw a circle around X which encompasses a number (to be chosen a priori) of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/gVpJF.gif\"/></p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/sh1zX.gif\"/></p>\n<p>From the illustration above, it is clear that Likelihood of <code>X</code> given <code>GREEN</code> is smaller than Likelihood of <code>X</code> given <code>RED</code>, since the circle encompasses <code>1</code> <code>GREEN</code> object and <code>3</code> <code>RED</code> ones. Thus:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/DLCqA.gif\"/></p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/cJzst.gif\"/></p>\n<p>Although the prior probabilities indicate that <code>X</code> may belong to <code>GREEN</code> (given that there are twice as many <code>GREEN</code> compared to <code>RED</code>) the likelihood indicates otherwise; that the class membership of <code>X</code> is <code>RED</code> (given that there are more <code>RED</code> objects in the vicinity of <code>X</code> than <code>GREEN</code>). In the Bayesian analysis, the final classification is produced by combining both sources of information, i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule (named after Rev. Thomas Bayes 1702-1761).</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/N8MPA.gif\"/></p>\n<p>Finally, we classify X as <code>RED</code> since its class membership achieves the largest posterior probability.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Naive Bayes comes under supervising machine learning which used to make classifications of data sets.\nIt is used to predict things based on its prior knowledge and independence assumptions.</p>\n<p>They call it <strong>naive</strong> because it’s assumptions (it assumes that all of the features in the dataset are equally important and independent) are really optimistic and rarely true in most real-world applications.</p>\n<p>It is classification algorithm which makes the decision for the unknown data set. It is based on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\" rel=\"nofollow noreferrer\">Bayes Theorem</a> which describe the probability of an event based on its prior knowledge.</p>\n<p>Below diagram shows how naive Bayes works</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0QOII.png\"/></a></p>\n<p><strong>Formula to predict NB:</strong></p>\n<p><a href=\"https://i.sstatic.net/0QOII.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/t5voX.png\"/></a></p>\n<p><strong>How to use Naive Bayes Algorithm ?</strong></p>\n<p>Let's take an example of how N.B woks</p>\n<p>Step 1: First we find out Likelihood of table which shows the probability of yes or no in below diagram.\nStep 2: Find the posterior probability of each class.</p>\n<p><a href=\"https://i.sstatic.net/t5voX.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ZSv4b.png\"/></a></p>\n<pre><code>Problem: Find out the possibility of whether the player plays in Rainy condition?\n\nP(Yes|Rainy) = P(Rainy|Yes) * P(Yes) / P(Rainy)\n\nP(Rainy|Yes) = 2/9 = 0.222\nP(Yes) = 9/14 = 0.64\nP(Rainy) = 5/14 = 0.36\n\nNow, P(Yes|Rainy) = 0.222*0.64/0.36 = 0.39 which is lower probability which means chances of the match played is low.\n</code></pre>\n<p>For more reference refer these <a href=\"https://medium.com/@jiteshmohite.619/introduction-to-naive-bayes-29c9c6c061b8\" rel=\"nofollow noreferrer\">blog.</a></p>\n<p>Refer GitHub Repository <a href=\"https://github.com/jiteshmohite/Naive-Bayes-Examples\" rel=\"nofollow noreferrer\">Naive-Bayes-Examples</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax\" rel=\"noreferrer\">tensorflow API docs</a> they use a keyword called <code>logits</code>. What is it? A lot of methods are written like:</p>\n<pre><code>tf.nn.softmax(logits, name=None)\n</code></pre>\n<p>If <code>logits</code> is just a generic <code>Tensor</code> input, why is it named <code>logits</code>?</p>\n<hr/>\n<p>Secondly, what is the difference between the following two methods?</p>\n<pre><code>tf.nn.softmax(logits, name=None)\ntf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)\n</code></pre>\n<p>I know what <code>tf.nn.softmax</code> does, but not the other. An example would be really helpful.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The softmax+logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.  It means, in particular, the sum of the inputs may not equal 1, that the values are <em>not</em> probabilities (you might have an input of 5). Internally, it first applies softmax to the unscaled output, and then and then computes the cross entropy of those values vs. what they \"should\" be as defined by the labels.</p>\n<p><code>tf.nn.softmax</code> produces the result of applying the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" rel=\"noreferrer\">softmax function</a> to an input tensor.  The softmax \"squishes\" the inputs so that <code>sum(input) = 1</code>, and it does the mapping by interpreting the inputs as log-probabilities (logits) and then converting them back into raw probabilities between 0 and 1.  The shape of output of a softmax is the same as the input:</p>\n<pre><code>a = tf.constant(np.array([[.1, .3, .5, .9]]))\nprint s.run(tf.nn.softmax(a))\n[[ 0.16838508  0.205666    0.25120102  0.37474789]]\n</code></pre>\n<p>See <a href=\"https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization\">this answer</a> for more about why softmax is used extensively in DNNs.</p>\n<p><code>tf.nn.softmax_cross_entropy_with_logits</code> combines the softmax step with the calculation of the cross-entropy loss after applying the softmax function, but it does it all together in a more mathematically careful way.  It's similar to the result of:</p>\n<pre><code>sm = tf.nn.softmax(x)\nce = cross_entropy(sm)\n</code></pre>\n<p>The cross entropy is a summary metric: it sums across the elements.  The output of <code>tf.nn.softmax_cross_entropy_with_logits</code> on a shape <code>[2,5]</code> tensor is of shape <code>[2,1]</code> (the first dimension is treated as the batch).</p>\n<p>If you want to do optimization to minimize the cross entropy <strong>AND</strong> you're softmaxing after your last layer, you should use <code>tf.nn.softmax_cross_entropy_with_logits</code> instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.  Otherwise, you'll end up hacking it by adding little epsilons here and there.</p>\n<p><strong>Edited 2016-02-07:</strong>\nIf you have single-class labels, where an object can only belong to one class, you might now  consider using <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> so that you don't have to convert your labels to a dense one-hot array.  This function was added after release 0.6.0.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Short version:</strong></p>\n<p>Suppose you have two tensors, where <code>y_hat</code> contains computed scores for each class (for example, from y = W*x +b) and <code>y_true</code> contains one-hot encoded true labels. </p>\n<pre><code>y_hat  = ... # Predicted label, e.g. y = tf.matmul(X, W) + b\ny_true = ... # True label, one-hot encoded\n</code></pre>\n<p>If you interpret the scores in <code>y_hat</code> as unnormalized log probabilities, then they are <strong>logits</strong>.</p>\n<p>Additionally, the total cross-entropy loss computed in this manner:</p>\n<pre><code>y_hat_softmax = tf.nn.softmax(y_hat)\ntotal_loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), [1]))\n</code></pre>\n<p>is essentially equivalent to the total cross-entropy loss computed with the function <code>softmax_cross_entropy_with_logits()</code>:</p>\n<pre><code>total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\n</code></pre>\n<p><strong>Long version:</strong></p>\n<p>In the output layer of your neural network, you will probably compute an array that contains the class scores for each of your training instances, such as from a computation <code>y_hat = W*x + b</code>. To serve as an example, below I've created a <code>y_hat</code> as a 2 x 3 array, where the rows correspond to the training instances and the columns correspond to classes. So here there are 2 training instances and 3 classes.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nsess = tf.Session()\n\n# Create example y_hat.\ny_hat = tf.convert_to_tensor(np.array([[0.5, 1.5, 0.1],[2.2, 1.3, 1.7]]))\nsess.run(y_hat)\n# array([[ 0.5,  1.5,  0.1],\n#        [ 2.2,  1.3,  1.7]])\n</code></pre>\n<p>Note that the values are not normalized (i.e. the rows don't add up to 1). In order to normalize them, we can apply the softmax function, which interprets the input as unnormalized log probabilities (aka <strong>logits</strong>) and outputs normalized linear probabilities. </p>\n<pre><code>y_hat_softmax = tf.nn.softmax(y_hat)\nsess.run(y_hat_softmax)\n# array([[ 0.227863  ,  0.61939586,  0.15274114],\n#        [ 0.49674623,  0.20196195,  0.30129182]])\n</code></pre>\n<p>It's important to fully understand what the softmax output is saying. Below I've shown a table that more clearly represents the output above. It can be seen that, for example, the probability of training instance 1 being \"Class 2\" is 0.619. The class probabilities for each training instance are normalized, so the sum of each row is 1.0.</p>\n<pre><code>                      Pr(Class 1)  Pr(Class 2)  Pr(Class 3)\n                    ,--------------------------------------\nTraining instance 1 | 0.227863   | 0.61939586 | 0.15274114\nTraining instance 2 | 0.49674623 | 0.20196195 | 0.30129182\n</code></pre>\n<p>So now we have class probabilities for each training instance, where we can take the argmax() of each row to generate a final classification. From above, we may generate that training instance 1 belongs to \"Class 2\" and training instance 2 belongs to \"Class 1\". </p>\n<p>Are these classifications correct? We need to measure against the true labels from the training set. You will need a one-hot encoded <code>y_true</code> array, where again the rows are training instances and columns are classes. Below I've created an example <code>y_true</code> one-hot array where the true label for training instance 1 is \"Class 2\" and the true label for training instance 2 is \"Class 3\".</p>\n<pre><code>y_true = tf.convert_to_tensor(np.array([[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]))\nsess.run(y_true)\n# array([[ 0.,  1.,  0.],\n#        [ 0.,  0.,  1.]])\n</code></pre>\n<p>Is the probability distribution in <code>y_hat_softmax</code> close to the probability distribution in <code>y_true</code>? We can use <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\">cross-entropy loss</a> to measure the error.</p>\n<p><a href=\"https://i.sstatic.net/rODko.png\"><img alt=\"Formula for cross-entropy loss\" src=\"https://i.sstatic.net/rODko.png\"/></a></p>\n<p>We can compute the cross-entropy loss on a row-wise basis and see the results. Below we can see that training instance 1 has a loss of 0.479, while training instance 2 has a higher loss of 1.200. This result makes sense because in our example above, <code>y_hat_softmax</code> showed that training instance 1's highest probability was for \"Class 2\", which matches training instance 1 in <code>y_true</code>; however, the prediction for training instance 2 showed a highest probability for \"Class 1\", which does not match the true class \"Class 3\".</p>\n<pre><code>loss_per_instance_1 = -tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1])\nsess.run(loss_per_instance_1)\n# array([ 0.4790107 ,  1.19967598])\n</code></pre>\n<p>What we really want is the total loss over all the training instances. So we can compute:</p>\n<pre><code>total_loss_1 = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1]))\nsess.run(total_loss_1)\n# 0.83934333897877944\n</code></pre>\n<p><strong>Using softmax_cross_entropy_with_logits()</strong></p>\n<p>We can instead compute the total cross entropy loss using the <code>tf.nn.softmax_cross_entropy_with_logits()</code> function, as shown below. </p>\n<pre><code>loss_per_instance_2 = tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true)\nsess.run(loss_per_instance_2)\n# array([ 0.4790107 ,  1.19967598])\n\ntotal_loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\nsess.run(total_loss_2)\n# 0.83934333897877922\n</code></pre>\n<p>Note that <code>total_loss_1</code> and <code>total_loss_2</code> produce essentially equivalent results with some small differences in the very final digits. However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of <code>softmax_cross_entropy_with_logits()</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>tf.nn.softmax</code> computes the forward propagation through a softmax layer. You use it during <strong>evaluation</strong> of the model when you compute the probabilities that the model outputs.</p>\n<p><code>tf.nn.softmax_cross_entropy_with_logits</code> computes the cost for a softmax layer. It is only used during <strong>training</strong>. </p>\n<p>The logits are the <em>unnormalized log probabilities</em> output the model (the values output before the softmax normalization is applied to them).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 12:31:37Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/4752626/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is the difference between <em>epoch</em> and <em>iteration</em> when training a multi-layer perceptron?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the neural network terminology:</p>\n<ul>\n<li>one <strong>epoch</strong> = one forward pass and one backward pass of <em>all</em> the training examples</li>\n<li><strong>batch size</strong> = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.</li>\n<li>number of <strong>iterations</strong> =  number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).</li>\n</ul>\n<p>For example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.</p>\n<p>FYI: <a href=\"https://stats.stackexchange.com/q/164876/12359\">Tradeoff batch size vs. number of iterations to train a neural network</a></p>\n<hr/>\n<p>The term \"batch\" is ambiguous: some people use it to designate the entire training set, and some people use it to refer to the number of training examples in one forward/backward pass (as I did in this answer). To avoid that ambiguity and make clear that batch corresponds to the number of training examples in one forward/backward pass, one can use the term <strong>mini-batch</strong>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>Epoch</em> and <em>iteration</em> describe different things.</p>\n<hr/>\n<h3>Epoch</h3>\n<p>An <em>epoch</em> describes the number of times the algorithm sees the <em>entire</em> data set. So, each time the algorithm has seen all samples in the dataset, an epoch has been completed.</p>\n<h3>Iteration</h3>\n<p>An <em>iteration</em> describes the number of times a <em>batch</em> of data passed through the algorithm. In the case of neural networks, that means the <em>forward pass</em> and <em>backward pass</em>. So, every time you pass a batch of data through the NN, you completed an <em>iteration</em>.</p>\n<hr/>\n<h3>Example</h3>\n<p>An example might make it clearer.</p>\n<p>Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs.</p>\n<p>Therefore, in each epoch, you have 5 batches (10/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.\nSince you've specified 3 epochs, you have a total of 15 iterations (5*3 = 15) for training.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Many neural network training algorithms involve making multiple presentations of the entire data set to the neural network.  Often, a single presentation of the entire data set is referred to as an \"epoch\".  In contrast, some algorithms present data to the neural network a single case at a time.</p>\n<p>\"Iteration\" is a much more general term, but since you asked about it together with \"epoch\", I assume that your source is referring to the presentation of a single case to a neural network.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 10:35:43Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/307291/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I've been developing an internal website for a portfolio management tool.  There is a lot of text data, company names etc.  I've been really impressed with some search engines ability to very quickly respond to queries with \"Did you mean: xxxx\".</p>\n<p>I need to be able to intelligently take a user query and respond with not only raw search results but also with a \"Did you mean?\" response when there is a highly likely alternative answer etc</p>\n<p>[I'm developing in <a href=\"http://en.wikipedia.org/wiki/ASP.NET\" rel=\"noreferrer\">ASP.NET</a> (VB - don't hold it against me! )]</p>\n<p>UPDATE:\nOK, how can I mimic this without the millions of 'unpaid users'?</p>\n<ul>\n<li>Generate typos for each 'known' or 'correct' term and perform lookups?</li>\n<li>Some other more elegant method?</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's the explanation directly from the source ( almost ) </p>\n<h2><strong><a href=\"http://www.youtube.com/watch?v=syKY8CrHkck#t=22m03s\" rel=\"noreferrer\">Search 101!</a></strong></h2>\n<p>at min 22:03</p>\n<p>Worth watching!</p>\n<p>Basically and according  to Douglas Merrill former CTO of Google it is like this:</p>\n<p>1) You write a  ( misspelled )  word  in google </p>\n<p>2) You don't find what you wanted ( don't click on any results )</p>\n<p>3) You realize you misspelled the word  so you rewrite the word in the search box.</p>\n<p>4) You find what you want ( you click in the first links ) </p>\n<p>This pattern multiplied millions of times, shows what are the most common misspells and what are the most \"common\" corrections. </p>\n<p>This way Google can almost instantaneously, offer spell correction in every language.</p>\n<p>Also this means if overnight everyone start to spell night as \"nigth\" google would suggest that word instead.  </p>\n<p><strong>EDIT</strong></p>\n<p>@ThomasRutter: Douglas describe it as \"statistical machine learning\". </p>\n<p>They know who correct the query, because they know which query comes from which user ( using cookies ) </p>\n<p>If the users perform a query, and only 10% of the users click on a result and 90% goes back and type another query ( with the corrected word ) and this time that 90% clicks on a result, then they know they have found a correction. </p>\n<p>They can also know if those are \"related\" queries of two different, because they have information of all the links they show. </p>\n<p>Furthermore, they are now including the context into the spell check, so they can even suggest different word depending on the context. </p>\n<p>See this <a href=\"http://www.youtube.com/watch?v=v_UyVmITiYQ#t=44m06s\" rel=\"noreferrer\">demo of google wave</a> ( @ 44m 06s )  that shows how the context is taken into account to automatically correct the spelling.</p>\n<p><a href=\"http://www.youtube.com/watch?v=Sx3Fpw0XCXk\" rel=\"noreferrer\">Here</a> it is explained how that natural language processing works.</p>\n<p>And finally here is an awesome demo of what can be done adding automatic <a href=\"http://www.youtube.com/watch?v=v_UyVmITiYQ#t=1h12m47s\" rel=\"noreferrer\">machine translation</a> ( @ 1h 12m 47s )  to the mix. </p>\n<p><sub>\n  I've added anchors of minute and seconds to the videos to skip directly to the content, if they don't work, try reloading the page or scrolling by hand to the mark. \n</sub></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I found this article some time ago: <a href=\"http://www.norvig.com/spell-correct.html\" rel=\"noreferrer\"><strong>How to Write a Spelling Corrector</strong></a>, written by <a href=\"https://en.wikipedia.org/wiki/Peter_Norvig\" rel=\"noreferrer\">Peter Norvig</a> (Director of Research at Google Inc.).</p>\n<p>It's an interesting read about the \"spelling correction\" topic. The examples are in Python but it's clear and simple to understand, and I think that the algorithm can be easily \ntranslated to other languages.</p>\n<p>Below follows a short description of the algorithm.\nThe algorithm consists of two steps, preparation and word checking.</p>\n<p><strong>Step 1: Preparation - setting up the word database</strong></p>\n<p>Best is if you can use actual search words and their occurence.\nIf you don't have that a large set of text can be used instead.\nCount the occurrence (popularity) of each word.</p>\n<p><strong>Step 2. Word checking - finding words that are similar to the one checked</strong></p>\n<p>Similar means that the edit distance is low (typically 0-1 or 0-2). The edit distance is the minimum number of inserts/deletes/changes/swaps needed to transform one word to another.</p>\n<p>Choose the most popular word from the previous step and suggest it as a correction (if other than the word itself).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For the theory of \"did you mean\" algorithm you can refer to Chapter 3 of Introduction to Information Retrieval. It is available <a href=\"http://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf\" rel=\"noreferrer\">online</a> for free. <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/spelling-correction-1.html\" rel=\"noreferrer\">Section 3.3</a> (page 52) exactly answers your question. And to specifically answer your update you only need a dictionary of words and nothing else (including millions of users).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\">What are logits? What is the difference between softmax and softmax_cross_entropy_with_logits?</a>\n<span class=\"question-originals-answer-count\">\n                                (9 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2020-08-19 18:45:08Z\">4 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>In the following TensorFlow function, we must feed the activation of artificial neurons in the final layer. That I understand. But I don't understand why it is called logits? Isn't that a mathematical function? </p>\n<pre><code>loss_function = tf.nn.softmax_cross_entropy_with_logits(\n     logits = last_layer,\n     labels = target_output\n)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Logits is an overloaded term which can mean many different things:</p>\n<hr/>\n<p><strong>In Math</strong>, <a href=\"https://en.wikipedia.org/wiki/Logit\" rel=\"noreferrer\">Logit</a> is a function that maps probabilities (<code>[0, 1]</code>) to R (<code>(-inf, inf)</code>)</p>\n<p><a href=\"https://i.sstatic.net/zto5q.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zto5q.png\"/></a></p>\n<p>Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to &gt; 0.5.</p>\n<p><strong>In ML</strong>, it <a href=\"https://developers.google.com/machine-learning/glossary/#logits\" rel=\"noreferrer\">can be</a> </p>\n<blockquote>\n<p>the vector of raw (non-normalized) predictions that a classification\n  model generates, which is ordinarily then passed to a normalization\n  function. If the model is solving a multi-class classification\n  problem, logits typically become an input to the softmax function. The\n  softmax function then generates a vector of (normalized) probabilities\n  with one value for each possible class.</p>\n</blockquote>\n<p><strong>Logits also</strong> <a href=\"https://developers.google.com/machine-learning/glossary/#logits\" rel=\"noreferrer\">sometimes</a> refer to the element-wise inverse of the sigmoid function.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just adding this clarification so that anyone who scrolls down this much can at least gets it right, since there are so many wrong answers upvoted.</p>\n<p>Diansheng's <a href=\"https://stackoverflow.com/a/47010867/4334743\">answer</a> and JakeJ's <a href=\"https://stackoverflow.com/a/47278567/4334743\">answer</a> get it right.<br/>\n<a href=\"https://stackoverflow.com/a/52111173/4334743\">A new answer</a> posted by Shital Shah is an even better and more complete answer.</p>\n<hr/>\n<p>Yes, <code>logit</code>  as a mathematical <a href=\"https://en.wikipedia.org/wiki/Logit\" rel=\"noreferrer\">function</a> in statistics, <strong>but the <code>logit</code> used in context of neural networks is different.</strong> Statistical <code>logit</code> doesn't even make any sense here.</p>\n<hr/>\n<p>I couldn't find a formal definition anywhere, but <code>logit</code> basically means:</p>\n<blockquote>\n<p>The raw predictions which come out of the last layer of the neural network.<br/>\n  1. This is the very tensor on which you apply the <a href=\"https://en.wikipedia.org/wiki/Arg_max\" rel=\"noreferrer\"><code>argmax</code></a> function to get the predicted class.<br/>\n  2. This is the very tensor which you feed into the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" rel=\"noreferrer\"><code>softmax</code></a> function to get the probabilities for the predicted classes.</p>\n</blockquote>\n<hr/>\n<p>Also, from a <a href=\"https://www.tensorflow.org/tutorials/layers\" rel=\"noreferrer\">tutorial</a> on official tensorflow website:</p>\n<blockquote>\n<h3>Logits Layer</h3>\n<p>The final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default):</p>\n<pre><code>logits = tf.layers.dense(inputs=dropout, units=10)\n</code></pre>\n</blockquote>\n<hr/>\n<p>If you are still confused, the situation is like this:</p>\n<pre><code>raw_predictions = neural_net(input_layer)\npredicted_class_index_by_raw = argmax(raw_predictions)\nprobabilities = softmax(raw_predictions)\npredicted_class_index_by_prob = argmax(probabilities)\n</code></pre>\n<p>where, <code>predicted_class_index_by_raw</code> and <code>predicted_class_index_by_prob</code> will be equal.</p>\n<p><strong>Another name for <code>raw_predictions</code> in the above code is <code>logit</code></strong>.</p>\n<hr/>\n<p><del>As for the <strong>why</strong> <code>logit</code>... I have no idea. Sorry.</del><br/>\n[Edit: See <a href=\"https://stackoverflow.com/a/52111173/4334743\">this answer</a> for  the historical motivations behind the term.]</p>\n<hr/>\n<h2>Trivia</h2>\n<p>Although, if you want to, you can apply statistical <code>logit</code> to <code>probabilities</code> that come out of the <code>softmax</code> function. </p>\n<p>If the probability of a certain class is <code>p</code>,<br/>\nThen the <strong>log-odds</strong> of that class is <code>L = logit(p)</code>.</p>\n<p>Also, the probability of that class can be recovered as <code>p = sigmoid(L)</code>, using the <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\" rel=\"noreferrer\"><code>sigmoid</code></a> function.</p>\n<p>Not very useful to calculate log-odds though.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Summary</strong></p>\n<p>In context of deep learning the <a href=\"https://www.tensorflow.org/tutorials/estimators/cnn#logits_layer\" rel=\"noreferrer\">logits layer</a> means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer. The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.</p>\n<p><strong>Historical Context</strong></p>\n<p>Where does this term comes from? In 1930s and 40s, several people were trying to adapt linear regression to the problem of predicting probabilities. However linear regression produces output from -infinity to +infinity while for probabilities our desired output is 0 to 1. One way to do this is by somehow mapping the probabilities 0 to 1 to -infinity to +infinity and then use linear regression as usual. One such mapping is cumulative normal distribution that was used by Chester Ittner Bliss in 1934 and he called this \"probit\" model, short for \"probability unit\". However this function is computationally expensive while lacking some of the desirable properties for multi-class classification. In 1944 Joseph Berkson used the function <code>log(p/(1-p))</code> to do this mapping and called it logit, short for \"logistic unit\". The term logistic regression derived from this as well.</p>\n<p><strong>The Confusion</strong></p>\n<p>Unfortunately the term logits is abused in deep learning. From pure mathematical perspective logit is a <em>function</em> that performs above mapping. In deep learning people started calling the layer \"logits layer\" that feeds in to logit function. Then people started calling the output <em>values</em> of this layer \"logit\" creating the confusion with logit <em>the function</em>.</p>\n<p><strong>TensorFlow Code</strong></p>\n<p>Unfortunately TensorFlow code further adds in to confusion by names like <code>tf.nn.softmax_cross_entropy_with_logits</code>. What does logits mean here? It just means the input of the function is supposed to be the output of last neuron layer as described above. The <code>_with_logits</code> suffix is <a href=\"https://github.com/tensorflow/tensorflow/issues/6531\" rel=\"noreferrer\">redundant, confusing and pointless</a>. Functions should be named without regards to such very specific contexts because they are simply mathematical operations that can be performed on values derived from many other domains. In fact TensorFlow has another similar function <code>sparse_softmax_cross_entropy</code> where they fortunately forgot to add <code>_with_logits</code> suffix creating inconsistency and adding in to confusion. PyTorch on the other hand simply names its function without these kind of suffixes.</p>\n<p><strong>Reference</strong></p>\n<p>The <a href=\"http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf\" rel=\"noreferrer\">Logit/Probit lecture slides</a> is one of the best resource to understand logit. I have also updated <a href=\"https://en.wikipedia.org/wiki/Logit\" rel=\"noreferrer\">Wikipedia article</a> with some of above information.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n                                As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, <a href=\"/help/reopen-questions\">visit the help center</a> for guidance.\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2012-11-26 16:25:59Z\">11 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p><a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\" rel=\"nofollow noreferrer\">Artificial neural networks</a> (ANNs) and <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\" rel=\"nofollow noreferrer\">support vector machines</a> (SVMs) are two popular strategies for <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\" rel=\"nofollow noreferrer\">supervised machine learning</a> and classification. It's not often clear which method is better for a particular project, and I'm certain the answer is always \"it depends.\" Often, a combination of both, along with <a href=\"https://en.wikipedia.org/wiki/Bayesian_statistics\" rel=\"nofollow noreferrer\">Bayesian</a> classification, is used.</p>\n<p>These questions on Stack Overflow have already been asked regarding ANN vs SVM:</p>\n<ul>\n<li><p><em><a href=\"https://stackoverflow.com/questions/8326485/ann-and-svm-classification\">ANN and SVM classification</a></em></p>\n</li>\n<li><p><em><a href=\"https://stackoverflow.com/questions/7316671/what-the-difference-among-ann-svm-and-knn-in-my-classification-question\">What's the difference between ANN, SVM and KNN classifiers?</a></em></p>\n</li>\n<li><p><em><a href=\"https://stackoverflow.com/questions/2434536/support-vector-machine-or-artificial-neural-network-for-text-processing\">Support vector machine or artificial neural network for text processing</a></em></p>\n</li>\n</ul>\n<p>In this question, I'd like to know <em>specifically</em> what aspects of an ANN (specifically, a <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" rel=\"nofollow noreferrer\">multilayer perceptron</a>) might make it desirable to use over an SVM? The reason I ask is because it's easy to answer the <em>opposite</em> question: Support Vector Machines are often superior to ANNs because they avoid two major weaknesses of ANNs:</p>\n<p>(1) ANNs often converge on <em>local minima</em> rather than global minima, meaning that they are essentially \"missing the big picture\" sometimes (or missing the forest for the trees)</p>\n<p>(2) ANNs often <em>overfit</em> if training goes on too long, meaning that for any given pattern, an ANN might start to consider the noise as part of the pattern.</p>\n<p>SVMs don't suffer from either of these two problems. However, it's not readily apparent that SVMs are meant to be a total replacement for ANNs. So what <em>specific</em> advantage(s) does an ANN have over an SVM that might make it applicable for certain situations? I've listed <em>specific</em> advantages of an SVM over an ANN, now I'd like to see a list of ANN advantages (if any).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Judging from the examples you provide, I'm assuming that by ANNs, you mean multilayer feed-forward networks (FF nets for short), such as multilayer perceptrons, because those are in direct competition with SVMs.</p>\n<p>One specific benefit that these models have over SVMs is that their size is fixed: they are <em>parametric</em> models, while SVMs are non-parametric. That is, in an ANN you have a bunch of hidden layers with sizes <em>h</em><sub>1</sub> through <em>h</em><sub><em>n</em></sub> depending on the number of features, plus bias parameters, and those make up your model. By contrast, an SVM (at least a kernelized one) consists of a set of support vectors, selected from the training set, with a weight for each. In the worst case, the number of support vectors is exactly the number of training samples (though that mainly occurs with small training sets or in degenerate cases) and in general its model size scales linearly. In natural language processing, SVM classifiers with tens of thousands of support vectors, each having hundreds of thousands of features, is not unheard of.</p>\n<p>Also, <a href=\"https://en.wikipedia.org/wiki/Online_machine_learning\" rel=\"nofollow noreferrer\">online training</a> of FF nets is very simple compared to online SVM fitting, and predicting can be quite a bit faster.</p>\n<p>All of the above pertains to the general case of kernelized SVMs. Linear SVM are a special case in that they <em>are</em> parametric and allow online learning with simple algorithms such as stochastic gradient descent.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One obvious advantage of artificial neural networks over support vector machines is that artificial neural networks may have any number of outputs, while support vector machines have only one. The most direct way to create an n-ary classifier with support vector machines is to create n support vector machines and train each of them one by one. On the other hand, an n-ary classifier with neural networks can be trained in one go. Additionally, the neural network will make more sense because it is one whole, whereas the support vector machines are isolated systems. This is especially useful if the outputs are inter-related.</p>\n<p>For example, if the goal was to classify hand-written digits, ten support vector machines would do. Each support vector machine would recognize exactly one digit, and fail to recognize all others. Since each handwritten digit cannot be meant to hold more information than just its class, it makes no sense to try to solve this with an artificial neural network.</p>\n<p>However, suppose the goal was to model a person's hormone balance (for several hormones) as a function of easily measured physiological factors such as time since last meal, heart rate, etc ... Since these factors are all inter-related, artificial neural network regression makes more sense than support vector machine regression.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One thing to note is that the two are actually very related.  Linear SVMs are equivalent to single-layer <a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\" rel=\"nofollow noreferrer\">NN</a>'s (i.e., <a href=\"https://en.wikipedia.org/wiki/Perceptron\" rel=\"nofollow noreferrer\">perceptrons</a>), and multi-layer NNs can be expressed in terms of SVMs.  See <a href=\"http://ronan.collobert.com/pub/matos/2004_links_icml.pdf\" rel=\"nofollow noreferrer\">here</a> for some details.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Given a 1D array of indices:</p>\n<pre><code>a = array([1, 0, 3])\n</code></pre>\n<p>I want to one-hot encode this as a 2D array:</p>\n<pre><code>b = array([[0,1,0,0], [1,0,0,0], [0,0,0,1]])\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Create a zeroed array <code>b</code> with enough columns, i.e. <code>a.max() + 1</code>.<br/>\nThen, for each row <code>i</code>, set the <code>a[i]</code>th column to <code>1</code>.</p>\n<pre><code>&gt;&gt;&gt; a = np.array([1, 0, 3])\n&gt;&gt;&gt; b = np.zeros((a.size, a.max() + 1))\n&gt;&gt;&gt; b[np.arange(a.size), a] = 1\n\n&gt;&gt;&gt; b\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.]])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>&gt;&gt;&gt; values = [1, 0, 3]\n&gt;&gt;&gt; n_values = np.max(values) + 1\n&gt;&gt;&gt; np.eye(n_values)[values]\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.]])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In case you are using keras, there is a built in utility for that:</p>\n<pre><code>from keras.utils.np_utils import to_categorical   \n\ncategorical_labels = to_categorical(int_labels, num_classes=3)\n</code></pre>\n<p>And it does pretty much the same as <a href=\"https://stackoverflow.com/a/29831596/4228275\">@YXD's answer</a> (see <a href=\"https://github.com/keras-team/keras/blob/2d183db0372e5ac2a686608cb9da0a9bd4319764/keras/utils/np_utils.py#L9\" rel=\"noreferrer\">source-code</a>).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What does <code>view()</code> do to the tensor <code>x</code>? What do negative values mean?</p>\n<pre><code>x = x.view(-1, 16 * 5 * 5)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>view()</code> reshapes the tensor without copying memory, similar to numpy's <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\" rel=\"noreferrer\"><code>reshape()</code></a>.</p>\n<p>Given a tensor <code>a</code> with 16 elements:</p>\n<pre><code>import torch\na = torch.range(1, 16)\n</code></pre>\n<p>To reshape this tensor to make it a <code>4 x 4</code> tensor, use:</p>\n<pre><code>a = a.view(4, 4)\n</code></pre>\n<p>Now <code>a</code> will be a <code>4 x 4</code> tensor. <em>Note that after the reshape the total number of elements need to remain the same. Reshaping the tensor <code>a</code> to a <code>3 x 5</code> tensor would not be appropriate.</em></p>\n<h3>What is the meaning of parameter -1?</h3>\n<p>If there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (<em>Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1</em>). This is a way of telling the library: \"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\".</p>\n<p>This can be seen in <a href=\"https://stackoverflow.com/revisions/42479902/9\">this model definition code</a>. After the line <code>x = self.pool(F.relu(self.conv2(x)))</code> in the forward function, you will have a 16 depth feature map. You have to flatten this to give it to the fully connected layer. So you tell PyTorch to reshape the tensor you obtained to have specific number of columns and tell it to decide the number of rows by itself.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>view()</code> reshapes a tensor by 'stretching' or 'squeezing' its elements into the shape you specify:</p>\n<p><a href=\"https://i.sstatic.net/ORqaP.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ORqaP.png\"/></a></p>\n<hr/>\n<h3>How does <code>view()</code> work?</h3>\n<p>First let's look at what a tensor is under the hood:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th><a href=\"https://i.sstatic.net/ee7Hj.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ee7Hj.png\"/></a></th>\n<th><a href=\"https://i.sstatic.net/26Q9g.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/26Q9g.png\"/></a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Tensor and its underlying <code>storage</code></td>\n<td>e.g. the right-hand tensor (shape (3,2)) can be computed from the left-hand one with <code>t2 = t1.view(3,2)</code></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Here you see PyTorch makes a tensor by converting an underlying block of contiguous memory into a matrix-like object by adding a <code>shape</code> and <code>stride</code> attribute:</p>\n<ul>\n<li><code>shape</code> states how long each dimension is</li>\n<li><code>stride</code> states how many steps you need to take in memory til you reach the next element in each dimension</li>\n</ul>\n<blockquote>\n<p><code>view(dim1,dim2,...)</code> returns a <em>view</em> of the same underlying information, but reshaped to a tensor of shape <code>dim1 x dim2 x ...</code> (by modifying the <code>shape</code> and <code>stride</code> attributes).</p>\n</blockquote>\n<p>Note this implicitly assumes that the new and old dimensions have the same product (i.e. the old and new tensor have the same volume).</p>\n<hr/>\n<h3>PyTorch <a href=\"https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\">-1</a></h3>\n<p><code>-1</code> is a PyTorch alias for \"infer this dimension given the others have all been specified\" (i.e. the quotient of the original product by the new product). It is a convention taken from <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\" rel=\"noreferrer\"><code>numpy.reshape()</code></a>.</p>\n<p>Hence <code>t1.view(3,2)</code> in our example would be equivalent to <code>t1.view(3,-1)</code> or <code>t1.view(-1,2)</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's do some examples, from simpler to more difficult.</p>\n<ol>\n<li><p>The <code>view</code> method returns a tensor with the same data as the <code>self</code> tensor (which means that the returned tensor has the same number of elements), but with a different shape.  For example:</p>\n<pre><code>a = torch.arange(1, 17)  # a's shape is (16,)\n\na.view(4, 4) # output below\n  1   2   3   4\n  5   6   7   8\n  9  10  11  12\n 13  14  15  16\n[torch.FloatTensor of size 4x4]\n\na.view(2, 2, 4) # output below\n(0 ,.,.) = \n1   2   3   4\n5   6   7   8\n\n(1 ,.,.) = \n 9  10  11  12\n13  14  15  16\n[torch.FloatTensor of size 2x2x4]\n</code></pre></li>\n<li><p>Assuming that <code>-1</code> is not one of the parameters, when you multiply them together, the result must be equal to the number of elements in the tensor.  If you do: <code>a.view(3, 3)</code>, it will raise a <code>RuntimeError</code> because shape (3 x 3) is invalid for input with 16 elements. In other words: 3 x 3 does not equal 16 but 9.</p></li>\n<li><p>You can use <code>-1</code> as one of the parameters that you pass to the function, but only once. All that happens is that the method will do the math for you on how to fill that dimension.  For example <code>a.view(2, -1, 4)</code> is equivalent to <code>a.view(2, 2, 4)</code>. [16 / (2 x 4) = 2]</p></li>\n<li><p>Notice that the returned tensor <strong>shares the same data</strong>. If you make a change in the \"view\" you are changing the original tensor's data:</p>\n<pre><code>b = a.view(4, 4)\nb[0, 2] = 2\na[2] == 3.0\nFalse\n</code></pre></li>\n<li><p>Now, for a more complex use case.  The documentation says that each new view dimension must either be a subspace of an original dimension, or only span <strong><em>d, d + 1, ..., d + k</em></strong> that satisfy the following contiguity-like condition that for all <strong><em>i = 0, ..., k - 1, stride[i] = stride[i + 1] x size[i + 1]</em></strong>. Otherwise, <code>contiguous()</code> needs to be called before the tensor can be viewed.  For example:</p>\n<pre><code>a = torch.rand(5, 4, 3, 2) # size (5, 4, 3, 2)\na_t = a.permute(0, 2, 3, 1) # size (5, 3, 2, 4)\n\n# The commented line below will raise a RuntimeError, because one dimension\n# spans across two contiguous subspaces\n# a_t.view(-1, 4)\n\n# instead do:\na_t.contiguous().view(-1, 4)\n\n# To see why the first one does not work and the second does,\n# compare a.stride() and a_t.stride()\na.stride() # (24, 6, 2, 1)\na_t.stride() # (24, 2, 1, 6)\n</code></pre>\n<p>Notice that for <code>a_t</code>, <strong><em>stride[0] != stride[1] x size[1]</em></strong> since <strong><em>24 != 2 x 3</em></strong></p></li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How do I print the summary of a model in PyTorch like what <code>model.summary()</code> does in Keras:</p>\n<pre><code>Model Summary:\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_1 (InputLayer)             (None, 1, 15, 27)     0                                            \n____________________________________________________________________________________________________\nconvolution2d_1 (Convolution2D)  (None, 8, 15, 27)     872         input_1[0][0]                    \n____________________________________________________________________________________________________\nmaxpooling2d_1 (MaxPooling2D)    (None, 8, 7, 27)      0           convolution2d_1[0][0]            \n____________________________________________________________________________________________________\nflatten_1 (Flatten)              (None, 1512)          0           maxpooling2d_1[0][0]             \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (None, 1)             1513        flatten_1[0][0]                  \n====================================================================================================\nTotal params: 2,385\nTrainable params: 2,385\nNon-trainable params: 0\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, you can get exact Keras representation, using the <a href=\"https://github.com/sksq96/pytorch-summary\" rel=\"noreferrer\">pytorch-summary</a> package.</p>\n<p>Example for VGG16:</p>\n<pre><code>from torchvision import models\nfrom torchsummary import summary\n\nvgg = models.vgg16()\nsummary(vgg, (3, 224, 224))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n              ReLU-2         [-1, 64, 224, 224]               0\n            Conv2d-3         [-1, 64, 224, 224]          36,928\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6        [-1, 128, 112, 112]          73,856\n              ReLU-7        [-1, 128, 112, 112]               0\n            Conv2d-8        [-1, 128, 112, 112]         147,584\n              ReLU-9        [-1, 128, 112, 112]               0\n        MaxPool2d-10          [-1, 128, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]         295,168\n             ReLU-12          [-1, 256, 56, 56]               0\n           Conv2d-13          [-1, 256, 56, 56]         590,080\n             ReLU-14          [-1, 256, 56, 56]               0\n           Conv2d-15          [-1, 256, 56, 56]         590,080\n             ReLU-16          [-1, 256, 56, 56]               0\n        MaxPool2d-17          [-1, 256, 28, 28]               0\n           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n             ReLU-19          [-1, 512, 28, 28]               0\n           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n             ReLU-21          [-1, 512, 28, 28]               0\n           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n             ReLU-23          [-1, 512, 28, 28]               0\n        MaxPool2d-24          [-1, 512, 14, 14]               0\n           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n             ReLU-26          [-1, 512, 14, 14]               0\n           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n             ReLU-28          [-1, 512, 14, 14]               0\n           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n             ReLU-30          [-1, 512, 14, 14]               0\n        MaxPool2d-31            [-1, 512, 7, 7]               0\n           Linear-32                 [-1, 4096]     102,764,544\n             ReLU-33                 [-1, 4096]               0\n          Dropout-34                 [-1, 4096]               0\n           Linear-35                 [-1, 4096]      16,781,312\n             ReLU-36                 [-1, 4096]               0\n          Dropout-37                 [-1, 4096]               0\n           Linear-38                 [-1, 1000]       4,097,000\n================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 218.59\nParams size (MB): 527.79\nEstimated Total Size (MB): 746.96\n----------------------------------------------------------------\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While you will not get as detailed information about the model as in Keras' model.summary, simply printing the model will give you some idea about the different layers involved and their specifications.</p>\n<p>For instance:</p>\n<pre><code>from torchvision import models\nmodel = models.vgg16()\nprint(model)\n</code></pre>\n<p>The output in this case would be something as follows:</p>\n<pre><code>VGG (\n  (features): Sequential (\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU (inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU (inplace)\n    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU (inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU (inplace)\n    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU (inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU (inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU (inplace)\n    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU (inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU (inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU (inplace)\n    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU (inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU (inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU (inplace)\n    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (classifier): Sequential (\n    (0): Dropout (p = 0.5)\n    (1): Linear (25088 -&gt; 4096)\n    (2): ReLU (inplace)\n    (3): Dropout (p = 0.5)\n    (4): Linear (4096 -&gt; 4096)\n    (5): ReLU (inplace)\n    (6): Linear (4096 -&gt; 1000)\n  )\n)\n</code></pre>\n<p>Now you could, as mentioned by <a href=\"https://stackoverflow.com/users/2704763/kashyap\">Kashyap</a>, use the <code>state_dict</code> method to get the weights of the different layers. But using this listing of the layers would perhaps provide more direction is creating a helper function to get that Keras like model summary!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In order to use torchsummary type:</p>\n<pre><code>from torchsummary import summary\n</code></pre>\n<p>Install it first if you don't have it.</p>\n<pre><code>pip install torchsummary \n</code></pre>\n<p>And then you can try it, but note for some reason it is not working unless I set model to cuda <code>alexnet.cuda</code>:</p>\n<pre><code>from torchsummary import summary\nhelp(summary)\nimport torchvision.models as models\nalexnet = models.alexnet(pretrained=False)\nalexnet.cuda()\nsummary(alexnet, (3, 224, 224))\nprint(alexnet)\n</code></pre>\n<p>The <code>summary</code> must take the input size and batch size is set to -1 meaning any batch size we provide.</p>\n<p>If we set <code>summary(alexnet, (3, 224, 224), 32)</code> this means use the <code>bs=32</code>.</p>\n<pre><code>summary(model, input_size, batch_size=-1, device='cuda')\n</code></pre>\n<hr/>\n<p>Out:</p>\n<pre><code>Help on function summary in module torchsummary.torchsummary:\n\nsummary(model, input_size, batch_size=-1, device='cuda')\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [32, 64, 55, 55]          23,296\n              ReLU-2           [32, 64, 55, 55]               0\n         MaxPool2d-3           [32, 64, 27, 27]               0\n            Conv2d-4          [32, 192, 27, 27]         307,392\n              ReLU-5          [32, 192, 27, 27]               0\n         MaxPool2d-6          [32, 192, 13, 13]               0\n            Conv2d-7          [32, 384, 13, 13]         663,936\n              ReLU-8          [32, 384, 13, 13]               0\n            Conv2d-9          [32, 256, 13, 13]         884,992\n             ReLU-10          [32, 256, 13, 13]               0\n           Conv2d-11          [32, 256, 13, 13]         590,080\n             ReLU-12          [32, 256, 13, 13]               0\n        MaxPool2d-13            [32, 256, 6, 6]               0\nAdaptiveAvgPool2d-14            [32, 256, 6, 6]               0\n          Dropout-15                 [32, 9216]               0\n           Linear-16                 [32, 4096]      37,752,832\n             ReLU-17                 [32, 4096]               0\n          Dropout-18                 [32, 4096]               0\n           Linear-19                 [32, 4096]      16,781,312\n             ReLU-20                 [32, 4096]               0\n           Linear-21                 [32, 1000]       4,097,000\n================================================================\nTotal params: 61,100,840\nTrainable params: 61,100,840\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 18.38\nForward/backward pass size (MB): 268.12\nParams size (MB): 233.08\nEstimated Total Size (MB): 519.58\n----------------------------------------------------------------\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace)\n    (3): Dropout(p=0.5)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the <a href=\"https://www.udacity.com/course/viewer#!/c-ud730/l-6370362152/m-6379811820\" rel=\"nofollow noreferrer\">Udacity's deep learning class</a>, the softmax of <code>y_i</code> is simply the exponential divided by the sum of exponential of the whole Y vector:</p>\n<p><a href=\"https://i.sstatic.net/iP8Du.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/iP8Du.png\"/></a></p>\n<p>Where <code>S(y_i)</code> is the softmax function of <code>y_i</code> and <code>e</code> is the exponential and <code>j</code> is the no. of columns in the input vector Y.</p>\n<p>I've tried the following:</p>\n<pre><code>import numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\nscores = [3.0, 1.0, 0.2]\nprint(softmax(scores))\n</code></pre>\n<p>which returns:</p>\n<pre><code>[ 0.8360188   0.11314284  0.05083836]\n</code></pre>\n<p>But the suggested solution was:</p>\n<pre><code>def softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\n</code></pre>\n<p>which produces the <strong>same output as the first implementation</strong>, even though the first implementation explicitly takes the difference of each column and the max and then divides by the sum.</p>\n<p>Can someone show mathematically why? Is one correct and the other one wrong?</p>\n<p>Are the implementation similar in terms of code and time complexity? Which is more efficient?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>They're both correct, but yours is preferred from the point of view of numerical stability.</p>\n<p>You start with</p>\n<pre><code>e ^ (x - max(x)) / sum(e^(x - max(x))\n</code></pre>\n<p>By using the fact that a^(b - c) = (a^b)/(a^c) we have</p>\n<pre><code>= e ^ x / (e ^ max(x) * sum(e ^ x / e ^ max(x)))\n\n= e ^ x / sum(e ^ x)\n</code></pre>\n<p>Which is what the other answer says. You could replace max(x) with any variable and it would cancel out.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>(Well... much confusion here, both in the question and in the answers...)</p>\n<p>To start with, the two solutions (i.e. yours and the suggested one) are <strong>not</strong> equivalent; they <strong>happen</strong> to be equivalent only for the special case of 1-D score arrays. You would have discovered it if you had tried also the 2-D score array in the Udacity quiz provided example.</p>\n<p>Results-wise, the only actual difference between the two solutions is the <code>axis=0</code> argument. To see that this is the case, let's try your solution (<code>your_softmax</code>) and one where the only difference is the <code>axis</code> argument:</p>\n<pre><code>import numpy as np\n\n# your solution:\ndef your_softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\n# correct solution:\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0) # only difference\n</code></pre>\n<p>As I said, for a 1-D score array, the results are indeed identical:</p>\n<pre><code>scores = [3.0, 1.0, 0.2]\nprint(your_softmax(scores))\n# [ 0.8360188   0.11314284  0.05083836]\nprint(softmax(scores))\n# [ 0.8360188   0.11314284  0.05083836]\nyour_softmax(scores) == softmax(scores)\n# array([ True,  True,  True], dtype=bool)\n</code></pre>\n<p>Nevertheless, here are the results for the 2-D score array given in the Udacity quiz as a test example:</p>\n<pre><code>scores2D = np.array([[1, 2, 3, 6],\n                     [2, 4, 5, 6],\n                     [3, 8, 7, 6]])\n\nprint(your_softmax(scores2D))\n# [[  4.89907947e-04   1.33170787e-03   3.61995731e-03   7.27087861e-02]\n#  [  1.33170787e-03   9.84006416e-03   2.67480676e-02   7.27087861e-02]\n#  [  3.61995731e-03   5.37249300e-01   1.97642972e-01   7.27087861e-02]]\n\nprint(softmax(scores2D))\n# [[ 0.09003057  0.00242826  0.01587624  0.33333333]\n#  [ 0.24472847  0.01794253  0.11731043  0.33333333]\n#  [ 0.66524096  0.97962921  0.86681333  0.33333333]]\n</code></pre>\n<p>The results are different - the second one is indeed identical with the one expected in the Udacity quiz, where all columns indeed sum to 1, which is not the case with the first (wrong) result.</p>\n<p>So, all the fuss was actually for an implementation detail - the <code>axis</code> argument. According to the <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\" rel=\"noreferrer\">numpy.sum documentation</a>:</p>\n<blockquote>\n<p>The default, axis=None, will sum all of the elements of the input array</p>\n</blockquote>\n<p>while here we want to sum row-wise, hence <code>axis=0</code>. For a 1-D array, the sum of the (only) row and the sum of all the elements happen to be identical, hence your identical results in that case...</p>\n<p>The <code>axis</code> issue aside, your implementation (i.e. your choice to subtract the max first) is actually <strong>better</strong> than the suggested solution! In fact, it is the recommended way of implementing the softmax function - see <a href=\"http://cs231n.github.io/linear-classify/#softmax\" rel=\"noreferrer\">here</a> for the justification (numeric stability, also pointed out by some other answers here).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So, this is really a comment to desertnaut's answer but I can't comment on it yet due to my reputation. As he pointed out, your version is only correct if your input consists of a single sample. If your input consists of several samples, it is wrong. <strong>However, desertnaut's solution is also wrong.</strong> The problem is that once he takes a 1-dimensional input and then he takes a 2-dimensional input. Let me show this to you.</p>\n<pre><code>import numpy as np\n\n# your solution:\ndef your_softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\n# desertnaut solution (copied from his answer): \ndef desertnaut_softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0) # only difference\n\n# my (correct) solution:\ndef softmax(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis] # necessary step to do broadcasting\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] # dito\n    return e_x / div\n</code></pre>\n<p>Lets take desertnauts example:</p>\n<pre><code>x1 = np.array([[1, 2, 3, 6]]) # notice that we put the data into 2 dimensions(!)\n</code></pre>\n<p>This is the output:</p>\n<pre><code>your_softmax(x1)\narray([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])\n\ndesertnaut_softmax(x1)\narray([[ 1.,  1.,  1.,  1.]])\n\nsoftmax(x1)\narray([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])\n</code></pre>\n<p>You can see that desernauts version would fail in this situation. (It would not if the input was just one dimensional like np.array([1, 2, 3, 6]).</p>\n<p>Lets now use 3 samples since thats the reason why we use a 2 dimensional input. The following x2 is not the same as the one from desernauts example. </p>\n<pre><code>x2 = np.array([[1, 2, 3, 6],  # sample 1\n               [2, 4, 5, 6],  # sample 2\n               [1, 2, 3, 6]]) # sample 1 again(!)\n</code></pre>\n<p>This input consists of a batch with 3 samples. But sample one and three are essentially the same. We now expect 3 rows of softmax activations where the first should be the same as the third and also the same as our activation of x1!</p>\n<pre><code>your_softmax(x2)\narray([[ 0.00183535,  0.00498899,  0.01356148,  0.27238963],\n       [ 0.00498899,  0.03686393,  0.10020655,  0.27238963],\n       [ 0.00183535,  0.00498899,  0.01356148,  0.27238963]])\n\n\ndesertnaut_softmax(x2)\narray([[ 0.21194156,  0.10650698,  0.10650698,  0.33333333],\n       [ 0.57611688,  0.78698604,  0.78698604,  0.33333333],\n       [ 0.21194156,  0.10650698,  0.10650698,  0.33333333]])\n\nsoftmax(x2)\narray([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])\n</code></pre>\n<p>I hope you can see that this is only the case with my solution.</p>\n<pre><code>softmax(x1) == softmax(x2)[0]\narray([[ True,  True,  True,  True]], dtype=bool)\n\nsoftmax(x1) == softmax(x2)[2]\narray([[ True,  True,  True,  True]], dtype=bool)\n</code></pre>\n<p>Additionally, here is the results of TensorFlows softmax implementation:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nbatch = np.asarray([[1,2,3,6],[2,4,5,6],[1,2,3,6]])\nx = tf.placeholder(tf.float32, shape=[None, 4])\ny = tf.nn.softmax(x)\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(y, feed_dict={x: batch})\n</code></pre>\n<p>And the result:</p>\n<pre><code>array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037045],\n       [ 0.01203764,  0.08894681,  0.24178252,  0.657233  ],\n       [ 0.00626879,  0.01704033,  0.04632042,  0.93037045]], dtype=float32)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/1832076/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-05-20 16:34:39Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/1832076/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>In terms of artificial intelligence and machine learning, what is the difference between supervised and unsupervised learning?\nCan you provide a basic, easy explanation with an example? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since you ask this very basic question, it looks like it's worth specifying what Machine Learning itself is.</p>\n<p>Machine Learning is a class of algorithms which is data-driven, i.e. unlike \"normal\" algorithms it is the data that \"tells\" what the \"good answer\" is. Example: a hypothetical non-machine learning algorithm for face detection in images would try to define what a face is (round skin-like-colored disk, with dark area where you expect the eyes etc). A machine learning algorithm would not have such coded definition, but would \"learn-by-examples\": you'll show several images of faces and not-faces and a good algorithm will eventually learn and be able to predict whether or not an unseen image is a face.</p>\n<p>This particular example of face detection is <strong>supervised</strong>, which means that your examples must be <em>labeled</em>, or explicitly say which ones are faces and which ones aren't.</p>\n<p>In an <strong>unsupervised</strong> algorithm your examples are not <em>labeled</em>, i.e. you don't say anything. Of course, in such a case the algorithm itself cannot \"invent\" what a face is, but it can try to <a href=\"http://en.wikipedia.org/wiki/Cluster_analysis\" rel=\"noreferrer\">cluster</a> the data into different groups, e.g. it can distinguish that faces are very different from landscapes, which are very different from horses.</p>\n<p>Since another answer mentions it (though, in an incorrect way): there are \"intermediate\" forms of supervision, i.e. <strong>semi-supervised</strong> and <strong>active learning</strong>. Technically, these are supervised methods in which there is some \"smart\" way to avoid a large number of labeled examples. In active learning, the algorithm itself decides which thing you should label (e.g. it can be pretty sure about a landscape and a horse, but it might ask you to confirm if a gorilla is indeed the picture of a face). In semi-supervised learning, there are two different algorithms which start with the labeled examples, and then \"tell\" each other the way they think about some large number of unlabeled data. From this \"discussion\" they learn.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Supervised learning</strong> is when the data you feed your algorithm with is \"tagged\" or \"labelled\", to help your logic make decisions.</p>\n<p>Example: Bayes spam filtering, where you have to flag an item as spam to refine the results.</p>\n<p><strong>Unsupervised learning</strong> are types of algorithms that try to find correlations without any external inputs other than the raw data.</p>\n<p>Example: data mining clustering algorithms.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Supervised learning</h2>\n<p>Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems.</p>\n<h2>Unsupervised learning</h2>\n<p>In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering</p>\n<p>Pattern Recognition and Machine Learning (Bishop, 2006)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When should I use <code>.eval()</code>? I understand it is supposed to allow me to \"evaluate my model\". How do I turn it back off for training?</p>\n<p>Example training <a href=\"https://github.com/natanielruiz/deep-head-pose/blob/master/code/train_hopenet.py\" rel=\"noreferrer\">code</a> using <code>.eval()</code>.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>model.eval()</code> is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation, and <code>.eval()</code> will do it for you. In addition, the common practice for evaluating/validation is using <code>torch.no_grad()</code> in pair with <code>model.eval()</code> to turn off gradients computation:</p>\n<pre class=\"lang-py prettyprint-override\"><code># evaluate model:\nmodel.eval()\n\nwith torch.no_grad():\n    ...\n    out_data = model(data)\n    ...\n</code></pre>\n<p>BUT, don't forget to turn back to <code>training</code> mode after eval step:</p>\n<pre class=\"lang-py prettyprint-override\"><code># training step\n...\nmodel.train()\n...\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th><a href=\"https://stackoverflow.com/a/66526891/9067615\"><code>model.train()</code></a></th>\n<th><code>model.eval()</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sets model in <strong>train</strong>ing mode: <br/><br/> • normalisation layers<sup>1</sup> use per-batch statistics <br/> • activates <code>Dropout</code> layers<sup>2</sup></td>\n<td>Sets model in <strong>eval</strong>uation (inference) mode: <br/><br/> • normalisation layers use running statistics <br/> • de-activates <code>Dropout</code> layers</td>\n</tr>\n<tr>\n<td></td>\n<td>Equivalent to <code>model.train(False)</code>.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>You can turn off evaluation mode by running <code>model.train()</code>. You should use it when running your model as an inference engine - i.e. when testing, validating, and predicting (though practically it will make no difference if your model does not include any of the <a href=\"https://stackoverflow.com/a/66526891/9067615\">differently behaving layers</a>).</p>\n<hr/>\n<sup>\n<ol>\n<li>e.g. <code>BatchNorm</code>, <code>InstanceNorm</code></li>\n<li>This includes sub-modules of RNN modules <a href=\"https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train\">etc</a>.</li>\n</ol>\n</sup>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\" rel=\"noreferrer\"><code>model.eval</code></a> is a method of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html\" rel=\"noreferrer\"><code>torch.nn.Module</code></a>:</p>\n<blockquote>\n<h3><code>eval()</code></h3>\n<p>Sets the module in evaluation mode.</p>\n<p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout\" rel=\"noreferrer\"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p>\n<p>This is equivalent with <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=eval#torch.nn.Module.train\" rel=\"noreferrer\"><code>self.train(False)</code></a>.</p>\n</blockquote>\n<p>The opposite method is <a href=\"https://stackoverflow.com/a/51433411/5884955\"><code>model.train</code></a> explained nicely by Umang Gupta.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 12:32:56Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/12146914/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>When we have to predict the value of a <a href=\"https://en.wikipedia.org/wiki/Categorical_variable\" rel=\"noreferrer\">categorical</a> (or discrete) outcome we use <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" rel=\"noreferrer\">logistic regression</a>. I believe we use <a href=\"https://en.wikipedia.org/wiki/Linear_regression\" rel=\"noreferrer\">linear regression</a> to also predict the value of an outcome given the input values.</p>\n<p>Then, what is the difference between the two methodologies?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ul>\n<li><p><strong>Linear regression output as probabilities</strong></p>\n<p>It's tempting to use the linear regression output as probabilities but it's a mistake because the output can be negative, and greater than 1 whereas probability can not. As regression might actually\nproduce probabilities that could be less than 0, or even bigger than\n1, logistic regression was introduced. </p>\n<p>Source: <a href=\"http://gerardnico.com/wiki/data_mining/simple_logistic_regression\" rel=\"noreferrer\">http://gerardnico.com/wiki/data_mining/simple_logistic_regression</a></p>\n<p><a href=\"https://i.sstatic.net/rhVmk.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/rhVmk.jpg\"/></a></p></li>\n<li><p><strong>Outcome</strong></p>\n<p>In linear regression, the outcome (dependent variable) is continuous.\nIt can have any one of an infinite number of possible values. </p>\n<p>In logistic regression, the outcome (dependent variable) has only a limited number of possible values. </p></li>\n<li><p><strong>The dependent variable</strong></p>\n<p>Logistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue,\n1st/2nd/3rd/4th, etc.  </p>\n<p>Linear regression is used when your response variable is continuous. For instance, weight, height, number of hours,  etc.</p></li>\n<li><p><strong>Equation</strong></p>\n<p>Linear regression gives an equation which is of the form Y = mX + C,\nmeans equation with degree 1. </p>\n<p>However, logistic regression gives an equation which is of the form \nY = e<sup>X</sup> + e<sup>-X</sup></p></li>\n<li><p><strong>Coefficient interpretation</strong></p>\n<p>In linear regression, the coefficient interpretation of independent variables are quite straightforward (i.e. holding all other variables constant, with a unit increase in this variable, the dependent variable is expected to increase/decrease by xxx). </p>\n<p>However, in logistic regression, depends on the family (binomial, Poisson,\netc.) and link (log, logit, inverse-log, etc.) you use, the interpretation is different. </p></li>\n<li><p><strong>Error minimization technique</strong></p>\n<p>Linear regression uses <em>ordinary least squares</em> method to minimise the\nerrors and arrive at a best possible fit, while logistic regression\nuses <em>maximum likelihood</em> method to arrive at the solution.</p>\n<p>Linear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically. </p>\n<p>Logistic regression is just the opposite. Using the logistic loss function causes large errors to be penalized to an asymptotically constant.</p>\n<p>Consider linear regression on categorical {0, 1} outcomes to see why this is a problem. If your model predicts the outcome is 38, when the truth is 1, you've lost nothing. Linear regression would try to reduce that 38, logistic wouldn't (as much)<sup><a href=\"https://stats.stackexchange.com/a/29340\">2</a></sup>.</p></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values.</p>\n<p>For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be <em>any</em>, there are so many possible values that a linear regression model would be chosen.</p>\n<p>If, instead, you wanted to predict, based on size, whether a house would sell for more than $200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than $200K, or No, the house will not.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just to add on the previous answers.</p>\n<p><strong>Linear regression</strong></p>\n<p>Is meant to resolve the problem of predicting/estimating the output value for a given element X (say f(x)). The result of the prediction is a continuous function where the values may be positive or negative. In this case you normally have an input dataset with lots of <em>examples</em> and the output value for each one of them. The goal is to be able to <em>fit</em> a model to this data set so you are able to predict that output for new different/never seen elements. Following is the classical example of fitting a line to set of points, but in general linear regression could be used to fit more complex models (using higher polynomial degrees):</p>\n<p><a href=\"https://i.sstatic.net/4NRfx.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/4NRfx.png\"/></a></p>\n<p><strong>Resolving the problem</strong></p>\n<p>Linear regression can be solved in two different ways:</p>\n<ol>\n<li>Normal equation (direct way to solve the problem)</li>\n<li>Gradient descent (Iterative approach)</li>\n</ol>\n<p><strong>Logistic regression</strong></p>\n<p>Is meant to resolve <strong>classification</strong> problems where given an element you have to classify the same in N categories. Typical examples are, for example, given a mail to classify it as spam or not, or given a vehicle find to which category it belongs (car, truck, van, etc ..). That's basically the output is a finite set of discrete values.</p>\n<p><strong>Resolving the problem</strong></p>\n<p>Logistic regression problems could be resolved only by using Gradient descent. The formulation in general is very similar to linear regression the only difference is the usage of different hypothesis function. In linear regression the hypothesis has the form:</p>\n<pre><code>h(x) = theta_0 + theta_1*x_1 + theta_2*x_2 .. \n</code></pre>\n<p>where theta is the model we are trying to fit and [1, x_1, x_2, ..] is the input vector. In logistic regression the hypothesis function is different:</p>\n<pre><code>g(x) = 1 / (1 + e^-x)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/G4vfu.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/G4vfu.png\"/></a></p>\n<p>This function has a nice property, basically it maps any value to the range [0,1] which is appropiate to handle propababilities during the classificatin. For example in case of a binary classification g(X) could be interpreted as the probability to belong to the positive class. In this case normally you have different classes that are separated with a <em>decision boundary</em> which basically a <em>curve</em> that decides the separation between the different classes. Following is an example of dataset separated in two classes.</p>\n<p><a href=\"https://i.sstatic.net/YHMpC.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/YHMpC.png\"/></a></p>\n<ol>\n<li><p>You can also use the below code to generate the linear regression\ncurve\nq_df = details_df\n# q_df = pd.get_dummies(q_df)</p>\n<pre><code>q_df = pd.get_dummies(q_df, columns=[\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\",\n    \"7\",\n    \"8\",\n    \"9\"\n])\n\nq_1_df = q_df[\"1\"]\nq_df = q_df.drop([\"2\", \"3\", \"4\", \"5\"], axis=1)\n\n(import statsmodels.api as sm)\n\nx = sm.add_constant(q_df)\ntrain_x, test_x, train_y, test_y =    sklearn.model_selection.train_test_split(\nx, q3_rechange_delay_df, test_size=0.2, random_state=123 )\n</code></pre>\n<p>lmod = sm.OLS(train_y, train_x).fit() lmod.summary()</p>\n<p>lmod.predict()[:10]</p>\n<p>lmod.get_prediction().summary_frame()[:10]</p>\n<p>sm.qqplot(lmod.resid,line=\"q\") plt.title(\"Q-Q plot of Standardized\nResiduals\") plt.show()</p>\n</li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 12:34:09Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/34518656/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>When I trained my neural network with Theano or Tensorflow, they will report a variable called \"loss\" per epoch.</p>\n<p>How should I interpret this variable? Higher loss is better or worse, or what does it mean for the final performance (accuracy) of my neural network?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The lower the <strong>loss,</strong> the better a model (unless the model has over-fitted to the training data). The loss is calculated on <strong>training</strong> and <strong>validation</strong> and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.</p>\n<p>In the case of neural networks, the loss is usually <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\" rel=\"noreferrer\">negative log-likelihood</a> and <a href=\"https://en.wikipedia.org/wiki/Residual_sum_of_squares\" rel=\"noreferrer\">residual sum of squares</a> for classification and regression respectively. Then naturally, the main objective in a learning model is to reduce (minimize) the loss function's value with respect to the model's parameters by changing the weight vector values through different optimization methods, such as backpropagation in neural networks.</p>\n<p>Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s).</p>\n<p>The <strong>accuracy</strong> of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.</p>\n<p>For example, if the number of test samples is 1000 and model classifies 952 of those correctly, then the model's accuracy is 95.2%.</p>\n<p><a href=\"https://i.sstatic.net/Vnf0p.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Vnf0p.png\"/></a></p>\n<p>There are also some subtleties while reducing the loss value. For instance, you may run into the problem of <a href=\"https://en.wikipedia.org/wiki/Overfitting\" rel=\"noreferrer\">over-fitting</a> in which the model \"memorizes\" the training examples and becomes kind of ineffective for the test set. Over-fitting also occurs in cases where you do not employ a <a href=\"https://en.wikipedia.org/wiki/Regularization_%28mathematics%29\" rel=\"noreferrer\">regularization</a>, you have a very complex model (the number of free parameters <code>W</code> is large) or the number of data points <code>N</code> is very low.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>They are two different metrics to evaluate your model's performance usually being used in different phases.</p>\n<p>Loss is often used in the training process to find the \"best\" parameter values for your model (e.g. weights in neural network). It is what you try to optimize in the training by updating weights.</p>\n<p>Accuracy is more from an applied perspective. Once you find the optimized parameters above, you use this metrics to evaluate how accurate your model's prediction is compared to the true data.</p>\n<p>Let us use a toy classification example. You want to predict gender from one's weight and height. You have 3 data, they are as follows:(0 stands for male, 1 stands for female)</p>\n<p>y1 = 0, x1_w = 50kg, x2_h = 160cm;</p>\n<p>y2 = 0, x2_w = 60kg, x2_h = 170cm;</p>\n<p>y3 = 1, x3_w = 55kg, x3_h = 175cm;</p>\n<p>You use a simple logistic regression model, that is</p>\n<p><a href=\"https://i.sstatic.net/OdyMX.png\" rel=\"nofollow noreferrer\"><img alt=\"a simple logistic regression model, y = 1/(1+exp{-(b1x_w+b2x_h)})\" src=\"https://i.sstatic.net/OdyMX.png\"/></a></p>\n<p>How do you find b1 and b2? You define a loss first and use optimization method to minimize the loss in an iterative way by updating b1 and b2.</p>\n<p>In our example, a typical loss for this binary classification problem can be:\n(a minus sign should be added in front of the summation sign)</p>\n<p><img alt=\"\" src=\"https://i.sstatic.net/sVSxH.gif\"/></p>\n<p>We don't know what b1 and b2 should be. Let us make a random guess say b1 = 0.1 and b2 = -0.03. Then what is our loss now?</p>\n<p><img alt=\"\" src=\"https://latex.codecogs.com/gif.latex?%5Chat%7By%7D_1%20%3D%20%5Cfrac%7B1%7D%7B%201%20+%20e%5E%7B%20-%280.1%20%5Ccdot%2050%20-%200.03%20%5Ccdot%20160%29%20%7D%20%7D%20%3D%200.549834%20%3D%200.55\"/></p>\n<p><img alt=\"\" src=\"https://latex.codecogs.com/gif.latex?%5Chat%7By%7D_2%20%3D%20%5Cfrac%7B1%7D%7B%201%20+%20e%5E%7B%20-%280.1%20%5Ccdot%2060%20-%200.03%20%5Ccdot%20170%29%20%7D%20%7D%20%3D%200.7109495%20%3D%200.71\"/></p>\n<p><img alt=\"\" src=\"https://latex.codecogs.com/gif.latex?%5Chat%7By%7D_3%20%3D%20%5Cfrac%7B1%7D%7B%201%20+%20e%5E%7B%20-%280.1%20%5Ccdot%2055%20-%200.03%20%5Ccdot%20175%29%20%7D%20%7D%20%3D%200.5621765%20%3D%200.56\"/></p>\n<p>so the loss is</p>\n<p><img alt=\"\" src=\"https://latex.codecogs.com/gif.latex?-%5Clog%281-0.55%29%20-%5Clog%281-0.71%29%20-%20%5Clog%280.56%29%20%5Csimeq%202.6162\"/></p>\n<p>Then your learning algorithm (e.g. gradient descent) will find a way to update b1 and b2 to decrease the loss.</p>\n<p>What if b1=0.1 and b2=-0.03 is the final b1 and b2 (output from gradient descent), what is the accuracy now?</p>\n<p>Let's assume if y_hat &gt;= 0.5, we decide our prediction is female(1), otherwise it would be 0. Therefore, our algorithm predicts y1 = 1, y2 = 1 and y3 = 1. What is our accuracy? We make a wrong prediction on y1 and y2 and make a correct one on y3. So now our accuracy is 1/3 = 33.33%.</p>\n<p>PS: In <a href=\"https://stackoverflow.com/a/34519264\">Amir's answer</a>, back-propagation is said to be an optimization method in NN. I think it would be treated as a way to find gradient for weights in NN. Common optimization method in NN are GradientDescent and Adam.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just to clarify the Training/Validation/Test data sets:\nThe training set is used to perform the initial training of the model, initializing the weights of the neural network.</p>\n<p>The validation set is used after the neural network has been trained. It is used for tuning the network's hyperparameters, and comparing how changes to them affect the predictive accuracy of the model. Whereas the training set can be thought of as being used to build the neural network's gate weights, the validation set allows fine tuning of the parameters or architecture of the neural network model. It's useful as it allows repeatable comparison of these different parameters/architectures against the same data and networks weights, to observe how parameter/architecture changes affect the predictive power of the network.</p>\n<p>Then the test set is used only to test the predictive accuracy of the trained neural network on previously unseen data, after training and parameter/architecture selection with the training and validation data sets.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How do I initialize weights and biases of a network (via e.g. He or Xavier initialization)?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>Single layer</h1>\n<p>To initialize the weights of a single layer, use a function from <a href=\"https://pytorch.org/docs/master/nn.init.html\" rel=\"noreferrer\"><code>torch.nn.init</code></a>. For instance:</p>\n<pre><code>conv1 = torch.nn.Conv2d(...)\ntorch.nn.init.xavier_uniform(conv1.weight)\n</code></pre>\n<p>Alternatively, you can modify the parameters by writing to <code>conv1.weight.data</code> (which is a <a href=\"http://pytorch.org/docs/master/tensors.html#torch.Tensor\" rel=\"noreferrer\"><code>torch.Tensor</code></a>). Example:</p>\n<pre><code>conv1.weight.data.fill_(0.01)\n</code></pre>\n<p>The same applies for biases:</p>\n<pre><code>conv1.bias.data.fill_(0.01)\n</code></pre>\n<h2><code>nn.Sequential</code> or custom <code>nn.Module</code></h2>\n<p>Pass an initialization function to <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module.apply\" rel=\"noreferrer\"><code>torch.nn.Module.apply</code></a>. It will initialize the weights in the entire <code>nn.Module</code> recursively.</p>\n<blockquote>\n<p><strong>apply(<em>fn</em>):</strong> Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).</p>\n</blockquote>\n<p>Example:</p>\n<pre><code>def init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)\n\nnet = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\nnet.apply(init_weights)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>We compare different mode of weight-initialization using the same neural-network(NN) architecture.</h2>\n<h3>All Zeros or Ones</h3>\n<p>If you follow the principle of <a href=\"https://en.wikipedia.org/wiki/Occam's_razor\" rel=\"noreferrer\">Occam's razor</a>, you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case.</p>\n<p>With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.</p>\n<pre><code>    # initialize two NN's with 0 and 1 constant weights\n    model_0 = Net(constant_weight=0)\n    model_1 = Net(constant_weight=1)\n</code></pre>\n<ul>\n<li>After 2 epochs:</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/jpjTk.png\" rel=\"noreferrer\"><img alt=\"plot of training loss with weight initialization to constant\" src=\"https://i.sstatic.net/jpjTk.png\"/></a></p>\n<pre><code>Validation Accuracy\n9.625% -- All Zeros\n10.050% -- All Ones\nTraining Loss\n2.304  -- All Zeros\n1552.281  -- All Ones\n</code></pre>\n<h3>Uniform Initialization</h3>\n<p>A <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution\" rel=\"noreferrer\">uniform distribution</a> has the equal probability of picking any number from a set of numbers. </p>\n<p>Let's see how well the neural network trains using a uniform weight initialization, where <code>low=0.0</code> and <code>high=1.0</code>.</p>\n<p>Below, we'll see another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, we can:</p>\n<blockquote>\n<ol>\n<li>Define a function that assigns weights by the type of network layer, <em>then</em> </li>\n<li>Apply those weights to an initialized model using <code>model.apply(fn)</code>, which applies a function to each model layer.</li>\n</ol>\n</blockquote>\n<pre><code>    # takes in a module and applies the specified weight initialization\n    def weights_init_uniform(m):\n        classname = m.__class__.__name__\n        # for every Linear layer in a model..\n        if classname.find('Linear') != -1:\n            # apply a uniform distribution to the weights and a bias=0\n            m.weight.data.uniform_(0.0, 1.0)\n            m.bias.data.fill_(0)\n\n    model_uniform = Net()\n    model_uniform.apply(weights_init_uniform)\n</code></pre>\n<ul>\n<li>After 2 epochs:</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/rTTP9.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/rTTP9.png\"/></a></p>\n<pre><code>Validation Accuracy\n36.667% -- Uniform Weights\nTraining Loss\n3.208  -- Uniform Weights\n</code></pre>\n<h2>General rule for setting weights</h2>\n<p>The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. </p>\n<blockquote>\n<p>Good practice is to start your weights in the range of [-y, y] where <code>y=1/sqrt(n)</code><br/>\n  (n is the number of inputs to a given neuron).</p>\n</blockquote>\n<pre><code>    # takes in a module and applies the specified weight initialization\n    def weights_init_uniform_rule(m):\n        classname = m.__class__.__name__\n        # for every Linear layer in a model..\n        if classname.find('Linear') != -1:\n            # get the number of the inputs\n            n = m.in_features\n            y = 1.0/np.sqrt(n)\n            m.weight.data.uniform_(-y, y)\n            m.bias.data.fill_(0)\n\n    # create a new model with these weights\n    model_rule = Net()\n    model_rule.apply(weights_init_uniform_rule)\n</code></pre>\n<p>below we compare performance of NN, weights initialized with uniform distribution [-0.5,0.5) versus the one whose weight is initialized using <strong>general rule</strong></p>\n<ul>\n<li>After 2 epochs:</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/AAmh6.png\" rel=\"noreferrer\"><img alt=\"plot showing performance of uniform initialization of weight versus general rule of initialization\" src=\"https://i.sstatic.net/AAmh6.png\"/></a></p>\n<pre><code>Validation Accuracy\n75.817% -- Centered Weights [-0.5, 0.5)\n85.208% -- General Rule [-y, y)\nTraining Loss\n0.705  -- Centered Weights [-0.5, 0.5)\n0.469  -- General Rule [-y, y)\n</code></pre>\n<h2>normal distribution to initialize the weights</h2>\n<blockquote>\n<p>The normal distribution should have a mean of 0 and a standard deviation of <code>y=1/sqrt(n)</code>, where n is the number of inputs to NN</p>\n</blockquote>\n<pre><code>    ## takes in a module and applies the specified weight initialization\n    def weights_init_normal(m):\n        '''Takes in a module and initializes all linear layers with weight\n           values taken from a normal distribution.'''\n\n        classname = m.__class__.__name__\n        # for every Linear layer in a model\n        if classname.find('Linear') != -1:\n            y = m.in_features\n        # m.weight.data shoud be taken from a normal distribution\n            m.weight.data.normal_(0.0,1/np.sqrt(y))\n        # m.bias.data should be 0\n            m.bias.data.fill_(0)\n</code></pre>\n<p>below we show the performance of two NN one initialized using <strong>uniform-distribution</strong> and the other using <strong>normal-distribution</strong></p>\n<ul>\n<li>After 2 epochs:</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/144Mn.png\" rel=\"noreferrer\"><img alt=\"performance of weight initialization using uniform-distribution versus the normal distribution\" src=\"https://i.sstatic.net/144Mn.png\"/></a> </p>\n<pre><code>Validation Accuracy\n85.775% -- Uniform Rule [-y, y)\n84.717% -- Normal Distribution\nTraining Loss\n0.329  -- Uniform Rule [-y, y)\n0.443  -- Normal Distribution\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>To initialize layers, you typically don't need to do anything.</strong></p>\n<p>PyTorch will do it for you. If you think about it, this makes a lot of sense. Why should we initialize layers, when PyTorch can do that following the latest trends?</p>\n<p>For instance, the <a href=\"https://github.com/pytorch/pytorch/blob/af7dc23124a6e3e7b8af0637e3b027f3a8b3fb76/torch/nn/modules/linear.py#L101\" rel=\"noreferrer\"><code>Linear</code></a> layer's <code>__init__</code> method will do <a href=\"http://kaiminghe.com/\" rel=\"noreferrer\">Kaiming He</a> initialization:</p>\n<pre><code>init.kaiming_uniform_(self.weight, a=math.sqrt(5))\nif self.bias is not None:\n    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n    bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n    init.uniform_(self.bias, -bound, bound)\n</code></pre>\n<p>Similarly, this holds for other layers types. For e.g., <code>Conv2d</code>, check <a href=\"https://github.com/pytorch/pytorch/blob/029a968212b018192cb6fc64075e68db9985c86a/torch/nn/modules/conv.py#L49\" rel=\"noreferrer\">here</a>.</p>\n<p><strong>NOTE:</strong> The advantage of proper initialization is faster training speed. If your problem requires special initialization, you can still do it afterwards.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a machine learning classification problem with 80% categorical variables. Must I use one hot encoding if I want to use some classifier for the classification? Can i pass the data to a classifier without the encoding? </p>\n<p>I am trying to do the following for feature selection:</p>\n<ol>\n<li><p>I read the train file:</p>\n<pre><code>num_rows_to_read = 10000\ntrain_small = pd.read_csv(\"../../dataset/train.csv\",   nrows=num_rows_to_read)\n</code></pre></li>\n<li><p>I change the type of the categorical features to 'category':</p>\n<pre><code>non_categorial_features = ['orig_destination_distance',\n                          'srch_adults_cnt',\n                          'srch_children_cnt',\n                          'srch_rm_cnt',\n                          'cnt']\n\nfor categorical_feature in list(train_small.columns):\n    if categorical_feature not in non_categorial_features:\n        train_small[categorical_feature] = train_small[categorical_feature].astype('category')\n</code></pre></li>\n<li><p>I use one hot encoding: </p>\n<pre><code>train_small_with_dummies = pd.get_dummies(train_small, sparse=True)\n</code></pre></li>\n</ol>\n<p>The problem is that the 3'rd part often get stuck, although I am using a strong machine.</p>\n<p>Thus, without the one hot encoding I can't do any feature selection, for determining the importance of the features.</p>\n<p>What do you recommend?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Approach 1: You can use pandas' <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\" rel=\"noreferrer\"><code>pd.get_dummies</code></a>.</strong></p>\n<p><strong>Example 1:</strong></p>\n<pre><code>import pandas as pd\ns = pd.Series(list('abca'))\npd.get_dummies(s)\nOut[]: \n     a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n</code></pre>\n<p><strong>Example 2:</strong></p>\n<p>The following will transform a given column into one hot. Use prefix to have multiple dummies.</p>\n<pre><code>import pandas as pd\n        \ndf = pd.DataFrame({\n          'A':['a','b','a'],\n          'B':['b','a','c']\n        })\ndf\nOut[]: \n   A  B\n0  a  b\n1  b  a\n2  a  c\n\n# Get one hot encoding of columns B\none_hot = pd.get_dummies(df['B'])\n# Drop column B as it is now encoded\ndf = df.drop('B',axis = 1)\n# Join the encoded df\ndf = df.join(one_hot)\ndf  \nOut[]: \n       A  a  b  c\n    0  a  0  1  0\n    1  b  1  0  0\n    2  a  0  0  1\n</code></pre>\n<p><strong>Approach 2: Use Scikit-learn</strong></p>\n<p>Using a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\" rel=\"noreferrer\"><code>OneHotEncoder</code></a> has the advantage of being able to <code>fit</code> on some training data and then <code>transform</code> on some other data using the same instance. We also have <code>handle_unknown</code> to further control what the encoder does with <em>unseen</em> data.</p>\n<p>Given a dataset with three features and four samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding.</p>\n<pre><code>&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder\n&gt;&gt;&gt; enc = OneHotEncoder()\n&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])   \nOneHotEncoder(categorical_features='all', dtype=&lt;class 'numpy.float64'&gt;,\n   handle_unknown='error', n_values='auto', sparse=True)\n&gt;&gt;&gt; enc.n_values_\narray([2, 3, 4])\n&gt;&gt;&gt; enc.feature_indices_\narray([0, 2, 5, 9], dtype=int32)\n&gt;&gt;&gt; enc.transform([[0, 1, 1]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])\n</code></pre>\n<p>Here is the link for this example: <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Much easier to use Pandas for basic one-hot encoding. If you're looking for more options you can use <code>scikit-learn</code>.</p>\n<p>For basic one-hot encoding with <strong>Pandas</strong> you pass your data frame into the <strong>get_dummies</strong> function.</p>\n<p>For example, if I have a dataframe called <strong>imdb_movies</strong>:</p>\n<p><a href=\"https://i.sstatic.net/OOM7e.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/OOM7e.png\"/></a></p>\n<p>...and I want to one-hot encode the Rated column, I do this:</p>\n<pre><code>pd.get_dummies(imdb_movies.Rated)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/ilJux.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ilJux.png\"/></a></p>\n<p>This returns a new <code>dataframe</code> with a column for every \"<strong>level</strong>\" of rating that exists, along with either a 1 or 0 specifying the presence of that rating for a given observation.</p>\n<p>Usually, we want this to be part of the original <code>dataframe</code>. In this case, we attach our new dummy coded frame onto the original frame using \"<strong>column-binding</strong>.</p>\n<p>We can column-bind by using Pandas <strong>concat</strong> function:</p>\n<pre><code>rated_dummies = pd.get_dummies(imdb_movies.Rated)\npd.concat([imdb_movies, rated_dummies], axis=1)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/GvQo2.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/GvQo2.png\"/></a></p>\n<p>We can now run an analysis on our full <code>dataframe</code>.</p>\n<p><strong>SIMPLE UTILITY FUNCTION</strong></p>\n<p>I would recommend making yourself a <strong>utility function</strong> to do this quickly:</p>\n<pre><code>def encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    return(res)\n</code></pre>\n<p><strong>Usage</strong>:</p>\n<pre><code>encode_and_bind(imdb_movies, 'Rated')\n</code></pre>\n<p><strong>Result</strong>:</p>\n<p><a href=\"https://i.sstatic.net/CJALw.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/CJALw.png\"/></a></p>\n<p>Also, as per @pmalbu comment, if you would like the function to <strong>remove the original feature_to_encode</strong> then use this version:</p>\n<pre><code>def encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n</code></pre>\n<p>You can encode multiple features at the same time as follows:</p>\n<pre><code>features_to_encode = ['feature_1', 'feature_2', 'feature_3',\n                      'feature_4']\nfor feature in features_to_encode:\n    res = encode_and_bind(train_set, feature)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can do it with <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html\" rel=\"noreferrer\"><code>numpy.eye</code></a> and a using the array element selection mechanism:</p>\n<pre><code>import numpy as np\nnb_classes = 6\ndata = [[2, 3, 4, 0]]\n\ndef indices_to_one_hot(data, nb_classes):\n    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n    targets = np.array(data).reshape(-1)\n    return np.eye(nb_classes)[targets]\n</code></pre>\n<p>The the return value of <code>indices_to_one_hot(nb_classes, data)</code> is now</p>\n<pre><code>array([[[ 0.,  0.,  1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.],\n        [ 1.,  0.,  0.,  0.,  0.,  0.]]])\n</code></pre>\n<p>The <code>.reshape(-1)</code> is there to make sure you have the right labels format (you might also have <code>[[2], [3], [4], [0]]</code>).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 15:51:42Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/13610074/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Is there a rule-of-thumb for how to best divide data into training and validation sets? Is an even 50/50 split advisable? Or are there clear advantages of having more training data relative to validation data (or vice versa)? Or is this choice pretty much application dependent?</p>\n<p>I have been mostly using an 80% / 20% of training and validation data, respectively, but I chose this division without any principled reason. Can someone who is more experienced in machine learning advise me?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.</p>\n<p>If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).</p>\n<p>Assuming you have enough data to do proper held-out test data (rather than cross-validation), the following is an instructive way to get a handle on variances:</p>\n<ol>\n<li>Split your data into training and testing (80/20 is indeed a good starting point)</li>\n<li>Split the <em>training</em> data into training and validation (again, 80/20 is a fair split).</li>\n<li>Subsample random selections of your training data, train the classifier with this, and record the performance on the validation set</li>\n<li>Try a series of runs with different amounts of training data: randomly sample 20% of it, say, 10 times and observe performance on the validation data, then do the same with 40%, 60%, 80%. You should see both greater performance with more data, but also lower variance across the different random samples</li>\n<li>To get a handle on variance due to the size of test data, perform the same procedure in reverse. Train on all of your training data, then randomly sample a percentage of your <em>validation</em> data a number of times, and observe performance. You should now find that the mean performance on small samples of your validation data is roughly the same as the performance on all the validation data, but the variance is much higher with smaller numbers of test samples</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the <a href=\"http://en.wikipedia.org/wiki/Pareto_principle\" rel=\"noreferrer\">Pareto principle</a>. It's usually a safe bet if you use that ratio.</p>\n<p>However, depending on the training/validation methodology you employ, the ratio may change. For example: if you use 10-fold cross validation, then you would end up with a validation set of 10% at each fold.</p>\n<p>There has been some research into <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.1337&amp;rep=rep1&amp;type=pdf\" rel=\"noreferrer\">what is the proper ratio between the training set and the validation set</a>:</p>\n<blockquote>\n<p>The fraction of patterns reserved for the validation set should be\n  inversely proportional to the square root of the number of free\n  adjustable parameters.</p>\n</blockquote>\n<p>In their conclusion they specify a formula:</p>\n<blockquote>\n<p>Validation set (v) to training set (t) size ratio, v/t, scales like\n  ln(N/h-max), where N is the number of families of recognizers and\n  h-max is the largest complexity of those families.</p>\n</blockquote>\n<p>What they mean by complexity is: </p>\n<blockquote>\n<p>Each family of recognizer is characterized by its complexity, which\n  may or may not be related to the <a href=\"http://en.wikipedia.org/wiki/VC_dimension\" rel=\"noreferrer\">VC-dimension</a>, the description\n  length, the number of adjustable parameters, or other measures of\n  complexity.</p>\n</blockquote>\n<p>Taking the first rule of thumb (i.e.validation set should be inversely proportional to the square root of the number of free adjustable parameters), you can conclude that if you have 32 adjustable parameters, the square root of 32 is ~5.65, the fraction should be 1/5.65 or 0.177 (v/t). Roughly 17.7% should be reserved for validation and 82.3% for training.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Last year, I took Prof: Andrew Ng’s online machine learning course. His recommendation was:</p>\n<p><strong>Training</strong>: 60%</p>\n<p><strong>Cross-validation</strong>: 20%</p>\n<p><strong>Testing</strong>: 20%</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How do I save a trained <strong>Naive Bayes classifier</strong> to <strong>disk</strong> and use it to <strong>predict</strong> data?</p>\n<p>I have the following sample program from the scikit-learn website:</p>\n<pre><code>from sklearn import datasets\niris = datasets.load_iris()\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\nprint \"Number of mislabeled points : %d\" % (iris.target != y_pred).sum()\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Classifiers are just objects that can be pickled and dumped like any other. To continue your example:</p>\n<pre><code>import cPickle\n# save the classifier\nwith open('my_dumped_classifier.pkl', 'wb') as fid:\n    cPickle.dump(gnb, fid)    \n\n# load it again\nwith open('my_dumped_classifier.pkl', 'rb') as fid:\n    gnb_loaded = cPickle.load(fid)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can also use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"noreferrer\">joblib.dump</a> and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"noreferrer\">joblib.load</a> which is much more efficient at handling numerical arrays than the default python pickler.</p>\n<p>Joblib is included in scikit-learn:</p>\n<pre><code>&gt;&gt;&gt; import joblib\n&gt;&gt;&gt; from sklearn.datasets import load_digits\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n\n&gt;&gt;&gt; digits = load_digits()\n&gt;&gt;&gt; clf = SGDClassifier().fit(digits.data, digits.target)\n&gt;&gt;&gt; clf.score(digits.data, digits.target)  # evaluate training error\n0.9526989426822482\n\n&gt;&gt;&gt; filename = '/tmp/digits_classifier.joblib.pkl'\n&gt;&gt;&gt; _ = joblib.dump(clf, filename, compress=9)\n\n&gt;&gt;&gt; clf2 = joblib.load(filename)\n&gt;&gt;&gt; clf2\nSGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,\n       fit_intercept=True, learning_rate='optimal', loss='hinge', n_iter=5,\n       n_jobs=1, penalty='l2', power_t=0.5, rho=0.85, seed=0,\n       shuffle=False, verbose=0, warm_start=False)\n&gt;&gt;&gt; clf2.score(digits.data, digits.target)\n0.9526989426822482\n</code></pre>\n<p>Edit: in Python 3.8+ it's now possible to use pickle for efficient pickling of object with large numerical arrays as attributes if you use pickle protocol 5 (which is not the default).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What you are looking for is called <strong>Model persistence</strong> in sklearn words and it is documented in <a href=\"http://scikit-learn.org/stable/tutorial/basic/tutorial.html#model-persistence\">introduction</a> and in <a href=\"http://scikit-learn.org/stable/modules/model_persistence.html\">model persistence</a> sections.</p>\n<p>So you have initialized your classifier and trained it for a long time with</p>\n<pre><code>clf = some.classifier()\nclf.fit(X, y)\n</code></pre>\n<p>After this you have two options:</p>\n<p><strong>1) Using Pickle</strong></p>\n<pre><code>import pickle\n# now you can save it to a file\nwith open('filename.pkl', 'wb') as f:\n    pickle.dump(clf, f)\n\n# and later you can load it\nwith open('filename.pkl', 'rb') as f:\n    clf = pickle.load(f)\n</code></pre>\n<p><strong>2) Using Joblib</strong></p>\n<pre><code>from sklearn.externals import joblib\n# now you can save it to a file\njoblib.dump(clf, 'filename.pkl') \n# and later you can load it\nclf = joblib.load('filename.pkl')\n</code></pre>\n<p>One more time it is helpful to read the above-mentioned links </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\" rel=\"noreferrer\">train_test_split</a> from <code>sklearn.cross_validation</code>, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data. </p>\n<p>I know that a workaround would be to use <code>train_test_split</code> two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Numpy solution. We will shuffle the whole dataset first (<code>df.sample(frac=1, random_state=42)</code>) and then split our data set into the following parts:</p>\n<ul>\n<li>60% - train set,</li>\n<li>20% - validation set,</li>\n<li>20% - test set</li>\n</ul>\n<hr/>\n<pre><code>In [305]: train, validate, test = \\\n              np.split(df.sample(frac=1, random_state=42), \n                       [int(.6*len(df)), int(.8*len(df))])\n\nIn [306]: train\nOut[306]:\n          A         B         C         D         E\n0  0.046919  0.792216  0.206294  0.440346  0.038960\n2  0.301010  0.625697  0.604724  0.936968  0.870064\n1  0.642237  0.690403  0.813658  0.525379  0.396053\n9  0.488484  0.389640  0.599637  0.122919  0.106505\n8  0.842717  0.793315  0.554084  0.100361  0.367465\n7  0.185214  0.603661  0.217677  0.281780  0.938540\n\nIn [307]: validate\nOut[307]:\n          A         B         C         D         E\n5  0.806176  0.008896  0.362878  0.058903  0.026328\n6  0.145777  0.485765  0.589272  0.806329  0.703479\n\nIn [308]: test\nOut[308]:\n          A         B         C         D         E\n4  0.521640  0.332210  0.370177  0.859169  0.401087\n3  0.333348  0.964011  0.083498  0.670386  0.169619\n</code></pre>\n<p><code>[int(.6*len(df)), int(.8*len(df))]</code> - is an <code>indices_or_sections </code> array for <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html\" rel=\"noreferrer\">numpy.split()</a>.</p>\n<p>Here is a small demo for <code>np.split()</code> usage - let's split 20-elements array into the following parts: 80%, 10%, 10%:</p>\n<pre><code>In [45]: a = np.arange(1, 21)\n\nIn [46]: a\nOut[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nIn [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])\nOut[47]:\n[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n array([17, 18]),\n array([19, 20])]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>However, one approach to dividing the dataset into <code>train</code>, <code>test</code>, <code>cv</code> with <code>0.6</code>, <code>0.2</code>, <code>0.2</code> would be to use the <code>train_test_split</code> method twice.</p>\n<pre><code>from sklearn.model_selection import train_test_split\n\nx, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h3>Note:</h3>\n<p>Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets.</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndef train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df.index)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.iloc[perm[:train_end]]\n    validate = df.iloc[perm[train_end:validate_end]]\n    test = df.iloc[perm[validate_end:]]\n    return train, validate, test\n</code></pre>\n<h3>Demonstration</h3>\n<pre><code>np.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\ndf\n</code></pre>\n<p><a href=\"https://i.sstatic.net/ThpsQ.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ThpsQ.png\"/></a></p>\n<pre><code>train, validate, test = train_validate_test_split(df)\n\ntrain\n</code></pre>\n<p><a href=\"https://i.sstatic.net/XNRBT.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/XNRBT.png\"/></a></p>\n<pre><code>validate\n</code></pre>\n<p><a href=\"https://i.sstatic.net/PpyC8.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/PpyC8.png\"/></a></p>\n<pre><code>test\n</code></pre>\n<p><a href=\"https://i.sstatic.net/U6CaT.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/U6CaT.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is it possible to specify your own distance function using scikit-learn K-Means Clustering?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's a small kmeans that uses any of the 20-odd distances in\n<a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html#scipy.spatial.distance.cdist\" rel=\"noreferrer\">scipy.spatial.distance</a>, or a user function.<br/>\nComments would be welcome (this has had only one user so far, not enough);\nin particular, what are your N, dim, k, metric ?</p>\n<pre><code>#!/usr/bin/env python\n# kmeans.py using any of the 20-odd metrics in scipy.spatial.distance\n# kmeanssample 2 pass, first sample sqrt(N)\n\nfrom __future__ import division\nimport random\nimport numpy as np\nfrom scipy.spatial.distance import cdist  # $scipy/spatial/distance.py\n    # http://docs.scipy.org/doc/scipy/reference/spatial.html\nfrom scipy.sparse import issparse  # $scipy/sparse/csr.py\n\n__date__ = \"2011-11-17 Nov denis\"\n    # X sparse, any cdist metric: real app ?\n    # centres get dense rapidly, metrics in high dim hit distance whiteout\n    # vs unsupervised / semi-supervised svm\n\n#...............................................................................\ndef kmeans( X, centres, delta=.001, maxiter=10, metric=\"euclidean\", p=2, verbose=1 ):\n    \"\"\" centres, Xtocentre, distances = kmeans( X, initial centres ... )\n    in:\n        X N x dim  may be sparse\n        centres k x dim: initial centres, e.g. random.sample( X, k )\n        delta: relative error, iterate until the average distance to centres\n            is within delta of the previous average distance\n        maxiter\n        metric: any of the 20-odd in scipy.spatial.distance\n            \"chebyshev\" = max, \"cityblock\" = L1, \"minkowski\" with p=\n            or a function( Xvec, centrevec ), e.g. Lqmetric below\n        p: for minkowski metric -- local mod cdist for 0 &lt; p &lt; 1 too\n        verbose: 0 silent, 2 prints running distances\n    out:\n        centres, k x dim\n        Xtocentre: each X -&gt; its nearest centre, ints N -&gt; k\n        distances, N\n    see also: kmeanssample below, class Kmeans below.\n    \"\"\"\n    if not issparse(X):\n        X = np.asanyarray(X)  # ?\n    centres = centres.todense() if issparse(centres) \\\n        else centres.copy()\n    N, dim = X.shape\n    k, cdim = centres.shape\n    if dim != cdim:\n        raise ValueError( \"kmeans: X %s and centres %s must have the same number of columns\" % (\n            X.shape, centres.shape ))\n    if verbose:\n        print \"kmeans: X %s  centres %s  delta=%.2g  maxiter=%d  metric=%s\" % (\n            X.shape, centres.shape, delta, maxiter, metric)\n    allx = np.arange(N)\n    prevdist = 0\n    for jiter in range( 1, maxiter+1 ):\n        D = cdist_sparse( X, centres, metric=metric, p=p )  # |X| x |centres|\n        xtoc = D.argmin(axis=1)  # X -&gt; nearest centre\n        distances = D[allx,xtoc]\n        avdist = distances.mean()  # median ?\n        if verbose &gt;= 2:\n            print \"kmeans: av |X - nearest centre| = %.4g\" % avdist\n        if (1 - delta) * prevdist &lt;= avdist &lt;= prevdist \\\n        or jiter == maxiter:\n            break\n        prevdist = avdist\n        for jc in range(k):  # (1 pass in C)\n            c = np.where( xtoc == jc )[0]\n            if len(c) &gt; 0:\n                centres[jc] = X[c].mean( axis=0 )\n    if verbose:\n        print \"kmeans: %d iterations  cluster sizes:\" % jiter, np.bincount(xtoc)\n    if verbose &gt;= 2:\n        r50 = np.zeros(k)\n        r90 = np.zeros(k)\n        for j in range(k):\n            dist = distances[ xtoc == j ]\n            if len(dist) &gt; 0:\n                r50[j], r90[j] = np.percentile( dist, (50, 90) )\n        print \"kmeans: cluster 50 % radius\", r50.astype(int)\n        print \"kmeans: cluster 90 % radius\", r90.astype(int)\n            # scale L1 / dim, L2 / sqrt(dim) ?\n    return centres, xtoc, distances\n\n#...............................................................................\ndef kmeanssample( X, k, nsample=0, **kwargs ):\n    \"\"\" 2-pass kmeans, fast for large N:\n        1) kmeans a random sample of nsample ~ sqrt(N) from X\n        2) full kmeans, starting from those centres\n    \"\"\"\n        # merge w kmeans ? mttiw\n        # v large N: sample N^1/2, N^1/2 of that\n        # seed like sklearn ?\n    N, dim = X.shape\n    if nsample == 0:\n        nsample = max( 2*np.sqrt(N), 10*k )\n    Xsample = randomsample( X, int(nsample) )\n    pass1centres = randomsample( X, int(k) )\n    samplecentres = kmeans( Xsample, pass1centres, **kwargs )[0]\n    return kmeans( X, samplecentres, **kwargs )\n\ndef cdist_sparse( X, Y, **kwargs ):\n    \"\"\" -&gt; |X| x |Y| cdist array, any cdist metric\n        X or Y may be sparse -- best csr\n    \"\"\"\n        # todense row at a time, v slow if both v sparse\n    sxy = 2*issparse(X) + issparse(Y)\n    if sxy == 0:\n        return cdist( X, Y, **kwargs )\n    d = np.empty( (X.shape[0], Y.shape[0]), np.float64 )\n    if sxy == 2:\n        for j, x in enumerate(X):\n            d[j] = cdist( x.todense(), Y, **kwargs ) [0]\n    elif sxy == 1:\n        for k, y in enumerate(Y):\n            d[:,k] = cdist( X, y.todense(), **kwargs ) [0]\n    else:\n        for j, x in enumerate(X):\n            for k, y in enumerate(Y):\n                d[j,k] = cdist( x.todense(), y.todense(), **kwargs ) [0]\n    return d\n\ndef randomsample( X, n ):\n    \"\"\" random.sample of the rows of X\n        X may be sparse -- best csr\n    \"\"\"\n    sampleix = random.sample( xrange( X.shape[0] ), int(n) )\n    return X[sampleix]\n\ndef nearestcentres( X, centres, metric=\"euclidean\", p=2 ):\n    \"\"\" each X -&gt; nearest centre, any metric\n            euclidean2 (~ withinss) is more sensitive to outliers,\n            cityblock (manhattan, L1) less sensitive\n    \"\"\"\n    D = cdist( X, centres, metric=metric, p=p )  # |X| x |centres|\n    return D.argmin(axis=1)\n\ndef Lqmetric( x, y=None, q=.5 ):\n    # yes a metric, may increase weight of near matches; see ...\n    return (np.abs(x - y) ** q) .mean() if y is not None \\\n        else (np.abs(x) ** q) .mean()\n\n#...............................................................................\nclass Kmeans:\n    \"\"\" km = Kmeans( X, k= or centres=, ... )\n        in: either initial centres= for kmeans\n            or k= [nsample=] for kmeanssample\n        out: km.centres, km.Xtocentre, km.distances\n        iterator:\n            for jcentre, J in km:\n                clustercentre = centres[jcentre]\n                J indexes e.g. X[J], classes[J]\n    \"\"\"\n    def __init__( self, X, k=0, centres=None, nsample=0, **kwargs ):\n        self.X = X\n        if centres is None:\n            self.centres, self.Xtocentre, self.distances = kmeanssample(\n                X, k=k, nsample=nsample, **kwargs )\n        else:\n            self.centres, self.Xtocentre, self.distances = kmeans(\n                X, centres, **kwargs )\n\n    def __iter__(self):\n        for jc in range(len(self.centres)):\n            yield jc, (self.Xtocentre == jc)\n\n#...............................................................................\nif __name__ == \"__main__\":\n    import random\n    import sys\n    from time import time\n\n    N = 10000\n    dim = 10\n    ncluster = 10\n    kmsample = 100  # 0: random centres, &gt; 0: kmeanssample\n    kmdelta = .001\n    kmiter = 10\n    metric = \"cityblock\"  # \"chebyshev\" = max, \"cityblock\" L1,  Lqmetric\n    seed = 1\n\n    exec( \"\\n\".join( sys.argv[1:] ))  # run this.py N= ...\n    np.set_printoptions( 1, threshold=200, edgeitems=5, suppress=True )\n    np.random.seed(seed)\n    random.seed(seed)\n\n    print \"N %d  dim %d  ncluster %d  kmsample %d  metric %s\" % (\n        N, dim, ncluster, kmsample, metric)\n    X = np.random.exponential( size=(N,dim) )\n        # cf scikits-learn datasets/\n    t0 = time()\n    if kmsample &gt; 0:\n        centres, xtoc, dist = kmeanssample( X, ncluster, nsample=kmsample,\n            delta=kmdelta, maxiter=kmiter, metric=metric, verbose=2 )\n    else:\n        randomcentres = randomsample( X, ncluster )\n        centres, xtoc, dist = kmeans( X, randomcentres,\n            delta=kmdelta, maxiter=kmiter, metric=metric, verbose=2 )\n    print \"%.0f msec\" % ((time() - t0) * 1000)\n\n    # also ~/py/np/kmeans/test-kmeans.py\n</code></pre>\n<p>Some notes added 26mar 2012:</p>\n<p>1) for cosine distance, first normalize all the data vectors to |X| = 1; then</p>\n<pre><code>cosinedistance( X, Y ) = 1 - X . Y = Euclidean distance |X - Y|^2 / 2\n</code></pre>\n<p>is fast. For bit vectors, keep the norms separately from the vectors\ninstead of expanding out to floats\n(although some programs may expand for you).\nFor sparse vectors, say 1 % of N, X . Y should take time O( 2 % N ),\nspace O(N); but I don't know which programs do that.</p>\n<p>2)\n<a href=\"http://scikit-learn.org/stable/modules/clustering.html\" rel=\"noreferrer\">Scikit-learn clustering</a>\ngives an excellent overview of k-means, mini-batch-k-means ...\nwith code that works on scipy.sparse matrices.</p>\n<p>3) Always check cluster sizes after k-means.\nIf you're expecting roughly equal-sized clusters, but they come out\n<code>[44 37  9  5  5] %</code> ... (sound of head-scratching).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Unfortunately no: scikit-learn current implementation of k-means only uses Euclidean distances.</p>\n<p>It is not trivial to extend k-means to other distances and denis' answer above is not the correct way to implement k-means for other metrics.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just use nltk instead where you can do this, e.g.</p>\n<pre><code>from nltk.cluster.kmeans import KMeansClusterer\nNUM_CLUSTERS = &lt;choose a value&gt;\ndata = &lt;sparse matrix that you would normally give to scikit&gt;.toarray()\n\nkclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\nassigned_clusters = kclusterer.cluster(data, assign_clusters=True)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Does it call <code>forward()</code> in <code>nn.Module</code>? I thought when we call the model, <code>forward</code> method is being used.\nWhy do we need to specify train()?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>model.train()</code> tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.</p>\n<p>More details:\n<code>model.train()</code> sets the mode to train\n(see <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train\" rel=\"noreferrer\">source code</a>). You can call either <code>model.eval()</code> or <code>model.train(mode=False)</code> to tell that you are testing.\nIt is somewhat intuitive to expect <code>train</code> function to train model but it does not do that. It just sets the mode.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is the code for <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train\" rel=\"noreferrer\"><code>nn.Module.train()</code></a>:</p>\n<pre><code>def train(self, mode=True):\n        r\"\"\"Sets the module in training mode.\"\"\"      \n        self.training = mode\n        for module in self.children():\n            module.train(mode)\n        return self\n</code></pre>\n<p>Here is the code for <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\" rel=\"noreferrer\"><code>nn.Module.eval()</code></a>:</p>\n<pre><code>def eval(self):\n        r\"\"\"Sets the module in evaluation mode.\"\"\"\n        return self.train(False)\n</code></pre>\n<p>By default, the <code>self.training</code> flag is set to <code>True</code>, i.e., modules are in train mode by default. When <code>self.training</code> is <code>False</code>, the module is in the opposite state, eval mode.</p>\n<p>Of the most commonly used layers, only <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html\" rel=\"noreferrer\"><code>Dropout</code></a> and <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html\" rel=\"noreferrer\"><code>BatchNorm</code></a> care about that flag.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th><code>model.train()</code></th>\n<th><a href=\"https://stackoverflow.com/a/66843176/9067615\"><code>model.eval()</code></a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sets model in <strong>train</strong>ing mode i.e. <br/> <br/> • <code>BatchNorm</code> layers use per-batch statistics <br/> • <code>Dropout</code> layers activated <a href=\"https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train\">etc</a></td>\n<td>Sets model in <strong>eval</strong>uation (inference) mode i.e. <br/><br/> • <code>BatchNorm</code> layers use running statistics <br/> • <code>Dropout</code> layers de-activated etc</td>\n</tr>\n<tr>\n<td></td>\n<td>Equivalent to <code>model.train(False)</code>.</td>\n</tr>\n</tbody>\n</table>\n</div><sup>\n<p><strong>Note:</strong> neither of these function calls run forward / backward passes. They tell the model <em>how</em> to act <em>when</em> run.</p>\n<p>This is important as <a href=\"https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train\">some modules (layers)</a> (e.g. <code>Dropout</code>, <code>BatchNorm</code>) are designed to behave differently during training vs inference, and hence the model will produce unexpected results if run in the wrong mode.</p>\n</sup>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/2595176/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2019-01-01 13:54:14Z\">5 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/2595176/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Suppose I'm working on some classification problem. (Fraud detection and comment spam are two problems I'm working on right now, but I'm curious about any classification task in general.)</p>\n<p>How do I know which classifier I should use? </p>\n<ol>\n<li>Decision tree</li>\n<li>SVM</li>\n<li>Bayesian</li>\n<li>Neural network</li>\n<li>K-nearest neighbors</li>\n<li>Q-learning</li>\n<li>Genetic algorithm</li>\n<li>Markov decision processes</li>\n<li>Convolutional neural networks</li>\n<li>Linear regression or logistic regression</li>\n<li>Boosting, bagging, ensambling</li>\n<li>Random hill climbing or simulated annealing</li>\n<li>...</li>\n</ol>\n<p>In which cases is one of these the \"natural\" first choice, and what are the principles for choosing that one?</p>\n<p>Examples of the type of answers I'm looking for (from Manning et al.'s <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/choosing-what-kind-of-classifier-to-use-1.html\" rel=\"noreferrer\"><em>Introduction to Information Retrieval</em></a> book):</p>\n<p>a. <em>If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes)</em>.</p>\n<p>I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data.</p>\n<p>b. <em>If you have a ton of data, then the classifier doesn't really matter so much, so you should probably just choose a classifier with good scalability.</em></p>\n<ol start=\"2\">\n<li><p>What are other guidelines? Even answers like \"if you'll have to explain your model to some upper management person, then maybe you should use a decision tree, since the decision rules are fairly transparent\" are good. I care less about implementation/library issues, though.</p></li>\n<li><p>Also, for a somewhat separate question, besides standard Bayesian classifiers, are there 'standard state-of-the-art' methods for comment spam detection (as opposed to email spam)?</p></li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/BZJiN.png\"/></p>\n<p>First of all, you need to identify your problem. It depends upon what kind of data you have and what your desired task is.</p>\n<blockquote>\n<p>If you are <code>Predicting Category</code> :</p>\n<ul>\n<li>You have <code>Labeled Data</code>\n<ul>\n<li>You need to follow <code>Classification Approach</code> and its algorithms</li>\n</ul></li>\n<li>You don't have <code>Labeled Data</code>\n<ul>\n<li>You need to go for <code>Clustering Approach</code></li>\n</ul></li>\n</ul>\n<p>If you are <code>Predicting Quantity</code> :</p>\n<ul>\n<li>You need to go for <code>Regression Approach</code></li>\n</ul>\n<p>Otherwise</p>\n<ul>\n<li>You can go for <code>Dimensionality Reduction Approach</code></li>\n</ul>\n</blockquote>\n<p>There are different algorithms within each approach mentioned above. The choice of a particular algorithm depends upon the size of the dataset.</p>\n<p><em>Source: <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\" rel=\"noreferrer\">http://scikit-learn.org/stable/tutorial/machine_learning_map/</a></em></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://en.wikipedia.org/wiki/Model_selection\" rel=\"noreferrer\">Model selection</a> using <a href=\"http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29\" rel=\"noreferrer\">cross validation</a> may be what you need.</p>\n<h3>Cross validation</h3>\n<p>What you do is simply to split your dataset into k non-overlapping subsets (folds), train a model using k-1 folds and predict its performance using the fold you left out. This you do for each possible combination of folds (first leave 1st fold out, then 2nd, ... , then kth, and train with the remaining folds). After finishing, you estimate the mean performance of all folds (maybe also the variance/standard deviation of the performance).</p>\n<p>How to choose the parameter k depends on the time you have. Usual values for k are 3, 5, 10 or even N, where N is the size of your data (that's the same as <em>leave-one-out cross validation</em>). I prefer 5 or 10.</p>\n<h3>Model selection</h3>\n<p>Let's say you have 5 methods (ANN, SVM, KNN, etc) and 10 parameter combinations for each method (depending on the method). You simply have to run cross validation for each method and parameter combination (5 * 10 = 50) and select the best model, method and parameters. Then you re-train with the best method and parameters on all your data and you have your final model.</p>\n<p>There are some more things to say. If, for example, you use a <em>lot of methods and parameter combinations</em> for each, it's very likely you will overfit. In cases like these, you have to use <em>nested cross validation</em>.</p>\n<h3>Nested cross validation</h3>\n<p>In <em>nested cross validation</em>, you perform cross validation on the model selection algorithm.</p>\n<p>Again, you first split your data into k folds. After each step, you choose k-1 as your training data and the remaining one as your test data. Then you run model selection (the procedure I explained above) for each possible combination of those k folds. After finishing this, you will have k models, one for each combination of folds. After that, you test each model with the remaining test data and choose the best one. Again, after having the last model you train a new one with the same method and parameters on all the data you have. That's your final model.</p>\n<p>Of course, there are many variations of these methods and other things I didn't mention. If you need more information about these look for some publications about these topics.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The book \"<strong>OpenCV</strong>\" has a great two pages on this on <strong>pages 462-463</strong>. Searching the Amazon preview for the word \"discriminative\" (probably google books also) will let you see the pages in question. These two pages are the greatest gem I have found in this book.</p>\n<p>In short:</p>\n<ul>\n<li><p><strong>Boosting</strong> - <strong>often effective</strong> when a <strong>large amount of training data</strong> is available.</p></li>\n<li><p><strong>Random trees</strong> - often <strong>very effective</strong> and can also perform <strong>regression</strong>.</p></li>\n<li><p><strong>K-nearest neighbors</strong> - <strong>simplest</strong> thing you can do, <strong>often effective</strong> but <strong>slow</strong> and requires <strong>lots of memory</strong>.</p></li>\n<li><p><strong>Neural networks</strong> - <strong>Slow to train</strong> but very <strong>fast to run</strong>, still optimal performer for <strong>letter recognition</strong>.</p></li>\n<li><p><strong>SVM</strong> - <strong>Among the best</strong> with <strong>limited data</strong>, but <strong>losing against boosting</strong> or <strong>random trees</strong> only when <strong>large data sets</strong> are available.</p></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to train a CNN to categorize text by topic. When I use binary cross-entropy I get ~80% accuracy, with categorical cross-entropy I get ~50% accuracy.</p>\n<p>I don't understand why this is. It's a multiclass problem, doesn't that mean that I have to use categorical cross-entropy and that the results with binary cross-entropy are meaningless?</p>\n<pre class=\"lang-python prettyprint-override\"><code>model.add(embedding_layer)\nmodel.add(Dropout(0.25))\n# convolution layers\nmodel.add(Conv1D(nb_filter=32,\n                    filter_length=4,\n                    border_mode='valid',\n                    activation='relu'))\nmodel.add(MaxPooling1D(pool_length=2))\n# dense layers\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Dropout(0.25))\nmodel.add(Activation('relu'))\n# output layer\nmodel.add(Dense(len(class_id_index)))\nmodel.add(Activation('softmax'))\n</code></pre>\n<p>Then I compile it either it like this using <code>categorical_crossentropy</code> as the loss function:</p>\n<pre class=\"lang-python prettyprint-override\"><code>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n</code></pre>\n<p>or </p>\n<pre class=\"lang-python prettyprint-override\"><code>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n</code></pre>\n<p>Intuitively it makes sense why I'd want to use categorical cross-entropy, I don't understand why I get good results with binary, and poor results with categorical.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The reason for this apparent performance discrepancy between categorical &amp; binary cross entropy is what user xtof54 has already reported in <a href=\"https://stackoverflow.com/a/44498955/4685471\">his answer below</a>, i.e.:</p>\n<blockquote>\n<p>the accuracy computed with the Keras method <code>evaluate</code> is just plain\nwrong when using binary_crossentropy with more than 2 labels</p>\n</blockquote>\n<p>I would like to elaborate more on this, demonstrate the actual underlying issue, explain it, and offer a remedy.</p>\n<p>This behavior is not a bug; the underlying reason is a rather subtle &amp; undocumented issue at how Keras actually <em>guesses</em> which accuracy to use, depending on the loss function you have selected, when you include simply <code>metrics=['accuracy']</code> in your model compilation. In other words, while your first compilation option</p>\n<pre class=\"lang-python prettyprint-override\"><code>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n</code></pre>\n<p>is valid, your second one:</p>\n<pre class=\"lang-python prettyprint-override\"><code>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n</code></pre>\n<p>will not produce what you expect, but the reason is not the use of binary cross entropy (which, at least in principle, is an absolutely valid loss function).</p>\n<p>Why is that? If you check the <a href=\"https://github.com/fchollet/keras/blob/master/keras/metrics.py\" rel=\"noreferrer\">metrics source code</a>, Keras does not define a single accuracy metric, but several different ones, among them <code>binary_accuracy</code> and <code>categorical_accuracy</code>. What happens <a href=\"https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L876\" rel=\"noreferrer\">under the hood</a> is that, since you have selected binary cross entropy as your loss function and have not specified a particular accuracy metric, Keras (wrongly...) infers that you are interested in the <code>binary_accuracy</code>, and this is what it returns - while in fact you are interested in the <code>categorical_accuracy</code>.</p>\n<p>Let's verify that this is the case, using the <a href=\"https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\" rel=\"noreferrer\">MNIST CNN example</a> in Keras, with the following modification:</p>\n<pre class=\"lang-python prettyprint-override\"><code>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # WRONG way\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=2,  # only 2 epochs, for demonstration purposes\n          verbose=1,\n          validation_data=(x_test, y_test))\n\n# Keras reported accuracy:\nscore = model.evaluate(x_test, y_test, verbose=0) \nscore[1]\n# 0.9975801164627075\n\n# Actual accuracy calculated manually:\nimport numpy as np\ny_pred = model.predict(x_test)\nacc = sum([np.argmax(y_test[i])==np.argmax(y_pred[i]) for i in range(10000)])/10000\nacc\n# 0.98780000000000001\n\nscore[1]==acc\n# False    \n</code></pre>\n<p>To remedy this, i.e. to use indeed binary cross entropy as your loss function (as I said, nothing wrong with this, at least in principle) while still getting the <strong>categorical</strong> accuracy required by the problem at hand, you should ask explicitly for <code>categorical_accuracy</code> in the model compilation as follows:</p>\n<pre class=\"lang-python prettyprint-override\"><code>from keras.metrics import categorical_accuracy\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n</code></pre>\n<p>In the MNIST example, after training, scoring, and predicting the test set as I show above, the two metrics now are the same, as they should be:</p>\n<pre class=\"lang-python prettyprint-override\"><code># Keras reported accuracy:\nscore = model.evaluate(x_test, y_test, verbose=0) \nscore[1]\n# 0.98580000000000001\n\n# Actual accuracy calculated manually:\ny_pred = model.predict(x_test)\nacc = sum([np.argmax(y_test[i])==np.argmax(y_pred[i]) for i in range(10000)])/10000\nacc\n# 0.98580000000000001\n\nscore[1]==acc\n# True    \n</code></pre>\n<p>System setup:</p>\n<pre class=\"lang-python prettyprint-override\"><code>Python version 3.5.3\nTensorflow version 1.2.1\nKeras version 2.0.4\n</code></pre>\n<p><strong>UPDATE</strong>: After my post, I discovered that this issue had already been identified in <a href=\"https://stackoverflow.com/questions/45799474/keras-model-evaluate-vs-model-predict-accuracy-difference-in-multi-class-nlp-ta/45834857#45834857\">this answer</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It all depends on the type of classification problem you are dealing with. There are three main categories</p>\n<ul>\n<li><strong>binary</strong> classification (two target classes),</li>\n<li><strong>multi-class</strong> classification (more than two <strong>exclusive</strong> targets),</li>\n<li><strong>multi-label</strong> classification (more than two <strong>non exclusive</strong> targets), in which multiple target classes can be on at the same time.</li>\n</ul>\n<p>In the first case, binary cross-entropy should be used and targets should be encoded as one-hot vectors.</p>\n<p>In the second case, categorical cross-entropy should be used and targets should be encoded as one-hot vectors.</p>\n<p>In the last case, binary cross-entropy should be used and targets should be encoded as one-hot vectors. Each output neuron (or unit) is considered as a separate random binary variable, and the loss for the entire vector of outputs is the product of the loss of single binary variables. Therefore it is the product of binary cross-entropy for each single output unit.</p>\n<p>The binary cross-entropy is defined as</p>\n<p><a href=\"https://i.sstatic.net/Qb9x9.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Qb9x9.png\"/></a></p>\n<p>and categorical cross-entropy is defined as</p>\n<p><a href=\"https://i.sstatic.net/g0b6Y.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/g0b6Y.png\"/></a></p>\n<p>where <code>c</code> is the index running over the number of classes <code>C</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I came across an \"inverted\" issue — I was getting good results with categorical_crossentropy (with 2 classes) and poor with binary_crossentropy. It seems that problem was with wrong activation function. The correct settings were:</p>\n<ul>\n<li>for <code>binary_crossentropy</code>: sigmoid activation, scalar target </li>\n<li>for <code>categorical_crossentropy</code>: softmax activation, one-hot encoded target</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm new to Tensorflow and would greatly benefit from some visualizations of what I'm doing. I understand that Tensorboard is a useful visualization tool, but how do I run it on my remote Ubuntu machine?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is what I do to avoid the issues of making the remote server accept your local external IP:</p>\n<ul>\n<li>when I ssh into the machine, I use the option <code>-L</code> to transfer the port <code>6006</code> of the remote server into the port <code>16006</code> of my machine (for instance):\n<code>\nssh -L 16006:127.0.0.1:6006 olivier@my_server_ip\n</code></li>\n</ul>\n<p>What it does is that everything on the port <code>6006</code> of the server (in <code>127.0.0.1:6006</code>) will be <strong>forwarded</strong> to my machine on the port <code>16006</code>.</p>\n<hr/>\n<ul>\n<li>You can then launch tensorboard on the remote machine using a standard <code>tensorboard --logdir log</code> with the default <code>6006</code>port</li>\n<li>On your local machine, go to <a href=\"http://127.0.0.1:16006\">http://127.0.0.1:16006</a> and enjoy your remote TensorBoard.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can port-forward with another <code>ssh</code> command that need not be tied to how you are connecting to the server (as an alternative to the other answer). Thus, the ordering of the below steps is arbitrary.</p>\n<ol>\n<li><p>from your <b>local</b> machine, run</p>\n<p><code>ssh -N -f -L localhost:16006:localhost:6006 &lt;user@remote&gt;</code></p>\n</li>\n<li><p>on the <b>remote</b> machine, run:</p>\n<p><code>tensorboard --logdir &lt;path&gt; --port 6006</code></p>\n</li>\n<li><p>Then, navigate to (in this example) <a href=\"http://localhost:16006\" rel=\"noreferrer\">http://localhost:16006</a> on your local machine.</p>\n</li>\n</ol>\n<p>(explanation of ssh command:</p>\n<p><code>-N</code> : no remote commands</p>\n<p><code>-f</code> : put ssh in the background</p>\n<p><code>-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt;</code> :</p>\n<p>forward <code>&lt;machine1&gt;:&lt;portA&gt;</code> (local scope) to <code>&lt;machine2&gt;:&lt;portB&gt;</code> (remote scope)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You don't need to do anything fancy. Just run:</p>\n<pre><code>tensorboard --host 0.0.0.0 &lt;other args here&gt;\n</code></pre>\n<p>and connect with your server url and port. The <code>--host 0.0.0.0</code> tells tensorflow to listen from connections on all IPv4 addresses on the local machine.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can I extract the underlying decision-rules (or 'decision paths') from a trained tree in a decision tree as a textual list?</p>\n<p>Something like:</p>\n<pre><code>if A&gt;0.4 then if B&lt;0.2 then if C&gt;0.8 then class='X'\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I believe that this answer is more correct than the other answers here:</p>\n<pre><code>from sklearn.tree import _tree\n\ndef tree_to_code(tree, feature_names):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    print \"def tree({}):\".format(\", \".join(feature_names))\n\n    def recurse(node, depth):\n        indent = \"  \" * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print \"{}if {} &lt;= {}:\".format(indent, name, threshold)\n            recurse(tree_.children_left[node], depth + 1)\n            print \"{}else:  # if {} &gt; {}\".format(indent, name, threshold)\n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            print \"{}return {}\".format(indent, tree_.value[node])\n\n    recurse(0, 1)\n</code></pre>\n<p>This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10.</p>\n<pre><code>def tree(f0):\n  if f0 &lt;= 6.0:\n    if f0 &lt;= 1.5:\n      return [[ 0.]]\n    else:  # if f0 &gt; 1.5\n      if f0 &lt;= 4.5:\n        if f0 &lt;= 3.5:\n          return [[ 3.]]\n        else:  # if f0 &gt; 3.5\n          return [[ 4.]]\n      else:  # if f0 &gt; 4.5\n        return [[ 5.]]\n  else:  # if f0 &gt; 6.0\n    if f0 &lt;= 8.5:\n      if f0 &lt;= 7.5:\n        return [[ 7.]]\n      else:  # if f0 &gt; 7.5\n        return [[ 8.]]\n    else:  # if f0 &gt; 8.5\n      return [[ 9.]]\n</code></pre>\n<p>Here are some stumbling blocks that I see in other answers:</p>\n<ol>\n<li>Using <code>tree_.threshold == -2</code> to decide whether a node is a leaf isn't a good idea. What if it's a real decision node with a threshold of -2? Instead, you should look at <code>tree.feature</code> or <code>tree.children_*</code>.</li>\n<li>The line <code>features = [feature_names[i] for i in tree_.feature]</code> crashes with my version of sklearn, because some values of <code>tree.tree_.feature</code> are -2 (specifically for leaf nodes).</li>\n<li>There is no need to have multiple if statements in the recursive function, just one is fine.</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I created my own function to extract the rules from the decision trees created by sklearn:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n# dummy data:\ndf = pd.DataFrame({'col1':[0,1,2,3],'col2':[3,4,5,6],'dv':[0,1,0,1]})\n\n# create decision tree\ndt = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1)\ndt.fit(df.ix[:,:2], df.dv)\n</code></pre>\n<p>This function first starts with the nodes (identified by -1 in the child arrays) and then recursively finds the parents. I call this a node's 'lineage'.  Along the way, I grab the values I need to create if/then/else SAS logic:</p>\n<pre><code>def get_lineage(tree, feature_names):\n     left      = tree.tree_.children_left\n     right     = tree.tree_.children_right\n     threshold = tree.tree_.threshold\n     features  = [feature_names[i] for i in tree.tree_.feature]\n\n     # get ids of child nodes\n     idx = np.argwhere(left == -1)[:,0]     \n\n     def recurse(left, right, child, lineage=None):          \n          if lineage is None:\n               lineage = [child]\n          if child in left:\n               parent = np.where(left == child)[0].item()\n               split = 'l'\n          else:\n               parent = np.where(right == child)[0].item()\n               split = 'r'\n\n          lineage.append((parent, split, threshold[parent], features[parent]))\n\n          if parent == 0:\n               lineage.reverse()\n               return lineage\n          else:\n               return recurse(left, right, parent, lineage)\n\n     for child in idx:\n          for node in recurse(left, right, child):\n               print node\n</code></pre>\n<p>The sets of tuples below contain everything I need to create SAS if/then/else statements. I do not like using <code>do</code> blocks in SAS which is why I create logic describing a node's entire path. The single integer after the tuples is the ID of the terminal node in a path. All of the preceding tuples combine to create that node.</p>\n<pre><code>In [1]: get_lineage(dt, df.columns)\n(0, 'l', 0.5, 'col1')\n1\n(0, 'r', 0.5, 'col1')\n(2, 'l', 4.5, 'col2')\n3\n(0, 'r', 0.5, 'col1')\n(2, 'r', 4.5, 'col2')\n(4, 'l', 2.5, 'col1')\n5\n(0, 'r', 0.5, 'col1')\n(2, 'r', 4.5, 'col2')\n(4, 'r', 2.5, 'col1')\n6\n</code></pre>\n<p><img alt=\"GraphViz output of example tree\" src=\"https://i.sstatic.net/SWwtO.png\"/></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Scikit learn introduced a delicious new method called <code>export_text</code> in version 0.21 (May 2019) to extract the rules from a tree. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html\" rel=\"noreferrer\">Documentation here</a>. It's no longer necessary to create a custom function.</p>\n<p>Once you've fit your model, you just need two lines of code. First, import <code>export_text</code>:</p>\n<pre><code>from sklearn.tree import export_text\n</code></pre>\n<p>Second, create an object that will contain your rules. To make the rules look more readable, use the <code>feature_names</code> argument and pass a list of your feature names. For example, if your model is called <code>model</code> and your features are named in a dataframe called <code>X_train</code>, you could create an object called <code>tree_rules</code>:</p>\n<pre><code>tree_rules = export_text(model, feature_names=list(X_train.columns))\n</code></pre>\n<p>Then just print or save <code>tree_rules</code>. Your output will look like this:</p>\n<pre><code>|--- Age &lt;= 0.63\n|   |--- EstimatedSalary &lt;= 0.61\n|   |   |--- Age &lt;= -0.16\n|   |   |   |--- class: 0\n|   |   |--- Age &gt;  -0.16\n|   |   |   |--- EstimatedSalary &lt;= -0.06\n|   |   |   |   |--- class: 0\n|   |   |   |--- EstimatedSalary &gt;  -0.06\n|   |   |   |   |--- EstimatedSalary &lt;= 0.40\n|   |   |   |   |   |--- EstimatedSalary &lt;= 0.03\n|   |   |   |   |   |   |--- class: 1\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In most of the models, there is a <em>steps</em> parameter indicating the <em>number of steps to run over data</em>. But yet I see in most practical usage, we also execute the fit function N <em>epochs</em>. </p>\n<p>What is the difference between running 1000 steps with 1 epoch and running 100 steps with 10 epoch? Which one is better in practice? Any logic changes between consecutive epochs? Data shuffling?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A training step is one gradient update. In one step <code>batch_size</code> examples are processed.</p>\n<p>An epoch consists of one full cycle through the training data. This is usually many steps. As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of:</p>\n<pre><code>2,000 images / (10 images / step) = 200 steps.\n</code></pre>\n<p>If you choose your training image randomly (and independently) in each step, you normally do not call it epoch. [This is where my answer differs from the previous one. Also see my comment.]</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>An epoch usually means one iteration over all of the training data.  For instance if you have 20,000 images and a batch size of 100 then the epoch should contain 20,000 / 100 = 200 steps.  However I usually just set a fixed number of steps like 1000 per epoch even though I have a much larger data set.  At the end of the epoch I check the average cost and if it improved I save a checkpoint.  There is no difference between steps from one epoch to another.  I just treat them as checkpoints.</p>\n<p>People often shuffle around the data set between epochs.  I prefer to use the random.sample function to choose the data to process in my epochs. So say I want to do 1000 steps with a batch size of 32.  I will just randomly pick 32,000 samples from the pool of training data.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In easy words \n<br/><strong>Epoch:</strong> Epoch is considered as number of one pass from entire dataset\n<br/><strong>Steps:</strong> In tensorflow one steps is considered as number of epochs multiplied by examples divided by batch size\n<br/></p>\n<pre><code>steps = (epoch * examples)/batch size\nFor instance\nepoch = 100, examples = 1000 and batch_size = 1000\nsteps = 100\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-11 12:00:40Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/42883547/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Can anyone please clearly explain the difference between 1D, 2D, and 3D convolutions in convolutional neural networks (in deep learning) with the use of examples?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to explain with picture from <a href=\"https://arxiv.org/abs/1412.0767\" rel=\"noreferrer\">C3D</a>.</p>\n<p>In a nutshell, <strong>convolutional direction</strong> &amp; <strong>output shape</strong> is important!</p>\n<p><a href=\"https://i.sstatic.net/owWjX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/owWjX.png\"/></a></p>\n<p>↑↑↑↑↑ <em><strong>1D Convolutions - Basic</strong></em> ↑↑↑↑↑</p>\n<ul>\n<li>just <strong>1</strong>-direction (time-axis) to calculate conv</li>\n<li>input = [W], filter = [k], output = [W]</li>\n<li>ex) input = [1,1,1,1,1], filter = [0.25,0.5,0.25], output = [1,1,1,1,1]</li>\n<li>output-shape is 1D array</li>\n<li>example) graph smoothing</li>\n</ul>\n<h3>tf.nn.conv1d code Toy Example</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nsess = tf.Session()\n\nones_1d = np.ones(5)\nweight_1d = np.ones(3)\nstrides_1d = 1\n\nin_1d = tf.constant(ones_1d, dtype=tf.float32)\nfilter_1d = tf.constant(weight_1d, dtype=tf.float32)\n\nin_width = int(in_1d.shape[0])\nfilter_width = int(filter_1d.shape[0])\n\ninput_1d   = tf.reshape(in_1d, [1, in_width, 1])\nkernel_1d = tf.reshape(filter_1d, [filter_width, 1, 1])\noutput_1d = tf.squeeze(tf.nn.conv1d(input_1d, kernel_1d, strides_1d, padding='SAME'))\nprint sess.run(output_1d)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/hvMaU.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/hvMaU.png\"/></a></p>\n<p>↑↑↑↑↑ <em><strong>2D Convolutions - Basic</strong></em> ↑↑↑↑↑</p>\n<ul>\n<li><strong>2</strong>-direction (x,y) to calculate conv</li>\n<li>output-shape is <strong>2D</strong> Matrix</li>\n<li>input = [W, H], filter = [k,k] output = [W,H]</li>\n<li>example) <a href=\"https://en.wikipedia.org/wiki/Sobel_operator\" rel=\"noreferrer\">Sobel Egde Fllter</a></li>\n</ul>\n<h3>tf.nn.conv2d - Toy Example</h3>\n<pre><code>ones_2d = np.ones((5,5))\nweight_2d = np.ones((3,3))\nstrides_2d = [1, 1, 1, 1]\n\nin_2d = tf.constant(ones_2d, dtype=tf.float32)\nfilter_2d = tf.constant(weight_2d, dtype=tf.float32)\n\nin_width = int(in_2d.shape[0])\nin_height = int(in_2d.shape[1])\n\nfilter_width = int(filter_2d.shape[0])\nfilter_height = int(filter_2d.shape[1])\n\ninput_2d   = tf.reshape(in_2d, [1, in_height, in_width, 1])\nkernel_2d = tf.reshape(filter_2d, [filter_height, filter_width, 1, 1])\n\noutput_2d = tf.squeeze(tf.nn.conv2d(input_2d, kernel_2d, strides=strides_2d, padding='SAME'))\nprint sess.run(output_2d)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/IvDQP.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/IvDQP.png\"/></a></p>\n<p>↑↑↑↑↑ <em><strong>3D Convolutions - Basic</strong></em> ↑↑↑↑↑</p>\n<ul>\n<li><strong>3</strong>-direction (x,y,z) to calcuate conv</li>\n<li>output-shape is <strong>3D</strong> Volume</li>\n<li>input = [W,H,<strong>L</strong>], filter = [k,k,<strong>d</strong>] output = [W,H,M]</li>\n<li><strong>d &lt; L</strong> is important! for making volume output</li>\n<li>example) C3D</li>\n</ul>\n<h3>tf.nn.conv3d - Toy Example</h3>\n<pre><code>ones_3d = np.ones((5,5,5))\nweight_3d = np.ones((3,3,3))\nstrides_3d = [1, 1, 1, 1, 1]\n\nin_3d = tf.constant(ones_3d, dtype=tf.float32)\nfilter_3d = tf.constant(weight_3d, dtype=tf.float32)\n\nin_width = int(in_3d.shape[0])\nin_height = int(in_3d.shape[1])\nin_depth = int(in_3d.shape[2])\n\nfilter_width = int(filter_3d.shape[0])\nfilter_height = int(filter_3d.shape[1])\nfilter_depth = int(filter_3d.shape[2])\n\ninput_3d   = tf.reshape(in_3d, [1, in_depth, in_height, in_width, 1])\nkernel_3d = tf.reshape(filter_3d, [filter_depth, filter_height, filter_width, 1, 1])\n\noutput_3d = tf.squeeze(tf.nn.conv3d(input_3d, kernel_3d, strides=strides_3d, padding='SAME'))\nprint sess.run(output_3d)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/49cdt.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/49cdt.png\"/></a></p>\n<p>↑↑↑↑↑ <em><strong>2D Convolutions with 3D input</strong></em>  - LeNet, VGG, ..., ↑↑↑↑↑</p>\n<ul>\n<li>Eventhough input is 3D ex) 224x224x3, 112x112x32</li>\n<li>output-shape is not <strong>3D</strong> Volume, but <strong>2D</strong> Matrix</li>\n<li>because filter depth = <strong>L</strong> must be matched with input channels = <strong>L</strong></li>\n<li><strong>2</strong>-direction (x,y) to calcuate conv! not 3D</li>\n<li>input = [W,H,<strong>L</strong>], filter = [k,k,<strong>L</strong>] output = [W,H]</li>\n<li>output-shape is <strong>2D</strong> Matrix</li>\n<li>what if we want to train N filters (N is number of filters)</li>\n<li>then output shape is (stacked 2D) <strong>3D = 2D x N</strong> matrix.</li>\n</ul>\n<h3>conv2d - LeNet, VGG, ... for 1 filter</h3>\n<pre><code>in_channels = 32 # 3 for RGB, 32, 64, 128, ... \nones_3d = np.ones((5,5,in_channels)) # input is 3d, in_channels = 32\n# filter must have 3d-shpae with in_channels\nweight_3d = np.ones((3,3,in_channels)) \nstrides_2d = [1, 1, 1, 1]\n\nin_3d = tf.constant(ones_3d, dtype=tf.float32)\nfilter_3d = tf.constant(weight_3d, dtype=tf.float32)\n\nin_width = int(in_3d.shape[0])\nin_height = int(in_3d.shape[1])\n\nfilter_width = int(filter_3d.shape[0])\nfilter_height = int(filter_3d.shape[1])\n\ninput_3d   = tf.reshape(in_3d, [1, in_height, in_width, in_channels])\nkernel_3d = tf.reshape(filter_3d, [filter_height, filter_width, in_channels, 1])\n\noutput_2d = tf.squeeze(tf.nn.conv2d(input_3d, kernel_3d, strides=strides_2d, padding='SAME'))\nprint sess.run(output_2d)\n</code></pre>\n<h3>conv2d - LeNet, VGG, ... for N filters</h3>\n<pre><code>in_channels = 32 # 3 for RGB, 32, 64, 128, ... \nout_channels = 64 # 128, 256, ...\nones_3d = np.ones((5,5,in_channels)) # input is 3d, in_channels = 32\n# filter must have 3d-shpae x number of filters = 4D\nweight_4d = np.ones((3,3,in_channels, out_channels))\nstrides_2d = [1, 1, 1, 1]\n\nin_3d = tf.constant(ones_3d, dtype=tf.float32)\nfilter_4d = tf.constant(weight_4d, dtype=tf.float32)\n\nin_width = int(in_3d.shape[0])\nin_height = int(in_3d.shape[1])\n\nfilter_width = int(filter_4d.shape[0])\nfilter_height = int(filter_4d.shape[1])\n\ninput_3d   = tf.reshape(in_3d, [1, in_height, in_width, in_channels])\nkernel_4d = tf.reshape(filter_4d, [filter_height, filter_width, in_channels, out_channels])\n\n#output stacked shape is 3D = 2D x N matrix\noutput_3d = tf.nn.conv2d(input_3d, kernel_4d, strides=strides_2d, padding='SAME')\nprint sess.run(output_3d)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/RghcS.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/RghcS.png\"/></a>\n↑↑↑↑↑ <em><strong>Bonus 1x1 conv in CNN</strong></em>  - GoogLeNet, ..., ↑↑↑↑↑</p>\n<ul>\n<li>1x1 conv is confusing when you think this as 2D image filter like sobel</li>\n<li>for 1x1 conv in CNN, input is 3D shape as above picture.</li>\n<li>it calculate depth-wise filtering</li>\n<li>input = [W,H,L], filter = <strong>[1,1,L]</strong> output = [W,H]</li>\n<li>output stacked shape is <strong>3D = 2D x N</strong> matrix.</li>\n</ul>\n<h3>tf.nn.conv2d - special case 1x1 conv</h3>\n<pre><code>in_channels = 32 # 3 for RGB, 32, 64, 128, ... \nout_channels = 64 # 128, 256, ...\nones_3d = np.ones((1,1,in_channels)) # input is 3d, in_channels = 32\n# filter must have 3d-shpae x number of filters = 4D\nweight_4d = np.ones((3,3,in_channels, out_channels))\nstrides_2d = [1, 1, 1, 1]\n\nin_3d = tf.constant(ones_3d, dtype=tf.float32)\nfilter_4d = tf.constant(weight_4d, dtype=tf.float32)\n\nin_width = int(in_3d.shape[0])\nin_height = int(in_3d.shape[1])\n\nfilter_width = int(filter_4d.shape[0])\nfilter_height = int(filter_4d.shape[1])\n\ninput_3d   = tf.reshape(in_3d, [1, in_height, in_width, in_channels])\nkernel_4d = tf.reshape(filter_4d, [filter_height, filter_width, in_channels, out_channels])\n\n#output stacked shape is 3D = 2D x N matrix\noutput_3d = tf.nn.conv2d(input_3d, kernel_4d, strides=strides_2d, padding='SAME')\nprint sess.run(output_3d)\n</code></pre>\n<h3>Animation (2D Conv with 3D-inputs)</h3>\n<p><a href=\"https://i.sstatic.net/FjvuN.gif\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/FjvuN.gif\"/></a></p>\n<ul>\n<li>Original Link : <a href=\"https://sites.google.com/site/nttrungmtwiki/home/it/data-science---python/tensorflow/tensorflow-and-deep-learning-part-3?tmpl=%2Fsystem%2Fapp%2Ftemplates%2Fprint%2F&amp;showPrintDialog=1\" rel=\"noreferrer\">LINK</a></li>\n<li>The author: Martin Görner</li>\n<li>Twitter: @martin_gorner</li>\n<li>Google +: plus.google.com/+MartinGorne</li>\n</ul>\n<h3>Bonus 1D Convolutions with 2D input</h3>\n<p><a href=\"https://i.sstatic.net/woaXM.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/woaXM.jpg\"/></a>\n↑↑↑↑↑ <em><strong>1D Convolutions with 1D input</strong></em>   ↑↑↑↑↑</p>\n<p><a href=\"https://i.sstatic.net/9VBtu.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/9VBtu.jpg\"/></a>\n↑↑↑↑↑ <em><strong>1D Convolutions with 2D input</strong></em>   ↑↑↑↑↑</p>\n<ul>\n<li>Eventhough input is 2D ex) 20x14</li>\n<li>output-shape is not <strong>2D</strong> , but <strong>1D</strong> Matrix</li>\n<li>because filter height = <strong>L</strong> must be matched with input height = <strong>L</strong></li>\n<li><strong>1</strong>-direction (x) to calcuate conv! not 2D</li>\n<li>input = [W,<strong>L</strong>], filter = [k,<strong>L</strong>] output = [W]</li>\n<li>output-shape is <strong>1D</strong> Matrix</li>\n<li>what if we want to train N filters (N is number of filters)</li>\n<li>then output shape is (stacked 1D) <strong>2D = 1D x N</strong> matrix.</li>\n</ul>\n<h3>Bonus C3D</h3>\n<pre><code>in_channels = 32 # 3, 32, 64, 128, ... \nout_channels = 64 # 3, 32, 64, 128, ... \nones_4d = np.ones((5,5,5,in_channels))\nweight_5d = np.ones((3,3,3,in_channels,out_channels))\nstrides_3d = [1, 1, 1, 1, 1]\n\nin_4d = tf.constant(ones_4d, dtype=tf.float32)\nfilter_5d = tf.constant(weight_5d, dtype=tf.float32)\n\nin_width = int(in_4d.shape[0])\nin_height = int(in_4d.shape[1])\nin_depth = int(in_4d.shape[2])\n\nfilter_width = int(filter_5d.shape[0])\nfilter_height = int(filter_5d.shape[1])\nfilter_depth = int(filter_5d.shape[2])\n\ninput_4d   = tf.reshape(in_4d, [1, in_depth, in_height, in_width, in_channels])\nkernel_5d = tf.reshape(filter_5d, [filter_depth, filter_height, filter_width, in_channels, out_channels])\n\noutput_4d = tf.nn.conv3d(input_4d, kernel_5d, strides=strides_3d, padding='SAME')\nprint sess.run(output_4d)\n\nsess.close()\n</code></pre>\n<h3>Input &amp; Output in Tensorflow</h3>\n<p><a href=\"https://i.sstatic.net/I25ty.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/I25ty.png\"/></a></p>\n<p><a href=\"https://i.sstatic.net/xIdEq.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/xIdEq.png\"/></a></p>\n<h3>Summary</h3>\n<p><a href=\"https://i.sstatic.net/HCWgp.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/HCWgp.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Following the answer from @runhani I am adding a few more details to make the explanation a bit more clear and will try to explain this a bit more (and of course with exmaples from TF1 and TF2).</p>\n<p>One of the main additional bits I'm including are, </p>\n<ul>\n<li>Emphasis on applications</li>\n<li>Usage of <code>tf.Variable</code></li>\n<li>Clearer explanation of inputs/kernels/outputs 1D/2D/3D convolution </li>\n<li>The effects of stride/padding</li>\n</ul>\n<h2>1D Convolution</h2>\n<p>Here's how you might do 1D convolution using TF 1 and TF 2.</p>\n<p>And to be specific my data has following shapes,</p>\n<ul>\n<li>1D vector - <code>[batch size, width, in channels]</code> (e.g. <code>1, 5, 1</code>)</li>\n<li>Kernel - <code>[width, in channels, out channels]</code> (e.g. <code>5, 1, 4</code>)</li>\n<li>Output - <code>[batch size, width, out_channels]</code> (e.g. <code>1, 5, 4</code>)</li>\n</ul>\n<h3>TF1 example</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ninp = tf.placeholder(shape=[None, 5, 1], dtype=tf.float32)\nkernel = tf.Variable(tf.initializers.glorot_uniform()([5, 1, 4]), dtype=tf.float32)\nout = tf.nn.conv1d(inp, kernel, stride=1, padding='SAME')\n\nwith tf.Session() as sess:\n  tf.global_variables_initializer().run()\n  print(sess.run(out, feed_dict={inp: np.array([[[0],[1],[2],[3],[4]],[[5],[4],[3],[2],[1]]])}))\n</code></pre>\n<h3>TF2 Example</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ninp = np.array([[[0],[1],[2],[3],[4]],[[5],[4],[3],[2],[1]]]).astype(np.float32)\nkernel = tf.Variable(tf.initializers.glorot_uniform()([5, 1, 4]), dtype=tf.float32)\nout = tf.nn.conv1d(inp, kernel, stride=1, padding='SAME')\nprint(out)\n\n</code></pre>\n<p>It's way less work with TF2 as TF2 does not need <code>Session</code> and <code>variable_initializer</code> for example.</p>\n<h3>What might this look like in real-life?</h3>\n<p>So let's understand what this is doing using a signal smoothing example. On the left you got the original and on the right you got output of a Convolution 1D which has 3 output channels.</p>\n<p><a href=\"https://i.sstatic.net/w23RC.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/w23RC.png\"/></a></p>\n<h3>What do multiple channels mean?</h3>\n<p>Multiple channels are basically multiple feature representations of an input. In this example you have three representations obtained by three different filters. The first channel is the equally-weighted smoothing filter. The second is a filter that weights the middle of the filter more than the boundaries. The final filter does the opposite of the second. So you can see how these different filters bring about different effects.</p>\n<h3>Deep learning applications of 1D convolution</h3>\n<p>1D convolution has been successful used for the <a href=\"https://www.aclweb.org/anthology/D14-1181.pdf\" rel=\"noreferrer\">sentence classification</a> task. </p>\n<h2>2D Convolution</h2>\n<p>Off to 2D convolution. If you are a deep learning person, chances that you haven't come across 2D convolution is … well about zero. It is used in CNNs for image classification, object detection, etc. as well as in NLP problems that involve images (e.g. image caption generation).</p>\n<p>Let's try an example, I got a convolution kernel with the following filters here,</p>\n<ul>\n<li>Edge detection kernel (3x3 window)</li>\n<li>Blur kernel (3x3 window)</li>\n<li>Sharpen kernel (3x3 window)</li>\n</ul>\n<p>And to be specific my data has following shapes,</p>\n<ul>\n<li>Image (black and white) - <code>[batch_size, height, width, 1]</code> (e.g. <code>1, 340, 371, 1</code>)</li>\n<li>Kernel (aka filters) - <code>[height, width, in channels, out channels]</code> (e.g. <code>3, 3, 1, 3</code>)</li>\n<li>Output (aka feature maps) - <code>[batch_size, height, width, out_channels]</code> (e.g. <code>1, 340, 371, 3</code>)</li>\n</ul>\n<h3>TF1 Example,</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\nim = np.array(Image.open(&lt;some image&gt;).convert('L'))#/255.0\n\nkernel_init = np.array(\n    [\n     [[[-1, 1.0/9, 0]],[[-1, 1.0/9, -1]],[[-1, 1.0/9, 0]]],\n     [[[-1, 1.0/9, -1]],[[8, 1.0/9,5]],[[-1, 1.0/9,-1]]],\n     [[[-1, 1.0/9,0]],[[-1, 1.0/9,-1]],[[-1, 1.0/9, 0]]]\n     ])\n\ninp = tf.placeholder(shape=[None, image_height, image_width, 1], dtype=tf.float32)\nkernel = tf.Variable(kernel_init, dtype=tf.float32)\nout = tf.nn.conv2d(inp, kernel, strides=[1,1,1,1], padding='SAME')\n\nwith tf.Session() as sess:\n  tf.global_variables_initializer().run()\n  res = sess.run(out, feed_dict={inp: np.expand_dims(np.expand_dims(im,0),-1)})\n\n</code></pre>\n<h3>TF2 Example</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\nim = np.array(Image.open(&lt;some image&gt;).convert('L'))#/255.0\nx = np.expand_dims(np.expand_dims(im,0),-1)\n\nkernel_init = np.array(\n    [\n     [[[-1, 1.0/9, 0]],[[-1, 1.0/9, -1]],[[-1, 1.0/9, 0]]],\n     [[[-1, 1.0/9, -1]],[[8, 1.0/9,5]],[[-1, 1.0/9,-1]]],\n     [[[-1, 1.0/9,0]],[[-1, 1.0/9,-1]],[[-1, 1.0/9, 0]]]\n     ])\n\nkernel = tf.Variable(kernel_init, dtype=tf.float32)\n\nout = tf.nn.conv2d(x, kernel, strides=[1,1,1,1], padding='SAME')\n</code></pre>\n<h3>What might this look like in real life?</h3>\n<p>Here you can see the output produced by above code. The first image is the original and going clock-wise you have outputs of the 1st filter, 2nd filter and 3 filter.\n<a href=\"https://i.sstatic.net/NuldH.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/NuldH.png\"/></a></p>\n<h3>What do multiple channels mean?</h3>\n<p>In the context if 2D convolution, it is much easier to understand what these multiple channels mean. Say you are doing face recognition. You can think of (this is a very unrealistic simplification but gets the point across) each filter represents an eye, mouth, nose, etc. So that each feature map would be a binary representation of whether that feature is there in the image you provided. I don't think I need to stress that for a face recognition model those are very valuable features. More information in this <a href=\"https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks-e3f054dd5daa\" rel=\"noreferrer\">article</a>.</p>\n<p>This is an illustration of what I'm trying to articulate.</p>\n<p><a href=\"https://i.sstatic.net/9bi5k.gif\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/9bi5k.gif\"/></a></p>\n<h3>Deep learning applications of 2D convolution</h3>\n<p>2D convolution is very prevalent in the realm of deep learning. </p>\n<p>CNNs (Convolution Neural Networks) use 2D convolution operation for almost all computer vision tasks (e.g. Image classification, object detection, video classification). </p>\n<h2>3D Convolution</h2>\n<p>Now it becomes increasingly difficult to illustrate what's going as the number of dimensions increase. But with good understanding of how 1D and 2D convolution works, it's very straight-forward to generalize that understanding to 3D convolution. So here goes.</p>\n<p>And to be specific my data has following shapes,</p>\n<ul>\n<li>3D data (LIDAR) - <code>[batch size, height, width, depth, in channels]</code> (e.g. <code>1, 200, 200, 200, 1</code>)</li>\n<li>Kernel - <code>[height, width, depth, in channels, out channels]</code> (e.g. <code>5, 5, 5, 1, 3</code>)</li>\n<li>Output - <code>[batch size, width, height, width, depth, out_channels]</code> (e.g. <code>1, 200, 200, 2000, 3</code>)</li>\n</ul>\n<h3>TF1 Example</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n\ninp = tf.placeholder(shape=[None, 200, 200, 200, 1], dtype=tf.float32)\nkernel = tf.Variable(tf.initializers.glorot_uniform()([5,5,5,1,3]), dtype=tf.float32)\nout = tf.nn.conv3d(inp, kernel, strides=[1,1,1,1,1], padding='SAME')\n\nwith tf.Session() as sess:\n  tf.global_variables_initializer().run()\n  res = sess.run(out, feed_dict={inp: np.random.normal(size=(1,200,200,200,1))})\n\n</code></pre>\n<h3>TF2 Example</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nx = np.random.normal(size=(1,200,200,200,1))\nkernel = tf.Variable(tf.initializers.glorot_uniform()([5,5,5,1,3]), dtype=tf.float32)\nout = tf.nn.conv3d(x, kernel, strides=[1,1,1,1,1], padding='SAME') \n</code></pre>\n<h3>Deep learning applications of 3D convolution</h3>\n<p>3D convolution has been used when developing machine learning applications involving LIDAR (Light Detection and Ranging) data which is 3 dimensional in nature.</p>\n<h2>What... more jargon?: Stride and padding</h2>\n<p>Alright you're nearly there. So hold on. Let's see what is stride and padding is. They are quite intuitive if you think about them.</p>\n<p>If you stride across a corridor, you get there faster in fewer steps. But it also means that you observed lesser surrounding than if you walked across the room. Let's now reinforce our understanding with a pretty picture too! Let's understand these via 2D convolution.</p>\n<h3>Understanding stride</h3>\n<p><a href=\"https://i.sstatic.net/XD2O4.png\" rel=\"noreferrer\"><img alt=\"Convolution stride\" src=\"https://i.sstatic.net/XD2O4.png\"/></a></p>\n<p>When you use <code>tf.nn.conv2d</code> for example, you need to set it as a vector of 4 elements. There's no reason to get intimidated by this. It just contain the strides in the following order.</p>\n<ul>\n<li><p>2D Convolution - <code>[batch stride, height stride, width stride, channel stride]</code>. Here, batch stride and channel stride you just set to one (I've been implementing deep learning models for 5 years and never had to set them to anything except one). So that leaves you only with 2 strides to set.</p></li>\n<li><p>3D Convolution - <code>[batch stride, height stride, width stride, depth stride, channel stride]</code>. Here you worry about height/width/depth strides only.</p></li>\n</ul>\n<h3>Understanding padding</h3>\n<p>Now, you notice that no matter how small your stride is (i.e. 1) there is an unavoidable dimension reduction happening during convolution (e.g. width is 3 after convolving a 4 unit wide image). This is undesirable especially when building deep convolution neural networks. This is where padding comes to the rescue. There are two most commonly used padding types. </p>\n<ul>\n<li><code>SAME</code> and <code>VALID</code></li>\n</ul>\n<p>Below you can see the difference.</p>\n<p><a href=\"https://i.sstatic.net/O01D7.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/O01D7.png\"/></a></p>\n<p><strong>Final word</strong>: If you are very curious, you might be wondering. We just dropped a bomb on whole automatic dimension reduction and now talking about having different strides. But the best thing about stride is that you control when where and how the dimensions get reduced.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In summary, In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data.</p>\n<p>In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional. Mostly used on Image data.</p>\n<p>In 3D CNN, kernel moves in 3 directions. Input and output data of 3D CNN is 4 dimensional. Mostly used on 3D Image data (MRI, CT Scans).</p>\n<p>You can find more details here: <a href=\"https://medium.com/@xzz201920/conv1d-conv2d-and-conv3d-8a59182c4d6\" rel=\"noreferrer\">https://medium.com/@xzz201920/conv1d-conv2d-and-conv3d-8a59182c4d6</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Where is an explicit connection between the <code>optimizer</code> and the <code>loss</code>?</p>\n<p>How does the optimizer know where to get the gradients of the loss without a call liks this <code>optimizer.step(loss)</code>?</p>\n<p>-More context-</p>\n<p>When I minimize the loss, I didn't have to pass the gradients to the optimizer.</p>\n<pre><code>loss.backward() # Back Propagation\noptimizer.step() # Gradient Descent\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Without delving too deep into the internals of pytorch, I can offer a simplistic answer:</p>\n<p>Recall that when initializing <code>optimizer</code> you explicitly tell it what parameters (tensors) of the model it should be updating. The gradients are \"stored\" by the tensors themselves (they have a <a href=\"https://pytorch.org/docs/master/autograd.html#torch.Tensor.grad\" rel=\"noreferrer\"><code>grad</code></a> and a <a href=\"https://pytorch.org/docs/master/autograd.html#torch.Tensor.requires_grad\" rel=\"noreferrer\"><code>requires_grad</code></a> attributes) once you call <code>backward()</code> on the loss. After computing the gradients for all tensors in the model, calling <code>optimizer.step()</code> makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored <code>grad</code> to update their values.</p>\n<p>More info on computational graphs and the additional \"grad\" information stored in pytorch tensors can be found in <a href=\"https://stackoverflow.com/a/63869655/1714410\">this answer</a>.</p>\n<p>Referencing the parameters by the optimizer can sometimes cause troubles, e.g., when the model is moved to GPU <em>after</em> initializing the optimizer.\nMake sure you are done setting up your model <em>before</em> constructing the optimizer. See <a href=\"https://stackoverflow.com/a/66096687/1714410\">this answer</a> for more details.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Perhaps this will clarify a little the connection between <code>loss.backward</code> and <code>optim.step</code> (although the other answers are to the point).</p>\n<pre><code># Our \"model\"\nx = torch.tensor([1., 2.], requires_grad=True)\ny = 100*x\n\n# Compute loss\nloss = y.sum()\n\n# Compute gradient of the loss w.r.t. to the parameters  \nprint(x.grad)     # None\nloss.backward()      \nprint(x.grad)     # tensor([100., 100.])\n\n# MOdify the parameters by subtracting the gradient\noptim = torch.optim.SGD([x], lr=0.001)\nprint(x)        # tensor([1., 2.], requires_grad=True)\noptim.step()\nprint(x)        # tensor([0.9000, 1.9000], requires_grad=True)\n</code></pre>\n<p><code>loss.backward()</code> sets the <code>grad</code> attribute of all tensors with <code>requires_grad=True</code>\nin the computational graph of which loss is the leaf (only <code>x</code> in this case).</p>\n<p>Optimizer just iterates through the list of parameters (tensors) it received on initialization and everywhere where a tensor has <code>requires_grad=True</code>, it subtracts the value of its gradient stored in its <code>.grad</code> property (simply multiplied by the learning rate in case of SGD). It doesn't need to know with respect to what loss the gradients were computed it just wants to access that <code>.grad</code> property so it can do <code>x = x - lr * x.grad</code></p>\n<p><strong>Note</strong> that if we were doing this in a train loop we would call <code>optim.zero_grad()</code> because in each train step we want to compute new gradients - we don't care about gradients from the previous batch. Not zeroing grads would lead to gradient accumulation across batches.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you call <code>loss.backward()</code>, all it does is compute gradient of loss w.r.t all the parameters in loss that have <code>requires_grad = True</code> and store them in <code>parameter.grad</code> attribute for every parameter.</p>\n<p><code>optimizer.step()</code> updates all the parameters based on <code>parameter.grad</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-08-16 00:28:35Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/9782071/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I've been reading some things on neural networks and I understand the general principle of a single layer neural network. I understand the need for aditional layers, but why are nonlinear activation functions used?</p>\n<p>This question is followed by this one: <a href=\"https://stackoverflow.com/questions/9785754/what-is-a-derivative-of-the-activation-function-used-for-in-backpropagation\">What is a derivative of the activation function used for in backpropagation?</a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The purpose of the activation function is to introduce <strong><em>non-linearity into the network</em></strong></p>\n<p>in turn, this allows you to model a response variable (aka target variable, class label, or score) that varies non-linearly with its explanatory variables</p>\n<p><em>non-linear</em> means that the output cannot be reproduced from a linear combination of the inputs (which is not the same as output that renders to a straight line--the word for this is <em>affine</em>).</p>\n<p>another way to think of it: without a <em>non-linear</em> activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function (see definition just above).</p>\n<pre><code>&gt;&gt;&gt; in_vec = NP.random.rand(10)\n&gt;&gt;&gt; in_vec\n  array([ 0.94,  0.61,  0.65,  0.  ,  0.77,  0.99,  0.35,  0.81,  0.46,  0.59])\n\n&gt;&gt;&gt; # common activation function, hyperbolic tangent\n&gt;&gt;&gt; out_vec = NP.tanh(in_vec)\n&gt;&gt;&gt; out_vec\n array([ 0.74,  0.54,  0.57,  0.  ,  0.65,  0.76,  0.34,  0.67,  0.43,  0.53])\n</code></pre>\n<p>A common activation function used in backprop (<strong><em>hyperbolic tangent</em></strong>) evaluated from -2 to 2:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/W6vkA.png\"/></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A linear activation function can be used, however on very limited occasions. In fact to understand activation functions better it is important to look at the ordinary least-square or simply the linear regression. A linear regression aims at finding the optimal weights that result in minimal vertical effect between the explanatory and target variables, when combined with the input. In short, if the expected output reflects the linear regression as shown below then linear activation functions can be used: (Top Figure). But  as in the second figure below linear function will not produce the desired results:(Middle figure). However, a non-linear function as shown below would produce the desired results:</p>\n<p><a href=\"https://i.sstatic.net/EoiEP.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/EoiEP.png\"/></a> </p>\n<p>Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, regardless of how complex their architecture is. Input to networks is usually linear transformation (input * weight), but real world and problems are non-linear. To make the incoming data nonlinear, we use nonlinear mapping called activation function. An activation function is a decision making function that determines the presence of a particular neural feature. It is mapped between 0 and 1, where zero means absence of the feature, while one means its presence. Unfortunately, the small changes occurring in the weights cannot be reflected in the activation values because it can only take either 0 or 1. Therefore, nonlinear functions must be continuous and differentiable between this range.\nA neural network must be able to take any input from -infinity to +infinite, but it should be able to map it to an output that ranges between {0,1} or between {-1,1} in some cases - thus the need for activation function. Non-linearity is needed in activation functions because its aim in a neural network is to produce a nonlinear decision boundary via non-linear combinations of the weight and inputs.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A feed-forward neural network with linear activation and any number of hidden layers is equivalent to just a linear neural neural network with no hidden layer. For example lets consider the neural network in figure with two hidden layers and no activation\n<a href=\"https://i.sstatic.net/KEFNX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/KEFNX.png\"/></a></p>\n<pre><code>y = h2 * W3 + b3 \n  = (h1 * W2 + b2) * W3 + b3\n  = h1 * W2 * W3 + b2 * W3 + b3 \n  = (x * W1 + b1) * W2 * W3 + b2 * W3 + b3 \n  = x * W1 * W2 * W3 + b1 * W2 * W3 + b2 * W3 + b3 \n  = x * W' + b'\n</code></pre>\n<p>We can do the last step because combination of several linear transformation can be replaced with one transformation and combination of several bias term is just a single bias. The outcome is same even if we add some linear activation.</p>\n<p>So we could replace this neural net with a single layer neural net.This can be extended to <code>n</code> layers. This indicates adding layers doesn't increase the approximation power of a linear neural net at all. We need non-linear activation functions to approximate non-linear functions and most real world problems are highly complex and non-linear. In fact when the activation function is non-linear, then a two-layer neural network with sufficiently large number of hidden units can be proven to be a universal function approximator.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-11 15:02:03Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/4674623/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Why do we have to normalize the input for a neural network?</p>\n<p>I understand that sometimes, when for example the input values are non-numerical a certain transformation must be performed, but when we have a numerical input? Why the numbers must be in a certain interval?</p>\n<p>What will happen if the data is not normalized?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It's explained well <a href=\"http://www.faqs.org/faqs/ai-faq/neural-nets/part2/\" rel=\"noreferrer\">here</a>.</p>\n<blockquote>\n<p>If the input variables are combined linearly, as in an MLP [multilayer perceptron], then it is\n  rarely strictly necessary to standardize the inputs, at least in theory. The\n  reason is that any rescaling of an input vector can be effectively undone by\n  changing the corresponding weights and biases, leaving you with the exact\n  same outputs as you had before. However, there are a variety of practical\n  reasons why standardizing the inputs can make training faster and reduce the\n  chances of getting stuck in local optima. Also, weight decay and Bayesian\n  estimation can be done more conveniently with standardized inputs. </p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In neural networks, it is good idea not just to normalize data but also to scale them. This is intended for faster approaching to global minima at error surface. See the following pictures:\n<img alt=\"error surface before and after normalization\" src=\"https://i.sstatic.net/QJpco.png\"/></p>\n<p><img alt=\"error surface before and after scaling\" src=\"https://i.sstatic.net/NYl4T.png\"/></p>\n<p>Pictures are taken from the <a href=\"https://www.coursera.org/learn/neural-networks\" rel=\"noreferrer\">coursera course</a> about neural networks. Author of the <a href=\"https://www.coursera.org/learn/neural-networks\" rel=\"noreferrer\">course</a> is Geoffrey Hinton. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Some inputs to NN might not have a 'naturally defined' range of values. For example, the average value might be slowly, but continuously increasing over time (for example a number of records in the database). </p>\n<p>In such case feeding this raw value into your network will not work very well. You will teach your network on values from lower part of range, while the actual inputs will be from the higher part of this range (and quite possibly above range, that the network has learned to work with). </p>\n<p>You should normalize this value. You could for example tell the network by how much the value has changed since the previous input. This increment usually can be defined with high probability in a specific range, which makes it a good input for network.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to understand the role of the <code>Flatten</code> function in Keras. Below is my code, which is a simple two-layer network. It takes in 2-dimensional data of shape (3, 2), and outputs 1-dimensional data of shape (1, 4):</p>\n<pre><code>model = Sequential()\nmodel.add(Dense(16, input_shape=(3, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(4))\nmodel.compile(loss='mean_squared_error', optimizer='SGD')\n\nx = np.array([[[1, 2], [3, 4], [5, 6]]])\n\ny = model.predict(x)\n\nprint y.shape\n</code></pre>\n<p>This prints out that <code>y</code> has shape (1, 4). However, if I remove the <code>Flatten</code> line, then it prints out that <code>y</code> has shape (1, 3, 4).</p>\n<p>I don't understand this. From my understanding of neural networks, the <code>model.add(Dense(16, input_shape=(3, 2)))</code> function is creating a hidden fully-connected layer, with 16 nodes. Each of these nodes is connected to each of the 3x2 input elements. Therefore, the 16 nodes at the output of this first layer are already \"flat\". So, the output shape of the first layer should be (1, 16). Then, the second layer takes this as an input, and outputs data of shape (1, 4).</p>\n<p>So if the output of the first layer is already \"flat\" and of shape (1, 16), why do I need to further flatten it?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you read the Keras documentation entry for <a href=\"https://keras.io/layers/core/#dense\" rel=\"noreferrer\"><code>Dense</code></a>, you will see that this call:</p>\n<pre><code>Dense(16, input_shape=(5,3))\n</code></pre>\n<p>would result in a <code>Dense</code> network with 3 inputs and 16 outputs which would be applied independently for each of 5 steps. So, if <code>D(x)</code> transforms 3 dimensional vector to 16-d vector, what you'll get as output from your layer would be a sequence of vectors: <code>[D(x[0,:]), D(x[1,:]),..., D(x[4,:])]</code> with shape <code>(5, 16)</code>. In order to have the behavior you specify you may first <code>Flatten</code> your input to a 15-d vector and then apply <code>Dense</code>:</p>\n<pre><code>model = Sequential()\nmodel.add(Flatten(input_shape=(3, 2)))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(4))\nmodel.compile(loss='mean_squared_error', optimizer='SGD')\n</code></pre>\n<p><strong>EDIT:</strong>\nAs some people struggled to understand - here you have an explaining image:</p>\n<p><a href=\"https://i.sstatic.net/Wk8eV.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Wk8eV.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://i.sstatic.net/lmrin.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/lmrin.png\"/></a>\nThis is how Flatten works converting Matrix to single array.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><sub>short read:</sub></p>\n<blockquote>\n<p>Flattening a tensor means to remove all of the dimensions except for one. This is exactly what the Flatten layer does.</p>\n</blockquote>\n<p><sub>long read:</sub></p>\n<p>If we take the original model (with the Flatten layer) created in consideration we can get the following model summary:</p>\n<pre><code>Layer (type)                 Output Shape              Param #   \n=================================================================\nD16 (Dense)                  (None, 3, 16)             48        \n_________________________________________________________________\nA (Activation)               (None, 3, 16)             0         \n_________________________________________________________________\nF (Flatten)                  (None, 48)                0         \n_________________________________________________________________\nD4 (Dense)                   (None, 4)                 196       \n=================================================================\nTotal params: 244\nTrainable params: 244\nNon-trainable params: 0\n</code></pre>\n<p>For this summary the next image will hopefully provide little more sense on the input and output sizes for each layer.</p>\n<p>The output shape for the Flatten layer as you can read is <code>(None, 48)</code>. Here is the tip. You should read it <code>(1, 48)</code> or <code>(2, 48)</code> or ... or <code>(16, 48)</code> ... or <code>(32, 48)</code>, ...</p>\n<p>In fact, <code>None</code> on that position means any batch size. For the inputs to recall, the first dimension means the batch size and the second means the number of input features.</p>\n<p>The role of the <strong>Flatten layer</strong> in Keras is super simple:</p>\n<p>A flatten operation on a tensor reshapes the tensor to have the shape that is equal to the number of elements contained in tensor <strong>non including the batch dimension</strong>.</p>\n<p><a href=\"https://i.sstatic.net/IBt6j.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/IBt6j.jpg\"/></a></p>\n<hr/>\n<p>Note: I used the <code>model.summary()</code> method to provide the output shape and parameter details.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This:</p>\n<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor data in dataloader:\n    inputs, labels = data\n    outputs = model(inputs)\n</code></pre>\n<p>Gives the error:</p>\n<blockquote>\n<p>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same</p>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You get this error because your model is on the GPU, but your data is on the CPU. So, you need to send your input tensors to the GPU.</p>\n<pre class=\"lang-py prettyprint-override\"><code>inputs, labels = data                         # this is what you had\ninputs, labels = inputs.cuda(), labels.cuda() # add this line\n</code></pre>\n<p>Or like this, to stay consistent with the rest of your code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ninputs, labels = inputs.to(device), labels.to(device)\n</code></pre>\n<p>The <strong>same error</strong> will be raised if your input tensors are on the GPU but your model weights aren't. In this case, you need to send your model weights to the GPU.</p>\n<pre><code>model = MyModel()\n\nif torch.cuda.is_available():\n    model.cuda()\n</code></pre>\n<p>See the documentation for <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda\" rel=\"noreferrer\"><code>cuda()</code></a>, and its opposite, <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu\" rel=\"noreferrer\"><code>cpu()</code></a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The new API is to use <code>.to()</code> method.</p>\n<p>The advantage is obvious and important.\nYour device may tomorrow be something other than \"cuda\":</p>\n<ul>\n<li>cpu</li>\n<li>cuda</li>\n<li>mkldnn</li>\n<li>opengl</li>\n<li>opencl</li>\n<li>ideep</li>\n<li>hip</li>\n<li>msnpu</li>\n<li>xla</li>\n</ul>\n<p>So try to avoid <code>model.cuda()</code>\nIt is not wrong to check for the device</p>\n<pre><code>dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n</code></pre>\n<p>or to hardcode it:</p>\n<pre><code>dev=torch.device(\"cuda\") \n</code></pre>\n<p>same as:</p>\n<pre><code>dev=\"cuda\"\n</code></pre>\n<p>In general you can use this code:</p>\n<pre><code>model.to(dev)\ndata = data.to(dev)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Notice that (from pytorch documentation):</p>\n<blockquote>\n<p>If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.</p>\n</blockquote>\n<p>That is, you might need to do:</p>\n<pre><code>model = model.to(\"cuda\")\ndata = data.to(\"cuda\")\n</code></pre>\n<p>Instead of just:</p>\n<pre><code>model.to(\"cuda\")\ndata.to(\"cuda\")\n</code></pre>\n<p>With the first approach you'll be in the safe side.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:</p>\n<p>As far as I understand, <strong>Conda</strong> and <strong>Poetry</strong> have different purposes but are largely redundant:</p>\n<ul>\n<li>Conda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.</li>\n<li>Poetry is primarily a Python package manager (say, an upgrade of <strong>pip</strong>), but it can also create and manage Python environments (say, an upgrade of <strong>Pyenv</strong>).</li>\n</ul>\n<p>My idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.</p>\n<p>I've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like <code>poetry shell</code> or <code>poetry run</code>, only <code>poetry init</code>, <code>poetry install</code> etc (after activating the Conda environment).</p>\n<p>For full disclosure, my <em>environment.yml</em> file (for Conda) looks like this:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: N\n\nchannels:\n  - defaults\n  - conda-forge\n\ndependencies:\n  - python=3.9\n  - cudatoolkit\n  - cudnn\n</code></pre>\n<p>and my <em>poetry.toml</em> file looks like that:</p>\n<pre class=\"lang-ini prettyprint-override\"><code>[tool.poetry]\nname = \"N\"\nauthors = [\"B\"]\n\n[tool.poetry.dependencies]\npython = \"3.9\"\ntorch = \"^1.10.1\"\n\n[build-system]\nrequires = [\"poetry-core&gt;=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre>\n<p>To be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.</p>\n<p>Does this project design look reasonable to you?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>2024-04-05 update:</h1>\n<p>It looks like my tips proved to be useful to many people, but they are not needed anymore. Just use <a href=\"https://pixi.sh/\" rel=\"noreferrer\"><strong>Pixi</strong></a>. It's still alpha, but it works great, and provides the features of the Conda + Poetry setup in a simpler and more unified way. In particular, Pixi supports:</p>\n<ul>\n<li>installing packages both from Conda channels and from PyPi,</li>\n<li>lockfiles,</li>\n<li>creating multiple features and environments (prod, dev, etc.),</li>\n<li>very efficient package version resolution, not just faster than Conda (which is very slow), but in my experience also faster than Mamba, Poetry and pip.</li>\n</ul>\n<h2>Making a Pixi env look like a Conda env</h2>\n<p>One non-obvious tip about Pixi is that you can easily make your project's Pixi environment visible as a Conda environment, which may be useful e.g. in VS Code, which allows choosing Python interpreters and Jupyter kernels from detected Conda environments. All you need to do is something like:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>ln -s /path/to/my/project/.pixi/envs/default /path/to/conda/base/envs/conda-name-of-my-env\n</code></pre>\n<p>The first path is the path to your Pixi environment, which resides in your project directory, under <code>.pixi/envs</code>, and the second path needs to be within one of Conda's environment directories, which can be found with <code>conda config --show envs_dirs</code>.</p>\n<h1>Original answer:</h1>\n<p>I have experience with a Conda + Poetry setup, and it's been working fine. The great majority of my dependencies are specified in <code>pyproject.toml</code>, but when there's something that's unavailable in PyPI, or installing it with Conda is easier, I add it to <code>environment.yml</code>. Moreover, Conda is used as a virtual environment manager, which works well with Poetry: there is no need to use <code>poetry run</code> or <code>poetry shell</code>, it is enough to activate the right Conda environment.</p>\n<h2>Tips for creating a reproducible environment</h2>\n<ol>\n<li>Add Poetry, possibly with a version number (if needed), as a dependency in <code>environment.yml</code>, so that you get Poetry installed when you run <code>conda create</code>, along with Python and other non-PyPI dependencies.</li>\n<li>Add <code>conda-lock</code>, which gives you lock files for Conda dependencies, just like you have <code>poetry.lock</code> for Poetry dependencies.</li>\n<li>Consider using <code>mamba</code> which is generally compatible with <code>conda</code>, but is better at resolving conflicts, and is also much faster. An additional benefit is that all users of your setup will use the same  package resolver, independent from the locally-installed version of Conda.</li>\n<li>By default, use Poetry for adding Python dependencies. Install packages via Conda if there's a reason to do so (e.g. in order to get a CUDA-enabled version). In such a case, it is best to specify the package's exact version in <code>environment.yml</code>, and after it's installed, to add an entry with the same version specification to Poetry's <code>pyproject.toml</code> (without <code>^</code> or <code>~</code> before the version number). This will let Poetry know that the package is there and should not be upgraded.</li>\n<li>If you use a different channels that provide the same packages, it might be not obvious which channel a particular package will be downloaded from. One solution is to specify the channel for the package using the :: notation (see the <code>pytorch</code> entry below), and another solution is to enable <a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html#strict\" rel=\"noreferrer\">strict channel priority</a>. Unfortunately, in Conda 4.x there is no way to enable this option through <code>environment.yml</code>.</li>\n<li>Note that Python adds <a href=\"https://docs.python.org/3/library/site.html#site.USER_SITE\" rel=\"noreferrer\">user site-packages</a> to <code>sys.path</code>, which may cause lack of reproducibility if the user has installed Python packages outside Conda environments. One possible solution is to make sure that the <a href=\"https://docs.python.org/3/using/cmdline.html#envvar-PYTHONNOUSERSITE\" rel=\"noreferrer\"><code>PYTHONNOUSERSITE</code></a> environment variable is set to <code>True</code> (or to any other non-empty value).</li>\n</ol>\n<h2>Example</h2>\n<p><code>environment.yml</code>:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: my_project_env\nchannels:\n  - pytorch\n  - conda-forge\n  # We want to have a reproducible setup, so we don't want default channels,\n  # which may be different for different users. All required channels should\n  # be listed explicitly here.\n  - nodefaults\ndependencies:\n  - python=3.10.*  # or don't specify the version and use the latest stable Python\n  - mamba\n  - pip  # pip must be mentioned explicitly, or conda-lock will fail\n  - poetry=1.*  # or 1.1.*, or no version at all -- as you want\n  - tensorflow=2.8.0\n  - pytorch::pytorch=1.11.0\n  - pytorch::torchaudio=0.11.0\n  - pytorch::torchvision=0.12.0\n\n# Non-standard section listing target platforms for conda-lock:\nplatforms:\n  - linux-64\n</code></pre>\n<p><code>virtual-packages.yml</code> (may be used e.g. when we want <code>conda-lock</code> to generate CUDA-enabled lock files even on platforms without CUDA):</p>\n<pre><code>subdirs:\n  linux-64:\n    packages:\n      __cuda: 11.5\n</code></pre>\n<h3>First-time setup</h3>\n<p>You can avoid playing with the bootstrap env and simplify the example below if you have <code>conda-lock</code>, <code>mamba</code> and <code>poetry</code> already installed outside your target environment.</p>\n<pre class=\"lang-bash prettyprint-override\"><code># Create a bootstrap env\nconda create -p /tmp/bootstrap -c conda-forge mamba conda-lock poetry='1.*'\nconda activate /tmp/bootstrap\n\n# Create Conda lock file(s) from environment.yml\nconda-lock -k explicit --conda mamba\n# Set up Poetry\npoetry init --python=~3.10  # version spec should match the one from environment.yml\n# Fix package versions installed by Conda to prevent upgrades\npoetry add --lock tensorflow=2.8.0 torch=1.11.0 torchaudio=0.11.0 torchvision=0.12.0\n# Add conda-lock (and other packages, as needed) to pyproject.toml and poetry.lock\npoetry add --lock conda-lock\n\n# Remove the bootstrap env\nconda deactivate\nrm -rf /tmp/bootstrap\n\n# Add Conda spec and lock files\ngit add environment.yml virtual-packages.yml conda-linux-64.lock\n# Add Poetry spec and lock files\ngit add pyproject.toml poetry.lock\ngit commit\n</code></pre>\n<h3>Usage</h3>\n<p>The above setup may seem complex, but it can be used in a fairly simple way.</p>\n<h4>Creating the environment</h4>\n<pre class=\"lang-bash prettyprint-override\"><code>conda create --name my_project_env --file conda-linux-64.lock\nconda activate my_project_env\npoetry install\n</code></pre>\n<h4>Activating the environment</h4>\n<pre class=\"lang-bash prettyprint-override\"><code>conda activate my_project_env\n</code></pre>\n<h4>Updating the environment</h4>\n<pre class=\"lang-bash prettyprint-override\"><code># Re-generate Conda lock file(s) based on environment.yml\nconda-lock -k explicit --conda mamba\n# Update Conda packages based on re-generated lock file\nmamba update --file conda-linux-64.lock\n# Update Poetry packages and re-generate poetry.lock\npoetry update\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To anyone using <a href=\"https://stackoverflow.com/a/71110028/18686384\">@michau's</a> answer but having issues including poetry in the <code>environment.yml</code>. Currently, poetry versions 1.2 or greater aren't <a href=\"https://github.com/conda-forge/poetry-feedstock/pull/72\" rel=\"noreferrer\">supported by conda-forge</a>. You can still include poetry v1.2 in the <code>.yml</code> with the below as an alternative:</p>\n<pre><code>dependencies:\n  - python=3.9.*\n  - mamba\n  - pip \n  - pip:\n    - \"poetry&gt;=1.2\"\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should avoid using multiple package or environment managers together, as it can cause incompatibilities or errors.</p>\n<p>I think this is incorrect, however:</p>\n<blockquote>\n<p>My idea is to use both and compartmentalize their roles--let Conda be the environment manager and Poetry the package manager.</p>\n</blockquote>\n<p>Doing this is basically pointless; you're just installing two different computer programs to do the same thing.</p>\n<p>The <em>real</em> difference between the two is that Poetry is a Python-only package manager, while Conda is mostly-Python (but has some support for other languages) with fewer packages. Nearly all the big packages will be available from Conda, but if you need to use a niche package, you might not find it there.</p>\n<p>There's also a lot less quality control on PythonPI than on Conda channels, so Poetry may install incorrect/conflicting versions of packages, although it does a much better job than <code>pip</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have asked a <a href=\"https://stackoverflow.com/questions/5684370/finding-k-nearest-neighbors-for-a-given-vector\">question</a> a few days back on how to find the nearest neighbors for a given vector. My vector is now 21 dimensions and before I proceed further, because I am not from the domain of Machine Learning nor Math, I am beginning to ask myself some fundamental questions:</p>\n<ul>\n<li>Is Euclidean distance a good metric for finding the nearest neighbors in the first place? If not, what are my options?</li>\n<li>In addition, how does one go about deciding the right threshold for determining the k-neighbors? Is there some analysis that can be done to figure this value out?</li>\n<li>Previously, I was suggested to use kd-Trees but the Wikipedia page clearly says that for high-dimensions, kd-Tree is almost equivalent to a brute-force search. In that case, what is the best way to find nearest-neighbors in a million point dataset efficiently?</li>\n</ul>\n<p>Can someone please clarify the some (or all) of the above questions?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I currently study such problems -- classification, nearest neighbor searching -- for music information retrieval.</p>\n<p>You may be interested in <strong>Approximate Nearest Neighbor</strong> (<strong>ANN</strong>) algorithms. The idea is that you allow the algorithm to return sufficiently <em>near neighbors</em> (perhaps not the nearest neighbor); in doing so, you reduce complexity. You mentioned the <strong>kd-tree</strong>; that is one example. But as you said, <strong>kd-tree</strong> works poorly in high dimensions. In fact, <em>all</em> current indexing techniques (based on space partitioning) degrade to linear search for sufficiently high dimensions [1][2][3].</p>\n<p>Among <strong>ANN</strong> algorithms proposed recently, perhaps the most popular is <strong>Locality-Sensitive Hashing</strong> (<strong>LSH</strong>), which maps a set of points in a high-dimensional space into a set of bins, i.e., a hash table [1][3]. But unlike traditional hashes, a <em>locality-sensitive</em> hash places <em>nearby</em> points into the same bin.</p>\n<p><strong>LSH</strong> has some huge advantages. First, it is simple. You just compute the hash for all points in your database, then make a hash table from them. To query, just compute the hash of the query point, then retrieve all points in the same bin from the hash table.</p>\n<p>Second, there is a rigorous theory that supports its performance. It can be shown that the query time is <em>sublinear</em> in the size of the database, i.e., faster than linear search. How much faster depends upon how much approximation we can tolerate.</p>\n<p>Finally, <strong>LSH</strong> is compatible with any Lp norm for <code>0 &lt; p &lt;= 2</code>. Therefore, to answer your first question, you can use <strong>LSH</strong> with the Euclidean distance metric, or you can use it with the Manhattan (L1) distance metric. There are also variants for Hamming distance and cosine similarity.</p>\n<p>A decent overview was written by Malcolm Slaney and Michael Casey for IEEE Signal Processing Magazine in 2008 [4].</p>\n<p><strong>LSH</strong> has been applied seemingly everywhere. You may want to give it a try.</p>\n<hr/>\n<p>[1] Datar, Indyk, Immorlica, Mirrokni, \"Locality-Sensitive Hashing Scheme Based on p-Stable Distributions,\" 2004.</p>\n<p>[2] Weber, Schek, Blott, \"A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces,\" 1998.</p>\n<p>[3] Gionis, Indyk, Motwani, \"Similarity search in high dimensions via hashing,\" 1999.</p>\n<p>[4] Slaney, Casey, \"Locality-sensitive hashing for finding nearest neighbors\", 2008.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>I. The Distance Metric</strong></p>\n<p>First, the number of features (columns) in a data set is not a factor in selecting a distance metric for use in kNN. There are quite a few published studies directed to precisely this question, and the usual bases for comparison are: </p>\n<ul>\n<li><p>the underlying statistical\ndistribution of your data;</p></li>\n<li><p>the relationship among the features\nthat comprise your data (are they\nindependent--i.e., what does the\ncovariance matrix look like); and</p></li>\n<li><p>the coordinate space from which your\ndata was obtained.</p></li>\n</ul>\n<p>If you have no prior knowledge of the distribution(s) from which your data was sampled, at least one (well documented and thorough) <a href=\"http://books.google.com/books?id=rc7A_dFH-rUC&amp;pg=PA79&amp;lpg=PA79&amp;dq=comparison%20performance%20knn%20distance%20%20metric%20euclidean&amp;source=bl&amp;ots=544CPNGVrl&amp;sig=BIonTj289nFB8iAXe-Ow2tCMxzs&amp;hl=en&amp;ei=oOWzTdzuC-vQiALWt6mvBg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=3&amp;sqi=2&amp;ved=0CC4Q6AEwAg#v=onepage&amp;q&amp;f=false\">study</a> concludes that Euclidean distance is the best choice.</p>\n<p>YEuclidean metric used in mega-scale Web Recommendation Engines as well as in current academic research. Distances calculated by Euclidean have intuitive meaning and the computation scales--i.e., Euclidean distance is calculated the same way, whether the two points are in two dimension or in twenty-two dimension space.</p>\n<p>It has only failed for me a few times, each of those cases Euclidean distance failed because the underlying (cartesian) coordinate system was a poor choice. And you'll usually recognize this because for instance path lengths (distances) are no longer additive--e.g., when the metric space is a chessboard, Manhattan distance is better than Euclidean, likewise when the metric space is Earth and your distances are trans-continental flights, a distance metric suitable for a polar coordinate system is a good idea (e.g., London to Vienna is is 2.5 hours, Vienna to St. Petersburg is another 3 hrs, more or less in the same direction, yet London to St. Petersburg isn't 5.5 hours, instead, is a little over 3 hrs.)</p>\n<p>But apart from those cases in which your data belongs in a non-cartesian coordinate system, the choice of distance metric is usually not material. (See this <a href=\"http://truepvd.blogspot.com/2007/02/knn-distance-metric-comparisons.html\">blog post</a> from a CS student, comparing several distance metrics by examining their effect on kNN classifier--chi square give the best results, but the differences are not large; A more comprehensive study is in the academic paper, <a href=\"http://books.google.com/books?id=rc7A_dFH-rUC&amp;pg=PA79&amp;lpg=PA79&amp;dq=comparison%20performance%20knn%20distance%20%20metric%20euclidean&amp;source=bl&amp;ots=544CPNGVrl&amp;sig=BIonTj289nFB8iAXe-Ow2tCMxzs&amp;hl=en&amp;ei=oOWzTdzuC-vQiALWt6mvBg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=3&amp;sqi=2&amp;ved=0CC4Q6AEwAg#v=onepage&amp;q&amp;f=false\">Comparative Study of Distance Functions for Nearest Neighbors</a>--Mahalanobis (essentially Euclidean normalized by to account for dimension covariance) was the best in this study.</p>\n<p>One important proviso: for distance metric calculations to be meaningful, you must <strong><em>re-scale</em></strong> your data--rarely is it possible to build a kNN model to generate accurate predictions without doing this. For instance, if you are building a kNN model to predict athletic performance, and your expectation variables are height (cm), weight (kg), bodyfat (%), and resting pulse (beats per minute), then a typical data point might look something like this: [ 180.4, 66.1, 11.3, 71 ]. Clearly the distance calculation will be dominated by height, while the contribution by bodyfat % will be almost negligible. Put another way, if instead, the data were reported differently, so that bodyweight was in grams rather than kilograms, then the original value of 86.1, would be 86,100, which would have a large effect on your results, which is exactly what you don't want. Probably the most common scaling technique is subtracting the mean and dividing by the standard deviation (mean and sd refer calculated separately for each column, or feature in that data set; X refers to an individual entry/cell within a data row):</p>\n<pre><code>X_new = (X_old - mu) / sigma\n</code></pre>\n<p><br/>\n<strong>II. The Data Structure</strong></p>\n<p>If you are concerned about performance of the kd-tree structure, A <strong>Voronoi Tessellation</strong> is a conceptually simple container but that will drastically improve performance and scales better than kd-Trees.<br/><br/><img alt=\"dat\" src=\"https://i.sstatic.net/uUKgA.png\"/></p>\n<p>This is not the most common way to persist kNN training data, though the application of VT for this purpose, as well as the consequent performance advantages, are well-documented (see e.g. this <a href=\"http://academic.research.microsoft.com/Paper/579371.aspx\">Microsoft Research report</a>). The practical significance of this is that, provided you are using a 'mainstream' language (e.g., in the <a href=\"http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html\">TIOBE Index</a>) then you ought to find a library to perform VT. I know in Python and R, there are multiple options for each language (e.g., the <em>voronoi</em> package for R available on <a href=\"http://mirrors.softliste.de/cran/\">CRAN</a>)</p>\n<p>Using a VT for kNN works like this::</p>\n<p>From your data, randomly select w points--these are your Voronoi centers. A Voronoi cell encapsulates all neighboring points that are nearest to each center. Imagine if you assign a different color to each of Voronoi centers, so that each point assigned to a given center is painted that color. As long as you have a sufficient density, doing this will nicely show the boundaries of each Voronoi center (as the boundary that separates two colors.</p>\n<p>How to select the Voronoi Centers? I use two orthogonal guidelines. After random selecting the w points, calculate the VT for your training data. Next check the number of data points assigned to each Voronoi center--these values should be about the same (given uniform point density across your data space). In two dimensions, this would cause a VT with tiles of the same size.That's the first rule, here's the second. Select w by iteration--run your kNN algorithm with w as a variable parameter, and measure performance (time required to return a prediction by querying the VT).</p>\n<p>So imagine you have one million data points..... If the points were persisted in an ordinary 2D data structure, or in a kd-tree, you would perform on average a couple million distance calculations for <em>each</em> new data points whose response variable you wish to predict. Of course, those calculations are performed on a single data set. With a V/T, the nearest-neighbor search is performed in two steps one after the other, against two different populations of data--first against the Voronoi centers, then once the nearest center is found, the points inside the cell corresponding to that center are searched to find the actual nearest neighbor (by successive distance calculations) Combined, these two look-ups are much faster than a single brute-force look-up. That's easy to see: for 1M data points, suppose you select 250 Voronoi centers to tesselate your data space. On average, each Voronoi cell will have 4,000 data points. So instead of performing on average 500,000 distance calculations (brute force), you perform far lesss, on average just 125 + 2,000.<br/><br/></p>\n<p><strong>III. Calculating the Result (the predicted response variable)</strong></p>\n<p>There are two steps to calculating the predicted value from a set of kNN training data. The first is identifying n, or <em>the number of nearest neighbors</em> to use for this calculation. The second is <em>how to weight their contribution</em> to the predicted value.</p>\n<p>W/r/t the first component, you can determine the best value of n by solving an optimization problem (very similar to least squares optimization). That's the theory; in practice, most people just use n=3. In any event, it's simple to run your kNN algorithm over a set of test instances (to calculate predicted values) for n=1, n=2, n=3, etc. and plot the error as a function of n. If you just want a plausible value for n to get started, again, just use n = 3.</p>\n<p>The second component is how to weight the contribution of each of the neighbors (assuming n &gt; 1).</p>\n<p>The simplest weighting technique is just multiplying each neighbor by a weighting coefficient, which is just the 1/(dist * K), or the inverse of the distance from that neighbor to the test instance often multiplied by some empirically derived constant, K. I am not a fan of this technique because it often over-weights the closest neighbors (and concomitantly under-weights the more distant ones); the significance of this is that a given prediction can be almost entirely dependent on a single neighbor, which in turn increases the algorithm's sensitivity to noise.</p>\n<p>A must better weighting function, which substantially avoids this limitation is the <strong><em>gaussian function</em></strong>, which in python, looks like this:</p>\n<pre><code>def weight_gauss(dist, sig=2.0) :\n    return math.e**(-dist**2/(2*sig**2))\n</code></pre>\n<p>To calculate a predicted value using your kNN code, you would identify the n nearest neighbors to the data point whose response variable you wish to predict ('test instance'), then call the weight_gauss function, once for each of the n neighbors, passing in the distance between each neighbor the the test point.This function will return the weight for each neighbor, which is then used as that neighbor's coefficient in the weighted average calculation.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What you are facing is known as the <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\" rel=\"noreferrer\"><em>curse of dimensionality</em></a>. It is sometimes useful to run an algorithm like PCA or <strike>ICA</strike> to make sure that you really need all 21 dimensions and possibly find a linear transformation which would allow you to use less than 21 with approximately the same result quality.</p>\n<p><strong>Update:</strong>\nI encountered them in a book called Biomedical Signal Processing by Rangayyan (I hope I remember it correctly). <strike>ICA is not a trivial technique, but it was developed by researchers in Finland and I think Matlab code for it is publicly available for download.</strike> PCA is a more widely used technique and I believe you should be able to find its R or other software implementation. PCA is performed by solving linear equations iteratively. I've done it too long ago to remember how. = )</p>\n<p>The idea is that you break up your signals into independent eigenvectors (discrete eigenfunctions, really) and their eigenvalues, 21 in your case. Each eigenvalue shows the amount of contribution each eigenfunction provides to each of your measurements. If an eigenvalue is tiny, you can very closely represent the signals without using its corresponding eigenfunction at all, and that's how you get rid of a dimension.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I recently started studying deep learning and other ML techniques, and I started searching for frameworks that simplify the process of build a net and training it, then I found TensorFlow, having little experience in the field, for me, it seems that speed is a big factor for making a big ML system even more if working with deep learning, so why python was chosen by Google to make TensorFlow? Wouldn't it be better to make it over an language that can be compiled and not interpreted?</p>\n<p>What are the advantages of using Python over a language like C++ for machine learning?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The most important thing to realize about TensorFlow is that, for the most part, <em>the core is not written in Python</em>:  It's written in a combination of highly-optimized C++ and CUDA (Nvidia's language for programming GPUs).  Much of that happens, in turn, by using <a href=\"http://eigen.tuxfamily.org/index.php?title=Main_Page\" rel=\"noreferrer\">Eigen</a> (a high-performance C++ and CUDA numerical library) and <a href=\"https://developer.nvidia.com/cudnn\" rel=\"noreferrer\">NVidia's cuDNN</a> (a very optimized DNN library for <a href=\"https://developer.nvidia.com/cuda-gpus\" rel=\"noreferrer\">NVidia GPUs</a>, for functions such as <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"noreferrer\">convolutions</a>).</p>\n<p>The model for TensorFlow is that the programmer uses \"some language\" (most likely Python!) to express the model.  This model, written in the TensorFlow constructs such as:</p>\n<pre><code>h1 = tf.nn.relu(tf.matmul(l1, W1) + b1)\nh2 = ...\n</code></pre>\n<p>is not actually executed when the Python is run.  Instead, what's actually created is a <a href=\"https://www.tensorflow.org/get_started/graph_viz\" rel=\"noreferrer\">dataflow graph</a> that says to take particular inputs, apply particular operations, supply the results as the inputs to other operations, and so on.  <em>This model is executed by fast C++ code, and for the most part, the data going between operations is never copied back to the Python code</em>.</p>\n<p>Then the programmer \"drives\" the execution of this model by pulling on nodes -- for training, usually in Python, and for serving, sometimes in Python and sometimes in raw C++:</p>\n<pre><code>sess.run(eval_results)\n</code></pre>\n<p>This one Python (or C++ function call) uses either an in-process call to C++ or an <a href=\"https://en.wikipedia.org/wiki/Remote_procedure_call\" rel=\"noreferrer\">RPC</a> for the distributed version to call into the C++ TensorFlow server to tell it to execute, and then copies back the results.</p>\n<p><strong>So, with that said, let's re-phrase the question:  Why did TensorFlow choose  Python as the first well-supported language for expressing and controlling the training of models?</strong></p>\n<p>The answer to that is simple:  Python is probably <em>the</em> most comfortable language for a large range of data scientists and machine learning experts that's also that easy to integrate and have control a C++ backend, while also being general, widely-used both inside and outside of Google, and open source.  Given that with the basic model of TensorFlow, the performance of Python isn't that important, it was a natural fit.  It's also a huge plus that <a href=\"http://www.numpy.org/\" rel=\"noreferrer\">NumPy</a> makes it easy to do pre-processing in Python -- also with high performance -- before feeding it in to TensorFlow for the truly CPU-heavy things.</p>\n<p>There's also a bunch of complexity in expressing the model that isn't used when executing it -- shape inference (e.g., if you do matmul(A, B), what is the shape of the resulting data?) and automatic <a href=\"https://en.wikipedia.org/wiki/Gradient\" rel=\"noreferrer\">gradient</a> computation.  It turns out to have been nice to be able to express those in Python, though I think in the long term they'll probably move to the C++ backend to make adding other languages easier.</p>\n<p>(The hope, of course, is to support other languages in the future for creating and expressing models.  It's already quite straightforward to run inference using several other languages -- C++ works now, someone from Facebook contributed <a href=\"https://golang.org/\" rel=\"noreferrer\">Go</a> bindings that we're reviewing now, etc.)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>TF is not written in python. It is written in C++ (and uses high-performant numerical <a href=\"https://en.wikipedia.org/wiki/Eigen_(C%2B%2B_library)\" rel=\"noreferrer\">libraries</a> and <a href=\"https://www.geforce.com/hardware/technology/cuda\" rel=\"noreferrer\">CUDA</a> code) and you can check this by looking at their <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"noreferrer\">github</a>. So <a href=\"https://youtu.be/t64ortpgS-E?t=4m38s\" rel=\"noreferrer\">the core is written not in python</a> but TF provide an interface to many other languages (<a href=\"https://www.tensorflow.org/api_docs/\" rel=\"noreferrer\">python, C++, Java, Go</a>)</p>\n<p><a href=\"https://i.sstatic.net/yAI6m.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/yAI6m.png\"/></a></p>\n<p>If you come from a data analysis world, you can think about it like numpy (not written in python, but provides an interface to Python) or if you are a web-developer - think about it as a database (PostgreSQL, MySQL, which can be invoked from Java, Python, PHP)</p>\n<hr/>\n<p>Python frontend (the language in which people write models in TF) is the most popular due to <a href=\"https://www.quora.com/Why-is-Python-so-popular-in-machine-learning\" rel=\"noreferrer\">many</a> <a href=\"https://www.quora.com/Why-is-Python-considered-a-good-language-for-AI-and-Machine-Learning\" rel=\"noreferrer\">reasons</a>. In my opinion the main reason is historical: majority of ML users already use it (another popular choice is R) so if you will not provide an interface to python, your library is most probably doomed to obscurity.</p>\n<hr/>\n<p>But being written in python does not mean that your model is executed in python. On the contrary, if you written your model in the right way Python is never executed during the evaluation of the TF graph (except of <a href=\"https://www.tensorflow.org/api_docs/python/tf/py_func\" rel=\"noreferrer\">tf.py_func()</a>, which exists for debugging and should be avoided in real model exactly because it is executed on Python's side).</p>\n<p>This is different from for example numpy. For example if you do <code>np.linalg.eig(np.matmul(A, np.transpose(A))</code> (which is <code>eig(AA')</code>), the operation will compute transpose in some fast language (C++ or fortran), return it to python, take it from python together with A, and compute a multiplication in some fast language and return it to python, then compute eigenvalues and return it to python. So nonetheless expensive operations like matmul and eig are calculated efficiently, you still lose time by moving the results to python back and force. <strong>TF does not do it</strong>, once you defined the graph your tensors flow not in python but in C++/CUDA/something else.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Python allows you to create extension modules using C and C++, interfacing with native code, and still getting the advantages that Python gives you.</p>\n<p>TensorFlow uses Python, yes, but it also contains large amounts of <a href=\"https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&amp;q=&amp;type=Code\" rel=\"nofollow\">C++</a>.</p>\n<p>This allows a simpler interface for experimentation with less human-thought overhead with Python, and add performance by programming the most important parts in C++.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am unable to understand the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" rel=\"noreferrer\">page</a> of the <code>StandardScaler</code> in the documentation of <code>sklearn</code>.</p>\n<p>Can anyone explain this to me in simple terms?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1><strong>Intro</strong></h1>\n<p>I assume that you have a matrix <code>X</code> where each <strong>row/line</strong> is a <strong>sample/observation</strong> and each <strong>column</strong> is a <strong>variable/feature</strong> (this is the expected input for any <code>sklearn</code> ML function by the way -- <code>X.shape</code> should be <code>[number_of_samples, number_of_features]</code>).</p>\n<hr/>\n<h1><strong>Core of method</strong></h1>\n<p>The main idea is to <strong>normalize/standardize</strong> i.e. <code>μ = 0</code> and <code>σ = 1</code> your features/variables/columns of <code>X</code>, <em>individually</em>,  <strong>before</strong> applying any machine learning model.</p>\n<blockquote>\n<p><strong><code>StandardScaler()</code></strong> will <strong>normalize the features</strong> i.e. each\n<strong>column</strong> of X, <strong>INDIVIDUALLY</strong>, so that each column/feature/variable will have <code>μ = 0</code> and <code>σ = 1</code>.</p>\n</blockquote>\n<hr/>\n<p><strong>P.S:</strong> I find the most upvoted answer on this page, wrong.\nI am quoting \"each value in the dataset will have the sample mean value subtracted\" -- This is neither true nor correct.</p>\n<hr/>\n<p>See also: <a href=\"https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832?source=friends_link&amp;sk=54480b89c4ca00d80965d77b3baf8d7e\" rel=\"noreferrer\">How and why to Standardize your data: A python tutorial</a></p>\n<hr/>\n<h1><strong>Example with code</strong></h1>\n<pre><code>from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 4 samples/observations and 2 variables/features\ndata = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\nprint(data)\n[[0, 0],\n [1, 0],\n [0, 1],\n [1, 1]])\n\nprint(scaled_data)\n[[-1. -1.]\n [ 1. -1.]\n [-1.  1.]\n [ 1.  1.]]\n</code></pre>\n<p><strong>Verify that the mean of each feature (column) is 0:</strong></p>\n<pre><code>scaled_data.mean(axis = 0)\narray([0., 0.])\n</code></pre>\n<p><strong>Verify that the std of each feature (column) is 1:</strong></p>\n<pre><code>scaled_data.std(axis = 0)\narray([1., 1.])\n</code></pre>\n<hr/>\n<h1>Appendix: <strong>The maths</strong></h1>\n<p><a href=\"https://i.sstatic.net/obywE.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/obywE.png\"/></a></p>\n<hr/>\n<p><strong>UPDATE 08/2020</strong>: Concerning the input parameters <code>with_mean</code> and <code>with_std</code> to <code>False</code>/<code>True</code>, I have provided an answer here: <a href=\"https://stackoverflow.com/a/57381708/5025009\">StandardScaler difference between “with_std=False or True” and “with_mean=False or True”</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The idea behind <code>StandardScaler</code> is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.<br/>\nIn case of multivariate data, this is done feature-wise (in other words independently for each column of the data).<br/>\nGiven the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>StandardScaler performs the task of <strong>Standardization</strong>. Usually a dataset contains variables that are different in scale. For e.g. an Employee dataset will contain AGE column with values <strong>on scale 20-70</strong> and SALARY column with values <strong>on scale 10000-80000</strong>.<br> As these two columns are different in scale, they are Standardized to have common scale while building machine learning model.</br></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I try to understand LSTMs and how to build them with Keras. I found out, that there are principally the 4 modes to run a RNN (the 4 right ones in the picture)</p>\n<p><a href=\"https://i.sstatic.net/b4sus.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/b4sus.jpg\"/></a>\nImage source: <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"noreferrer\">Andrej Karpathy</a></p>\n<p>Now I wonder how a minimalistic code snippet for each of them would look like in Keras.\nSo something like</p>\n<pre><code>model = Sequential()\nmodel.add(LSTM(128, input_shape=(timesteps, data_dim)))\nmodel.add(Dense(1))\n</code></pre>\n<p>for each of the 4 tasks, maybe with a little bit of explanation.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So:</p>\n<ol>\n<li><p><strong>One-to-one</strong>: you could use a <code>Dense</code> layer as you are not processing sequences:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.add(Dense(output_size, input_shape=input_shape))\n</code></pre>\n</li>\n<li><p><strong>One-to-many</strong>: this option is not supported well as chaining models is not very easy in <code>Keras</code>, so the following version is the easiest one:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.add(RepeatVector(number_of_times, input_shape=input_shape))\nmodel.add(LSTM(output_size, return_sequences=True))\n</code></pre>\n</li>\n<li><p><strong>Many-to-one</strong>: actually, your code snippet is (almost) an example of this approach:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Sequential()\nmodel.add(LSTM(1, input_shape=(timesteps, data_dim)))\n</code></pre>\n</li>\n<li><p><strong>Many-to-many</strong>: This is the easiest snippet when the length of the input and output matches the number of recurrent steps:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Sequential()\nmodel.add(LSTM(1, input_shape=(timesteps, data_dim), return_sequences=True))\n</code></pre>\n</li>\n<li><p><strong>Many-to-many when number of steps differ from input/output length</strong>: this is freaky hard in Keras. There are no easy code snippets to code that.</p>\n</li>\n</ol>\n<p><strong>EDIT: Ad 5</strong></p>\n<p>In one of my recent applications, we implemented something which might be similar to <em>many-to-many</em> from the 4th image. In case you want to have a network with the following architecture (when an input is longer than the output):</p>\n<pre class=\"lang-py prettyprint-override\"><code>                                        O O O\n                                        | | |\n                                  O O O O O O\n                                  | | | | | | \n                                  O O O O O O\n</code></pre>\n<p>You could achieve this in the following manner:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Sequential()\nmodel.add(LSTM(1, input_shape=(timesteps, data_dim), return_sequences=True))\nmodel.add(Lambda(lambda x: x[:, -N:, :])) #Select last N from output\n</code></pre>\n<p>Where <code>N</code> is the number of last steps you want to cover (on image <code>N = 3</code>).</p>\n<p>From this point getting to:</p>\n<pre class=\"lang-py prettyprint-override\"><code>                                        O O O\n                                        | | |\n                                  O O O O O O\n                                  | | | \n                                  O O O \n</code></pre>\n<p>is as simple as artificial padding sequence of length <code>N</code> using e.g. with <code>0</code> vectors, in order to adjust it to an appropriate size.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Great Answer by @Marcin Możejko</p>\n<p>I would <strong>add the following to NR.5</strong> (many to many with different in/out length):</p>\n<p>A) as Vanilla LSTM</p>\n<pre><code>model = Sequential()\nmodel.add(LSTM(N_BLOCKS, input_shape=(N_INPUTS, N_FEATURES)))\nmodel.add(Dense(N_OUTPUTS))\n</code></pre>\n<p>B) as Encoder-Decoder LSTM</p>\n<pre><code>model.add(LSTM(N_BLOCKS, input_shape=(N_INPUTS, N_FEATURES))  \nmodel.add(RepeatVector(N_OUTPUTS))\nmodel.add(LSTM(N_BLOCKS, return_sequences=True))  \nmodel.add(TimeDistributed(Dense(1)))\nmodel.add(Activation('linear')) \n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 15:53:30Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/12952729/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I noticed that LSH seems a good way to find similar items with high-dimension properties.</p>\n<p>After reading the paper <a href=\"http://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf\" rel=\"noreferrer\">http://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf</a>, I'm still confused with those formulas.</p>\n<p>Does anyone know a blog or article that explains that the easy way?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The best tutorial I have seen for LSH is in the book: Mining of Massive Datasets.\nCheck Chapter 3 - Finding Similar Items\n<a href=\"http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf\" rel=\"noreferrer\">http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf</a></p>\n<p>Also I recommend the below slide:\n<a href=\"http://www.cs.jhu.edu/%7Evandurme/papers/VanDurmeLallACL10-slides.pdf\" rel=\"noreferrer\">http://www.cs.jhu.edu/%7Evandurme/papers/VanDurmeLallACL10-slides.pdf</a> .\nThe example in the slide helps me a lot in understanding the hashing for cosine similarity.</p>\n<p>I borrow two slides from <a href=\"http://www.cs.jhu.edu/~vandurme/papers/VanDurmeLallACL10-slides.pdf\" rel=\"noreferrer\">Benjamin Van Durme &amp; Ashwin Lall, ACL2010</a> and try to explain the intuitions of LSH Families for Cosine Distance a bit.\n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/xYCSJ.png\"/></p>\n<ul>\n<li>In the figure, there are two circles w/ <strong>red</strong> and <strong>yellow</strong> colored, representing two two-dimensional data points. We are trying to find their <a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">cosine similarity</a> using LSH.</li>\n<li>The gray lines are some uniformly randomly picked planes.</li>\n<li>Depending on whether the data point locates above or below a gray line, we mark this relation as 0/1. </li>\n<li>On the upper-left corner, there are two rows of white/black squares, representing the signature of the two data points respectively. Each square is corresponding to a bit 0(white) or 1(black). </li>\n<li>So once you have a pool of planes, you can encode the data points with their location respective to the planes. Imagine that when we have more planes in the pool, the angular difference encoded in the signature is closer to the actual difference. Because only planes that resides between the two points will give the two data different bit value. </li>\n</ul>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/27nSY.png\"/></p>\n<ul>\n<li>Now we look at the signature of the two data points. As in the example, we use only 6 bits(squares) to represent each data. This is the LSH hash for the original data we have.</li>\n<li>The hamming distance between the two hashed value is 1, because their signatures only differ by 1 bit.</li>\n<li>Considering the length of the signature, we can calculate their angular similarity as shown in the graph.</li>\n</ul>\n<p>I have some sample code (just 50 lines) in python here which is using cosine similarity. \n<a href=\"https://gist.github.com/94a3d425009be0f94751\" rel=\"noreferrer\">https://gist.github.com/94a3d425009be0f94751</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Tweets in vector space can be a great example of high dimensional data.</p>\n<p>Check out my blog post on applying Locality Sensitive Hashing to tweets to find similar ones. </p>\n<p><a href=\"http://micvog.com/2013/09/08/storm-first-story-detection/\" rel=\"noreferrer\">http://micvog.com/2013/09/08/storm-first-story-detection/</a></p>\n<p>And because one picture is a thousand words check the picture below:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/cKVUs.png\"/>\n<a href=\"http://micvog.files.wordpress.com/2013/08/lsh1.png\" rel=\"noreferrer\">http://micvog.files.wordpress.com/2013/08/lsh1.png</a></p>\n<p>Hope it helps.\n@mvogiatzis</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's a presentation from Stanford that explains it. It made a big difference for me. Part two is more about LSH, but part one covers it as well.</p>\n<p>A picture of the overview (There are much more in the slides):</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/HYPWE.png\"/></p>\n<p>Near Neighbor Search in High Dimensional Data - Part1:\n<a href=\"http://www.stanford.edu/class/cs345a/slides/04-highdim.pdf\" rel=\"noreferrer\">http://www.stanford.edu/class/cs345a/slides/04-highdim.pdf</a></p>\n<p>Near Neighbor Search in High Dimensional Data - Part2:\n<a href=\"http://www.stanford.edu/class/cs345a/slides/05-LSH.pdf\" rel=\"noreferrer\">http://www.stanford.edu/class/cs345a/slides/05-LSH.pdf</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What would cause a Convolutional Neural Network to diverge?</p>\n<p>Specifics:</p>\n<p>I am using Tensorflow's iris_training model with some of my own data and keep getting</p>\n<blockquote>\n<p>ERROR:tensorflow:Model diverged with loss = NaN.</p>\n<p>Traceback...</p>\n<p>tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training.</p>\n</blockquote>\n<p>Traceback originated with line:</p>\n<pre><code> tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                        hidden_units=[300, 300, 300],\n                                        #optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001),                                                          \n                                        n_classes=11,\n                                        model_dir=\"/tmp/iris_model\")\n</code></pre>\n<p>I've tried adjusting the optimizer, using a zero for learning rate, and using no optimizer.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are lots of things I have seen make a model diverge.</p>\n<ol>\n<li><p>Too high of a learning rate.  You can often tell if this is the case if the loss begins to increase and then diverges to infinity.</p>\n</li>\n<li><p>I am not to familiar with the DNNClassifier but I am guessing it uses the categorical cross entropy cost function.  This involves taking the log of the prediction which diverges as the prediction approaches zero.  That is why people usually add a small epsilon value to the prediction to prevent this divergence. I am guessing the DNNClassifier probably does this or uses the tensorflow opp for it.  Probably not the issue.</p>\n</li>\n<li><p>Other numerical stability issues can exist such as division by zero where adding the epsilon can help.  Another less obvious one if the square root whose derivative can diverge if not properly simplified when dealing with finite precision numbers. Yet again I doubt this is the issue in the case of the DNNClassifier.</p>\n</li>\n<li><p>You may have an issue with the input data.  Try calling <code>assert not np.any(np.isnan(x))</code> on the input data to make sure you are not introducing the nan.  Also make sure all of the target values are valid.  Finally, make sure the data is properly normalized. You probably want to have the pixels in the range [-1, 1] and not [0, 255].</p>\n</li>\n<li><p>The labels must be in the domain of the loss function, so if using a logarithmic-based loss function all labels must be non-negative (as noted by evan pu and the comments below).</p>\n</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're training for cross entropy, you want to add a small number like 1e-8 to your output probability.</p>\n<p>Because log(0) is negative infinity, when your model trained enough the output distribution will be very skewed, for instance say I'm doing a 4 class output, in the beginning my probability looks like</p>\n<pre><code>0.25 0.25 0.25 0.25\n</code></pre>\n<p>but toward the end the probability will probably look like</p>\n<pre><code>1.0 0 0 0\n</code></pre>\n<p>And you take a cross entropy of this distribution everything will explode. The fix is to artifitially add a small number to all the terms to prevent this.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In my case I got NAN when setting distant integer LABELs. ie:</p>\n<ul>\n<li>Labels [0..100] the training was ok,</li>\n<li>Labels [0..100] plus one additional label 8000, then I got NANs.</li>\n</ul>\n<p>So, not use a very distant Label.</p>\n<p>EDIT\nYou can see the effect in the following simple code:</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nimport numpy as np\n\nX=np.random.random(size=(20,5))\ny=np.random.randint(0,high=5, size=(20,1))\n\nmodel = Sequential([\n            Dense(10, input_dim=X.shape[1]),\n            Activation('relu'),\n            Dense(5),\n            Activation('softmax')\n            ])\nmodel.compile(optimizer = \"Adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"] )\n\nprint('fit model with labels in range 0..5')\nhistory = model.fit(X, y, epochs= 5 )\n\nX = np.vstack( (X, np.random.random(size=(1,5))))\ny = np.vstack( ( y, [[8000]]))\nprint('fit model with labels in range 0..5 plus 8000')\nhistory = model.fit(X, y, epochs= 5 )\n</code></pre>\n<p>The result shows the NANs after adding the label 8000:</p>\n<pre><code>fit model with labels in range 0..5\nEpoch 1/5\n20/20 [==============================] - 0s 25ms/step - loss: 1.8345 - acc: 0.1500\nEpoch 2/5\n20/20 [==============================] - 0s 150us/step - loss: 1.8312 - acc: 0.1500\nEpoch 3/5\n20/20 [==============================] - 0s 151us/step - loss: 1.8273 - acc: 0.1500\nEpoch 4/5\n20/20 [==============================] - 0s 198us/step - loss: 1.8233 - acc: 0.1500\nEpoch 5/5\n20/20 [==============================] - 0s 151us/step - loss: 1.8192 - acc: 0.1500\nfit model with labels in range 0..5 plus 8000\nEpoch 1/5\n21/21 [==============================] - 0s 142us/step - loss: nan - acc: 0.1429\nEpoch 2/5\n21/21 [==============================] - 0s 238us/step - loss: nan - acc: 0.2381\nEpoch 3/5\n21/21 [==============================] - 0s 191us/step - loss: nan - acc: 0.2381\nEpoch 4/5\n21/21 [==============================] - 0s 191us/step - loss: nan - acc: 0.2381\nEpoch 5/5\n21/21 [==============================] - 0s 188us/step - loss: nan - acc: 0.2381\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can't figure out how the <code>sklearn.pipeline.Pipeline</code> works exactly.</p>\n<p>There are a few explanation in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\" rel=\"noreferrer\">doc</a>. For example what do they mean by:</p>\n<blockquote>\n<p>Pipeline of transforms with a final estimator.</p>\n</blockquote>\n<p>To make my question clearer, what are <code>steps</code>? How do they work?</p>\n<p><strong>Edit</strong></p>\n<p>Thanks to the answers I can make my question clearer:</p>\n<p>When I call pipeline and pass, as steps, two transformers and one estimator, e.g:</p>\n<pre><code>pipln = Pipeline([(\"trsfm1\",transformer_1),\n                  (\"trsfm2\",transformer_2),\n                  (\"estmtr\",estimator)])\n</code></pre>\n<p>What happens when I call this?</p>\n<pre><code>pipln.fit()\nOR\npipln.fit_transform()\n</code></pre>\n<p>I can't figure out how an estimator can be a transformer and how a transformer can be fitted.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Transformer</strong> in scikit-learn - some class that have fit and transform method, or fit_transform method.</p>\n<p><strong>Predictor</strong> - some class that has fit and predict methods, or fit_predict method.</p>\n<p><strong>Pipeline</strong> is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator.</p>\n<p><a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\" rel=\"noreferrer\">Here</a> is a good example of Pipeline usage.\nPipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and now you can do something like:</p>\n<pre><code>    vect = CountVectorizer()\n    tfidf = TfidfTransformer()\n    clf = SGDClassifier()\n\n    vX = vect.fit_transform(Xtrain)\n    tfidfX = tfidf.fit_transform(vX)\n    predicted = clf.fit_predict(tfidfX)\n\n    # Now evaluate all steps on test set\n    vX = vect.fit_transform(Xtest)\n    tfidfX = tfidf.fit_transform(vX)\n    predicted = clf.fit_predict(tfidfX)\n</code></pre>\n<p>With just:</p>\n<pre><code>pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\npredicted = pipeline.fit(Xtrain).predict(Xtrain)\n# Now evaluate all steps on test set\npredicted = pipeline.predict(Xtest)\n</code></pre>\n<p>With pipelines you can easily perform a grid-search over set of parameters for each step of this meta-estimator. As described in the link above. All steps except last one must be transforms, last step can be transformer or predictor.\n<strong>Answer to edit</strong>:\nWhen you call <code>pipln.fit()</code> - each transformer inside pipeline will be fitted on outputs of previous transformer (First transformer is learned on raw dataset).  Last estimator may be transformer or predictor, you can call fit_transform() on pipeline only if your last estimator is transformer (that implements fit_transform, or transform and fit methods separately), you can call fit_predict() or predict() on pipeline only if your last estimator is predictor. So you just can't call fit_transform or transform on pipeline, last step of which is predictor.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think that M0rkHaV has the right idea. Scikit-learn's pipeline class is a useful tool for encapsulating multiple different transformers alongside an estimator into one object, so that you only have to call your important methods once (<code>fit()</code>, <code>predict()</code>, etc). Let's break down the two major components:</p>\n<ol>\n<li><p><strong>Transformers</strong> are classes that implement both <code>fit()</code> and <code>transform()</code>. You might be familiar with some of the sklearn preprocessing tools, like <code>TfidfVectorizer</code> and <code>Binarizer</code>. If you look at the docs for these preprocessing tools, you'll see that they implement both of these methods. What I find pretty cool is that some estimators can also be used as transformation steps, e.g. <code>LinearSVC</code>!</p></li>\n<li><p><strong>Estimators</strong> are classes that implement both <code>fit()</code> and <code>predict()</code>. You'll find that many of the classifiers and regression models implement both these methods, and as such you can readily test many different models. It is possible to use another transformer as the final estimator (i.e., it doesn't necessarily implement <code>predict()</code>, but definitely implements <code>fit()</code>). All this means is that you wouldn't be able to call <code>predict()</code>.</p></li>\n</ol>\n<p>As for your edit: let's go through a text-based example. Using LabelBinarizer, we want to turn a list of labels into a list of binary values. </p>\n<pre><code>bin = LabelBinarizer()  #first we initialize\n\nvec = ['cat', 'dog', 'dog', 'dog'] #we have our label list we want binarized\n</code></pre>\n<p>Now, when the binarizer is fitted on some data, it will have a structure called <code>classes_</code> that contains the unique classes that the transformer 'knows' about. Without calling <code>fit()</code> the binarizer has no idea what the data looks like, so calling <code>transform()</code> wouldn't make any sense. This is true if you print out the list of classes before trying to fit the data.</p>\n<pre><code>print bin.classes_  \n</code></pre>\n<p>I get the following error when trying this:</p>\n<pre><code>AttributeError: 'LabelBinarizer' object has no attribute 'classes_'\n</code></pre>\n<p>But when you fit the binarizer on the <code>vec</code> list:</p>\n<pre><code>bin.fit(vec)\n</code></pre>\n<p>and try again </p>\n<pre><code>print bin.classes_\n</code></pre>\n<p>I get the following:</p>\n<pre><code>['cat' 'dog']\n\n\nprint bin.transform(vec)\n</code></pre>\n<p>And now, after calling transform on the <code>vec</code> object, we get the following:</p>\n<pre><code>[[0]\n [1]\n [1]\n [1]]\n</code></pre>\n<p>As for estimators being used as transformers, let us use the <code>DecisionTree</code> classifier as an example of a feature-extractor. Decision Trees are great for a lot of reasons, but for our purposes, what's important is that they have the ability to rank features that the <strong>tree</strong> found useful for predicting. When you call <code>transform()</code> on a Decision Tree, it will take your input data and find what <strong>it</strong> thinks are the most important features. So you can think of it transforming your data matrix (n rows by m columns) into a smaller matrix (n rows by k columns), where the k columns are the k most important features that the Decision Tree found.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>ML algorithms typically process tabular data. You may want to do preprocessing and post-processing of this data before and after your ML algorithm. A pipeline is a way to chain those data processing steps.</p>\n</blockquote>\n<h1><a href=\"https://www.neuraxio.com/en/blog/neuraxle/2019/10/26/neat-machine-learning-pipelines.html\" rel=\"noreferrer\">What are ML pipelines and how do they work?</a></h1>\n<p>A pipeline is a series of steps in which data is transformed. It comes from the old \"pipe and filter\" design pattern (for instance, you could think of unix bash commands with pipes “|” or redirect operators “&gt;”). However, pipelines are objects in the code. Thus, you may have a class for each filter (a.k.a. each pipeline step), and then another class to combine those steps into the final pipeline. Some pipelines may combine other pipelines in series or in parallel, have multiple inputs or outputs, and so on. We like to view <a href=\"https://www.neuraxle.org/\" rel=\"noreferrer\">Pipelining Machine Learning</a> as:</p>\n<ul>\n<li><a href=\"https://learn.microsoft.com/en-us/azure/architecture/patterns/pipes-and-filters\" rel=\"noreferrer\">Pipe and filters</a>. The pipeline’s steps process data, and they manage their inner state which can be learned from the data.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Composite_pattern\" rel=\"noreferrer\">Composites</a>. Pipelines can be nested: for example a whole pipeline can be treated as a single pipeline step in another pipeline. A pipeline step is not necessarily a pipeline, but a pipeline is itself at least a pipeline step by definition.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" rel=\"noreferrer\">Directed Acyclic Graphs (DAG)</a>. A pipeline step's output may be sent to many other steps, and then the resulting outputs can be recombined, and so on. Side note: despite pipelines are acyclic, they can process multiple items one by one, and if their state change (e.g.: using the fit_transform method each time), then they can be viewed as recurrently unfolding through time, keeping their states (think like an RNN). That’s an interesting way to see pipelines for doing online learning when putting them in production and training them on more data.</li>\n</ul>\n<h2>Methods of a Scikit-Learn Pipeline</h2>\n<p>Pipelines (or steps in the pipeline) <strong>must have those two methods</strong>:</p>\n<ul>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.fit\" rel=\"noreferrer\">fit</a>” to learn on the data and acquire state (e.g.: neural network’s neural weights are such state)</li>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.transform\" rel=\"noreferrer\">transform</a>\" (or \"predict\") to actually process the data and generate a prediction.</li>\n</ul>\n<p>It's also possible to call this method to chain both:</p>\n<ul>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.fit_transform\" rel=\"noreferrer\">fit_transform</a>” to fit and then transform the data, but in one pass, which allows for potential code optimizations when the two methods must be done one after the other directly.</li>\n</ul>\n<h1><a href=\"https://www.neuraxio.com/en/blog/scikit-learn/2020/01/03/what-is-wrong-with-scikit-learn.html\" rel=\"noreferrer\">Problems of the sklearn.pipeline.Pipeline class</a></h1>\n<blockquote>\n<p>Scikit-Learn’s “pipe and filter” design pattern is simply beautiful. But how to use it for Deep Learning, AutoML, and complex production-level pipelines?</p>\n</blockquote>\n<p>Scikit-Learn had its first release in 2007, which was a <a href=\"https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources#trends\" rel=\"noreferrer\">pre deep learning era</a>. However, it’s one of the most known and adopted machine learning library, and is still growing. On top of all, it uses the Pipe and Filter design pattern as a software architectural style - it’s what makes Scikit-Learn so fabulous, added to the fact it provides algorithms ready for use. However, it has massive issues when it comes to do the following, which we should be able to do in 2020 already:</p>\n<ul>\n<li><strong>Automatic Machine Learning (AutoML),</strong></li>\n<li><strong>Deep Learning Pipelines,</strong></li>\n<li><strong>More complex Machine Learning pipelines.</strong></li>\n</ul>\n<h2>Solutions that we’ve Found to Those Scikit-Learn's Problems</h2>\n<p>For sure, Scikit-Learn is very convenient and well-built. However, it needs a refresh. Here are our solutions with <a href=\"https://github.com/Neuraxio/Neuraxle\" rel=\"noreferrer\">Neuraxle</a> to make Scikit-Learn fresh and useable within modern computing projects!</p>\n<ul>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#inability-to-reasonably-do-automatic-machine-learning-automl\" rel=\"noreferrer\">Inability to Reasonably do Automatic Machine Learning (AutoML)</a>\n<ul>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-defining-the-search-space-hyperparameter-distributions\" rel=\"noreferrer\">Problem: Defining the Search Space (Hyperparameter Distributions)</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-defining-hyperparameters-in-the-constructor-is-limiting\" rel=\"noreferrer\">Problem: Defining Hyperparameters in the Constructor is Limiting</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-different-train-and-test-behavior\" rel=\"noreferrer\">Problem: Different Train and Test Behavior</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-you-trained-a-pipeline-and-you-want-feedback-statistics-on-its-learning\" rel=\"noreferrer\">Problem: You trained a Pipeline and You Want Feedback on its Learning.</a></li>\n</ul>\n</li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#inability-to-reasonably-do-deep-learning-pipelines\" rel=\"noreferrer\">Inability to Reasonably do Deep Learning Pipelines</a>\n<ul>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-scikit-learn-hardly-allows-for-mini-batch-gradient-descent-incremental-fit\" rel=\"noreferrer\">Problem: Scikit-Learn Hardly Allows for Mini-Batch Gradient Descent (Incremental Fit)</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-initializing-the-pipeline-and-deallocating-resources\" rel=\"noreferrer\">Problem: Initializing the Pipeline and Deallocating Resources</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-it-is-difficult-to-use-other-deep-learning-dl-libraries-in-scikit-learn\" rel=\"noreferrer\">Problem: It is Difficult to Use Other Deep Learning (DL) Libraries in Scikit-Learn</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-the-ability-to-transform-output-labels\" rel=\"noreferrer\">Problem: The Ability to Transform Output Labels</a></li>\n</ul>\n</li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#not-ready-for-production-nor-for-complex-pipelines\" rel=\"noreferrer\">Not ready for Production nor for Complex Pipelines</a>\n<ul>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-processing-3d-4d-or-nd-data-in-your-pipeline-with-steps-made-for-lower-dimensionnal-data\" rel=\"noreferrer\">Problem: Processing 3D, 4D, or ND Data in your Pipeline with Steps Made for Lower-Dimensional Data</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-modify-a-pipeline-along-the-way-such-as-for-pre-training-or-fine-tuning\" rel=\"noreferrer\">Problem: Modify a Pipeline Along the Way, such as for Pre-Training or Fine-Tuning</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-getting-model-attributes-from-scikit-learn-pipeline\" rel=\"noreferrer\">Problem: Getting Model Attributes from Scikit-Learn Pipeline</a></li>\n<li><a href=\"https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-you-can-t-parallelize-nor-save-pipelines-using-steps-that-can-t-be-serialized-as-is-by-joblib\" rel=\"noreferrer\">Problem: You can't Parallelize nor Save Pipelines Using Steps that Can't be Serialized \"as-is\" by Joblib</a></li>\n</ul>\n</li>\n</ul>\n<h2>Additional pipeline methods and features offered through <a href=\"https://github.com/Neuraxio/Neuraxle\" rel=\"noreferrer\">Neuraxle</a></h2>\n<p>Note: if a step of a pipeline doesn’t need to have one of the fit or transform methods, it could inherit from <a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.NonFittableMixin\" rel=\"noreferrer\">NonFittableMixin</a> or <a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.NonTransformableMixin\" rel=\"noreferrer\">NonTransformableMixin</a> to be provided a default implementation of one of those methods to do nothing.</p>\n<p>As a starter, it is possible for pipelines or their steps to also <strong>optionally define those methods</strong>:</p>\n<ul>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.setup\" rel=\"noreferrer\">setup</a>” which will call the “setup” method on each of its step. For instance, if a step contains a TensorFlow, PyTorch, or Keras neural network, the steps could create their neural graphs and register them to the GPU in the “setup” method before fit. It is discouraged to create the graphs directly in the constructors of the steps for several reasons, such as if the steps are copied before running many times with different hyperparameters within an Automatic Machine Learning algorithm that searches for the best hyperparameters for you.</li>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.teardown\" rel=\"noreferrer\">teardown</a>”, which is the opposite of the “setup” method: it clears resources.</li>\n</ul>\n<p>The <strong>following methods are provided by default</strong> to allow for managing hyperparameters:</p>\n<ul>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.get_hyperparams\" rel=\"noreferrer\">get_hyperparams</a>” will return you a dictionary of the hyperparameters. If your pipeline contains more pipelines (nested pipelines), then the hyperparameter’ keys are chained with double underscores “__” separators.</li>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.set_hyperparams\" rel=\"noreferrer\">set_hyperparams</a>” will allow you to set new hyperparameters in the same format of when you get them.</li>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.get_hyperparams_space\" rel=\"noreferrer\">get_hyperparams_space</a>” allows you to get the space of hyperparameter, which will be not empty if you defined one. So, the only difference with “get_hyperparams” here is that you’ll get statistic distributions as values instead of a precise value. For instance, one hyperparameter for the number of layers could be a <code>RandInt(1, 3)</code> which means 1 to 3 layers. You can call <code>.rvs()</code> on this dict to pick a value randomly and send it to “set_hyperparams” to try training on it.</li>\n<li>“<a href=\"https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.set_hyperparams_space\" rel=\"noreferrer\">set_hyperparams_space</a>” can be used to set a new space using the same hyperparameter distribution classes as in “get_hyperparams_space”.</li>\n</ul>\n<hr/>\n<p>For more info on our suggested solutions, read the entries in the big list with links above.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm learning different methods to convert categorical variables to numeric for machine-learning classifiers.  I came across the <code>pd.get_dummies</code> method and <code>sklearn.preprocessing.OneHotEncoder()</code> and I wanted to see how they differed in terms of performance and usage. </p>\n<p>I found a tutorial on how to use <code>OneHotEncoder()</code> on <a href=\"https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/\" rel=\"noreferrer\">https://xgdgsc.wordpress.com/2015/03/20/note-on-using-onehotencoder-in-scikit-learn-to-work-on-categorical-features/</a> since the <code>sklearn</code> documentation wasn't too helpful on this feature. I have a feeling I'm not doing it correctly...but</p>\n<p><strong>Can some explain the pros and cons of using <code>pd.dummies</code> over <code>sklearn.preprocessing.OneHotEncoder()</code> and vice versa?</strong> I know that <code>OneHotEncoder()</code> gives you a sparse matrix but other than that I'm not sure how it is used and what the benefits are over the <code>pandas</code> method.  Am I using it inefficiently? </p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nsns.set()\n\n%matplotlib inline\n\n#Iris Plot\niris = load_iris()\nn_samples, m_features = iris.data.shape\n\n#Load Data\nX, y = iris.data, iris.target\nD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))\n\nDF_data = pd.DataFrame(X,columns=iris.feature_names)\nDF_data[\"target\"] = pd.Series(y).map(D_target_dummy)\n#sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n#0                  5.1               3.5                1.4               0.2   \n#1                  4.9               3.0                1.4               0.2   \n#2                  4.7               3.2                1.3               0.2   \n#3                  4.6               3.1                1.5               0.2   \n#4                  5.0               3.6                1.4               0.2   \n#5                  5.4               3.9                1.7               0.4   \n\nDF_dummies = pd.get_dummies(DF_data[\"target\"])\n#setosa  versicolor  virginica\n#0         1           0          0\n#1         1           0          0\n#2         1           0          0\n#3         1           0          0\n#4         1           0          0\n#5         1           0          0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ndef f1(DF_data):\n    Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder()\n    DF_data[\"Dummies\"] = Enc_label.fit_transform(DF_data[\"target\"])\n    DF_dummies2 = pd.DataFrame(Enc_ohe.fit_transform(DF_data[[\"Dummies\"]]).todense(), columns = Enc_label.classes_)\n    return(DF_dummies2)\n\n%timeit pd.get_dummies(DF_data[\"target\"])\n#1000 loops, best of 3: 777 µs per loop\n\n%timeit f1(DF_data)\n#100 loops, best of 3: 2.91 ms per loop\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>For machine learning, you almost definitely want to use <code>sklearn.OneHotEncoder</code>.</strong> For other tasks like simple analyses, you might be able to use <code>pd.get_dummies</code>, which is a bit more convenient.</p>\n<p>Note that <code>sklearn.OneHotEncoder</code> has been updated in the latest version so that it <strong>does accept strings</strong> for categorical variables, as well as integers.</p>\n<p>The crux of it is that the <code>sklearn</code> encoder creates a function which <strong>persists</strong> and can <strong>then be applied to new data sets which use the same categorical variables, with consistent results</strong>.</p>\n<pre><code>from sklearn.preprocessing import OneHotEncoder\n\n# Create the encoder.\nencoder = OneHotEncoder(handle_unknown=\"ignore\")\nencoder.fit(X_train)    # Assume for simplicity all features are categorical.\n\n# Apply the encoder.\nX_train = encoder.transform(X_train)\nX_test = encoder.transform(X_test)\n</code></pre>\n<p>Note how we apply the same encoder we created via <code>X_train</code> to the new data set <code>X_test</code>.</p>\n<p>Consider what happens if <code>X_test</code> contains different levels than <code>X_train</code> for one of its variables. For example, let's say <code>X_train[\"color\"]</code> contains only <code>\"red\"</code> and <code>\"green\"</code>, but in addition to those, <code>X_test[\"color\"]</code> sometimes contains <code>\"blue\"</code>.</p>\n<p>If we use <code>pd.get_dummies</code>, <code>X_test</code> will end up with an additional <code>\"color_blue\"</code> column which <code>X_train</code> doesn't have, and the inconsistency will probably break our code later on, especially if we are feeding <code>X_test</code> to an <code>sklearn</code> model which we trained on <code>X_train</code>.</p>\n<p>And if we want to process the data like this in production, where we're receiving a single example at a time, <code>pd.get_dummies</code> won't be of use.</p>\n<p>With <code>sklearn.OneHotEncoder</code> on the other hand, once we've created the encoder, we can reuse it to produce the same output every time, with columns only for <code>\"red\"</code> and <code>\"green\"</code>. And we can explicitly control what happens when it encounters the new level <code>\"blue\"</code>: if we think that's impossible, then we can tell it to throw an error with <code>handle_unknown=\"error\"</code>; otherwise we can tell it to continue and simply set the red and green columns to 0, with <code>handle_unknown=\"ignore\"</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>OneHotEncoder</code> cannot process string values directly. If your nominal features are strings, then you need to first map them into integers.</p>\n<p><code>pandas.get_dummies</code> is kind of the opposite. By default, it only converts string columns into one-hot representation, unless columns are specified. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I really like Carl's answer and upvoted it.  I will just expand Carl's example a bit so that more people hopefully will appreciate that pd.get_dummies can handle unknown.  The two examples below shows that pd.get_dummies can accomplish the same thing in handling unknown as OHE . </p>\n<pre><code># data is from @dzieciou's comment above\n&gt;&gt;&gt; data =pd.DataFrame(pd.Series(['good','bad','worst','good', 'good', 'bad']))\n# new_data has two values that data does not have. \n&gt;&gt;&gt; new_data= pd.DataFrame(\npd.Series(['good','bad','worst','good', 'good', 'bad','excellent', 'perfect']))\n</code></pre>\n<h2>Using pd.get_dummies</h2>\n<pre><code>&gt;&gt;&gt; df = pd.get_dummies(data)\n&gt;&gt;&gt; col_list = df.columns.tolist()\n&gt;&gt;&gt; print(df)\n   0_bad  0_good  0_worst\n0      0       1        0\n1      1       0        0\n2      0       0        1\n3      0       1        0\n4      0       1        0\n5      1       0        0\n6      0       0        0\n7      0       0        0\n\n&gt;&gt;&gt; new_df = pd.get_dummies(new_data)\n# handle unknow by using .reindex and .fillna()\n&gt;&gt;&gt; new_df = new_df.reindex(columns=col_list).fillna(0.00)\n&gt;&gt;&gt; print(new_df)\n#    0_bad  0_good  0_worst\n# 0      0       1        0\n# 1      1       0        0\n# 2      0       0        1\n# 3      0       1        0\n# 4      0       1        0\n# 5      1       0        0\n# 6      0       0        0\n# 7      0       0        0\n</code></pre>\n<h2>Using OneHotEncoder</h2>\n<pre><code>&gt;&gt;&gt; encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n&gt;&gt;&gt; encoder.fit(data)\n&gt;&gt;&gt; encoder.transform(new_data)\n# array([[0., 1., 0.],\n#        [1., 0., 0.],\n#        [0., 0., 1.],\n#        [0., 1., 0.],\n#        [0., 1., 0.],\n#        [1., 0., 0.],\n#        [0., 0., 0.],\n#        [0., 0., 0.]])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is <a href=\"/help/closed-questions\">not about programming or software development</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about <a href=\"/help/on-topic\">a specific programming problem, a software algorithm, or software tools primarily used by programmers</a>. If you believe the question would be on-topic on <a href=\"https://stackexchange.com/sites\">another Stack Exchange site</a>, you can leave a comment to explain where the question may be able to be answered.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2024-10-08 10:35:35Z\">1 hour ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community is reviewing whether to reopen this question as of <span class=\"relativetime\" title=\"2024-10-08 10:40:10Z\">1 hour ago</span>.</p>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/6542274/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm currently trying to get an ANN to play a video game and  and I was hoping to get some help from the wonderful community here.</p>\n<p>I've settled on Diablo 2. Game play is thus in real-time and from an isometric viewpoint, with the player controlling a single avatar whom the camera is centered on. </p>\n<p>To make things concrete, the task is to get your character x experience points without having its health drop to 0, where experience point are gained through killing monsters. Here is an example of the gameplay:</p>\n<p><img alt=\"here\" src=\"https://i.sstatic.net/2NslT.jpg\"/></p>\n<p>Now, since I want the net to operate based solely on the information it gets from the pixels on the screen, it must learn a very rich representation in order to play efficiently, since this would presumably require it to know (implicitly at least) how divide the game world up into objects and how to interact with them.</p>\n<p>And all of this information must be taught to the net somehow. I can't for the life of me think of how to train this thing. My only idea is have a separate program visually extract something innately good/bad in the game (e.g. health, gold, experience) from the screen, and then use that stat in a reinforcement learning procedure. I think that will be <em>part</em> of the answer, but I don't think it'll be enough; there are just too many levels of abstraction from raw visual input to goal-oriented behavior for such limited feedback to train a net within my lifetime.</p>\n<p>So, my question: what other ways can you think of to train a net to do at least some part of this task? preferably without making thousands of labeled examples.</p>\n<p>Just for a little more direction: I'm looking for some other sources of reinforcement learning and/or any unsupervised methods for extracting useful information in this setting. Or a supervised algorithm if you can think of a way of getting labeled data out of a game world without having to manually label it.</p>\n<p><strong>UPDATE(04/27/12):</strong></p>\n<p>Strangely, I'm still working on this and seem to be making progress. The biggest secret to getting a ANN controller to work is to use the most advanced ANN architectures appropriate to the task. Hence I've been using a <a href=\"http://www.scholarpedia.org/article/Deep_belief_networks\" rel=\"noreferrer\">deep belief net</a> composed of factored <a href=\"http://cs.nyu.edu/~gwtaylor/thesis/4/\" rel=\"noreferrer\">conditional restricted Boltzmann machines</a> that I've trained in an unsupervised manner (on video of me playing the game) before fine tuning with <a href=\"http://www.stanford.edu/group/pdplab/pdphandbook/handbookch10.html#x26-1290009\" rel=\"noreferrer\">temporal difference back-propagation</a> (i.e. reinforcement learning with standard feed-forward ANNs).</p>\n<p><em>Still looking for more valuable input though, especially on the problem of action selection in real-time and how to encode color images for ANN processing :-)</em> </p>\n<p><strong>UPDATE(10/21/15):</strong></p>\n<p>Just remembered I asked this question back-in-the-day, and thought I should mention that this is no longer a crazy idea. Since my last update, DeepMind published their nature <a href=\"http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html\" rel=\"noreferrer\">paper on getting neural networks to play Atari games from visual inputs</a>. Indeed, the  only thing preventing me from using their architecture to play, a limited subset, of Diablo 2 is the lack of access to the underlying game engine. Rendering to the screen and then redirecting it to the network is just far too slow to train in a reasonable amount of time. Thus we probably won't see this sort of bot playing Diablo 2 anytime soon, but only because it'll be playing something either open-source or with API access to the rendering target. (Quake perhaps?)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can see that you are worried about how to train the ANN, but <strong>this project hides a complexity</strong> that you might not be aware of. Object/character recognition on computer games through <em>image processing</em> it's a highly challenging task (not say <em>crazy</em> for FPS and RPG games). I don't doubt of your skills and I'm also not saying it can't be done, but you can easily spend 10x more time working on recognizing stuff than implementing the ANN itself (assuming you already have experience with <em>digital image processing</em> techniques).</p>\n<p>I think your idea is very <strong>interesting and</strong> also very <strong>ambitious</strong>. At this point you might want to reconsider it. I sense that this project is something you are planning for the university, so if the focus of the work is really ANN you should probably pick another game, something more simple.</p>\n<p>I remember that someone else came looking for tips on a <a href=\"https://stackoverflow.com/q/4751091/176769\">different but somehow similar project</a> not too long ago. It's worth checking it out.</p>\n<p>On the other hand, there might be better/easier approaches for identifying objects in-game if you're accepting suggestions. But first, let's call this project for what you want it to be: a <strong>smart-bot</strong>. </p>\n<p><strong>One method</strong> for implementing bots <strong>accesses the memory of the game client</strong> to find relevant information, such as the location of the character on the screen and it's health. Reading computer memory is trivial, but figuring out exactly where in memory to look for is not. Memory scanners like <a href=\"http://www.cheatengine.org/index.php\" rel=\"noreferrer\">Cheat Engine</a> can be very helpful for this.</p>\n<p><strong>Another method</strong>, which works under the game, involves manipulating rendering information. All objects of the game must be rendered to the screen. This means that the locations of all 3D objects will eventually be sent to the video card for processing. Be ready for some serious debugging.</p>\n<p>In this answer I briefly described 2 methods to accomplish what you want through image processing. If you are interested in them you can find more about them on <a href=\"http://www.exploitingonlinegames.com/\" rel=\"noreferrer\">Exploiting Online Games</a> (chapter 6), an excellent book on the subject.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>UPDATE 2018-07-26:</strong> That's it! We are now approaching the point where this kind of game will be solvable! Using OpenAI and based on the game DotA 2, a team could make an AI that can <a href=\"https://www.youtube.com/watch?v=yEOEqaEgu94\" rel=\"noreferrer\">beat semi-professional gamers in a 5v5 game</a>. If you know DotA 2, you know this game is quite similar to Diablo-like games in terms of mechanics, but one could argue that it is even more complicated because of the team play.</p>\n<p>As expected, this was achieved thanks to the latest advances in reinforcement learning with deep learning, and using open game frameworks like OpenAI which eases the development of an AI since you get a neat API and also because you can accelerate the game (the AI played the equivalent of 180 years of gameplay against itself everyday!).</p>\n<p><a href=\"https://blog.openai.com/openai-five-benchmark/\" rel=\"noreferrer\">On the 5th of August 2018 (in 10 days!)</a>, it is planned to pit this AI against top DotA 2 gamers. If this works out, expect a big revolution, maybe not as mediatized as the solving of the Go game, but it will nonetheless be a huge milestone for games AI!</p>\n<p><strong>UPDATE 2017-01:</strong> The field is moving very fast since AlphaGo's success, and there are new frameworks to facilitate the development of machine learning algorithms on games almost every months. Here is a list of the latest ones I've found:</p>\n<ul>\n<li><a href=\"https://openai.com/blog/universe/\" rel=\"noreferrer\">OpenAI's Universe</a>: a platform to <strong>play virtually any game using machine learning</strong>. The API is in Python, and it runs the games behind a VNC remote desktop environment, so it can capture the images of any game! You can probably use Universe to play Diablo II through a machine learning algorithm!</li>\n<li><a href=\"https://openai.com/blog/openai-gym-beta/\" rel=\"noreferrer\">OpenAI's Gym</a>: Similar to Universe but targeting reinforcement learning algorithms specifically (so it's kind of a generalization of the framework used by AlphaGo but to a lot more games). There is <a href=\"https://www.udemy.com/artificial-intelligence-az/\" rel=\"noreferrer\">a course on Udemy</a> covering the application of machine learning to games like breakout or Doom using OpenAI Gym.</li>\n<li><a href=\"https://github.com/TorchCraft/TorchCraft\" rel=\"noreferrer\">TorchCraft</a>: a bridge between <a href=\"http://torch.ch/\" rel=\"noreferrer\">Torch</a> (machine learning framework) and StarCraft: Brood War.</li>\n<li><a href=\"https://github.com/sentdex/pygta5\" rel=\"noreferrer\">pyGTA5</a>: a project to build self-driving cars in GTA5 using only screen captures (with lots of <a href=\"https://www.youtube.com/watch?v=edWI4ZnWUGg\" rel=\"noreferrer\">videos online</a>).</li>\n</ul>\n<p>Very exciting times!</p>\n<p><strong>IMPORTANT UPDATE (2016-06):</strong> As noted by OP, this problem of training artificial networks to play games using only visual inputs is now being tackled by several serious institutions, with quite promising results, such as <a href=\"https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner\" rel=\"noreferrer\">DeepMind Deep-Qlearning-Network (DQN)</a>.</p>\n<p>And now, if you want to get to take on the next level challenge, you can use one of the various <strong>AI vision game development platforms</strong> such as <a href=\"https://github.com/Marqt/ViZDoom\" rel=\"noreferrer\">ViZDoom</a>, a highly optimized platform (7000 fps) to train networks to play Doom using only visual inputs:</p>\n<blockquote>\n<p>ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.\n  ViZDoom is based on ZDoom to provide the game mechanics.</p>\n</blockquote>\n<p>And the results are quite amazing, <a href=\"http://vizdoom.cs.put.edu.pl/\" rel=\"noreferrer\">see the videos on their webpage</a> and the <a href=\"http://vizdoom.cs.put.edu.pl/tutorial\" rel=\"noreferrer\">nice tutorial</a> (in Python) here!</p>\n<p>There is also a similar project for Quake 3 Arena, called <a href=\"http://www.cs.rochester.edu/trac/quagents\" rel=\"noreferrer\">Quagents</a>, which also provides easy API access to underlying game data, but you can scrap it and just use screenshots and the API only to control your agent.</p>\n<p>Why is such a platform useful if we only use screenshots? Even if you don't access underlying game data, such a platform provide:</p>\n<ul>\n<li><strong>high performance</strong> implementation of games (you can generate more data/plays/learning generations with less time so that your learning algorithms can converge faster!).</li>\n<li>a simple and <strong>responsive API</strong> to control your agents (ie, if you try to use human inputs to control a game, some of your commands may be lost, so you'd also deal with unreliability of your outputs...).</li>\n<li>easy setup of <strong>custom scenarios</strong>.</li>\n<li><strong>customizable rendering</strong> (can be useful to \"simplify\" the images you get to ease processing)</li>\n<li><strong>synchronized (\"turn-by-turn\") play</strong> (so you don't need your algorithm to work in realtime at first, that's a huge complexity reduction).</li>\n<li>additional convenience features such as crossplatform compatibility, retrocompatibility (you don't risk your bot not working with the game anymore when there is a new game update), etc.</li>\n</ul>\n<p>To summarize, the great thing about these platforms is that <strong>they alleviate much of the previous technical issues</strong> you had to deal with (how to manipulate game inputs, how to setup scenarios, etc.) so that <strong>you just have to deal with the learning algorithm</strong> itself.</p>\n<p>So now, get to work and make us the best AI visual bot ever ;)</p>\n<hr/>\n<p><strong>Old post</strong> describing the technical issues of developping an AI relying only on visual inputs:</p>\n<p>Contrary to some of my colleagues above, I do not think this problem is intractable. But it surely is a hella hard one!</p>\n<p>The first problem as pointed out above is that of the <strong>representation of the state of the game</strong>: you can't represent the full state with just a single image, you need to maintain some kind of <strong>memorization</strong> (health but also objects equipped and items available to use, quests and goals, etc.). To fetch such informations you have two ways: either by directly accessing the game data, which is the most reliable and easy; or either you can create an abstract representation of these informations by implementing some simple procedures (open inventory, take a screenshot, extract the data). Of course, extracting data from a screenshot will either have you to put in some supervised procedure (that you define completely) or unsupervised (via a machine learning algorithm, but then it'll scale up a lot the complexity...). For unsupervised machine learning, you will need to use a quite recent kind of algorithms called structural learning algorithms (which learn the structure of data rather than how to classify them or predict a value). One such algorithm is the Recursive Neural Network (not to confuse with Recurrent Neural Network) by Richard Socher: <a href=\"http://techtalks.tv/talks/54422/\" rel=\"noreferrer\">http://techtalks.tv/talks/54422/</a></p>\n<p>Then, another problem is that even when you have fetched all the data you need, the game is only <strong>partially observable</strong>. Thus you need to inject an abstract model of the world and feed it with processed information from the game, for example the location of your avatar, but also the location of quest items, goals and enemies outside the screen. You may maybe look into Mixture Particle Filters by Vermaak 2003 for this.</p>\n<p>Also, you need to have an <strong>autonomous agent</strong>, with <strong>goals</strong> dynamically generated. A well-known architecture you can try is BDI agent, but you will probably have to tweak it for this architecture to work in your practical case. As an alternative, there is also the Recursive Petri Net, which you can probably combine with all kinds of variations of the petri nets to achieve what you want since it is a very well studied and flexible framework, with great formalization and proofs procedures.</p>\n<p>And at last, even if you do all the above, you will need to find a way to emulate the game in <strong>accelerated speed</strong> (using a video may be nice, but the problem is that your algorithm will only spectate without control, and being able to try for itself is very important for learning). Indeed, it is well-known that current state-of-the-art algorithm takes a lot more time to learn the same thing a human can learn (even more so with reinforcement learning), thus if can't speed up the process (ie, if you can't speed up the game time), your algorithm won't even converge in a single lifetime...</p>\n<p>To conclude, what you want to achieve here is at <strong>the limit (and maybe a bit beyond) of current state-of-the-art algorithms</strong>. I think it may be possible, but even if it is, <strong>you are going to spend a hella lot of time</strong>, because this is not a theoretical problem but a <strong>practical problem</strong> you are approaching here, and thus you need to implement and <strong>combine a lot of different AI approaches</strong> in order to solve it.</p>\n<p>Several decades of research with a whole team working on it would may not suffice, so if you are alone and working on it in part-time (as you probably have a job for a living) you may spend a whole lifetime without reaching anywhere near a working solution.</p>\n<p>So my most important advice here would be that you <strong>lower down your expectations, and try to reduce the complexity</strong> of your problem by using all the information you can, and avoid as much as possible relying on screenshots (ie, try to hook directly into the game, look for DLL injection), and simplify some problems by implementing supervised procedures, do not let your algorithm learn everything (ie, drop image processing for now as much as possible and rely on internal game informations, later on if your algorithm works well, you can replace some parts of your AI program with image processing, thus gruadually attaining your full goal, for example if you can get something to work quite well, you can try to complexify your problem and replace supervised procedures and memory game data by unsupervised machine learning algorithms on screenshots).</p>\n<p>Good luck, and if it works, make sure to publish an article, you can surely get renowned for solving such a hard practical problem!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The problem you are pursuing is intractable in the way you have defined it. It is usually a mistake to think that a neural network would \"magically\" learn a rich reprsentation of a problem. A good fact to keep in mind when deciding whether ANN is the right tool for a task is that it is an interpolation method. Think, whether you can frame your problem as finding an approximation of a function, where you have many points from this function and lots of time for designing the network and training it.</p>\n<p>The problem you propose does not pass this test. Game control is not a function of the image on the screen. There is a lot of information the player has to keep in memory. For a simple example, it is often true that every time you enter a shop in a game, the screen looks the same. However, what you buy depends on the circumstances. No matter how complicated the network, if the screen pixels are its input, it would always perform the same action upon entering the store.</p>\n<p>Besides, there is the problem of scale. The task you propose is simply too complicated to learn in any reasonable amount of time. You should see <a href=\"http://aigamedev.com\">aigamedev.com</a> for how game AI works. Artitificial Neural Networks have been used successfully in some games, but in very limited manner. Game AI is difficult and often expensive to develop. If there was a general approach of constructing functional neural networks, the industry would have most likely seized on it. I recommend that you begin with much, much simpler examples, like tic-tac-toe.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When we calculate the F-Measure considering both Precision and Recall, we take the harmonic mean of the two measures instead of a simple arithmetic mean. </p>\n<p>What is the intuitive reason behind taking the harmonic mean and not a simple average?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To explain, consider for example, what the average of 30mph and 40mph is? if you drive for 1 hour at each speed, the average speed over the 2 hours is indeed the arithmetic average, 35mph.</p>\n<p>However if you drive for the same distance at each speed -- say 10 miles -- then the average speed over 20 miles is the harmonic mean of 30 and 40, about 34.3mph.</p>\n<p>The reason is that for the average to be valid, you really need the values to be in the same scaled units. Miles per hour need to be compared over the same number of hours; to compare over the same number of miles you need to average hours per mile instead, which is exactly what the harmonic mean does.</p>\n<p>Precision and recall both have true positives in the numerator, and different denominators. To average them it really only makes sense to average their reciprocals, thus the harmonic mean.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Because it punishes extreme values more.</p>\n<p>Consider a <em>trivial</em> method (e.g. always returning class A). There are infinite data elements of class B, and a single element of class A:</p>\n<pre><code>Precision: 0.0\nRecall:    1.0\n</code></pre>\n<p>When taking the arithmetic mean, it would have 50% correct. Despite being the <em>worst</em> possible outcome! With the harmonic mean, the F1-measure is 0.</p>\n<pre><code>Arithmetic mean: 0.5\nHarmonic mean:   0.0\n</code></pre>\n<p>In other words, to have a high F1, you need to <em>both</em> have a high precision and recall.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The above answers are well explained. This is just for a quick reference to understand the nature of the arithmetic mean and the harmonic mean with plots. As you can see from the plot, consider the X axis and Y axis as precision and recall, and the Z axis as the F1 Score. So, from the plot of the harmonic mean, both the precision and recall should contribute evenly for the F1 score to rise up unlike the Arithmetic mean.</p>\n<p>This is for the arithmetic mean.</p>\n<p><a href=\"https://i.sstatic.net/SYA7h.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/SYA7h.jpg\"/></a></p>\n<p>This is for the Harmonic mean.</p>\n<p><a href=\"https://i.sstatic.net/slxFc.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/slxFc.jpg\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-11 13:08:12Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/37370015/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>In reinforcement learning, what is the difference between <em>policy iteration</em> and <em>value iteration</em>? </p>\n<p>As much as I understand, in value iteration, you use the Bellman equation to solve for the optimal policy, whereas, in policy iteration, you randomly select a policy π, and find the reward of that policy. </p>\n<p>My doubt is that if you are selecting a random policy π in PI, how is it guaranteed to be the optimal policy, even if we are choosing several random policies.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's look at them side by side. The key parts for comparison are highlighted. Figures are from Sutton and Barto's book: <em>Reinforcement Learning: An Introduction</em>.</p>\n<p><a href=\"https://i.sstatic.net/wGuj5.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/wGuj5.png\"/></a>\nKey points:</p>\n<ol>\n<li><strong>Policy iteration</strong> includes: <strong>policy evaluation</strong> + <strong>policy improvement</strong>, and the two are repeated iteratively until policy converges.</li>\n<li><strong>Value iteration</strong> includes: <strong>finding optimal value function</strong> + one <strong>policy extraction</strong>. There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (i.e. converged).</li>\n<li><strong>Finding optimal value function</strong> can also be seen as a combination of policy improvement (due to max) and truncated policy evaluation (the reassignment of v_(s) after just one sweep of all states regardless of convergence).</li>\n<li>The algorithms for <strong>policy evaluation</strong> and <strong>finding optimal value function</strong> are highly similar except for a max operation (as highlighted)</li>\n<li>Similarly, the key step to <strong>policy improvement</strong> and <strong>policy extraction</strong> are identical except the former involves a stability check.</li>\n</ol>\n<p>In my experience, <em>policy iteration</em> is faster than <em>value iteration</em>, as a policy converges more quickly than a value function. I remember this is also described in the book.</p>\n<p>I guess the confusion mainly came from all these somewhat similar terms, which also confused me before. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In <strong>policy iteration</strong> algorithms, you start with a random policy, then find the value function of that policy (policy evaluation step), then find a new (improved) policy based on the previous value function, and so on. In this process, each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Given a policy, its value function can be obtained using the <em>Bellman operator</em>.</p>\n<p>In <strong>value iteration</strong>, you start with a random value function and then find a new (improved) value function in an iterative process, until reaching the optimal value function. Notice that you can derive easily the optimal policy from the optimal value function. This process is based on the <em>optimality Bellman operator</em>.</p>\n<p>In some sense, both algorithms share the same working principle, and they can be seen as two cases of the <a href=\"http://incompleteideas.net/book/ebook/node46.html\" rel=\"noreferrer\"><em>generalized policy iteration</em></a>. However, the optimality Bellman operator contains a <em>max</em> operator, which is non linear and, therefore, it has different features. In addition, it's possible to use hybrid methods between pure value iteration and pure policy iteration.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The basic difference is - </p>\n<p>In <strong>Policy Iteration</strong> - You randomly select a policy and find value function corresponding to it , then find a new (improved) policy based on the previous value function, and so on this will lead to optimal policy . </p>\n<p>In <strong>Value Iteration</strong> - You randomly select a value function ,  then find a new (improved) value function in an iterative process, until reaching the optimal value function , then derive optimal policy from that optimal value function .</p>\n<p>Policy iteration works on principle of “Policy evaluation —-&gt; Policy improvement”.</p>\n<p>Value Iteration works on principle of “ Optimal value function —-&gt; optimal policy”.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm working in a sentiment analysis problem the data looks like this:</p>\n<pre><code>label instances\n    5    1190\n    4     838\n    3     239\n    1     204\n    2     127\n</code></pre>\n<p>So my data is unbalanced since 1190 <code>instances</code> are labeled with <code>5</code>. For the classification Im using scikit's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\" rel=\"noreferrer\">SVC</a>. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches:</p>\n<p>First:</p>\n<pre><code>    wclf = SVC(kernel='linear', C= 1, class_weight={1: 10})\n    wclf.fit(X, y)\n    weighted_prediction = wclf.predict(X_test)\n\nprint 'Accuracy:', accuracy_score(y_test, weighted_prediction)\nprint 'F1 score:', f1_score(y_test, weighted_prediction,average='weighted')\nprint 'Recall:', recall_score(y_test, weighted_prediction,\n                              average='weighted')\nprint 'Precision:', precision_score(y_test, weighted_prediction,\n                                    average='weighted')\nprint '\\n clasification report:\\n', classification_report(y_test, weighted_prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, weighted_prediction)\n</code></pre>\n<p>Second:</p>\n<pre><code>auto_wclf = SVC(kernel='linear', C= 1, class_weight='auto')\nauto_wclf.fit(X, y)\nauto_weighted_prediction = auto_wclf.predict(X_test)\n\nprint 'Accuracy:', accuracy_score(y_test, auto_weighted_prediction)\n\nprint 'F1 score:', f1_score(y_test, auto_weighted_prediction,\n                            average='weighted')\n\nprint 'Recall:', recall_score(y_test, auto_weighted_prediction,\n                              average='weighted')\n\nprint 'Precision:', precision_score(y_test, auto_weighted_prediction,\n                                    average='weighted')\n\nprint '\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction)\n\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction)\n</code></pre>\n<p>Third:</p>\n<pre><code>clf = SVC(kernel='linear', C= 1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nfrom sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\nprint 'Accuracy:', accuracy_score(y_test, prediction)\nprint 'F1 score:', f1_score(y_test, prediction)\nprint 'Recall:', recall_score(y_test, prediction)\nprint 'Precision:', precision_score(y_test, prediction)\nprint '\\n clasification report:\\n', classification_report(y_test,prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, prediction)\n\n\nF1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n  sample_weight=sample_weight)\n/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n  sample_weight=sample_weight)\n/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n  sample_weight=sample_weight)\n 0.930416613529\n</code></pre>\n<p>However, Im getting warnings like this:</p>\n<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:\nDeprecationWarning: The default `weighted` averaging is deprecated,\nand from version 0.18, use of precision, recall or F-score with \nmulticlass or multilabel data or pos_label=None will result in an \nexception. Please set an explicit value for `average`, one of (None, \n'micro', 'macro', 'weighted', 'samples'). In cross validation use, for \ninstance, scoring=\"f1_weighted\" instead of scoring=\"f1\"\n</code></pre>\n<p>How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;).</p>\n<h2>Class weights</h2>\n<p>The weights from the <code>class_weight</code> parameter are used to <strong>train the classifier</strong>.\nThey <strong>are not used in the calculation of any of the metrics you are using</strong>: with different class weights, the numbers will be different simply because the classifier is different.</p>\n<p>Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.<br/>\nHow they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it.</p>\n<h2>The metrics</h2>\n<p>Once you have a classifier, you want to know how well it is performing.\nHere you can use the metrics you mentioned: <code>accuracy</code>, <code>recall_score</code>, <code>f1_score</code>...</p>\n<p>Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.</p>\n<p>I will not detail all these metrics but note that, with the exception of <code>accuracy</code>, they are naturally applied at the class level: as you can see in this <code>print</code> of a classification report they are defined for each class. They rely on concepts such as <code>true positives</code> or <code>false negative</code> that require defining which class is the <em>positive</em> one.</p>\n<pre><code>             precision    recall  f1-score   support\n\n          0       0.65      1.00      0.79        17\n          1       0.57      0.75      0.65        16\n          2       0.33      0.06      0.10        17\navg / total       0.52      0.60      0.51        50\n</code></pre>\n<h2>The warning</h2>\n<pre><code>F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The \ndefault `weighted` averaging is deprecated, and from version 0.18, \nuse of precision, recall or F-score with multiclass or multilabel data  \nor pos_label=None will result in an exception. Please set an explicit \nvalue for `average`, one of (None, 'micro', 'macro', 'weighted', \n'samples'). In cross validation use, for instance, \nscoring=\"f1_weighted\" instead of scoring=\"f1\".\n</code></pre>\n<p>You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!\nThe question could be rephrased: from the above classification report, how do you output <strong>one</strong> global number for the f1-score?\nYou could:</p>\n<ol>\n<li>Take the average of the f1-score for each class: that's the <code>avg / total</code> result above. It's also called <em>macro</em> averaging.</li>\n<li>Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka <em>micro</em> averaging.</li>\n<li>Compute a weighted average of the f1-score. Using <code>'weighted'</code> in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.</li>\n</ol>\n<p>These are 3 of the options in scikit-learn, the warning is there to say you <strong>have to pick one</strong>. So you have to specify an <code>average</code> argument for the score method.</p>\n<p>Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5.</p>\n<p>The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.</p>\n<h2>Computing scores</h2>\n<p>Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier <strong>has never seen</strong>.\nThis is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.</p>\n<p>Here's a way to do it using <code>StratifiedShuffleSplit</code>, which gives you a random splits of your data (after shuffling) that preserve the label distribution.</p>\n<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n# We use a utility to generate artificial classification data.\nX, y = make_classification(n_samples=100, n_informative=10, n_classes=3)\nsss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=0)\nfor train_idx, test_idx in sss:\n    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    print(f1_score(y_test, y_pred, average=\"macro\"))\n    print(precision_score(y_test, y_pred, average=\"macro\"))\n    print(recall_score(y_test, y_pred, average=\"macro\"))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Lot of very detailed answers here but I don't think you are answering the right questions. As I understand the question, there are two concerns:</p>\n<ol>\n<li>How to I score a multiclass problem?</li>\n<li>How do I deal with unbalanced data?</li>\n</ol>\n<h2>1.</h2>\n<p>You can use most of the scoring functions in scikit-learn with both multiclass problem as with single class problems. Ex.:</p>\n<pre><code>from sklearn.metrics import precision_recall_fscore_support as score\n\npredicted = [1,2,3,4,5,1,2,1,1,4,5] \ny_test = [1,2,3,4,5,1,2,1,1,4,1]\n\nprecision, recall, fscore, support = score(y_test, predicted)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\n</code></pre>\n<p>This way you end up with tangible and interpretable numbers for each of the classes.</p>\n<pre><code>| Label | Precision | Recall | FScore | Support |\n|-------|-----------|--------|--------|---------|\n| 1     | 94%       | 83%    | 0.88   | 204     |\n| 2     | 71%       | 50%    | 0.54   | 127     |\n| ...   | ...       | ...    | ...    | ...     |\n| 4     | 80%       | 98%    | 0.89   | 838     |\n| 5     | 93%       | 81%    | 0.91   | 1190    |\n</code></pre>\n<p>Then...</p>\n<h2>2.</h2>\n<p>... you can tell if the unbalanced data is even a problem. If the scoring for the less represented classes (class 1 and 2) are lower than for the classes with more training samples (class 4 and 5) then you know that the unbalanced data is in fact a problem, and you can act accordingly, as described in some of the other answers in this thread.\nHowever, if the same class distribution is present in the data you want to predict on, your unbalanced training data is a good representative of the data, and hence, the unbalance is a good thing.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Posed question</strong></p>\n<p>Responding to the question 'what metric should be used for multi-class classification with imbalanced data': Macro-F1-measure. \nMacro Precision and Macro Recall can be also used, but they are not so easily interpretable as for binary classificaion, they are already incorporated into F-measure, and excess metrics complicate methods comparison, parameters tuning, and so on. </p>\n<p>Micro averaging are sensitive to class imbalance: if your method, for example, works good for the most common labels and totally messes others, micro-averaged metrics show good results.</p>\n<p>Weighting averaging isn't well suited for imbalanced data, because it weights by counts of labels. Moreover, it is too hardly interpretable and unpopular: for instance, there is no mention of such an averaging in the following very detailed <a href=\"http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf\">survey</a> I strongly recommend to look through:</p>\n<blockquote>\n<p>Sokolova, Marina, and Guy Lapalme. \"A systematic analysis of\n  performance measures for classification tasks.\" Information Processing\n  &amp; Management 45.4 (2009): 427-437.</p>\n</blockquote>\n<p><strong>Application-specific question</strong></p>\n<p>However, returning to your task, I'd research 2 topics:</p>\n<ol>\n<li>metrics commonly used for your specific task - it lets (a) to\ncompare your method with others and understand if you do something\nwrong, and (b) to not explore this by yourself and reuse someone\nelse's findings; </li>\n<li>cost of different errors of your methods - for\nexample, use-case of your application may rely on 4- and 5-star\nreviewes only - in this case, good metric should count only these 2\nlabels.</li>\n</ol>\n<p><strong><em>Commonly used metrics.</em></strong>\nAs I can infer after looking through literature, there are 2 main evaluation metrics:</p>\n<ol>\n<li><strong><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">Accuracy</a></strong>, which is used, e.g. in </li>\n</ol>\n<blockquote>\n<p>Yu, April, and Daryl Chang. \"Multiclass Sentiment Prediction using\n  Yelp Business.\"</p>\n</blockquote>\n<p>(<a href=\"http://cs224d.stanford.edu/reports/YuApril.pdf\">link</a>) - note that the authors work with almost the same distribution of ratings, see Figure 5.</p>\n<blockquote>\n<p>Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class\n  relationships for sentiment categorization with respect to rating\n  scales.\" Proceedings of the 43rd Annual Meeting on Association for\n  Computational Linguistics. Association for Computational Linguistics,\n  2005.</p>\n</blockquote>\n<p>(<a href=\"http://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf\">link</a>)</p>\n<ol start=\"2\">\n<li><strong><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\">MSE</a></strong> (or, less often, Mean Absolute Error - <strong><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\">MAE</a></strong>) - see, for example,</li>\n</ol>\n<blockquote>\n<p>Lee, Moontae, and R. Grafe. \"Multiclass sentiment analysis with\n  restaurant reviews.\" Final Projects from CS N 224 (2010).</p>\n</blockquote>\n<p>(<a href=\"http://nlp.stanford.edu/courses/cs224n/2010/reports/pgrafe-moontae.pdf\">link</a>) - they explore both accuracy and MSE, considering the latter to be better</p>\n<blockquote>\n<p>Pappas, Nikolaos, Rue Marconi, and Andrei Popescu-Belis. \"Explaining\n  the Stars: Weighted Multiple-Instance Learning for Aspect-Based\n  Sentiment Analysis.\" Proceedings of the 2014 Conference on Empirical\n  Methods In Natural Language Processing. No. EPFL-CONF-200899. 2014.</p>\n</blockquote>\n<p>(<a href=\"http://www.aclweb.org/anthology/D14-1052\">link</a>) - they utilize scikit-learn for evaluation and baseline approaches and state that their code is available; however, I can't find it, so if you need it, write a letter to the authors, the work is pretty new and seems to be written in Python.</p>\n<p><strong><em>Cost of different errors</em>.</strong>\nIf you care more about avoiding gross blunders, e.g. assinging 1-star to 5-star review or something like that, look at MSE; \nif difference matters, but not so much, try MAE, since it doesn't square diff; \notherwise stay with Accuracy.</p>\n<p><strong>About approaches, not metrics</strong></p>\n<p>Try regression approaches, e.g. <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\">SVR</a>, since they generally outperforms Multiclass classifiers like SVC or OVA SVM.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n                                As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, <a href=\"/help/reopen-questions\">visit the help center</a> for guidance.\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2012-12-23 18:58:24Z\">11 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>Is there a rule of thumb (or set of examples) to determine when to use genetic algorithms as opposed to neural networks (and vice-versa) to solve a problem?</p>\n<p>I know there are cases in which you can have both methods mixed, but I am looking for a high-level comparison between the two methods.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From wikipedia:</p>\n<blockquote>\n<p>A <a href=\"http://en.wikipedia.org/wiki/Genetic_algorithms\" rel=\"noreferrer\">genetic algorithm</a> (GA) is a search technique used in computing to <strong>find</strong> exact or approximate <strong>solutions</strong> to optimization and search problems.</p>\n</blockquote>\n<p>and:</p>\n<blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Artificial_neural_network\" rel=\"noreferrer\">Neural networks</a> are non-linear statistical data modeling tools. They can be used to model complex relationships between inputs and outputs or to <strong>find patterns</strong> in data.</p>\n</blockquote>\n<p>If you have a problem where you can quantify the worth of a solution, a <strong>genetic algorithm</strong> can perform a <strong>directed search</strong> of the solution space. (E.g. find the shortest route between two points)</p>\n<p>When you have a number of items in different classes, a <strong>neural network</strong> can \"learn\" to <strong>classify</strong> items it has not \"seen\" before. (E.g. face recognition, voice recognition)</p>\n<p>Execution times must also be considered. A genetic algorithm takes a long time to find an acceptable solution. A neural network takes a long time to \"learn\", but then it can almost instantly classify new inputs.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A genetic algorithm (despite its sexy name) is, for most purposes, an <em>optimization technique</em>. It primarily boils down to you having a number of variables and wanting to find the best combination of values for these variables.  It just borrows techniques from natural evolution to get there.</p>\n<p>Neural networks are useful for <em>recognizing patterns</em>. They follow a simplistic model of the brain, and by changing a number of weights between them, attempt to predict outputs based on inputs.</p>\n<p>They are two fundamentally different entities, but sometimes the problems they are capable of solving overlap.  </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>GAs generate new patterns in a structure that you define.</p>\n<p>NNs classify (or recognize) existing patterns based on training data that you provide.</p>\n<p>GAs perform well at efficiently searching a large state-space of solutions, and converging on one or more good solutions, but not necessarily the 'best' solution.</p>\n<p>NNs can learn to recognize patterns (via training), but it is notoriously difficult to figure out what they have learned, i.e. to extract the knowledge from them once trained, and reuse the knowledge in some other (non-NN).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been trying to use tensorflow for two days now installing and reinstalling it over and over again in python2.7 and 3.4.  No matter what I do, I get this error message when trying to use tensorflow.placeholder()</p>\n<p>It's very boilerplate code:</p>\n<pre><code>tf_in = tf.placeholder(\"float\", [None, A]) # Features\n</code></pre>\n<p>No matter what I do I always get the trace back:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/willim/PycharmProjects/tensorflow/tensorflow.py\", line 2, in &lt;module&gt;\n    import tensorflow as tf\n  File \"/home/willim/PycharmProjects/tensorflow/tensorflow.py\", line 53, in &lt;module&gt;\n    tf_in = tf.placeholder(\"float\", [None, A]) # Features\nAttributeError: 'module' object has no attribute 'placeholder'\n</code></pre>\n<p>Anyone know how I can fix this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you have this error after an upgrade to TensorFlow 2.0, you can still use 1.X API by replacing:</p>\n<pre><code>import tensorflow as tf\n</code></pre>\n<p>by</p>\n<pre><code>import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Solution: Do not use \"tensorflow\" as your filename.</strong></p>\n<p>Notice that you use tensorflow.py as your filename. And I guess you write code like:</p>\n<pre><code>import tensorflow as tf\n</code></pre>\n<p>Then you are actually importing the script file \"tensorflow.py\" that is under your current working directory, rather than the \"real\" tensorflow module from Google.</p>\n<p>Here is the order in which a module will be searched when importing:</p>\n<blockquote>\n<ol>\n<li><p>The directory containing the input script (or the current directory when no file is specified). </p></li>\n<li><p>PYTHONPATH (a list of directory names,\n  with the same syntax as the shell variable PATH). </p></li>\n<li><p>The installation-dependent default.</p></li>\n</ol>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Instead of <code>tf.placeholder(shape=[None, 2], dtype=tf.float32)</code> use something like\n<code>tf.compat.v1.placeholder(shape=[None, 2], dtype=tf.float32)</code> if you don't want to disable v2 completely.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it. </p>\n<p>The naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.).</p>\n<p>Any idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>They likely use <a href=\"http://en.wikipedia.org/wiki/Information_extraction\" rel=\"nofollow noreferrer\">Information Extraction</a> techniques for this.</p>\n<p>Here is a demo of Stanford's <a href=\"https://nlp.stanford.edu/software/sutime.html\" rel=\"nofollow noreferrer\">SUTime</a> tool:</p>\n<p><a href=\"http://nlp.stanford.edu:8080/sutime/process\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu:8080/sutime/process</a></p>\n<p>You would extract attributes about n-grams (consecutive words) in a document:</p>\n<ul>\n<li>numberOfLetters</li>\n<li>numberOfSymbols</li>\n<li>length</li>\n<li>previousWord</li>\n<li>nextWord</li>\n<li>nextWordNumberOfSymbols<br/>\n...</li>\n</ul>\n<p>And then use a classification algorithm, and feed it positive and negative examples:</p>\n<pre><code>Observation  nLetters  nSymbols  length  prevWord  nextWord isPartOfDate  \n\"Feb.\"       3         1         4       \"Wed\"     \"29th\"   TRUE  \n\"DEC\"        3         0         3       \"company\" \"went\"   FALSE  \n...\n</code></pre>\n<p>You might get away with 50 examples of each, but the more the merrier. Then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before.</p>\n<p>It might learn rules such as</p>\n<ul>\n<li>if previous word is only characters and maybe periods...</li>\n<li>and current word is in \"february\", \"mar.\", \"the\" ...</li>\n<li>and next word is in \"twelfth\", any_number ...</li>\n<li>then is date</li>\n</ul>\n<p>Here is a <a href=\"http://videolectures.net/mlas06_nigam_tie/\" rel=\"nofollow noreferrer\">decent video</a> by a Google engineer on the subject</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>That's a technology Apple actually developed a very long time ago called <code>Apple Data Detectors</code>. You can read more about it here:</p>\n<p><a href=\"http://www.miramontes.com/writing/add-cacm/\">http://www.miramontes.com/writing/add-cacm/</a></p>\n<p>Essentially it parses the text and detects patterns that represent specific pieces of data, then applies OS-contextual actions to it. It's neat.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is called <em>temporal expression</em> identification and parsing.  Here are some Google searches to get you started: </p>\n<p><a href=\"https://www.google.com/#hl=en&amp;safe=off&amp;sclient=psy-ab&amp;q=timebank+timeml+timex\" rel=\"nofollow noreferrer\">https://www.google.com/#hl=en&amp;safe=off&amp;sclient=psy-ab&amp;q=timebank+timeml+timex</a></p>\n<p><a href=\"https://www.google.com/#hl=en&amp;safe=off&amp;sclient=psy-ab&amp;q=temporal+expression+tagger\" rel=\"nofollow noreferrer\">https://www.google.com/#hl=en&amp;safe=off&amp;sclient=psy-ab&amp;q=temporal+expression+tagger</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 16:01:17Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/17469835/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I have noticed that when One Hot encoding is used on a particular data set (a matrix) and used as training data for learning algorithms, it gives significantly better results with respect to prediction accuracy, compared to using the original matrix itself as training data. How does this performance increase happen? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Many learning algorithms either learn a single weight per feature, or they use distances between samples. The former is the case for linear models such as logistic regression, which are easy to explain.</p>\n<p>Suppose you have a dataset having only a single categorical feature \"nationality\", with values \"UK\", \"French\" and \"US\". Assume, without loss of generality, that these are encoded as 0, 1 and 2. You then have a weight w for this feature in a linear classifier, which will make some kind of decision based on the constraint w×x + b &gt; 0, or equivalently w×x &lt; b.</p>\n<p>The problem now is that the weight w cannot encode a three-way choice. The three possible values of w×x are 0, w and 2×w. Either these three all lead to the same decision (they're all &lt; b or ≥b) or \"UK\" and \"French\" lead to the same decision, or \"French\" and \"US\" give the same decision. There's no possibility for the model to learn that \"UK\" and \"US\" should be given the same label, with \"French\" the odd one out.</p>\n<p>By one-hot encoding, you effectively blow up the feature space to three features, which will each get their own weights, so the decision function is now w[UK]x[UK] + w[FR]x[FR] + w[US]x[US] &lt; b, where all the x's are booleans. In this space, such a linear function can express any sum/disjunction of the possibilities (e.g. \"UK or US\", which might be a predictor for someone speaking English).</p>\n<p>Similarly, any learner based on standard distance metrics (such as k-nearest neighbors) between samples will get confused without one-hot encoding. With the naive encoding and Euclidean distance, the distance between French and US is 1. The distance between US and UK is 2. But with the one-hot encoding, the pairwise distances between [1, 0, 0], [0, 1, 0] and [0, 0, 1] are all equal to √2.</p>\n<p>This is not true for all learning algorithms; decision trees and derived models such as random forests, if deep enough, can handle categorical variables without one-hot encoding.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Regarding the increase of the features by doing one-hot-encoding one can use feature hashing. When you do hashing, you can specify the number of buckets to be much less than the number of the newly introduced features.  </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>update: this question is related to Google Colab's \"Notebook settings: Hardware accelerator: GPU\". This question was written before the \"TPU\" option was added.</p>\n<p>Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run <a href=\"http://course.fast.ai/\" rel=\"noreferrer\">fast.ai</a> lesson on it for it to never complete - quickly running out of memory. I started investigating of why.</p>\n<p>The bottom line is that “free Tesla K80” is not \"free\" for all - for some only a small slice of it is \"free\". </p>\n<p>I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM.</p>\n<p>Clearly 0.5GB GPU RAM is insufficient for most ML/DL work.</p>\n<p>If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook):</p>\n<pre><code># memory footprint support libraries/code\n!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n!pip install gputil\n!pip install psutil\n!pip install humanize\nimport psutil\nimport humanize\nimport os\nimport GPUtil as GPU\nGPUs = GPU.getGPUs()\n# XXX: only one GPU on Colab and isn’t guaranteed\ngpu = GPUs[0]\ndef printm():\n process = psutil.Process(os.getpid())\n print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\nprintm()\n</code></pre>\n<p>Executing it in a jupyter notebook before running any other code gives me:</p>\n<pre><code>Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB\nGPU RAM Free: 566MB | Used: 10873MB | Util  95% | Total 11439MB\n</code></pre>\n<p>The lucky users who get access to the full card will see:</p>\n<pre><code>Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB\nGPU RAM Free: 11439MB | Used: 0MB | Util  0% | Total 11439MB\n</code></pre>\n<p>Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil?</p>\n<p>Can you confirm that you get similar results if you run this code on Google Colab notebook?</p>\n<p>If my calculations are correct, is there any way to get more of that GPU RAM on the free box?</p>\n<p>update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing!</p>\n<p><strong>note</strong>: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So to prevent another dozen of answers suggesting invalid in the context of this thread suggestion to !kill -9 -1, let's close this thread:</p>\n<p>The answer is simple:</p>\n<p><strong>As of this writing Google simply gives only 5% of GPU to some of us, whereas 100% to the others. Period.</strong></p>\n<p>dec-2019 update: The problem still exists - this question's upvotes continue still.</p>\n<p>mar-2019 update: A year later a Google employee @AmiF commented on the state of things, stating that the problem doesn't exist, and anybody who seems to have this problem needs to simply reset their runtime to recover memory. Yet, the upvotes continue, which to me this tells that the problem still exists, despite @AmiF's suggestion to the contrary.</p>\n<p>dec-2018 update: I have a theory that Google may have a blacklist of certain accounts, or perhaps browser fingerprints, when its robots detect a non-standard behavior. It could be a total coincidence, but for quite some time I had an issue with Google Re-captcha on any website that happened to require it, where I'd have to go through dozens of puzzles before I'd be allowed through, often taking me 10+ min to accomplish. This lasted for many months. All of a sudden as of this month I get no puzzles at all and any google re-captcha gets resolved with just a single mouse click, as it used to be almost a year ago. </p>\n<p>And why I'm telling this story? Well, because <strong>at the same time I was given 100% of the GPU RAM on Colab</strong>. That's why my suspicion is that if you are on a theoretical Google black list then you aren't being trusted to be given a lot of resources for free. I wonder if any of you find the same correlation between the limited GPU access and the Re-captcha nightmare. As I said, it could be totally a coincidence as well.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Last night I ran your snippet and got exactly what you got:</p>\n<pre><code>Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB\nGPU RAM Free: 566MB | Used: 10873MB | Util  95% | Total 11439MB\n</code></pre>\n<p>but today:</p>\n<pre><code>Gen RAM Free: 12.2 GB  I Proc size: 131.5 MB\nGPU RAM Free: 11439MB | Used: 0MB | Util   0% | Total 11439MB\n</code></pre>\n<p>I think the most probable reason is the GPUs are shared among VMs, so each time you restart the runtime you have chance to switch the GPU, and there is also probability you switch to one that is being used by other users.</p>\n<p>UPDATED:\nIt turns out that I can use GPU normally even when the GPU RAM Free is 504 MB, which I thought as the cause of ResourceExhaustedError I got last night. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you execute a cell that just has<br/>\n<strong>!kill -9 -1</strong><br/>\nin it, that'll cause all of your runtime's state (including memory, filesystem, and GPU) to be wiped clean and restarted.  Wait 30-60s and press the CONNECT button at the top-right to reconnect.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have five text files that I input to a CountVectorizer. When specifying <code>min_df</code> and <code>max_df</code> to the CountVectorizer instance what does the min/max document frequency exactly mean? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (five text files)?</p>\n<p>What are the differences when <code>min_df</code> and <code>max_df</code> are provided as integers or as floats?</p>\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer\" rel=\"noreferrer\">The documentation</a> doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of these two parameters. Could someone provide an explanation or example demonstrating <code>min_df</code> and <code>max_df</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>max_df</code> is used for removing terms that appear <strong>too frequently</strong>, also known as \"corpus-specific stop words\". For example:</p>\n<ul>\n<li><code>max_df = 0.50</code> means \"ignore terms that appear in <strong>more than 50% of the documents</strong>\".</li>\n<li><code>max_df = 25</code> means \"ignore terms that appear in <strong>more than 25 documents</strong>\".</li>\n</ul>\n<p>The default <code>max_df</code> is <code>1.0</code>, which means \"ignore terms that appear in <strong>more than 100% of the documents</strong>\". Thus, the default setting does not ignore any terms.</p>\n<hr/>\n<p><code>min_df</code> is used for removing terms that appear <strong>too infrequently</strong>. For example:</p>\n<ul>\n<li><code>min_df = 0.01</code> means \"ignore terms that appear in <strong>less than 1% of the documents</strong>\".</li>\n<li><code>min_df = 5</code> means \"ignore terms that appear in <strong>less than 5 documents</strong>\".</li>\n</ul>\n<p>The default <code>min_df</code> is <code>1</code>, which means \"ignore terms that appear in <strong>less than 1 document</strong>\". Thus, the default setting does not ignore any terms.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would add this point also for understanding <code>min_df</code> and <code>max_df</code> in tf-idf better.</p>\n<p>If you go with the default values, meaning considering all terms, you have generated definitely more tokens. So your clustering process (or any other thing you want to do with those terms later) will take a longer time. </p>\n<p>BUT the quality of your clustering should NOT be reduced. </p>\n<p>One might think that allowing all terms (e.g. too frequent terms or stop-words) to be present might lower the quality but in tf-idf it doesn't. Because tf-idf measurement instinctively will give a low score to those terms, effectively making them not influential (as they appear in many documents).</p>\n<p>So to sum it up, pruning the terms via <code>min_df</code> and <code>max_df</code> is to improve the performance, not the quality of clusters (as an example).</p>\n<p>And the crucial point is that if you set the <code>min</code> and <code>max</code> mistakenly, you would lose some important terms and thus lower the quality. So if you are unsure about the right threshold (it depends on your documents set), or if you are sure about your machine's processing capabilities, leave the <code>min</code>, <code>max</code> parameters unchanged.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As per the <code>CountVectorizer</code> documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\" rel=\"noreferrer\">here</a>.</p>\n<p>When using a float in the range <code>[0.0, 1.0]</code> they refer to the <strong>document</strong> frequency. That is the percentage of documents that contain the term.</p>\n<p>When using an int it refers to absolute number of documents that hold this term.</p>\n<p>Consider the example where you have 5 text files (or documents). If you set <code>max_df = 0.6</code> then that would translate to <code>0.6*5=3</code> documents. If you set <code>max_df = 2</code> then that would simply translate to 2 documents.</p>\n<p>The source code example below is copied from Github <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\" rel=\"noreferrer\">here</a> and shows how the <code>max_doc_count</code> is constructed from the <code>max_df</code>. The code for <code>min_df</code> is similar and can be found on the GH page.</p>\n<pre><code>max_doc_count = (max_df\n                 if isinstance(max_df, numbers.Integral)\n                 else max_df * n_doc)\n</code></pre>\n<p>The defaults for <code>min_df</code> and <code>max_df</code> are 1 and 1.0, respectively. This basically says <em>\"If my term is found in only 1 document, then it's ignored. Similarly if it's found in all documents (100% or 1.0) then it's ignored.\"</em></p>\n<p><code>max_df</code> and <code>min_df</code> are both used internally to calculate <code>max_doc_count</code> and <code>min_doc_count</code>, the maximum and minimum number of documents that a term must be found in. This is then passed to <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L665\" rel=\"noreferrer\"><code>self._limit_features</code></a> as the keyword arguments <code>high</code> and <code>low</code> respectively, the docstring for <code>self._limit_features</code> is</p>\n<pre><code>\"\"\"Remove too rare or too common features.\n\nPrune features that are non zero in more samples than high or less\ndocuments than low, modifying the vocabulary, and restricting it to\nat most the limit most frequent.\n\nThis does not prune samples with zero features.\n\"\"\"\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've noticed that a frequent occurrence during training is <code>NAN</code>s being introduced.</p>\n<p>Often times it seems to be introduced by weights in inner-product/fully-connected or convolution layers blowing up.</p>\n<p>Is this occurring because the gradient computation is blowing up? Or is it because of weight initialization (if so, why does weight initialization have this effect)? Or is it likely caused by the nature of the input data?</p>\n<p>The overarching question here is simply: <strong>What is the most common reason for NANs to occurring during training?</strong> And secondly, what are some methods for combatting this (and why do they work)?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I came across this phenomenon several times. Here are my observations:</p>\n<hr/>\n<h3>Gradient blow up</h3>\n<p><strong>Reason:</strong> large gradients throw the learning process off-track.</p>\n<p><strong>What you should expect:</strong> Looking at the runtime log, you should look at the loss values per-iteration. You'll notice that the loss starts to grow <em>significantly</em> from iteration to iteration, eventually the loss will be too large to be represented by a floating point variable and it will become <code>nan</code>.</p>\n<p><strong>What can you do:</strong> Decrease the <code>base_lr</code> (in the solver.prototxt) by an order of magnitude (at least). If you have several loss layers, you should inspect the log to see which layer is responsible for the gradient blow up and decrease the <code>loss_weight</code> (in train_val.prototxt) for that specific layer, instead of the general <code>base_lr</code>.</p>\n<hr/>\n<h3>Bad learning rate policy and params</h3>\n<p><strong>Reason:</strong> caffe fails to compute a valid learning rate and gets <code>'inf'</code> or <code>'nan'</code> instead, this invalid rate multiplies all updates and thus invalidating all parameters.</p>\n<p><strong>What you should expect:</strong> Looking at the runtime log, you should see that the learning rate itself becomes <code>'nan'</code>, for example:</p>\n<blockquote>\n<pre><code>... sgd_solver.cpp:106] Iteration 0, lr = -nan\n</code></pre>\n</blockquote>\n<p><strong>What can you do:</strong> fix all parameters affecting the learning rate in your <code>'solver.prototxt'</code> file.<br/>\nFor instance, if you use <code>lr_policy: \"poly\"</code> and you forget to define <code>max_iter</code> parameter, you'll end up with  <code>lr = nan</code>...<br/>\nFor more information about learning rate in caffe, see <a href=\"https://stackoverflow.com/q/30033096/1714410\">this thread</a>.</p>\n<hr/>\n<h3>Faulty Loss function</h3>\n<p><strong>Reason:</strong> Sometimes the computations of the loss in the loss layers causes <code>nan</code>s to appear. For example, Feeding <a href=\"https://stackoverflow.com/a/27645934/1714410\"><code>InfogainLoss</code> layer with non-normalized values</a>, using custom loss layer with bugs, etc.</p>\n<p><strong>What you should expect:</strong> Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a <code>nan</code> appears.</p>\n<p><strong>What can you do:</strong> See if you can reproduce the error, add printout to the loss layer and debug the error.</p>\n<p>For example: Once I used a loss that normalized the penalty by the frequency of label occurrence in a batch. It just so happened that if one of the training labels did not appear in the batch at all - the loss computed produced <code>nan</code>s. In that case, working with large enough batches (with respect to the number of labels in the set) was enough to avoid this error.</p>\n<hr/>\n<h3>Faulty input</h3>\n<p><strong>Reason:</strong> you have an input with <code>nan</code> in it!</p>\n<p><strong>What you should expect:</strong> once the learning process \"hits\" this faulty input - output becomes <code>nan</code>. Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a <code>nan</code> appears.</p>\n<p><strong>What can you do:</strong> re-build your input datasets (lmdb/leveldn/hdf5...) make sure you do not have bad image files in your training/validation set. For debug you can build a simple net that read the input layer, has a dummy loss on top of it and runs through all the inputs: if one of them is faulty, this dummy net should also produce <code>nan</code>.</p>\n<hr/>\n<h3>stride larger than kernel size in <code>\"Pooling\"</code> layer</h3>\n<p>For some reason, choosing <code>stride</code> &gt; <code>kernel_size</code> for pooling may results with <code>nan</code>s. For example:</p>\n<pre><code>layer {\n  name: \"faulty_pooling\"\n  type: \"Pooling\"\n  bottom: \"x\"\n  top: \"y\"\n  pooling_param {\n    pool: AVE\n    stride: 5\n    kernel: 3\n  }\n}\n</code></pre>\n<p>results with <code>nan</code>s in <code>y</code>.</p>\n<hr/>\n<h3>Instabilities in <code>\"BatchNorm\"</code></h3>\n<p>It was reported that under some settings <code>\"BatchNorm\"</code> layer may output <code>nan</code>s due to numerical instabilities.<br/>\nThis <a href=\"https://github.com/BVLC/caffe/pull/5136\" rel=\"noreferrer\">issue</a> was raised in bvlc/caffe and <a href=\"https://github.com/BVLC/caffe/pull/5136\" rel=\"noreferrer\">PR #5136</a> is attempting to fix it.</p>\n<hr/>\n<p>Recently, I became aware of <a href=\"https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto#L86-L88\" rel=\"noreferrer\"><code>debug_info</code></a> flag: setting <code>debug_info: true</code> in <code>'solver.prototxt'</code> will make caffe print to log more debug information (including gradient magnitudes and activation values) during training: This information can <a href=\"https://stackoverflow.com/q/40510706/1714410\">help in spotting gradient blowups and other problems in the training process</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In my case, not setting the bias in the convolution/deconvolution layers was the cause.</p>\n<p><strong>Solution:</strong> add the following to the convolution layer parameters.</p>\n<pre><code>bias_filler {\n      type: \"constant\"\n      value: 0\n    }\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One more solution for anyone stuck like I just was-</p>\n<p>I was receiving nan or inf losses on a network I setup with float16 dtype across the layers and input data.  After all else failed, it occurred to me to switch back to float32, and the nan losses were solved!</p>\n<p>So bottom line, if you switched dtype to float16, change it back to float32.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <a href=\"https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\" rel=\"noreferrer\">PyTorch documentation</a> says:</p>\n<blockquote>\n<p>Returns a new tensor with a dimension of size one inserted at the specified position. [...]</p>\n<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])\n&gt;&gt;&gt; torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n&gt;&gt;&gt; torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n</code></pre>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>unsqueeze</code> turns an n.d. tensor into an (n+1).d. one by adding an extra dimension of depth 1. However, since it is ambiguous which axis the new dimension should lie across (i.e. in which direction it should be \"unsqueezed\"), this needs to be specified by the <a href=\"https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\" rel=\"noreferrer\"><code>dim</code></a> argument.</p>\n<p>e.g. <code>unsqueeze</code> can be applied to a 2d tensor three different ways:</p>\n<p><a href=\"https://i.sstatic.net/NiJu4.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/NiJu4.png\"/></a></p>\n<p>The resulting unsqueezed tensors have the same information, but the <em>indices</em> used to access them are different.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you look at the shape of the array before and after, you see that before it was <code>(4,)</code> and after it is <code>(1, 4)</code> (when second parameter is <code>0</code>) and <code>(4, 1)</code> (when second parameter is <code>1</code>). So a <code>1</code> was inserted in the shape of the array at axis <code>0</code> or <code>1</code>, depending on the value of the second parameter.</p>\n<p>That is opposite of <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.squeeze.html\" rel=\"noreferrer\"><code>np.squeeze()</code></a> (nomenclature borrowed from MATLAB) which removes axes of size <code>1</code> (singletons).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It indicates the position on where to add the dimension. <code>torch.unsqueeze</code> adds an additional dimension to the tensor.</p>\n<p>So let's say you have a tensor of shape (3), if you add a dimension at the 0 position, it will be of shape (1,3), which means 1 row and 3 columns:</p>\n<ul>\n<li>If you have a 2D tensor of shape (2,2) add add an extra dimension at the <strong>0</strong> position, this will result of the tensor having a shape of (1,2,2), which means one channel, 2 rows and 2 columns. If you add at the 1 position, it will be of shape (2,1,2), so it will have 2 channels, 1 row and 2 columns.</li>\n<li>If you add at the <strong>1</strong> position, it will be (3,1), which means 3 rows and 1 column.</li>\n<li>If you add it at the <strong>2</strong> position, the tensor will be of shape (2,2,1), which means 2 channels, 2 rows and one column.</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>My problem:</strong></p>\n<p>I have a dataset which is a large JSON file. I read it and store it in the <code>trainList</code> variable.</p>\n<p>Next, I pre-process it - in order to be able to work with it.</p>\n<p>Once I have done that I start the classification:</p>\n<ol>\n<li>I use the <code>kfold</code> cross validation method in order to obtain the mean\naccuracy and train a classifier.</li>\n<li>I make the predictions and obtain the accuracy &amp; confusion matrix of that fold.</li>\n<li>After this, I would like to obtain the <code>True Positive(TP)</code>, <code>True Negative(TN)</code>, <code>False Positive(FP)</code> and <code>False Negative(FN)</code> values. I'll  use these parameters to obtain the <strong>Sensitivity</strong> and <strong>Specificity</strong>. </li>\n</ol>\n<p>Finally, I would use this to put in HTML in order to show a chart with the TPs of each label.</p>\n<p><strong>Code:</strong></p>\n<p>The variables I have for the moment:</p>\n<pre><code>trainList #It is a list with all the data of my dataset in JSON form\nlabelList #It is a list with all the labels of my data \n</code></pre>\n<p>Most part of the method:</p>\n<pre><code>#I transform the data from JSON form to a numerical one\nX=vec.fit_transform(trainList)\n\n#I scale the matrix (don't know why but without it, it makes an error)\nX=preprocessing.scale(X.toarray())\n\n#I generate a KFold in order to make cross validation\nkf = KFold(len(X), n_folds=10, indices=True, shuffle=True, random_state=1)\n\n#I start the cross validation\nfor train_indices, test_indices in kf:\n    X_train=[X[ii] for ii in train_indices]\n    X_test=[X[ii] for ii in test_indices]\n    y_train=[listaLabels[ii] for ii in train_indices]\n    y_test=[listaLabels[ii] for ii in test_indices]\n\n    #I train the classifier\n    trained=qda.fit(X_train,y_train)\n\n    #I make the predictions\n    predicted=qda.predict(X_test)\n\n    #I obtain the accuracy of this fold\n    ac=accuracy_score(predicted,y_test)\n\n    #I obtain the confusion matrix\n    cm=confusion_matrix(y_test, predicted)\n\n    #I should calculate the TP,TN, FP and FN \n    #I don't know how to continue\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For the multi-class case, everything you need can be found from the confusion matrix. For example, if your confusion matrix looks like this:</p>\n<p><a href=\"https://i.sstatic.net/luc1s.png\" rel=\"noreferrer\"><img alt=\"confusion matrix\" src=\"https://i.sstatic.net/luc1s.png\"/></a></p>\n<p>Then what you're looking for, per class, can be found like this:</p>\n<p><a href=\"https://i.sstatic.net/AuTKP.png\" rel=\"noreferrer\"><img alt=\"overlay\" src=\"https://i.sstatic.net/AuTKP.png\"/></a></p>\n<p>Using pandas/numpy, you can do this for all classes at once like so:</p>\n<pre class=\"lang-python prettyprint-override\"><code>FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \nFN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\nTP = np.diag(confusion_matrix)\nTN = confusion_matrix.values.sum() - (FP + FN + TP)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \n# Precision or positive predictive value\nPPV = TP/(TP+FP)\n# Negative predictive value\nNPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you have two lists that have the predicted and actual values; as it appears you do, you can pass them to a function that will calculate TP, FP, TN, FN with something like this:</p>\n<pre><code>def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)\n</code></pre>\n<p>From here I think you will be able to calculate rates of interest to you, and other performance measure like specificity and sensitivity.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>According to scikit-learn documentation,</p>\n<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix</a></p>\n<p>By definition a confusion matrix C is such that <code>C[i, j]</code> is equal to the number of observations known to be in group <code>i</code> but predicted to be in group <code>j</code>.</p>\n<p>Thus in binary classification, the count of true negatives is <code>C[0,0]</code>, false negatives is <code>C[1,0]</code>, true positives is <code>C[1,1]</code> and false positives is <code>C[0,1]</code>.</p>\n<pre><code>CM = confusion_matrix(y_true, y_pred)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-08-15 14:44:25Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/20027598/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I am trying to build a neural network from scratch.\nAcross all AI literature there is a consensus that weights should be initialized to random numbers in order for the network to converge faster.</p>\n<p><strong>But why are neural networks initial weights initialized as random numbers?</strong> </p>\n<p>I had read somewhere that this is done to \"break the symmetry\" and this makes the neural network learn faster. How does breaking the symmetry make it learn faster?</p>\n<p>Wouldn't initializing the weights to 0 be a better idea? That way the weights would be able to find their values (whether positive or negative) faster?</p>\n<p>Is there some other underlying philosophy behind randomizing the weights apart from hoping that they would be near their optimum values when initialized?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Breaking symmetry is essential here, and not for the reason of performance. Imagine first 2 layers of multilayer perceptron (input and hidden layers): </p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/agyRr.png\"/></p>\n<p>During forward propagation each unit in hidden layer gets signal:</p>\n<p><img alt=\"enter image description here\" src=\"https://latex.codecogs.com/gif.latex?a_i%20%3D%20%5Csum_%7Bi%7D%5EN%7BW_%7Bi%2Cj%7D%20%5Ccdot%20x_i%7D\"/></p>\n<p>That is, each hidden unit gets sum of inputs multiplied by the corresponding weight. </p>\n<p>Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, <strong>each hidden unit will get exactly the same signal</strong>. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs <code>sigmoid(sum(inputs))</code>). If all weights are zeros, which is even worse, every hidden unit will get zero signal. <strong>No matter what was the input - if all weights are the same, all units in hidden layer will be the same too</strong>. </p>\n<p>This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). Note, that this issue affects all architectures that use each-to-each connections. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Analogy:</strong></p>\n<p>Imagine that someone has dropped you from a helicopter to an unknown mountain top, and you're trapped there. Fog everywhere. You only know that you should get down to the sea level somehow. Which direction should you take to get down to <strong>the lowest possible point</strong>?</p>\n<p>If you couldn't reach sea level, the helicopter would take you again and drop you at the same mountain top. You would have to take the same directions again because you're <strong>\"initializing\"</strong> yourself to the same starting positions.</p>\n<p>However, each time the helicopter drops you somewhere randomly on the mountain, you would take different directions and steps. So, you would have a better chance of reaching the lowest possible point.</p>\n<p>That is what is meant by <strong>breaking the symmetry</strong>. The initialization is asymmetric (which is different), so you can find different solutions to the same problem.</p>\n<p>In this analogy, where you land is the <strong>weight</strong>. So, with different weights, there's a better chance of reaching the lowest (or lower) point.</p>\n<p>Also, it increases the <strong>entropy</strong> in the system so the system can create more information to help you find the lower points (local or global minimums).</p>\n<p><a href=\"https://i.sstatic.net/EGx2a.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/EGx2a.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The answer is pretty simple. The basic training algorithms are greedy in nature - they do not find the global optimum, but rather - \"nearest\" local solution. As the result, starting from any fixed initialization biases your solution towards some one particular set of weights. If you do it randomly (and possibly many times) then there is much less probable that you will get stuck in some weird part of the error surface.</p>\n<p>The same argument applies to other algorithms, which are not able to find a global optimum (k-means, EM, etc.) and does not apply to the global optimization techniques (like SMO algorithm for SVM).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How to load a model from an HDF5 file in Keras?</p>\n<p>What I tried:</p>\n<pre><code>model = Sequential()\n\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, init='uniform'))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2, init='uniform'))\nmodel.add(Activation('softmax'))\n\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\ncheckpointer = ModelCheckpoint(filepath=\"/weights.hdf5\", verbose=1, save_best_only=True)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2, callbacks=[checkpointer])\n</code></pre>\n<p>The above code successfully saves the best model to a file named weights.hdf5. What I want to do is then load that model. The below code shows how I tried to do so:</p>\n<pre><code>model2 = Sequential()\nmodel2.load_weights(\"/Users/Desktop/SquareSpace/weights.hdf5\")\n</code></pre>\n<p>This is the error I get:</p>\n<pre><code>IndexError                                Traceback (most recent call last)\n&lt;ipython-input-101-ec968f9e95c5&gt; in &lt;module&gt;()\n      1 model2 = Sequential()\n----&gt; 2 model2.load_weights(\"/Users/Desktop/SquareSpace/weights.hdf5\")\n\n/Applications/anaconda/lib/python2.7/site-packages/keras/models.pyc in load_weights(self, filepath)\n    582             g = f['layer_{}'.format(k)]\n    583             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n--&gt; 584             self.layers[k].set_weights(weights)\n    585         f.close()\n    586 \n\nIndexError: list index out of range\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you stored the complete model, not only the weights, in the HDF5 file, then it is as simple as</p>\n<pre><code>from keras.models import load_model\nmodel = load_model('model.h5')\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>load_weights</code> only sets the weights of your network. You still need to define its architecture before calling <code>load_weights</code>:</p>\n<pre><code>def create_model():\n   model = Sequential()\n   model.add(Dense(64, input_dim=14, init='uniform'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5)) \n   model.add(Dense(64, init='uniform'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5))\n   model.add(Dense(2, init='uniform'))\n   model.add(Activation('softmax'))\n   return model\n\ndef train():\n   model = create_model()\n   sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n   model.compile(loss='binary_crossentropy', optimizer=sgd)\n\n   checkpointer = ModelCheckpoint(filepath=\"/tmp/weights.hdf5\", verbose=1, save_best_only=True)\n   model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose=2, callbacks=[checkpointer])\n\ndef load_trained_model(weights_path):\n   model = create_model()\n   model.load_weights(weights_path)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>See the following sample code on how to Build a basic Keras Neural Net Model, save Model (JSON) &amp; Weights (HDF5) and load them:</p>\n<pre><code># create model\nmodel = Sequential()\nmodel.add(Dense(X.shape[1], input_dim=X.shape[1], activation='relu')) #Input Layer\nmodel.add(Dense(X.shape[1], activation='relu')) #Hidden Layer\nmodel.add(Dense(output_dim, activation='softmax')) #Output Layer\n\n# Compile &amp; Fit model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X,Y,nb_epoch=5,batch_size=100,verbose=1)    \n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"Data/model.json\", \"w\") as json_file:\n    json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n\n# serialize weights to HDF5\nmodel.save_weights(\"Data/model.h5\")\nprint(\"Saved model to disk\")\n\n# load json and create model\njson_file = open('Data/model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"Data/model.h5\")\nprint(\"Loaded model from disk\")\n\n# evaluate loaded model on test data \n# Define X_test &amp; Y_test data first\nloaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, Y_test, verbose=0)\nprint (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How do I add L1/L2 regularization in PyTorch without manually computing it?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <code>weight_decay &gt; 0</code> for L2 regularization:</p>\n<pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>See the <a href=\"http://pytorch.org/docs/optim.html#torch.optim.Adagrad\" rel=\"noreferrer\">documentation</a>. Add a <code>weight_decay</code> parameter to the optimizer for L2 regularization.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Previous answers, while technically correct, are inefficient performance wise and are not too modular (hard to apply on a per-layer basis, as provided by, say, <code>keras</code> layers).</p>\n<h2>PyTorch L2 implementation</h2>\n<p><strong>Why PyTorch implemented <code>L2</code> inside <code>torch.optim.Optimizer</code> instances?</strong></p>\n<p>Let's take a look at <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/optim/_functional.py#L143\" rel=\"noreferrer\"><code>torch.optim.SGD</code> source code</a> (currently as functional optimization procedure), especially this part:</p>\n<pre><code>for i, param in enumerate(params):\n    d_p = d_p_list[i]\n    # L2 weight decay specified HERE!\n    if weight_decay != 0:\n        d_p = d_p.add(param, alpha=weight_decay)\n</code></pre>\n<ul>\n<li>One can see, that <code>d_p</code> (derivative of parameter, gradient) is modified and re-assigned for faster computation (not saving the temporary variables)</li>\n<li>It has <code>O(N)</code> complexity without any complicated math like <code>pow</code></li>\n<li><strong>It does not involve <code>autograd</code> extending the graph without any need</strong></li>\n</ul>\n<p>Compare that to <code>O(n)</code> <code>**2</code> operations, addition and also taking part in backpropagation.</p>\n<h2>Math</h2>\n<p>Let's see <code>L2</code> equation with <code>alpha</code> regularization factor (same could be done for L1 ofc):</p>\n<p><a href=\"https://i.sstatic.net/eSqgj.png\" rel=\"noreferrer\"><img alt=\"L2\" src=\"https://i.sstatic.net/eSqgj.png\"/></a></p>\n<p>If we take derivative of any loss with <code>L2</code> regularization w.r.t. parameters <code>w</code> (it is independent of loss), we get:</p>\n<p><a href=\"https://i.sstatic.net/4NNzW.png\" rel=\"noreferrer\"><img alt=\"L2 deriv\" src=\"https://i.sstatic.net/4NNzW.png\"/></a></p>\n<p><strong>So it is simply an addition of <code>alpha * weight</code> for gradient of every weight!</strong> And this is exactly what PyTorch does above!</p>\n<h2>L1 Regularization layer</h2>\n<p>Using this (and some PyTorch magic), we can come up with quite generic L1 regularization layer, but let's look at first derivative of <code>L1</code> first (<code>sgn</code> is signum function, returning <code>1</code> for positive input and <code>-1</code> for negative, <code>0</code> for <code>0</code>):</p>\n<p><a href=\"https://i.sstatic.net/K6gqK.png\" rel=\"noreferrer\"><img alt=\"L1 derivative\" src=\"https://i.sstatic.net/K6gqK.png\"/></a></p>\n<p>Full code with <code>WeightDecay</code> interface located in <a href=\"https://github.com/szymonmaszke/torchlayers/blob/master/torchlayers/regularization.py#L150\" rel=\"noreferrer\">torchlayers third party library</a> providing stuff like regularizing only weights/biases/specifically named paramters (<strong>disclaimer: I'm the author</strong>), but the essence of the idea outlined below (see comments):</p>\n<pre><code>class L1(torch.nn.Module):\n    def __init__(self, module, weight_decay):\n        super().__init__()\n        self.module = module\n        self.weight_decay = weight_decay\n\n        # Backward hook is registered on the specified module\n        self.hook = self.module.register_full_backward_hook(self._weight_decay_hook)\n\n    # Not dependent on backprop incoming values, placeholder\n    def _weight_decay_hook(self, *_):\n        for param in self.module.parameters():\n            # If there is no gradient or it was zeroed out\n            # Zeroed out using optimizer.zero_grad() usually\n            # Turn on if needed with grad accumulation/more safer way\n            # if param.grad is None or torch.all(param.grad == 0.0):\n\n            # Apply regularization on it\n            param.grad = self.regularize(param)\n\n    def regularize(self, parameter):\n        # L1 regularization formula\n        return self.weight_decay * torch.sign(parameter.data)\n\n    def forward(self, *args, **kwargs):\n        # Simply forward and args and kwargs to module\n        return self.module(*args, **kwargs)\n</code></pre>\n<p>Read more about hooks <a href=\"https://stackoverflow.com/questions/65011884/understanding-backward-hooks\">in this answer</a> or respective PyTorch docs if needed.</p>\n<p>And usage is also pretty simple (should work with gradient accumulation and and PyTorch layers):</p>\n<pre><code>layer = L1(torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)) \n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 16:01:10Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/40898019/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm following a <a href=\"https://youtu.be/lN5jesocJjk?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5vhttp://\" rel=\"noreferrer\">tutorial</a> about machine learning basics and there is mentioned that something can be a <em>feature</em> or a <em>label</em>. </p>\n<p>From what I know, a feature is a property of data that is being used. I can't figure out what the label is, I know the meaning of the word, but I want to know what it means in the context of machine learning.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Briefly, feature is input; label is output.  This applies to both classification and regression problems.</p>\n<p>A feature is one column of the data in your input set.  For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc.  The label is the final choice, such as dog, fish, iguana, rock, etc.</p>\n<p>Once you've trained your model, you will give it sets of new input containing those features; it will return the predicted \"label\" (pet type) for that person.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Feature:</strong> </p>\n<p>In Machine Learning feature means property of your training data. Or you can say a column name in your training dataset.</p>\n<p>Suppose this is your training dataset </p>\n<pre><code>Height   Sex   Age\n 61.5     M     20\n 55.5     F     30\n 64.5     M     41\n 55.5     F     51\n .     .     .\n .     .     .\n .     .     .\n .     .     .\n</code></pre>\n<p>Then here <code>Height</code>, <code>Sex</code> and <code>Age</code> are the features.</p>\n<p><strong>label:</strong> </p>\n<p>The output you get from your model after training it is called a label. </p>\n<p>Suppose you fed the above dataset to some algorithm and generates a model to predict gender as Male or Female, In the above model you pass features like <code>age</code>, <code>height</code> etc. </p>\n<p>So after computing, it will return the gender as Male or Female. That's called a <strong>Label</strong></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>Here comes a more visual approach to explain the concept. Imagine you want to classify the animal shown in a photo.</p>\n</blockquote>\n<p>The possible classes of animals are e.g. cats or birds.\nIn that case the <strong>label</strong> would be the possible class associations e.g. cat or bird, that your machine learning algorithm will predict.</p>\n<p>The <strong>features</strong> are pattern, colors, forms that are part of your images e.g. furr, feathers, or more low-level interpretation, pixel values.</p>\n<p><a href=\"https://i.sstatic.net/yPivs.jpg\" rel=\"noreferrer\"><img alt=\"Bird\" src=\"https://i.sstatic.net/yPivs.jpg\"/></a>\n<strong>Label:</strong> Bird<br/>\n<strong>Features:</strong> Feathers</p>\n<p><a href=\"https://i.sstatic.net/LJX5s.png\" rel=\"noreferrer\"><img alt=\"Cat\" src=\"https://i.sstatic.net/LJX5s.png\"/></a></p>\n<p><strong>Label:</strong> Cat<br/>\n<strong>Features:</strong> Furr</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset consisting of both numeric and categorical data and I want to predict adverse outcomes for patients based on their medical characteristics. I defined a prediction pipeline for my dataset like so:</p>\n<pre class=\"lang-py prettyprint-override\"><code>X = dataset.drop(columns=['target'])\ny = dataset['target']\n\n# define categorical and numeric transformers\nnumeric_transformer = Pipeline(steps=[\n    ('knnImputer', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n#  dispatch object columns to the categorical_transformer and remaining columns to numerical_transformer\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, selector(dtype_exclude=\"object\")),\n    ('cat', categorical_transformer, selector(dtype_include=\"object\"))\n])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\n</code></pre>\n<p>However, when running this code, I get the following warning message:</p>\n<pre><code>ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\n    model score: 0.988\n</code></pre>\n<p>Can someone explain to me what this warning means? I am new to machine learning so am a little lost as to what I can do to improve the prediction model. As you can see from the numeric_transformer, I scaled the data through standardisation. I am also confused as to how the model score is quite high and whether this is a good or bad thing.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <em>warning</em> means what it mainly says: Suggestions to try to make the <em>solver</em> (the algorithm) converges.</p>\n<hr/>\n<p><code>lbfgs</code> stand for: \"Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm\". It is one of the solvers' algorithms provided by Scikit-Learn Library.</p>\n<p>The term <em>limited-memory</em> simply means it stores <strong>only a few</strong> vectors that represent the gradients approximation implicitly.</p>\n<p>It has better <strong>convergence</strong> on relatively <em>small</em> datasets.</p>\n<hr/>\n<p>But what is <em>algorithm convergence</em>?</p>\n<p>In simple words. If the error of solving is ranging within very small range (i.e., it is almost not changing), then that means the algorithm reached the solution (<em>not necessary to be the best solution as it might be stuck at what so-called \"local Optima\"</em>).</p>\n<p>On the other hand, if the error is <em><strong>varying noticeably</strong></em> (<em>even if the error is relatively small [like in your case the score was good], but rather the differences between the errors per iteration is greater than some tolerance</em>) then we say the algorithm did not converge.</p>\n<p>Now, you need to know that Scikit-Learn API sometimes provides the user the option to specify the maximum number of iterations the algorithm should take while it's searching for the solution in an iterative manner:</p>\n<pre><code>LogisticRegression(... solver='lbfgs', max_iter=100 ...)\n</code></pre>\n<p>As you can see, the default solver in LogisticRegression is 'lbfgs' and the maximum number of iterations is 100 by default.</p>\n<p>Final words, please, however, note that increasing the maximum number of iterations does not necessarily guarantee convergence, but it certainly helps!</p>\n<hr/>\n<h2>Update:</h2>\n<p>Based on your comment below, some tips to try (out of many) that might help the algorithm to converge are:</p>\n<ul>\n<li>Increase the <em>number of iterations</em>: As in this answer;</li>\n<li>Try a <em>different optimizer</em>: Look <a href=\"https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions/52388406#52388406\">here</a>;</li>\n<li>Scale your data: Look <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\" rel=\"noreferrer\">here</a>;</li>\n<li>Add engineered features: Look <a href=\"https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\" rel=\"noreferrer\">here</a>;</li>\n<li>Data pre-processing: Look <a href=\"https://datascience.stackexchange.com/questions/80421/very-low-cross-val-score-for-regression-with-big-corr-between-feature-and-res/80422#80422\">here - use case</a> and <a href=\"https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\" rel=\"noreferrer\">here</a>;</li>\n<li>Add more data: Look <a href=\"https://www.quora.com/How-do-you-determine-sample-size-for-machine-learning-classification/answer/Yahya-Almardeny\" rel=\"noreferrer\">here</a>.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you are getting the following error for any machine learning algorithm,</p>\n<blockquote>\n<p>ConvergenceWarning:</p>\n<p>lbfgs failed to converge (status=1): <br/>\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.</p>\n</blockquote>\n<p>increase the number of iterations (max_iter) or scale the data as shown in <em><a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\" rel=\"noreferrer\">6.3. Preprocessing data</a></em></p>\n<p>Please also refer to the documentation for alternative solver options: <em><a href=\"https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\" rel=\"noreferrer\">LogisticRegression()</a></em></p>\n<p>Then in that case you use an algorithm like</p>\n<pre><code>from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n</code></pre>\n<p>because sometimes it will happen due to iteration.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>to fix <strong>Convergence warning</strong> specify <strong>max_iter</strong> in the <strong>LogisticRegression</strong> to a higer value:</p>\n<pre><code>from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression(max_iter=3000)\nmodel.fit(X_train,y_train)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>OpenAI's REINFORCE and actor-critic example for reinforcement learning has the following code:</p>\n<p><a href=\"https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\" rel=\"nofollow noreferrer\">REINFORCE</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>policy_loss = torch.cat(policy_loss).sum()\n</code></pre>\n<p><a href=\"https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\" rel=\"nofollow noreferrer\">actor-critic</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n</code></pre>\n<p>One is using <a href=\"https://pytorch.org/docs/stable/generated/torch.cat.html\" rel=\"nofollow noreferrer\"><code>torch.cat</code></a>, the other uses <a href=\"https://pytorch.org/docs/stable/generated/torch.stack.html\" rel=\"nofollow noreferrer\"><code>torch.stack</code></a>, for similar use cases.</p>\n<p>As far as my understanding goes, the doc doesn't give any clear distinction between them.</p>\n<p><strong>I would be happy to know the differences between the functions.</strong></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.stack.html\" rel=\"noreferrer\"><code>stack</code></a></p>\n<blockquote>\n<p>Concatenates sequence of tensors along a <strong>new dimension</strong>.</p>\n</blockquote>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.cat.html\" rel=\"noreferrer\"><code>cat</code></a></p>\n<blockquote>\n<p>Concatenates the given sequence of seq tensors <strong>in the given dimension</strong>.</p>\n</blockquote>\n<p>So if <code>A</code> and <code>B</code> are of shape (3, 4):</p>\n<ul>\n<li><code>torch.cat([A, B], dim=0)</code> will be of shape (6, 4)</li>\n<li><code>torch.stack([A, B], dim=0)</code> will be of shape (2, 3, 4)</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre class=\"lang-py prettyprint-override\"><code>t1 = torch.tensor([[1, 2],\n                   [3, 4]])\n\nt2 = torch.tensor([[5, 6],\n                   [7, 8]])\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th><code>torch.stack</code></th>\n<th><code>torch.cat</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>'Stacks'</strong> a sequence of tensors along a new dimension: <br/><br/> <a href=\"https://i.sstatic.net/Ar9fu.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Ar9fu.png\"/></a> <br/><br/><br/><br/></td>\n<td>'Con<strong>cat</strong>enates' a sequence of tensors along an existing dimension: <br/><br/> <a href=\"https://i.sstatic.net/RsEEq.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/RsEEq.png\"/></a></td>\n</tr>\n</tbody>\n</table>\n</div><sup>\n<p>These functions are analogous to <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.stack.html\" rel=\"noreferrer\"><code>numpy.stack</code></a> and <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html\" rel=\"noreferrer\"><code>numpy.concatenate</code></a>.</p>\n</sup>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The original answer lacks a good example that is self-contained so here it goes:</p>\n<pre><code>import torch\n\n# stack vs cat\n\n# cat \"extends\" a list in the given dimension e.g. adds more rows or columns\n\nx = torch.randn(2, 3)\nprint(f'{x.size()}')\n\n# add more rows (thus increasing the dimensionality of the column space to 2 -&gt; 6)\nxnew_from_cat = torch.cat((x, x, x), 0)\nprint(f'{xnew_from_cat.size()}')\n\n# add more columns (thus increasing the dimensionality of the row space to 3 -&gt; 9)\nxnew_from_cat = torch.cat((x, x, x), 1)\nprint(f'{xnew_from_cat.size()}')\n\nprint()\n\n# stack serves the same role as append in lists. i.e. it doesn't change the original\n# vector space but instead adds a new index to the new tensor, so you retain the ability\n# get the original tensor you added to the list by indexing in the new dimension\nxnew_from_stack = torch.stack((x, x, x, x), 0)\nprint(f'{xnew_from_stack.size()}')\n\nxnew_from_stack = torch.stack((x, x, x, x), 1)\nprint(f'{xnew_from_stack.size()}')\n\nxnew_from_stack = torch.stack((x, x, x, x), 2)\nprint(f'{xnew_from_stack.size()}')\n\n# default appends at the from\nxnew_from_stack = torch.stack((x, x, x, x))\nprint(f'{xnew_from_stack.size()}')\n\nprint('I like to think of xnew_from_stack as a \\\"tensor list\\\" that you can pop from the front')\n</code></pre>\n<p>output:</p>\n<pre><code>torch.Size([2, 3])\ntorch.Size([6, 3])\ntorch.Size([2, 9])\ntorch.Size([4, 2, 3])\ntorch.Size([2, 4, 3])\ntorch.Size([2, 3, 4])\ntorch.Size([4, 2, 3])\nI like to think of xnew_from_stack as a \"tensor list\"\n</code></pre>\n<p>for reference here are the definitions:</p>\n<blockquote>\n<p>cat: Concatenates the given sequence of seq tensors in the given dimension. The consequence is that a specific dimension changes size e.g. dim=0 then you are adding elements to the row which increases the dimensionality of the column space.</p>\n</blockquote>\n<blockquote>\n<p>stack: Concatenates sequence of tensors along a new dimension. I like to think of this as the torch \"append\" operation since you can index/get your original tensor by \"poping it\" from the front. With no arguments, it appends tensors to the front of the tensor.</p>\n</blockquote>\n<hr/>\n<p>Related:</p>\n<ul>\n<li>here is the link from the pytorch forum with discussions on this: <a href=\"https://discuss.pytorch.org/t/best-way-to-convert-a-list-to-a-tensor/59949/8\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/best-way-to-convert-a-list-to-a-tensor/59949/8</a> I wish though that <code>tensor.torch</code> convert a nested list of tensors to a big tensor with many dimensions that respected the depth of the nested list.</li>\n</ul>\n<hr/>\n<h1>Update: With nested list of the same size</h1>\n<pre><code>def tensorify(lst):\n    \"\"\"\n    List must be nested list of tensors (with no varying lengths within a dimension).\n    Nested list of nested lengths [D1, D2, ... DN] -&gt; tensor([D1, D2, ..., DN)\n\n    :return: nested list D\n    \"\"\"\n    # base case, if the current list is not nested anymore, make it into tensor\n    if type(lst[0]) != list:\n        if type(lst) == torch.Tensor:\n            return lst\n        elif type(lst[0]) == torch.Tensor:\n            return torch.stack(lst, dim=0)\n        else:  # if the elements of lst are floats or something like that\n            return torch.tensor(lst)\n    current_dimension_i = len(lst)\n    for d_i in range(current_dimension_i):\n        tensor = tensorify(lst[d_i])\n        lst[d_i] = tensor\n    # end of loop lst[d_i] = tensor([D_i, ... D_0])\n    tensor_lst = torch.stack(lst, dim=0)\n    return tensor_lst\n</code></pre>\n<p>here is a few unit tests (I didn't write more tests but it worked with my real code so I trust it's fine. Feel free to help me by adding more tests if you want):</p>\n<pre><code>\ndef test_tensorify():\n    t = [1, 2, 3]\n    print(tensorify(t).size())\n    tt = [t, t, t]\n    print(tensorify(tt))\n    ttt = [tt, tt, tt]\n    print(tensorify(ttt))\n\nif __name__ == '__main__':\n    test_tensorify()\n    print('Done\\a')\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In Keras, we can return the output of <code>model.fit</code> to a history as follows:</p>\n<pre><code> history = model.fit(X_train, y_train, \n                     batch_size=batch_size, \n                     nb_epoch=nb_epoch,\n                     validation_data=(X_test, y_test))\n</code></pre>\n<p>Now, how to save the history attribute of the history object to a file for further uses (e.g. draw plots of acc or loss against epochs)?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What I use is the following:</p>\n<pre><code>with open('/trainHistoryDict', 'wb') as file_pi:\n    pickle.dump(history.history, file_pi)\n</code></pre>\n<p>In this way I save the history as a dictionary in case I want to plot the loss or accuracy later on. Later, when you want to load the history again, you can use:</p>\n<pre><code>with open('/trainHistoryDict', \"rb\") as file_pi:\n    history = pickle.load(file_pi)\n</code></pre>\n<h3>Why choose pickle over json?</h3>\n<p>The comment under <a href=\"https://stackoverflow.com/a/53101097/11659881\">this answer</a> accurately states:</p>\n<blockquote>\n<p>[Storing the history as json] does not work anymore in tensorflow keras. I had issues with: TypeError: Object of type 'float32' is not JSON serializable.</p>\n</blockquote>\n<p>There are ways to tell <code>json</code> how to encode <code>numpy</code> objects, which you can learn about from this <a href=\"https://stackoverflow.com/q/26646362/11659881\">other question</a>, so there's nothing wrong with using <code>json</code> in this case, it's just more complicated than simply dumping to a pickle file.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h3>Another way to do this:</h3>\n<p>As <code>history.history</code> is a <code>dict</code>, you can convert it as well to a <code>pandas</code> <code>DataFrame</code> object, which can then be saved to suit your needs.</p>\n<p>Step by step:</p>\n<pre><code>import pandas as pd\n\n# assuming you stored your model.fit results in a 'history' variable:\nhistory = model.fit(x_train, y_train, epochs=10)\n\n# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history.history) \n\n# save to json:  \nhist_json_file = 'history.json' \nwith open(hist_json_file, mode='w') as f:\n    hist_df.to_json(f)\n\n# or save to csv: \nhist_csv_file = 'history.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The easiest way:</p>\n<p>Saving:</p>\n<pre><code>np.save('my_history.npy',history.history)\n</code></pre>\n<p>Loading:</p>\n<pre><code>history=np.load('my_history.npy',allow_pickle='TRUE').item()\n</code></pre>\n<p>Then history is a dictionary and you can retrieve all desirable values using the keys.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n                                As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, <a href=\"/help/reopen-questions\">visit the help center</a> for guidance.\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2012-07-15 17:28:22Z\">12 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>Are there any machine learning libraries in C#? I'm after something like <a href=\"http://www.cs.waikato.ac.nz/~ml/weka/\" rel=\"noreferrer\">WEKA</a>.\nThank you.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Check out <a href=\"https://github.com/quozd/awesome-dotnet#machine-learning-and-data-science\" rel=\"nofollow noreferrer\">this awesome list</a> on GitHub. Of the frameworks listed, <code>Accord.NET</code> is open-source and the most popular with over 2,000 stars.</p>\n<p>Also, check out the official machine learning library for .NET provided by Microsoft: <a href=\"https://github.com/dotnet/machinelearning\" rel=\"nofollow noreferrer\">https://github.com/dotnet/machinelearning</a></p>\n<hr/>\n<p><strong>OLD</strong></p>\n<p>There's a neural network library called <a href=\"http://www.codeproject.com/KB/recipes/aforge.aspx\" rel=\"nofollow noreferrer\">AForge.net</a> on the codeproject. (Code hosted at <a href=\"http://code.google.com/p/aforge/\" rel=\"nofollow noreferrer\">Google code</a>) (Also checkout the <a href=\"http://www.aforgenet.com/\" rel=\"nofollow noreferrer\">AForge homepage</a> - According to the homepage, the new version now supports genetic algorithms and machine learning as well. It looks like it's progressed a lot since I last played with it)</p>\n<p>I don't know it's it's anything like WEKA as I've never used that.</p>\n<p>(there's also an article on it's <a href=\"http://www.codeproject.com/KB/recipes/aforge_neuro.aspx\" rel=\"nofollow noreferrer\">usage</a>)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can also <a href=\"http://weka.wikispaces.com/Use+WEKA+with+the+Microsoft+.NET+Framework\" rel=\"noreferrer\">use Weka with C#</a>.  The best solution is to use <a href=\"http://weka.wikispaces.com/IKVM+with+Weka+tutorial\" rel=\"noreferrer\"><b>IKVM</b>, as in this tutorial</a>, although you can also use bridging software.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Weka can be used from C# very easily as Shane stated, using IKVM and some 'glue code'. Folow the tutorial on <a href=\"http://weka.wikispaces.com/IKVM+with+Weka+tutorial\">weka page</a> to create the '.Net version' of weka, then you can try to run the following tests:</p>\n<pre><code>[Fact]\npublic void BuildAndClassify()\n{\n  var classifier = BuildClassifier();\n  AssertCanClassify(classifier);\n}\n\n[Fact]\npublic void DeserializeAndClassify()\n{\n  BuildClassifier().Serialize(\"test.weka\");\n  var classifier = Classifier.Deserialize&lt;LinearRegression&gt;(\"test.weka\");\n  AssertCanClassify(classifier);\n}\n\nprivate static void AssertCanClassify(LinearRegression classifier)\n{\n  var result = classifier.Classify(-402, -1);\n  Assert.InRange(result, 255.8d, 255.9d);\n}\n\nprivate static LinearRegression BuildClassifier()\n{\n  var trainingSet = new TrainingSet(\"attribute1\", \"attribute2\", \"class\")\n    .AddExample(-173, 3, -31)\n    .AddExample(-901, 1, 807)\n    .AddExample(-901, 1, 807)\n    .AddExample(-94, -2, -86);\n\n  return Classifier.Build&lt;LinearRegression&gt;(trainingSet);\n}\n</code></pre>\n<p>First test shows, how you build a classifier and classify a new Example with it, the second one shows, how you can use a persisted classifier from a file to classify an example. If you need too support discrete attributes, some modification will be necessery. The code above uses 2 helper classes:</p>\n<pre><code>public class TrainingSet\n{\n    private readonly List&lt;string&gt; _attributes = new List&lt;string&gt;();\n    private readonly List&lt;List&lt;object&gt;&gt; _examples = new List&lt;List&lt;object&gt;&gt;();\n\n    public TrainingSet(params string[] attributes)\n    {\n      _attributes.AddRange(attributes);\n    }\n\n    public int AttributesCount\n    {\n      get { return _attributes.Count; }\n    }\n\n    public int ExamplesCount\n    {\n      get { return _examples.Count; }\n    }\n\n    public TrainingSet AddExample(params object[] example)\n    {\n      if (example.Length != _attributes.Count)\n      {\n        throw new InvalidOperationException(\n          String.Format(\"Invalid number of elements in example. Should be {0}, was {1}.\", _attributes.Count,\n            _examples.Count));\n      }\n\n\n      _examples.Add(new List&lt;object&gt;(example));\n\n      return this;\n    }\n\n    public static implicit operator Instances(TrainingSet trainingSet)\n    {\n      var attributes = trainingSet._attributes.Select(x =&gt; new Attribute(x)).ToArray();\n      var featureVector = new FastVector(trainingSet.AttributesCount);\n\n      foreach (var attribute in attributes)\n      {\n        featureVector.addElement(attribute);\n      }\n\n      var instances = new Instances(\"Rel\", featureVector, trainingSet.ExamplesCount);\n      instances.setClassIndex(trainingSet.AttributesCount - 1);\n\n      foreach (var example in trainingSet._examples)\n      {\n        var instance = new Instance(trainingSet.AttributesCount);\n\n        for (var i = 0; i &lt; example.Count; i++)\n        {\n          instance.setValue(attributes[i], Convert.ToDouble(example[i]));\n        }\n\n        instances.add(instance);\n      }\n\n      return instances;\n    }\n}\n\npublic static class Classifier\n{\n    public static TClassifier Build&lt;TClassifier&gt;(TrainingSet trainingSet)\n      where TClassifier : weka.classifiers.Classifier, new()\n    {\n      var classifier = new TClassifier();\n      classifier.buildClassifier(trainingSet);\n      return classifier;\n    }\n\n    public static TClassifier Deserialize&lt;TClassifier&gt;(string filename)\n    {\n      return (TClassifier)SerializationHelper.read(filename);\n    }\n\n    public static void Serialize(this weka.classifiers.Classifier classifier, string filename)\n    {\n      SerializationHelper.write(filename, classifier);\n    }\n\n    public static double Classify(this weka.classifiers.Classifier classifier, params object[] example)\n    {\n      // instance lenght + 1, because class variable is not included in example\n      var instance = new Instance(example.Length + 1);\n\n      for (int i = 0; i &lt; example.Length; i++)\n      {\n        instance.setValue(i, Convert.ToDouble(example[i]));\n      }\n\n      return classifier.classifyInstance(instance);\n    }\n}\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.  </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you want to force Keras to use CPU</p>\n<h2>Way 1</h2>\n<pre><code>import os\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n</code></pre>\n<p>before Keras / Tensorflow is imported.</p>\n<h2>Way 2</h2>\n<p>Run your script as</p>\n<pre><code>$ CUDA_VISIBLE_DEVICES=\"\" ./your_keras_code.py\n</code></pre>\n<p>See also </p>\n<ol>\n<li><a href=\"https://github.com/keras-team/keras/issues/152\" rel=\"noreferrer\">https://github.com/keras-team/keras/issues/152</a></li>\n<li><a href=\"https://github.com/fchollet/keras/issues/4613\" rel=\"noreferrer\">https://github.com/fchollet/keras/issues/4613</a></li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This worked for me (win10), place before you import keras:</p>\n<pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A rather separable way of doing this is to use </p>\n<pre><code>import tensorflow as tf\nfrom keras import backend as K\n\nnum_cores = 4\n\nif GPU:\n    num_GPU = 1\n    num_CPU = 1\nif CPU:\n    num_CPU = 1\n    num_GPU = 0\n\nconfig = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\n                        inter_op_parallelism_threads=num_cores, \n                        allow_soft_placement=True,\n                        device_count = {'CPU' : num_CPU,\n                                        'GPU' : num_GPU}\n                       )\n\nsession = tf.Session(config=config)\nK.set_session(session)\n</code></pre>\n<p>Here, with <code>booleans</code> <code>GPU</code> and <code>CPU</code>, we indicate whether we would like to run our code with the GPU or CPU by rigidly defining the number of GPUs and CPUs the Tensorflow session is allowed to access. The variables <code>num_GPU</code> and <code>num_CPU</code> define this value. <code>num_cores</code> then sets the number of CPU cores available for usage via <code>intra_op_parallelism_threads</code> and <code>inter_op_parallelism_threads</code>.</p>\n<p>The <code>intra_op_parallelism_threads</code> variable dictates the number of threads a parallel operation in a single node in the computation graph is allowed to use (intra). While the <code>inter_ops_parallelism_threads</code> variable defines the number of threads accessible for parallel operations across the nodes of the computation graph (inter).</p>\n<p><code>allow_soft_placement</code> allows for operations to be run on the CPU if any of the following criterion are met:</p>\n<ol>\n<li><p>there is no GPU implementation for the operation</p></li>\n<li><p>there are no GPU devices known or registered</p></li>\n<li><p>there is a need to co-locate with other inputs from the CPU</p></li>\n</ol>\n<p>All of this is executed in the constructor of my class before any other operations, and is completely separable from any model or other code I use. </p>\n<p>Note: This requires <code>tensorflow-gpu</code> and <code>cuda</code>/<code>cudnn</code> to be installed because the option is given to use a GPU.</p>\n<p>Refs:</p>\n<ul>\n<li><p><a href=\"https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac\">What do the options in ConfigProto like allow_soft_placement and log_device_placement mean?</a></p></li>\n<li><p><a href=\"https://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads\">Meaning of inter_op_parallelism_threads and intra_op_parallelism_threads</a> </p></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Considering the <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py\">example code</a>.</p>\n<p>I would like to know How to apply gradient clipping on this network on the RNN where there is a possibility of exploding gradients.</p>\n<pre><code>tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\n</code></pre>\n<p>This is an example that could be used but where do I introduce this ?\nIn the def of RNN </p>\n<pre><code>    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n    _X = tf.split(0, n_steps, _X) # n_steps\ntf.clip_by_value(_X, -1, 1, name=None)\n</code></pre>\n<p>But this doesn't make sense as the tensor _X is the input and not the grad what is to be clipped? </p>\n<p>Do I have to define my own Optimizer for this or is there a simpler option?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Gradient clipping needs to happen after computing the gradients, but before applying them to update the model's parameters. In your example, both of those things are handled by the <code>AdamOptimizer.minimize()</code> method.</p>\n<p>In order to clip your gradients you'll need to explicitly compute, clip, and apply them as described in <a href=\"https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer#processing_gradients_before_applying_them\" rel=\"nofollow noreferrer\">this section in TensorFlow's API documentation</a>. Specifically you'll need to substitute the call to the <code>minimize()</code> method with something like the following:</p>\n<pre><code>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ngvs = optimizer.compute_gradients(cost)\ncapped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\ntrain_op = optimizer.apply_gradients(capped_gvs)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Despite what seems to be popular, you probably want to clip the whole gradient by its global norm:</p>\n<pre><code>optimizer = tf.train.AdamOptimizer(1e-3)\ngradients, variables = zip(*optimizer.compute_gradients(loss))\ngradients, _ = tf.clip_by_global_norm(gradients, 5.0)\noptimize = optimizer.apply_gradients(zip(gradients, variables))\n</code></pre>\n<p>Clipping each gradient matrix individually changes their relative scale but is also possible:</p>\n<pre><code>optimizer = tf.train.AdamOptimizer(1e-3)\ngradients, variables = zip(*optimizer.compute_gradients(loss))\ngradients = [\n    None if gradient is None else tf.clip_by_norm(gradient, 5.0)\n    for gradient in gradients]\noptimize = optimizer.apply_gradients(zip(gradients, variables))\n</code></pre>\n<p>In TensorFlow 2, a tape computes the gradients, the optimizers come from Keras, and we don't need to store the update op because it runs automatically without passing it to a session:</p>\n<pre><code>optimizer = tf.keras.optimizers.Adam(1e-3)\n# ...\nwith tf.GradientTape() as tape:\n  loss = ...\nvariables = ...\ngradients = tape.gradient(loss, variables)\ngradients, _ = tf.clip_by_global_norm(gradients, 5.0)\noptimizer.apply_gradients(zip(gradients, variables))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>It's easy for tf.keras!</strong></p>\n<pre><code>optimizer = tf.keras.optimizers.Adam(clipvalue=1.0)\n</code></pre>\n<p>This optimizer will clip all gradients to values between <code>[-1.0, 1.0]</code>.</p>\n<p>See the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#args_2\" rel=\"noreferrer\">docs</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is <a href=\"/help/closed-questions\">not about programming or software development</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about <a href=\"/help/on-topic\">a specific programming problem, a software algorithm, or software tools primarily used by programmers</a>. If you believe the question would be on-topic on <a href=\"https://stackexchange.com/sites\">another Stack Exchange site</a>, you can leave a comment to explain where the question may be able to be answered.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2023-05-10 10:27:00Z\">last year</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2023-05-10 10:27:25Z\">last year</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/27860652/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>\n<p><a href=\"http://arxiv.org/pdf/1402.3722v1.pdf\">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>\n<p>Can anyone help , please?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The idea of <code>word2vec</code> is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have</p>\n<pre><code>      v_c . v_w\n -------------------\n   sum_i(v_ci . v_w)\n</code></pre>\n<p>The numerator is basically the similarity between words <code>c</code> (the context) and <code>w</code> (the target) word. The denominator computes the similarity of all other contexts <code>ci</code> and the target word <code>w</code>. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts <code>ci</code>. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts <code>ci</code> at random. The end result is that if <code>cat</code> appears in the context of <code>food</code>, then the vector of <code>food</code> is more similar to the vector of <code>cat</code> (as measures by their dot product) than the vectors of <strong>several other randomly chosen words</strong> (e.g. <code>democracy</code>, <code>greed</code>, <code>Freddy</code>), instead of <strong>all other words in language</strong>. This makes <code>word2vec</code> much much faster to train.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Computing <strong><em>Softmax</em></strong> (Function to determine which words are similar to the current target word) is expensive since requires summing over all words in <strong>V</strong> (denominator), which is generally very large.</p>\n<p><a href=\"https://i.sstatic.net/Akfej.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Akfej.png\"/></a></p>\n<p><em>What can be done?</em></p>\n<p>Different strategies have been proposed to <strong>approximate</strong> the softmax. These approaches can be grouped into <strong>softmax-based</strong> and <strong>sampling-based</strong> approaches. <strong><em>Softmax-based</em></strong> approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency (e.g hierarchical softmax). <strong><em>Sampling-based</em></strong> approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax (They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute like negative sampling).</p>\n<p>The loss function in Word2vec is something like:</p>\n<p><a href=\"https://i.sstatic.net/4s4f6.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/4s4f6.png\"/></a></p>\n<p>Which logarithm can decompose into:</p>\n<p><a href=\"https://i.sstatic.net/6RDai.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/6RDai.png\"/></a></p>\n<p>With some mathematic and gradient formula (See more details at <a href=\"http://sebastianruder.com/word-embeddings-softmax/\" rel=\"noreferrer\">6</a>) it converted to:</p>\n<p><a href=\"https://i.sstatic.net/fua4s.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/fua4s.png\"/></a></p>\n<p>As you see it converted to binary classification task (y=1 positive class, y=0 negative class). As we need labels to perform our binary classification task, we designate all context words <em>c</em> as true labels (y=1, positive sample), and <em>k</em> randomly selected from corpora as false labels (y=0, negative sample).</p>\n<hr/>\n<p>Look at the following paragraph. Assume our target word is \"<strong>Word2vec</strong>\". With window of 3, our context words are: <code>The</code>, <code>widely</code>, <code>popular</code>, <code>algorithm</code>, <code>was</code>, <code>developed</code>. These context words consider as positive labels. We also need some negative labels. We randomly pick some words from corpus (<code>produce</code>, <code>software</code>, <code>Collobert</code>, <code>margin-based</code>, <code>probabilistic</code>) and consider them as negative samples. This technique that we picked some randomly example from corpus is called negative sampling.  </p>\n<p><a href=\"https://i.sstatic.net/nnnQX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/nnnQX.png\"/></a></p>\n<p><strong>Reference</strong> :</p>\n<ul>\n<li>(1) C. Dyer, <em>\"Notes on Noise Contrastive Estimation and Negative Sampling\"</em>, 2014</li>\n<li>(2) <a href=\"http://sebastianruder.com/word-embeddings-softmax/\" rel=\"noreferrer\">http://sebastianruder.com/word-embeddings-softmax/</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I wrote an tutorial article about negative sampling <a href=\"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling\" rel=\"noreferrer\">here</a>.</p>\n<p><strong>Why do we use negative sampling?</strong> -&gt; to reduce computational cost</p>\n<p>The cost function for vanilla Skip-Gram (SG) and Skip-Gram negative sampling (SGNS) looks like this:</p>\n<p><a href=\"https://i.sstatic.net/8bUdX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/8bUdX.png\"/></a></p>\n<p>Note that <code>T</code> is the number of all vocabs. It is equivalent to <code>V</code>. In the other words, <code>T</code> = <code>V</code>.</p>\n<p>The probability distribution <code>p(w_t+j|w_t)</code> in SG is computed for all <code>V</code> vocabs in the corpus with:</p>\n<p><a href=\"https://i.sstatic.net/3Lc8N.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/3Lc8N.png\"/></a></p>\n<p><code>V</code> can easily exceed tens of thousand when training Skip-Gram model. The probability needs to be computed <code>V</code> times, making it computationally expensive. Furthermore, the normalization factor in the denominator requires extra <code>V</code> computations. </p>\n<p>On the other hand, the probability distribution in SGNS is computed with:</p>\n<p><a href=\"https://i.sstatic.net/Tl4ST.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Tl4ST.png\"/></a></p>\n<p><code>c_pos</code> is a word vector for positive word, and <code>W_neg</code> is word vectors for all <code>K</code> negative samples in the output weight matrix. With SGNS, the probability needs to be computed only <code>K + 1</code> times, where <code>K</code> is typically between 5 ~ 20. Furthermore, no extra iterations are necessary to compute the normalization factor in the denominator. </p>\n<p>With SGNS, only a fraction of weights are updated for each training sample, whereas SG updates all millions of weights for each training sample.</p>\n<p><a href=\"https://i.sstatic.net/0bPat.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0bPat.png\"/></a></p>\n<p><strong>How does SGNS achieve this?</strong> -&gt; by transforming multi-classification task into binary classification task.</p>\n<p>With SGNS, word vectors are no longer learned by predicting context words of a center word. It learns to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution.</p>\n<p><a href=\"https://i.sstatic.net/qbZaH.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/qbZaH.png\"/></a></p>\n<p>In real life, you don't usually observe <code>regression</code> with random words like <code>Gangnam-Style</code>, or <code>pimples</code>. The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.</p>\n<p><a href=\"https://i.sstatic.net/7V7EO.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/7V7EO.png\"/></a></p>\n<p>In the above figure, current positive word-context pair is (<code>drilling</code>, <code>engineer</code>). <code>K=5</code> negative samples are <a href=\"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#neg_drawn\" rel=\"noreferrer\">randomly drawn</a> from the <a href=\"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#noise_dist\" rel=\"noreferrer\">noise distribution</a>: <code>minimized</code>, <code>primary</code>, <code>concerns</code>, <code>led</code>, <code>page</code>. As the model iterates through the training samples, weights are optimized so that the probability for positive pair will output <code>p(D=1|w,c_pos)≈1</code>, and probability for negative pairs will output <code>p(D=1|w,c_neg)≈0</code>. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm following <a href=\"https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python\" rel=\"noreferrer\">this tutorial</a> to make this ML prediction:</p>\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nstyle.use(\"ggplot\")\nfrom sklearn import svm\n\nx = [1, 5, 1.5, 8, 1, 9]\ny = [2, 8, 1.8, 8, 0.6, 11]\n\nplt.scatter(x,y)\nplt.show()\n\nX = np.array([[1,2],\n             [5,8],\n             [1.5,1.8],\n             [8,8],\n             [1,0.6],\n             [9,11]])\n\ny = [0,1,0,1,0,1]\nX.reshape(1, -1)\n\nclf = svm.SVC(kernel='linear', C = 1.0)\nclf.fit(X,y)\n\nprint(clf.predict([0.58,0.76]))\n</code></pre>\n<p>I'm using Python 3.6 and I get error \"Expected 2D array, got 1D array instead:\"\nI think the script is for older versions, but I don't know how to convert it to the 3.6 version.</p>\n<p>Already try with the:</p>\n<pre><code>X.reshape(1, -1)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are just supposed to provide the <code>predict</code> method with the same 2D array, but with one value that you want to process (or more). In short, you can just replace</p>\n<pre><code>[0.58,0.76]\n</code></pre>\n<p>With</p>\n<pre><code>[[0.58,0.76]]\n</code></pre>\n<p>And it should work.</p>\n<p>EDIT: This answer became popular so I thought I'd add a little more explanation about ML. The short version: we can only use <code>predict</code> on data that is of the same dimensionality as the training data (<code>X</code>) was. </p>\n<p>In the example in question, we give the computer a bunch of rows in <code>X</code> (with 2 values each) and we show it the correct responses in <code>y</code>. When we want to <code>predict</code> using new values, our program expects the same - a <strong>bunch</strong> of rows. Even if we want to do it to just one row (with two values), that row has to be part of another array. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The problem is occurring when you run prediction on the array <code>[0.58,0.76]</code>. Fix the problem by reshaping it before you call <code>predict()</code>:</p>\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nstyle.use(\"ggplot\")\nfrom sklearn import svm\n\nx = [1, 5, 1.5, 8, 1, 9]\ny = [2, 8, 1.8, 8, 0.6, 11]\n\nplt.scatter(x,y)\nplt.show()\n\nX = np.array([[1,2],\n             [5,8],\n             [1.5,1.8],\n             [8,8],\n             [1,0.6],\n             [9,11]])\n\ny = [0,1,0,1,0,1]\n\nclf = svm.SVC(kernel='linear', C = 1.0)\nclf.fit(X,y)\n\ntest = np.array([0.58, 0.76])\nprint test       # Produces: [ 0.58  0.76]\nprint test.shape # Produces: (2,) meaning 2 rows, 1 col\n\ntest = test.reshape(1, -1)\nprint test       # Produces: [[ 0.58  0.76]]\nprint test.shape # Produces (1, 2) meaning 1 row, 2 cols\n\nprint(clf.predict(test)) # Produces [0], as expected\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use the below approach.</p>\n<pre><code>reg = linear_model.LinearRegression()\nreg.fit(df[['year']],df.income)\n\nreg.predict([[2136]])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to grasp what TimeDistributed wrapper does in Keras.</p>\n<p>I get that TimeDistributed \"applies a layer to every temporal slice of an input.\"</p>\n<p>But I did some experiment and got the results that I cannot understand.</p>\n<p>In short, in connection to LSTM layer, TimeDistributed and just Dense layer bear same results.</p>\n<pre><code>model = Sequential()\nmodel.add(LSTM(5, input_shape = (10, 20), return_sequences = True))\nmodel.add(TimeDistributed(Dense(1)))\nprint(model.output_shape)\n\nmodel = Sequential()\nmodel.add(LSTM(5, input_shape = (10, 20), return_sequences = True))\nmodel.add((Dense(1)))\nprint(model.output_shape)\n</code></pre>\n<p>For both models, I got output shape of <strong>(None, 10, 1)</strong>.</p>\n<p>Can anyone explain the difference between TimeDistributed and Dense layer after an RNN layer?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In <code>keras</code> - while building a sequential model - usually the second dimension (one after sample dimension) - is related to a <code>time</code> dimension. This means that if for example, your data is <code>5-dim</code> with <code>(sample, time, width, length, channel)</code> you could apply a convolutional layer using <code>TimeDistributed</code> (which is applicable to <code>4-dim</code> with <code>(sample, width, length, channel)</code>) along a time dimension (applying the same layer to each time slice) in order to obtain <code>5-d</code> output.</p>\n<p>The case with <code>Dense</code> is that in <code>keras</code> from version 2.0 <code>Dense</code> is by default applied to only last dimension (e.g. if you apply <code>Dense(10)</code> to input with shape <code>(n, m, o, p)</code> you'll get output with shape <code>(n, m, o, 10)</code>) so in your case <code>Dense</code> and <code>TimeDistributed(Dense)</code> are equivalent.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>In Short</h1>\n<ul>\n<li><code>Dense</code> without <code>TimeDistributed</code> computes per<code>Batch</code></li>\n<li><code>TimeDistributed</code> with <code>Dense</code> computes per <code>Timestep</code></li>\n</ul>\n<h1>In Detail</h1>\n<pre class=\"lang-py prettyprint-override\"><code>B = 2 # number of batches\nd_model = 8 # embedding dimension\nT = 3 # number of timesteps\n\ndense_layer = tf.keras.layers.Dense(16)\ninp = np.random.randn(B, T, d_model)\n\n# using TimeDistributed layer\ninputs = tf.keras.Input(shape=(T, d_model)) # (B, T, d_model)\noutputs = tf.keras.layers.TimeDistributed(dense_layer)(inputs) # (B, T, 16)\nmodel1 = keras.Model(inputs, outputs)\n\notpt1 = model1(inp)\n</code></pre>\n<p><code>TimeDistributed</code> Layer applies the layer wrapped inside it to each timestep\nso the input shape to the <code>dense_layer</code> wrapped inside is <code>(B, d_model)</code>, so after the applying the <code>dense_layer</code> with weights of shape <code>(d_model, 16)</code> the output is <code>(B, 16)</code>, doing this for all time steps we get output of shape <code>(B, T, 16)</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Without using TimeDistributed layer\ninputs = tf.keras.Input(shape=(T, d_model)) # (B, T, d_model)\noutputs = dense_layer(inputs) # (B, T, 16)\nmodel2 = keras.Model(inputs, outputs)\n\notpt2 = model2(inp)\n</code></pre>\n<p>Without using the <code>TimeDistributed</code> the input shape to the <code>dense_layer</code> is <code>(B, T, d_model)</code> so the weights dimension is <code>(d_model, 16)</code> which applies to all the batches <code>B</code> to give output shape <code>(B, T, 16)</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>np.all(otpt1.numpy() == otpt2.numpy()) # True\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/11808074/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-07-20 18:17:41Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/11808074/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Expectation Maximization (EM) is a kind of probabilistic method to classify data. Please correct me if I am wrong if it is not a classifier. </p>\n<p>What is an intuitive explanation of this EM technique? What is <code>expectation</code> here and what is being <code>maximized</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>Note: the code behind this answer can be found <a href=\"https://github.com/ajcr/em-explanation/blob/master/em-notebook-2.ipynb\" rel=\"noreferrer\">here</a>.</em></p>\n<hr/>\n<p>Suppose we have some data sampled from two different groups, red and blue:</p>\n<p><a href=\"https://i.sstatic.net/JDJ1n.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/JDJ1n.png\"/></a></p>\n<p>Here, we can see which data point belongs to the red or blue group. This makes it easy to find the parameters that characterise each group. For example, the mean of the red group is around 3, the mean of the blue group is around 7 (and we could find the exact means if we wanted).</p>\n<p>This is, generally speaking, known as <em>maximum likelihood estimation</em>. Given some data, we compute the value of a parameter (or parameters) that best explains that data.</p>\n<p>Now imagine that we <em>cannot</em> see which value was sampled from which group. Everything looks purple to us:</p>\n<p><a href=\"https://i.sstatic.net/uvcRO.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/uvcRO.png\"/></a></p>\n<p>Here we have the knowledge that there are <em>two</em> groups of values, but we don't know which group any particular value belongs to. </p>\n<p>Can we still estimate the means for the red group and blue group that best fit this data?</p>\n<p>Yes, often we can! <strong>Expectation Maximisation</strong> gives us a way to do it. The very general idea behind the algorithm is this:</p>\n<ol>\n<li>Start with an initial estimate of what each parameter might be.</li>\n<li>Compute the <em>likelihood</em> that each parameter produces the data point.</li>\n<li>Calculate weights for each data point indicating whether it is more red or more blue based on the likelihood of it being produced by a parameter. Combine the weights with the data (<strong>expectation</strong>).</li>\n<li>Compute a better estimate for the parameters using the weight-adjusted data (<strong>maximisation</strong>).</li>\n<li>Repeat steps 2 to 4 until the parameter estimate converges (the process stops producing a different estimate).</li>\n</ol>\n<p>These steps need some further explanation, so I'll walk through the  problem described above.</p>\n<h1>Example: estimating mean and standard deviation</h1>\n<p>I'll use Python in this example, but the code should be fairly easy to understand if you're not familiar with this language. </p>\n<p>Suppose we have two groups, red and blue, with the values distributed as in the image above. Specifically, each group contains a value drawn from a <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\" rel=\"noreferrer\">normal distribution</a> with the following parameters:</p>\n<pre class=\"lang-Python prettyprint-override\"><code>import numpy as np\nfrom scipy import stats\n\nnp.random.seed(110) # for reproducible results\n\n# set parameters\nred_mean = 3\nred_std = 0.8\n\nblue_mean = 7\nblue_std = 2\n\n# draw 20 samples from normal distributions with red/blue parameters\nred = np.random.normal(red_mean, red_std, size=20)\nblue = np.random.normal(blue_mean, blue_std, size=20)\n\nboth_colours = np.sort(np.concatenate((red, blue))) # for later use...\n</code></pre>\n<p>Here is an image of these red and blue groups again (to save you from having to scroll up):</p>\n<p><a href=\"https://i.sstatic.net/JDJ1n.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/JDJ1n.png\"/></a></p>\n<p>When we can see the colour of each point (i.e. which group it belongs to), it's very easy to estimate the mean and standard deviation for each each group. We just pass the red and blue values to the builtin functions in NumPy. For example:</p>\n<pre class=\"lang-Python prettyprint-override\"><code>&gt;&gt;&gt; np.mean(red)\n2.802\n&gt;&gt;&gt; np.std(red)\n0.871\n&gt;&gt;&gt; np.mean(blue)\n6.932\n&gt;&gt;&gt; np.std(blue)\n2.195\n</code></pre>\n<p>But what if we <em>can't</em> see the colours of the points? That is, instead of red or blue, every point has been coloured purple.</p>\n<p>To try and recover the mean and standard deviation parameters for the red and blue groups, we can use Expectation Maximisation.</p>\n<p>Our first step (<strong>step 1</strong> above) is to guess at the parameter values for each group's mean and standard deviation. We don't have to guess intelligently; we can pick any numbers we like:</p>\n<pre class=\"lang-Python prettyprint-override\"><code># estimates for the mean\nred_mean_guess = 1.1\nblue_mean_guess = 9\n\n# estimates for the standard deviation\nred_std_guess = 2\nblue_std_guess = 1.7\n</code></pre>\n<p>These parameter estimates produce bell curves that look like this:</p>\n<p><a href=\"https://i.sstatic.net/GxLlr.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/GxLlr.png\"/></a></p>\n<p>These are bad estimates. Both means (the vertical dotted lines) look far off any kind of \"middle\" for sensible groups of points, for instance. We want to improve these estimates.</p>\n<p>The next step (<strong>step 2</strong>) is to compute the likelihood of each data point appearing under the current parameter guesses:</p>\n<pre class=\"lang-Python prettyprint-override\"><code>likelihood_of_red = stats.norm(red_mean_guess, red_std_guess).pdf(both_colours)\nlikelihood_of_blue = stats.norm(blue_mean_guess, blue_std_guess).pdf(both_colours)\n</code></pre>\n<p>Here, we have simply put each data point into the <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\" rel=\"noreferrer\">probability density function</a> for a normal distribution using our current guesses at the mean and standard deviation for red and blue. This tells us, for example, that with our current guesses the data point at 1.761 is <em>much</em> more likely to be red (0.189) than blue (0.00003). </p>\n<p>For each data point, we can turn these two likelihood values into weights (<strong>step 3</strong>) so that they sum to 1 as follows:</p>\n<pre class=\"lang-Python prettyprint-override\"><code>likelihood_total = likelihood_of_red + likelihood_of_blue\n\nred_weight = likelihood_of_red / likelihood_total\nblue_weight = likelihood_of_blue / likelihood_total\n</code></pre>\n<p>With our current estimates and our newly-computed weights, we can now compute <em>new</em> estimates for the mean and standard deviation of the red and blue groups (<strong>step 4</strong>).</p>\n<p>We twice compute the mean and standard deviation using <em>all</em> data points, but with the different weightings: once for the red weights and once for the blue weights.</p>\n<p>The key bit of intuition is that the greater the weight of a colour on a data point, the more the data point influences the next estimates for that colour's parameters. This has the effect of \"pulling\" the parameters in the right direction.</p>\n<pre class=\"lang-Python prettyprint-override\"><code>def estimate_mean(data, weight):\n    \"\"\"\n    For each data point, multiply the point by the probability it\n    was drawn from the colour's distribution (its \"weight\").\n\n    Divide by the total weight: essentially, we're finding where \n    the weight is centred among our data points.\n    \"\"\"\n    return np.sum(data * weight) / np.sum(weight)\n\ndef estimate_std(data, weight, mean):\n    \"\"\"\n    For each data point, multiply the point's squared difference\n    from a mean value by the probability it was drawn from\n    that distribution (its \"weight\").\n\n    Divide by the total weight: essentially, we're finding where \n    the weight is centred among the values for the difference of\n    each data point from the mean.\n\n    This is the estimate of the variance, take the positive square\n    root to find the standard deviation.\n    \"\"\"\n    variance = np.sum(weight * (data - mean)**2) / np.sum(weight)\n    return np.sqrt(variance)\n\n# new estimates for standard deviation\nblue_std_guess = estimate_std(both_colours, blue_weight, blue_mean_guess)\nred_std_guess = estimate_std(both_colours, red_weight, red_mean_guess)\n\n# new estimates for mean\nred_mean_guess = estimate_mean(both_colours, red_weight)\nblue_mean_guess = estimate_mean(both_colours, blue_weight)\n</code></pre>\n<p>We have new estimates for the parameters. To improve them again, we can jump back to step 2 and repeat the process. We do this until the estimates converge, or after some number of iterations have been performed (<strong>step 5</strong>).</p>\n<p>For our data, the first five iterations of this process look like this (recent iterations have stronger appearance):</p>\n<p><a href=\"https://i.sstatic.net/QbKmW.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/QbKmW.png\"/></a></p>\n<p>We see that the means are already converging on some values, and the shapes of the curves (governed by the standard deviation) are also becoming more stable. </p>\n<p>If we continue for 20 iterations, we end up with the following:</p>\n<p><a href=\"https://i.sstatic.net/RVFBC.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/RVFBC.png\"/></a></p>\n<p>The EM process has converged to the following values, which turn out to very close to the actual values (where we can see the colours - no hidden variables):</p>\n<pre><code>          | EM guess | Actual |  Delta\n----------+----------+--------+-------\nRed mean  |    2.910 |  2.802 |  0.108\nRed std   |    0.854 |  0.871 | -0.017\nBlue mean |    6.838 |  6.932 | -0.094\nBlue std  |    2.227 |  2.195 |  0.032\n</code></pre>\n<p>In the code above you may have noticed that the new estimation for standard deviation was computed using the previous iteration's estimate for the mean. Ultimately it does not matter if we compute a new value for the mean first as we are just finding the (weighted) variance of values around some central point. We will still see the estimates for the parameters converge.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>EM is an algorithm for maximizing a likelihood function when some of the variables in your model are unobserved (i.e. when you have latent variables).  </p>\n<p>You might fairly ask, if we're just trying to maximize a function, why don't we just use the existing machinery for maximizing a function.  Well, if you try to maximize this by taking derivatives and setting them to zero, you find that in many cases the first-order conditions don't have a solution.  There's a chicken-and-egg problem in that to solve for your model parameters you need to know the distribution of your unobserved data; but the distribution of your unobserved data is a function of your model parameters.  </p>\n<p>E-M tries to get around this by iteratively guessing a distribution for the unobserved data, then estimating the model parameters by maximizing something that is a lower bound on the actual likelihood function, and repeating until convergence:</p>\n<p>The EM algorithm </p>\n<p>Start with guess for values of your model parameters</p>\n<p>E-step:  For each datapoint that has missing values, use your model equation to solve for the distribution of the missing data given your current guess of the model parameters and given the observed data (note that you are solving for a distribution for each missing value, not for the expected value).  Now that we have a distribution for each missing value, we can calculate the <em>expectation</em> of the likelihood function with respect to the unobserved variables.  If our guess for the model parameter was correct, this expected likelihood will be the actual likelihood of our observed data; if the parameters were not correct, it will just be a lower bound.</p>\n<p>M-step:  Now that we've got an expected likelihood function with no unobserved variables in it, maximize the function as you would in the fully observed case, to get a new estimate of your model parameters.</p>\n<p>Repeat until convergence.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a straight-forward recipe to understand the Expectation Maximisation algorithm:</p>\n<p><strong>1-</strong> Read this <a href=\"http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf\" rel=\"noreferrer\">EM tutorial paper</a> by Do and Batzoglou.</p>\n<p><strong>2-</strong> You may have question marks in your head, have a look at the explanations on this maths stack exchange <a href=\"https://math.stackexchange.com/questions/25111/how-does-expectation-maximization-work\">page</a>.</p>\n<p><strong>3-</strong> Look at this code that I wrote in Python that explains the example in the EM tutorial paper of item 1:</p>\n<p><strong><em>Warning :</em></strong> The code may be messy/suboptimal, since I am not a Python developer. But it does the job.</p>\n<pre><code>import numpy as np\nimport math\n\n#### E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* #### \n\ndef get_mn_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Multinomial Distribution Log PMF\n    # ln (pdf)      =             multinomial coeff            *   product of probabilities\n    # ln[f(x|n, p)] = [ln(n!) - (ln(x1!)+ln(x2!)+...+ln(xk!))] + [x1*ln(p1)+x2*ln(p2)+...+xk*ln(pk)]     \n\n    multinomial_coeff_denom= 0\n    prod_probs = 0\n    for x in range(0,len(obs)): # loop through state counts in each observation\n        multinomial_coeff_denom = multinomial_coeff_denom + math.log(math.factorial(obs[x]))\n        prod_probs = prod_probs + obs[x]*math.log(probs[x])\n\n    multinomial_coeff = math.log(math.factorial(sum(obs))) -  multinomial_coeff_denom\n    likelihood = multinomial_coeff + prod_probs\n    return likelihood\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement&gt;delta):\n    expectation_A = np.zeros((5,2), dtype=float) \n    expectation_B = np.zeros((5,2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n        ll_A = get_mn_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) # loglikelihood of e given coin A\n        ll_B = get_mn_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) # loglikelihood of e given coin B\n\n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of A proportional to likelihood of A \n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of B proportional to likelihood of B                            \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))\n    j = j+1\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this:</p>\n<pre><code>x1  x2  x3\n \\  /   /\n  y1   /\n   \\  /\n    y2\n</code></pre>\n<p>So, I'd created a model with two layers and tried to merge them but it returns an error: <code>The first layer in a Sequential model must get an \"input_shape\" or \"batch_input_shape\" argument.</code> on the line <code>result.add(merged)</code>.</p>\n<p>Model:</p>\n<pre class=\"lang-python prettyprint-override\"><code>first = Sequential()\nfirst.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n\nsecond = Sequential()\nsecond.add(Dense(1, input_shape=(1,), activation='sigmoid'))\n\nresult = Sequential()\nmerged = Concatenate([first, second])\nada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)\nresult.add(merged)\nresult.compile(optimizer=ada_grad, loss=_loss_tensor, metrics=['accuracy'])\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You're getting the error because <code>result</code> defined as <code>Sequential()</code> is just a container for the model and you have not defined an input for it.</p>\n<p>Given what you're trying to build set <code>result</code> to take the third input <code>x3</code>.</p>\n<pre class=\"lang-python prettyprint-override\"><code>first = Sequential()\nfirst.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n\nsecond = Sequential()\nsecond.add(Dense(1, input_shape=(1,), activation='sigmoid'))\n\nthird = Sequential()\n# of course you must provide the input to result which will be your x3\nthird.add(Dense(1, input_shape=(1,), activation='sigmoid'))\n\n# lets say you add a few more layers to first and second.\n# concatenate them\nmerged = Concatenate([first, second])\n\n# then concatenate the two outputs\n\nresult = Concatenate([merged,  third])\n\nada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)\n\nresult.compile(optimizer=ada_grad, loss='binary_crossentropy',\n               metrics=['accuracy'])\n</code></pre>\n<p>However, my preferred way of building a model that has this type of input structure would be to use the <a href=\"https://keras.io/getting-started/functional-api-guide/\" rel=\"noreferrer\">functional api</a>.</p>\n<p>Here is an implementation of your requirements to get you started:</p>\n<pre class=\"lang-python prettyprint-override\"><code>from keras.models import Model\nfrom keras.layers import Concatenate, Dense, LSTM, Input, concatenate\nfrom keras.optimizers import Adagrad\n\nfirst_input = Input(shape=(2, ))\nfirst_dense = Dense(1, )(first_input)\n\nsecond_input = Input(shape=(2, ))\nsecond_dense = Dense(1, )(second_input)\n\nmerge_one = concatenate([first_dense, second_dense])\n\nthird_input = Input(shape=(1, ))\nmerge_two = concatenate([merge_one, third_input])\n\nmodel = Model(inputs=[first_input, second_input, third_input], outputs=merge_two)\nada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer=ada_grad, loss='binary_crossentropy',\n               metrics=['accuracy'])\n</code></pre>\n<p><strong>To answer the question in the comments:</strong></p>\n<ol>\n<li>How are result and merged connected? Assuming you mean how are they concatenated.</li>\n</ol>\n<p>Concatenation works like this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>  a        b         c\na b c   g h i    a b c g h i\nd e f   j k l    d e f j k l\n</code></pre>\n<p>i.e rows are just joined.</p>\n<ol start=\"2\">\n<li>Now, <code>x1</code> is input to first, <code>x2</code> is input into second and <code>x3</code> input into third.</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Adding to the above-accepted answer so that it helps those who are using <code>tensorflow 2.0</code> </p>\n<pre class=\"lang-py prettyprint-override\"><code>\nimport tensorflow as tf\n\n# some data\nc1 = tf.constant([[1, 1, 1], [2, 2, 2]], dtype=tf.float32)\nc2 = tf.constant([[2, 2, 2], [3, 3, 3]], dtype=tf.float32)\nc3 = tf.constant([[3, 3, 3], [4, 4, 4]], dtype=tf.float32)\n\n# bake layers x1, x2, x3\nx1 = tf.keras.layers.Dense(10)(c1)\nx2 = tf.keras.layers.Dense(10)(c2)\nx3 = tf.keras.layers.Dense(10)(c3)\n\n# merged layer y1\ny1 = tf.keras.layers.Concatenate(axis=1)([x1, x2])\n\n# merged layer y2\ny2 = tf.keras.layers.Concatenate(axis=1)([y1, x3])\n\n# print info\nprint(\"-\"*30)\nprint(\"x1\", x1.shape, \"x2\", x2.shape, \"x3\", x3.shape)\nprint(\"y1\", y1.shape)\nprint(\"y2\", y2.shape)\nprint(\"-\"*30)\n</code></pre>\n<p>Result:</p>\n<pre><code>------------------------------\nx1 (2, 10) x2 (2, 10) x3 (2, 10)\ny1 (2, 20)\ny2 (2, 30)\n------------------------------\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can experiment with <code>model.summary()</code> (notice the concatenate_XX (Concatenate) layer size)</p>\n<pre><code># merge samples, two input must be same shape\ninp1 = Input(shape=(10,32))\ninp2 = Input(shape=(10,32))\ncc1 = concatenate([inp1, inp2],axis=0) # Merge data must same row column\noutput = Dense(30, activation='relu')(cc1)\nmodel = Model(inputs=[inp1, inp2], outputs=output)\nmodel.summary()\n\n# merge row must same column size\ninp1 = Input(shape=(20,10))\ninp2 = Input(shape=(32,10))\ncc1 = concatenate([inp1, inp2],axis=1)\noutput = Dense(30, activation='relu')(cc1)\nmodel = Model(inputs=[inp1, inp2], outputs=output)\nmodel.summary()\n\n# merge column must same row size\ninp1 = Input(shape=(10,20))\ninp2 = Input(shape=(10,32))\ncc1 = concatenate([inp1, inp2],axis=1)\noutput = Dense(30, activation='relu')(cc1)\nmodel = Model(inputs=[inp1, inp2], outputs=output)\nmodel.summary()\n</code></pre>\n<p>You can view notebook here for detail:\n<a href=\"https://nbviewer.jupyter.org/github/anhhh11/DeepLearning/blob/master/Concanate_two_layer_keras.ipynb\" rel=\"noreferrer\">https://nbviewer.jupyter.org/github/anhhh11/DeepLearning/blob/master/Concanate_two_layer_keras.ipynb</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-12-05 01:10:05Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/41990250/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I know that there are a lot of explanations of what cross-entropy is, but I'm still confused.</p>\n<p>Is it only a method to describe the loss function? Can we use gradient descent algorithm to find the minimum using the loss function?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross-entropy is commonly used to quantify the difference between two probability distributions. In the context of machine learning, it is a measure of error for categorical multi-class classification problems. Usually the \"true\" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution.</p>\n<p>For example, suppose for a specific training instance, the true label is B (out of the possible labels A, B, and C). The one-hot distribution for this training instance is therefore:</p>\n<pre><code>Pr(Class A)  Pr(Class B)  Pr(Class C)\n        0.0          1.0          0.0\n</code></pre>\n<p>You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C.</p>\n<p>Now, suppose your machine learning algorithm predicts the following probability distribution:</p>\n<pre><code>Pr(Class A)  Pr(Class B)  Pr(Class C)\n      0.228        0.619        0.153\n</code></pre>\n<p>How close is the predicted distribution to the true distribution? That is what the cross-entropy loss determines. Use this formula:</p>\n<p><a href=\"https://i.sstatic.net/gNip2.png\" rel=\"noreferrer\"><img alt=\"Cross entropy loss formula\" src=\"https://i.sstatic.net/gNip2.png\"/></a></p>\n<p>Where <code>p(x)</code> is the true probability distribution (one-hot) and <code>q(x)</code> is the predicted probability distribution. The sum is over the three classes A, B, and C. In this case the loss is <strong>0.479</strong> :</p>\n<pre><code>H = - (0.0*ln(0.228) + 1.0*ln(0.619) + 0.0*ln(0.153)) = 0.479\n</code></pre>\n<h3>Logarithm base</h3>\n<p>Note that it does not matter what logarithm base you use as long as you consistently use the same one. As it happens, the Python Numpy <code>log()</code> function computes the natural log (log base e).</p>\n<h3>Python code</h3>\n<p>Here is the above example expressed in Python using Numpy:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\n\np = np.array([0, 1, 0])             # True probability (one-hot)\nq = np.array([0.228, 0.619, 0.153]) # Predicted probability\n\ncross_entropy_loss = -np.sum(p * np.log(q))\nprint(cross_entropy_loss)\n# 0.47965000629754095\n</code></pre>\n<p>So that is how \"wrong\" or \"far away\" your prediction is from the true distribution. A machine learning optimizer will attempt to minimize the loss (i.e. it will try to reduce the loss from 0.479 to 0.0).</p>\n<h3>Loss units</h3>\n<p>We see in the above example that the loss is 0.4797. Because we are using the natural log (log base e), the units are in <a href=\"https://en.wikipedia.org/wiki/Nat_(unit)\" rel=\"noreferrer\"><em>nats</em></a>, so we say that the loss is 0.4797 nats. If the log were instead log base 2, then the units are in bits. See <a href=\"https://machinelearningmastery.com/cross-entropy-for-machine-learning/\" rel=\"noreferrer\">this page</a> for further explanation.</p>\n<h3>More examples</h3>\n<p>To gain more intuition on what these loss values reflect, let's look at some extreme examples.</p>\n<p>Again, let's suppose the true (one-hot) distribution is:</p>\n<pre><code>Pr(Class A)  Pr(Class B)  Pr(Class C)\n        0.0          1.0          0.0\n</code></pre>\n<p>Now suppose your machine learning algorithm did a really great job and predicted class B with very high probability:</p>\n<pre><code>Pr(Class A)  Pr(Class B)  Pr(Class C)\n      0.001        0.998        0.001\n</code></pre>\n<p>When we compute the cross entropy loss, we can see that the loss is tiny, only 0.002:</p>\n<pre class=\"lang-py prettyprint-override\"><code>p = np.array([0, 1, 0])\nq = np.array([0.001, 0.998, 0.001])\nprint(-np.sum(p * np.log(q)))\n# 0.0020020026706730793\n</code></pre>\n<p>At the other extreme, suppose your ML algorithm did a terrible job and predicted class C with high probability instead. The resulting loss of 6.91 will reflect the larger error.</p>\n<pre><code>Pr(Class A)  Pr(Class B)  Pr(Class C)\n      0.001        0.001        0.998\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>p = np.array([0, 1, 0])\nq = np.array([0.001, 0.001, 0.998])\nprint(-np.sum(p * np.log(q)))\n# 6.907755278982137\n</code></pre>\n<p>Now, what happens in the middle of these two extremes? Suppose your ML algorithm can't make up its mind and predicts the three classes with nearly equal probability.</p>\n<pre><code>Pr(Class A)  Pr(Class B)  Pr(Class C)\n      0.333        0.333        0.334\n</code></pre>\n<p>The resulting loss is 1.10.</p>\n<pre class=\"lang-py prettyprint-override\"><code>p = np.array([0, 1, 0])\nq = np.array([0.333, 0.333, 0.334])\nprint(-np.sum(p * np.log(q)))\n# 1.0996127890016931\n</code></pre>\n<h3>Fitting into gradient descent</h3>\n<p>Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss). These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values. In the equation below, you would replace <code>J(theta)</code> with <code>H(p, q)</code>. But note that you need to compute the derivative of <code>H(p, q)</code> with respect to the parameters first.</p>\n<p><a href=\"https://i.sstatic.net/ZSyZE.jpg\" rel=\"noreferrer\"><img alt=\"gradient descent\" src=\"https://i.sstatic.net/ZSyZE.jpg\"/></a></p>\n<p>So to answer your original questions directly:</p>\n<blockquote>\n<p>Is it only a method to describe the loss function?</p>\n</blockquote>\n<p>Correct, cross-entropy describes the loss between two probability distributions. It is one of many possible loss functions.</p>\n<blockquote>\n<p>Then we can use, for example, gradient descent algorithm to find the\nminimum.</p>\n</blockquote>\n<p>Yes, the cross-entropy loss function can be used as part of gradient descent.</p>\n<p>Further reading: one of my <a href=\"https://stackoverflow.com/a/39499486/4561314\">other answers</a> related to TensorFlow.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In short, cross-entropy(CE) is the measure of how far is your predicted value from the true label.</p>\n<p><a href=\"https://i.sstatic.net/LRjRR.png\" rel=\"noreferrer\"></a></p>\n<p>The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1).</p>\n<p>And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels.</p>\n<p>So the weights are changed to reduce CE and thus finally leads to reduced difference between the prediction and true labels and thus better accuracy.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Adding to the above posts, the simplest form of cross-entropy loss is known as <strong>binary-cross-entropy</strong> (used as loss function for binary classification, e.g., with logistic regression), whereas the generalized version is <strong>categorical-cross-entropy</strong> (used as loss function for multi-class classification problems, e.g., with neural networks).</p>\n<p>The idea remains the same:</p>\n<ol>\n<li><p>when the model-computed (softmax) class-probability becomes close to 1 for the target label for a training instance (represented with one-hot-encoding, e.g.,), the corresponding CCE loss decreases to zero</p>\n</li>\n<li><p>otherwise it increases as the predicted probability corresponding to the target class becomes smaller.</p>\n</li>\n</ol>\n<p>The following figure demonstrates the concept (notice from the figure that BCE becomes low when both of y and p are high or both of them are low simultaneously, i.e., there is an agreement):</p>\n<p><a href=\"https://i.sstatic.net/k19Ub.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/k19Ub.png\"/></a></p>\n<p><strong>Cross-entropy</strong> is closely related to <strong>relative entropy</strong> or <strong>KL-divergence</strong> that computes distance between two probability distributions. For example, in between two discrete pmfs, the relation between them is shown in the following figure:</p>\n<p><a href=\"https://i.sstatic.net/Oa7Ot.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Oa7Ot.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/10565868/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-10 21:19:34Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/10565868/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>If we have 10 eigenvectors then we can have 10 neural nodes in input layer.If we have 5 output classes then we can have 5 nodes in output layer.But what is the criteria for choosing number of hidden layer in a MLP and how many neural nodes in 1 hidden layer?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong><em>how many hidden layers</em>?</strong> </p>\n<p>a model with <em>zero</em> hidden layers will resolve <em>linearly separable</em> data. So unless you already know your data isn't linearly separable, it doesn't hurt to verify this--why use a more complex model than the task requires? If it is linearly separable then a simpler technique will work, but a Perceptron will do the job as well.</p>\n<p>Assuming your data does require separation by a non-linear technique, then <strong><em>always start with one hidden layer</em></strong>. Almost certainly that's all you will need. If your data is separable using a MLP, then that MLP probably only needs a single hidden layer. There is theoretical justification for this, but my reason is purely empirical:  Many difficult classification/regression problems are solved using single-hidden-layer MLPs, yet I don't recall encountering any multiple-hidden-layer MLPs used to successfully model data--whether on ML bulletin boards, ML Textbooks, academic papers, etc. They exist, certainly, but the circumstances that justify their use is empirically quite rare.</p>\n<p><br/>\n<strong><em>How many nodes in the hidden layer?</em></strong> </p>\n<p>From the MLP academic literature. my own experience, etc., I have gathered and often rely upon several rules of thumb (<em>RoT</em>), and which I have also found to be reliable guides (ie., the guidance was accurate, and even when it wasn't, it was usually clear what to do next):</p>\n<p><a href=\"http://en.wikibooks.org/wiki/Artificial_Neural_Networks/Neural_Network_Basics\" rel=\"noreferrer\">RoT</a> based on improving convergence:</p>\n<blockquote>\n<p>When you begin the model building, err on the side of <strong>more</strong> nodes\n  in the hidden layer.</p>\n</blockquote>\n<p>Why? First, a few extra nodes in the hidden layer isn't likely do any any harm--your MLP will still converge. On the other hand, too few nodes in the hidden layer can prevent convergence. Think of it this way, additional nodes provides some excess capacity--additional weights to store/release signal to the network during iteration (training, or model building). Second, if you begin with additional nodes in your hidden layer, then it's easy to prune them later (during iteration progress). This is common and there are diagnostic techniques to assist you (e.g., Hinton Diagram, which is just a visual depiction of the weight matrices, a 'heat map' of the weight values,). </p>\n<p><a href=\"ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu\" rel=\"noreferrer\">RoTs</a> based on size of input layer and size of output layer:</p>\n<blockquote>\n<p><em>A rule of thumb is for the size of this [hidden] layer to be somewhere\n  between the input layer size ... and the output layer size....</em></p>\n<p><em>To calculate the number of hidden nodes we use a general rule of:\n  (Number of inputs + outputs) x 2/3</em></p>\n</blockquote>\n<p><a href=\"ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu\" rel=\"noreferrer\">RoT</a> based on principal components:</p>\n<blockquote>\n<p><em>Typically, we specify as many hidden nodes as dimensions [principal\n  components] needed to capture 70-90% of the variance of the input data\n  set</em>.</p>\n</blockquote>\n<p>And yet the <a href=\"ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu\" rel=\"noreferrer\">NN FAQ</a> author calls these Rules \"nonsense\" (literally) because they: ignore the number of training instances, the noise in the targets (values of the response variables), and the complexity of the feature space.</p>\n<p>In his view (and it always seemed to me that he knows what he's talking about), <em>choose the number of neurons in the hidden layer based on whether your MLP includes some form of regularization, or early stopping</em>.\n<br/><br/></p>\n<p><strong><em>The only valid technique for optimizing the number of neurons in the Hidden Layer:</em></strong></p>\n<p>During your model building, test obsessively; testing will reveal the signatures of \"incorrect\" network architecture. For instance, if you begin with an MLP having a hidden layer comprised of a small number of nodes (which you will gradually increase as needed, based on test results) your training and generalization error will both be high caused by bias and underfitting.</p>\n<p>Then increase the number of nodes in the hidden layer, one at a time, until the generalization error begins to increase, this time due to overfitting and high variance.</p>\n<hr/>\n<p>In practice, I do it this way:</p>\n<p><strong>input layer</strong>: the size of my data vactor (the number of features in my model) + 1 for the bias node and not including the response variable, of course</p>\n<p><strong>output layer</strong>: soley determined by my model: regression (one node) versus classification (number of nodes equivalent to the number of classes, assuming softmax)</p>\n<p><strong>hidden layer</strong>: <em>to start</em>, <em>one hidden layer</em> with a number of nodes equal to the size of the input layer. The \"ideal\" size is more likely to be smaller (i.e, some number of nodes between the number in the input layer and the number in the output layer) rather than larger--again, this is just an empirical observation, and the bulk of this observation is my own experience. If the project justified the additional time required, then I start with a single hidden layer comprised of a small number of nodes, then (as i explained just above) I add nodes to the Hidden Layer, one at a time, while calculating the generalization error, training error, bias, and variance. When generalization error has dipped and just before it begins to increase again, the number of nodes at that point is my choice. See figure below.</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/LO1Id.png\"/></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To automate the selection of the best number of layers and best number of neurons for each of the layers, you can use <strong><a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" rel=\"nofollow\">genetic optimization</a></strong>.</p>\n<p>The key pieces would be:</p>\n<ol>\n<li><strong>Chromosome</strong>: Vector that defines how many units in each hidden layer (e.g. [20,5,1,0,0] meaning 20 units in first hidden layer, 5 in second, ... , with layers 4 and 5 missing). You can set a limit on the maximum number number of layers to try, and the max number of units in each layer. You should also place restrictions of how the chromosomes are generated. E.g. [10, 0, 3, ... ] should not be generated, because any units after a missing layer (the '3,...') would be irrelevant and would waste evaluation cycles. </li>\n<li><strong>Fitness Function</strong>: A function that returns the reciprocal of the lowest training error in the cross-validation set of a network defined by a given chromosome. You could also include the number of total units, or computation time if you want to find the \"smallest/fastest yet most accurate network\".</li>\n</ol>\n<p>You can also consider:</p>\n<ul>\n<li><strong>Pruning</strong>: Start with a large network, then reduce the layers and hidden units, while keeping track of cross-validation set performance.</li>\n<li><strong>Growing</strong>: Start with a very small network, then add units and layers, and again keep track of CV set performance.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is very difficult to choose the number of neurons in a hidden layer, and to choose the number of hidden layers in your neural network.</p>\n<p>Usually, for most applications, one hidden layer is enough. Also, the number of neurons in that hidden layer should be between the number of inputs (10 in your example) and the number of outputs (5 in your example).</p>\n<p>But the best way to choose the number of neurons and hidden layers is experimentation. Train several neural networks with different numbers of hidden layers and hidden neurons, and measure the performance of those networks using <a href=\"http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29\" rel=\"nofollow\">cross-validation</a>. You can stick with the number that yields the best performing network.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm working on a classification problem with unbalanced classes (5% 1's). <strong>I want to predict the class, not the probability</strong>.</p>\n<p>In a binary classification problem, is scikit's <code>classifier.predict()</code> using <code>0.5</code> by default?\nIf it doesn't, what's the default method? If it does, how do I change it?</p>\n<p>In scikit some classifiers have the <code>class_weight='auto'</code> option, but not all do. With <code>class_weight='auto'</code>, would <code>.predict()</code> use the actual population proportion as a threshold?</p>\n<p>What would be the way to do this in a classifier like <code>MultinomialNB</code> that doesn't support <code>class_weight</code>? Other than using <code>predict_proba()</code> and then calculation the classes myself.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The threshold can be set using <code>clf.predict_proba()</code></p>\n<p>for example:</p>\n<pre><code>from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state = 2)\nclf.fit(X_train,y_train)\n# y_pred = clf.predict(X_test)  # default threshold is 0.5\ny_pred = (clf.predict_proba(X_test)[:,1] &gt;= 0.3).astype(bool) # set threshold as 0.3\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The threshold in scikit learn is 0.5 for binary classification and whichever class has the greatest probability for multiclass classification. In many problems a much better result may be obtained by adjusting the threshold. However, this must be done with care and NOT on the holdout test data but by cross validation on the training data. If you do any adjustment of the threshold on your test data you are just overfitting the test data.</p>\n<p>Most methods of adjusting the threshold is based on the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" rel=\"noreferrer\">receiver operating characteristics (ROC)</a> and <a href=\"https://en.wikipedia.org/wiki/Youden%27s_J_statistic\" rel=\"noreferrer\">Youden's J statistic</a> but it can also be done by other methods such as a search with a genetic algorithm.</p>\n<p>Here is a peer review journal article describing doing this in medicine:</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2515362/\" rel=\"noreferrer\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2515362/</a> </p>\n<p>So far as I know there is no package for doing it in Python but it is relatively simple (but inefficient) to find it with a brute force search in Python.</p>\n<p>This is some R code that does it. </p>\n<pre><code>## load data\nDD73OP &lt;- read.table(\"/my_probabilites.txt\", header=T, quote=\"\\\"\")\n\nlibrary(\"pROC\")\n# No smoothing\nroc_OP &lt;- roc(DD73OP$tc, DD73OP$prob)\nauc_OP &lt;- auc(roc_OP)\nauc_OP\nArea under the curve: 0.8909\nplot(roc_OP)\n\n# Best threshold\n# Method: Youden\n#Youden's J statistic (Youden, 1950) is employed. The optimal cut-off is the threshold that maximizes the distance to the identity (diagonal) line. Can be shortened to \"y\".\n#The optimality criterion is:\n#max(sensitivities + specificities)\ncoords(roc_OP, \"best\", ret=c(\"threshold\", \"specificity\", \"sensitivity\"), best.method=\"youden\")\n#threshold specificity sensitivity \n#0.7276835   0.9092466   0.7559022\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>is scikit's <code>classifier.predict()</code> using 0.5 by default?</p>\n</blockquote>\n<p>In probabilistic classifiers, yes. It's the only sensible threshold from a mathematical viewpoint, as others have explained.</p>\n<blockquote>\n<p>What would be the way to do this in a classifier like MultinomialNB that doesn't support <code>class_weight</code>?</p>\n</blockquote>\n<p>You can set the <code>class_prior</code>, which is the prior probability P(<em>y</em>) per class <em>y</em>. That effectively shifts the decision boundary. E.g.</p>\n<pre><code># minimal dataset\n&gt;&gt;&gt; X = [[1, 0], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1]\n# use empirical prior, learned from y\n&gt;&gt;&gt; MultinomialNB().fit(X,y).predict([1,1])\narray([0])\n# use custom prior to make 1 more likely\n&gt;&gt;&gt; MultinomialNB(class_prior=[.1, .9]).fit(X,y).predict([1,1])\narray([1])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using <code>linear_model.LinearRegression</code> from scikit-learn as a predictive model. It works and it's perfect. I have a problem to evaluate the predicted results using the <code>accuracy_score</code> metric.</p>\n<p>This is my true Data :</p>\n<pre><code>array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n</code></pre>\n<p><strong>My predicted Data:</strong></p>\n<pre><code>array([ 0.07094605,  0.1994941 ,  0.19270157,  0.13379635,  0.04654469,\n    0.09212494,  0.19952108,  0.12884365,  0.15685076, -0.01274453,\n    0.32167554,  0.32167554, -0.10023553,  0.09819648, -0.06755516,\n    0.25390082,  0.17248324])\n</code></pre>\n<p><strong>My code:</strong></p>\n<pre><code>accuracy_score(y_true, y_pred, normalize=False)\n</code></pre>\n<p><strong>Error message:</strong></p>\n<pre><code>ValueError: Can't handle mix of binary and continuous target\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Despite the plethora of wrong answers here that attempt to circumvent the error by numerically manipulating the predictions, the root cause of your error is a <em>theoretical</em> and not computational issue: you are trying to use a <em>classification</em> metric (accuracy) in a regression (i.e. numeric prediction) model (<code>LinearRegression</code>), which is <strong>meaningless</strong>.</p>\n<p>Just like the majority of performance metrics, accuracy compares apples to apples (i.e true labels of 0/1 with predictions again of 0/1); so, when you ask the function to compare binary true labels (apples) with continuous predictions (oranges), you get an expected error, where the message tells you exactly what the problem is from a <em>computational</em> point of view:</p>\n<pre><code>Classification metrics can't handle a mix of binary and continuous target\n</code></pre>\n<p>Despite that the message doesn't tell you directly that you are trying to compute a metric that is invalid for your problem (and we shouldn't actually expect it to go that far), it is certainly a good thing that scikit-learn at least gives you a direct and explicit warning that you are attempting something wrong; this is not necessarily the case with other frameworks - see for example the <a href=\"https://stackoverflow.com/questions/48775305/what-function-defines-accuracy-in-keras-when-the-loss-is-mean-squared-error-mse/48788577#48788577\">behavior of Keras in a very similar situation</a>, where you get no warning at all, and one just ends up complaining for low \"accuracy\" in a regression setting...</p>\n<p>I am super-surprised with all the other answers here (including the accepted &amp; highly upvoted one) effectively suggesting to manipulate the predictions in order to simply get rid of the error; it's true that, once we end up with a set of numbers, we can certainly start mingling with them in various ways (rounding, thresholding etc) in order to make our code behave, but this of course does not mean that our numeric manipulations are <em>meaningful</em> in the specific context of the ML problem we are trying to solve.</p>\n<p>So, to wrap up: the problem is that you are applying a metric (accuracy) that is <strong>inappropriate</strong> for your model (<code>LinearRegression</code>): if you are in a classification setting, you should change your model (e.g. use <code>LogisticRegression</code> instead); if you are in a regression (i.e. numeric prediction) setting, you should change the metric. Check the <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\" rel=\"nofollow noreferrer\">list of metrics available in scikit-learn</a>, where you can confirm that accuracy is used only in classification.</p>\n<p>Compare also the situation with a <a href=\"https://stackoverflow.com/questions/54450453/python-3-7-scikit-learn-classification-metrics-cant-handle-a-mix-of-binary-and\">recent SO question</a>, where the OP is trying to get the accuracy of a list of models:</p>\n<pre><code>models = []\nmodels.append(('SVM', svm.SVC()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SGDRegressor', linear_model.SGDRegressor())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n#models.append(('BayesianRidge', linear_model.BayesianRidge())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n#models.append(('LassoLars', linear_model.LassoLars())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n#models.append(('ARDRegression', linear_model.ARDRegression())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n#models.append(('PassiveAggressiveRegressor', linear_model.PassiveAggressiveRegressor())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n#models.append(('TheilSenRegressor', linear_model.TheilSenRegressor())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n#models.append(('LinearRegression', linear_model.LinearRegression())) #ValueError: Classification metrics can't handle a mix of binary and continuous targets\n</code></pre>\n<p>where the first 6 models work OK, while all the rest (commented-out) ones give the same error. By now, you should be able to convince yourself that all the commented-out models are regression (and not classification) ones, hence the justified error.</p>\n<p>A last important note: it may sound legitimate for someone to claim:</p>\n<blockquote>\n<p>OK, but I want to use linear regression and then just\nround/threshold the outputs, effectively treating the predictions as\n\"probabilities\" and thus converting the model into a classifier</p>\n</blockquote>\n<p>Actually, this has already been suggested in several other answers here, implicitly or not; again, this is an <strong>invalid</strong> approach (and the fact that you have negative predictions should have already alerted you that they cannot be interpreted as probabilities). Andrew Ng, in his popular Machine Learning course at Coursera, explains why this is a bad idea - see his <a href=\"https://www.youtube.com/watch?v=-la3q9d7AKQ&amp;t=0s&amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&amp;index=33\" rel=\"nofollow noreferrer\">Lecture 6.1 - Logistic Regression | Classification</a> at Youtube (explanation starts at ~ 3:00), as well as section <strong>4.2 Why Not Linear Regression [for classification]?</strong> of the (highly recommended and freely available) textbook <a href=\"https://www.statlearning.com/\" rel=\"nofollow noreferrer\">An Introduction to Statistical Learning</a> by Hastie, Tibshirani and coworkers...</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>accuracy_score</code> is a classification metric, you cannot use it for a regression problem.</p>\n<p>You can see the available regression metrics in the <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\" rel=\"noreferrer\">docs</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The problem is that the true y is binary (zeros and ones), while your predictions are not. You probably generated probabilities and not predictions, hence the result :)\nTry instead to generate class membership, and it should work!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a matrix <code>A</code> and I want 2 matrices <code>U</code> and <code>L</code> such that <code>U</code> contains the upper triangular elements of A (all elements above and not including diagonal) and similarly for <code>L</code>(all elements below and not including diagonal). Is there a <code>numpy</code> method to do this?</p>\n<p>e.g</p>\n<pre><code>A = array([[ 4.,  9., -3.],\n           [ 2.,  4., -2.],\n           [-2., -3.,  7.]])\n\nU = array([[ 0.,  9., -3.],\n           [ 0.,  0., -2.],\n           [ 0.,  0.,  0.]])\n\nL = array([[ 0.,  0.,  0.],\n           [ 2.,  0.,  0.],\n           [-2., -3.,  0.]])\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To <strong>extract the upper triangle values</strong> to a flat vector,\nyou can do something like the following:</p>\n<pre><code>import numpy as np\n\na = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(a)\n\n#array([[1, 2, 3],\n#       [4, 5, 6],\n#       [7, 8, 9]])\n\na[np.triu_indices(3)]\n#or\nlist(a[np.triu_indices(3)])\n\n#array([1, 2, 3, 5, 6, 9])\n</code></pre>\n<hr/>\n<p>Similarly, for the <strong>lower triangle</strong>, use <code>np.tril</code>.</p>\n<hr/>\n<h2><em>IMPORTANT</em></h2>\n<p>If you want to extract the values that are <strong>above the diagonal</strong> (or <strong>below</strong>) then use the <strong>k</strong> argument. This is usually used when the matrix is symmetric.</p>\n<pre><code>import numpy as np\n\na = np.array([[1,2,3],[4,5,6],[7,8,9]])\n\n#array([[1, 2, 3],\n#       [4, 5, 6],\n#       [7, 8, 9]])\n\na[np.triu_indices(3, k = 1)]\n\n# this returns the following\narray([2, 3, 6])\n</code></pre>\n<hr/>\n<h2><strong>EDIT (on 11.11.2019):</strong></h2>\n<p>To put back the extracted vector into a 2D symmetric array, one can follow my answer here: <a href=\"https://stackoverflow.com/a/58806626/5025009\">https://stackoverflow.com/a/58806626/5025009</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.triu.html\" rel=\"noreferrer\"><code>numpy.triu</code></a> (triangle-upper) and <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.tril.html\" rel=\"noreferrer\"><code>numpy.tril</code></a> (triangle-lower).</p>\n<p>Code example:</p>\n<pre><code>np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 0,  8,  9],\n       [ 0,  0, 12]])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use the <a href=\"https://docs.scipy.org/doc/numpy-1.10.0/reference/routines.array-creation.html#building-matrices\" rel=\"noreferrer\">Array Creation Routines</a> of <a href=\"https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.triu.html#numpy.triu\" rel=\"noreferrer\">numpy.triu</a> and <a href=\"https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.triu.html#numpy.tril\" rel=\"noreferrer\">numpy.tril</a> to return a copy of a matrix with the elements above or below the k-th diagonal zeroed.</p>\n<pre><code>    &gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n    &gt;&gt;&gt; a\n    array([[1, 2, 3],\n           [4, 5, 6],\n           [7, 8, 9]])\n\n    &gt;&gt;&gt; tri_upper_diag = np.triu(a, k=0)\n    &gt;&gt;&gt; tri_upper_diag\n    array([[1, 2, 3],\n           [0, 5, 6],\n           [0, 0, 9]])\n\n    &gt;&gt;&gt; tri_upper_no_diag = np.triu(a, k=1)\n    &gt;&gt;&gt; tri_upper_no_diag\n    array([[0, 2, 3],\n           [0, 0, 6],\n           [0, 0, 0]])\n\n    &gt;&gt;&gt; tri_lower_diag = np.tril(a, k=0)\n    &gt;&gt;&gt; tri_lower_diag\n    array([[1, 0, 0],\n           [4, 5, 0],\n           [7, 8, 9]])\n\n    &gt;&gt;&gt; tri_lower_no_diag = np.tril(a, k=-1)\n    &gt;&gt;&gt; tri_lower_no_diag\n    array([[0, 0, 0],\n           [4, 0, 0],\n           [7, 8, 0]])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between <code>sparse_categorical_crossentropy</code> and <code>categorical_crossentropy</code>? When should one loss be used as opposed to the other? For example, are these losses suitable for linear regression?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Simply:</p>\n<ul>\n<li><code>categorical_crossentropy</code> (<code>cce</code>) produces a one-hot array containing the probable match for each category,</li>\n<li><code>sparse_categorical_crossentropy</code> (<code>scce</code>) produces a category index of the <em>most likely</em> matching category.</li>\n</ul>\n<p>Consider a classification problem with 5 categories (or classes).</p>\n<ul>\n<li><p>In the case of <code>cce</code>, the one-hot target may be <code>[0, 1, 0, 0, 0]</code> and the model may predict <code>[.2, .5, .1, .1, .1]</code> (probably right)</p>\n</li>\n<li><p>In the case of <code>scce</code>, the target index may be [1] and the model may predict: [.5].</p>\n</li>\n</ul>\n<p>Consider now a classification problem with 3 classes.</p>\n<ul>\n<li>In the case of <code>cce</code>, the one-hot target might be <code>[0, 0, 1]</code> and the model may predict <code>[.5, .1, .4]</code> (probably inaccurate, given that it gives more probability to the first class)</li>\n<li>In the case of <code>scce</code>, the target index might be <code>[0]</code>, and the model may predict <code>[.5]</code></li>\n</ul>\n<p>Many categorical models produce <code>scce</code> output because you save space, but lose A LOT of information (for example, in the 2nd example, index 2 was also very close.)  I generally prefer <code>cce</code> output for model reliability.</p>\n<p>There are a number of situations to use <code>scce</code>, including:</p>\n<ul>\n<li>when your classes are mutually exclusive, i.e. you don't care at all about other close-enough predictions,</li>\n<li>the number of categories is large to the prediction output becomes overwhelming.</li>\n</ul>\n<p><strong>220405</strong>: response to \"one-hot encoding\" comments:</p>\n<p>one-hot encoding is used for a category feature INPUT to select a specific category (e.g. male versus female).  This encoding allows the model to train more efficiently: training weight is a product of category, which is 0 for all categories except for the given one.</p>\n<p><code>cce</code> and <code>scce</code> are a model OUTPUT. <code>cce</code> is a probability array of each category, totally 1.0. <code>scce</code> shows the MOST LIKELY category, totally 1.0.</p>\n<p><code>scce</code> is technically a one-hot array, just like a hammer used as a door stop is still a hammer, but its purpose is different. <code>cce</code> is NOT one-hot.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was also confused with this one. Fortunately, the excellent keras documentation came to the rescue. Both have the same loss function and are ultimately doing the same thing, only difference is in the representation of the true labels.</p>\n<ul>\n<li>Categorical Cross Entropy [<a href=\"https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class\" rel=\"noreferrer\">Doc</a>]:</li>\n</ul>\n<blockquote>\n<p>Use this crossentropy loss function when there are two or more label\nclasses. We expect labels to be provided in a one_hot representation.</p>\n</blockquote>\n<pre><code>&gt;&gt;&gt; y_true = [[0, 1, 0], [0, 0, 1]]\n&gt;&gt;&gt; y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n&gt;&gt;&gt; # Using 'auto'/'sum_over_batch_size' reduction type.  \n&gt;&gt;&gt; cce = tf.keras.losses.CategoricalCrossentropy()\n&gt;&gt;&gt; cce(y_true, y_pred).numpy()\n1.177\n</code></pre>\n<ul>\n<li>Sparse Categorical Cross Entropy [<a href=\"https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class\" rel=\"noreferrer\">Doc</a>]:</li>\n</ul>\n<blockquote>\n<p>Use this crossentropy loss function when there are two or more label\nclasses. We expect labels to be provided as integers.</p>\n</blockquote>\n<pre><code>&gt;&gt;&gt; y_true = [1, 2]\n&gt;&gt;&gt; y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n&gt;&gt;&gt; # Using 'auto'/'sum_over_batch_size' reduction type.  \n&gt;&gt;&gt; scce = tf.keras.losses.SparseCategoricalCrossentropy()\n&gt;&gt;&gt; scce(y_true, y_pred).numpy()\n1.177\n</code></pre>\n<p>One good example of the sparse-categorical-cross-entropy is the fasion-mnist dataset.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\nfashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n\nprint(y_train_full.shape) # (60000,)\nprint(y_train_full.dtype) # uint8\n\ny_train_full[:10]\n# array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_ops.py#L3304\" rel=\"noreferrer\">From the TensorFlow source code</a>, the <code>sparse_categorical_crossentropy</code> is defined as <code>categorical crossentropy</code> with integer targets:</p>\n<pre><code>def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n  \"\"\"Categorical crossentropy with integer targets.\n  Arguments:\n      target: An integer tensor.\n      output: A tensor resulting from a softmax\n          (unless `from_logits` is True, in which\n          case `output` is expected to be the logits).\n      from_logits: Boolean, whether `output` is the\n          result of a softmax, or is a tensor of logits.\n      axis: Int specifying the channels axis. `axis=-1` corresponds to data\n          format `channels_last', and `axis=1` corresponds to data format\n          `channels_first`.\n  Returns:\n      Output tensor.\n  Raises:\n      ValueError: if `axis` is neither -1 nor one of the axes of `output`.\n  \"\"\"\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/backend.py#L4426\" rel=\"noreferrer\">From the TensorFlow source code</a>, the <code>categorical_crossentropy</code> is defined as categorical cross-entropy between an output tensor and a target tensor.</p>\n<pre><code>def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n  \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n  Arguments:\n      target: A tensor of the same shape as `output`.\n      output: A tensor resulting from a softmax\n          (unless `from_logits` is True, in which\n          case `output` is expected to be the logits).\n      from_logits: Boolean, whether `output` is the\n          result of a softmax, or is a tensor of logits.\n      axis: Int specifying the channels axis. `axis=-1` corresponds to data\n          format `channels_last', and `axis=1` corresponds to data format\n          `channels_first`.\n  Returns:\n      Output tensor.\n  Raises:\n      ValueError: if `axis` is neither -1 nor one of the axes of `output`.\n  \"\"\"\n\n</code></pre>\n<p>The meaning of integer targets is that the target labels should be in the form of an integer list that shows the index of class, for example:</p>\n<ul>\n<li><p>For <code>sparse_categorical_crossentropy</code>, For class 1 and class 2 targets, in a 5-class classification problem, the list should be [1,2]. Basically, the targets should be in integer form in order to call <code>sparse_categorical_crossentropy</code>. This is called sparse since the target representation requires much less space than one-hot encoding. For example, a batch with <code>b</code> targets and <code>k</code> classes needs <code>b * k</code> space to be represented in one-hot, whereas a batch with <code>b</code> targets and <code>k</code> classes needs <code>b</code> space to be represented in integer form.  </p></li>\n<li><p>For <code>categorical_crossentropy</code>, for class 1 and class 2 targets, in a 5-class classification problem, the list should be <code>[[0,1,0,0,0], [0,0,1,0,0]]</code>. Basically, the targets should be in one-hot form in order to call <code>categorical_crossentropy</code>.</p></li>\n</ul>\n<p>The representation of the targets are the only difference, the results should be the same since they are both calculating categorical crossentropy.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-02-11 15:04:24Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/36274638/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>In Computer Vision and Object Detection, a common evaluation method is mAP.\nWhat is it and how is it calculated?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Quotes are from the above mentioned <a href=\"http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf\" rel=\"noreferrer\">Zisserman paper - 4.2 Evaluation of Results (Page 11)</a>:</p>\n<p>First an \"overlap criterion\" is defined as an intersection-over-union greater than 0.5. (e.g. if a predicted box satisfies this criterion with respect to a ground-truth box, it is considered a detection). Then a matching is made between the GT boxes and the predicted boxes using this \"greedy\" approach:</p>\n<blockquote>\n<p>Detections output by a method were assigned to ground truth objects\n  satisfying the overlap criterion in order ranked by the (decreasing)\n  confidence output. Multiple detections of the same object in an image\n  were considered false detections e.g. 5 detections of a single object\n  counted as 1 correct detection and 4 false detections</p>\n</blockquote>\n<p>Hence each predicted box is either True-Positive or False-Positive.\nEach ground-truth box is True-Positive.\nThere are no True-Negatives.</p>\n<p>Then the average precision is computed by averaging the precision values on the precision-recall curve where the recall is in the range [0, 0.1, ..., 1] (e.g. average of 11 precision values). To be more precise, we consider a slightly corrected PR curve, where for each curve point (p, r), if there is a different curve point (p', r') such that p' &gt; p and r' &gt;= r, we replace p with maximum p' of those points.</p>\n<p>What is still unclear to me is what is done with those GT boxes that are <strong>never</strong> detected (even if the confidence is 0). This means that there are certain recall values that the precision-recall curve will never reach, and this makes the average precision computation above undefined.</p>\n<p><strong>Edit:</strong></p>\n<p>Short answer: in the region where the recall is unreachable, the precision drops to 0.</p>\n<p>One way to explain this is to assume that when the threshold for the confidence approaches 0, an infinite number of <strong>predicted</strong> bounding boxes light up all over the image.  The precision then immediately goes to 0 (since there is only a finite number of GT boxes) and the recall keeps growing on this flat curve until we reach 100%.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>mAP is Mean Average Precision.</p>\n<p>Its use is different in the field of Information Retrieval (Reference <a href=\"https://en.wikipedia.org/wiki/Information_retrieval#Mean_average_precision\" rel=\"noreferrer\">[1]</a> <a href=\"http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\" rel=\"noreferrer\">[2]</a> )and Multi-Class classification (Object Detection) settings.</p>\n<p>To calculate it for Object Detection, you calculate the average precision for each class in your data based on your model predictions. Average precision is related to the area under the precision-recall curve for a class. Then Taking the mean of these average individual-class-precision gives you the Mean Average Precision. </p>\n<p>To calculate Average Precision, see <a href=\"http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf\" rel=\"noreferrer\">[3]</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>For detection, a common way to determine if one object proposal was\n  right is <em>Intersection over Union</em> (IoU, IU). This takes the set <code>A</code>\n  of proposed object pixels and the set of true object pixels <code>B</code> and\n  calculates:</p>\n</blockquote>\n<p><img src=\"https://i.sstatic.net/9SfLJ.gif\" width=\"200\"/></p>\n<p>Commonly, IoU &gt; 0.5 means that it was a hit, otherwise it was a fail. For each class, one can calculate the</p>\n<ul>\n<li>True Positive TP(c): a proposal was made for class c and there actually was an object of class c</li>\n<li>False Positive FP(c): a proposal was made for class c, but there is no object of class c</li>\n<li>Average Precision for class c:<br/>\n<img src=\"https://i.sstatic.net/ndCCu.gif\" width=\"180\"/></li>\n</ul>\n<blockquote>\n<p>The mAP (mean average precision) is then:</p>\n</blockquote>\n<p><img src=\"https://i.sstatic.net/NSaRb.gif\" width=\"450\"/></p>\n<p><em>Note:</em>\nIf one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect). One can denote this with mAP@p, where p \\in (0, 1) is the IoU.</p>\n<p><code>mAP@[.5:.95]</code> means that the mAP is calculated over multiple thresholds and then again being averaged</p>\n<p><em>Edit:</em> \nFor more detailed Information see the COCO <a href=\"http://cocodataset.org/#detections-eval\" rel=\"noreferrer\">Evaluation metrics</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the correct way to perform gradient clipping in pytorch?</p>\n<p>I have an exploding gradients problem.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A more complete example from <a href=\"https://github.com/pytorch/pytorch/issues/309\" rel=\"noreferrer\">here</a>:</p>\n<pre><code>optimizer.zero_grad()        \nloss, hidden = model(data, hidden, targets)\nloss.backward()\n\ntorch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\noptimizer.step()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\" rel=\"noreferrer\"><code>clip_grad_norm</code></a> (which is actually deprecated in favor of <code>clip_grad_norm_</code> following the more consistent syntax of a trailing <code>_</code> when in-place modification is performed) clips the norm of the <em>overall</em> gradient by concatenating all parameters passed to the function, as can be seen from <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html\" rel=\"noreferrer\">the documentation</a>:</p>\n<blockquote>\n<p>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</p>\n</blockquote>\n<p>From your example it looks like that you want <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html\" rel=\"noreferrer\"><code>clip_grad_value_</code></a> instead which has a similar syntax and also modifies the gradients in-place:</p>\n<pre><code>clip_grad_value_(model.parameters(), clip_value)\n</code></pre>\n<p>Another option is to register a <a href=\"https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#forward-and-backward-function-hooks\" rel=\"noreferrer\">backward hook</a>. This takes the current gradient as an input and may return a tensor which will be used in-place of the previous gradient, i.e. modifying it. This hook is called each time after a gradient has been computed, i.e. there's no need for manually clipping once the hook has been registered:</p>\n<pre><code>for p in model.parameters():\n    p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Reading through <a href=\"http://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191\" rel=\"noreferrer\">the forum discussion</a> gave this:</p>\n<pre><code>clipping_value = 1 # arbitrary value of your choosing\ntorch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n</code></pre>\n<p>I'm sure there is more depth to it than only this code snippet.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was following a tutorial which was available at <a href=\"http://blog.christianperone.com/?p=1589\" rel=\"noreferrer\">Part 1</a> &amp; <a href=\"http://blog.christianperone.com/?p=1747\" rel=\"noreferrer\">Part 2</a>. Unfortunately the author didn't have the time for the final section which involved using cosine similarity to actually find the distance between two documents. I followed the examples in the article with the help of the following link from <a href=\"https://stackoverflow.com/questions/11911469/tfidf-for-search-queries\">stackoverflow</a>, included is the code mentioned in the above link (just so as to make life easier)</p>\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport numpy.linalg as LA\n\ntrain_set = [\"The sky is blue.\", \"The sun is bright.\"]  # Documents\ntest_set = [\"The sun in the sky is bright.\"]  # Query\nstopWords = stopwords.words('english')\n\nvectorizer = CountVectorizer(stop_words = stopWords)\n#print vectorizer\ntransformer = TfidfTransformer()\n#print transformer\n\ntrainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\ntestVectorizerArray = vectorizer.transform(test_set).toarray()\nprint 'Fit Vectorizer to train set', trainVectorizerArray\nprint 'Transform Vectorizer to test set', testVectorizerArray\n\ntransformer.fit(trainVectorizerArray)\nprint\nprint transformer.transform(trainVectorizerArray).toarray()\n\ntransformer.fit(testVectorizerArray)\nprint \ntfidf = transformer.transform(testVectorizerArray)\nprint tfidf.todense()\n</code></pre>\n<p>as a result of the above code I have the following matrix</p>\n<pre><code>Fit Vectorizer to train set [[1 0 1 0]\n [0 1 0 1]]\nTransform Vectorizer to test set [[0 1 1 1]]\n\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.70710678  0.          0.70710678]]\n\n[[ 0.          0.57735027  0.57735027  0.57735027]]\n</code></pre>\n<p>I am not sure how to use this output in order to calculate cosine similarity, I know how to implement cosine similarity with respect to two vectors of similar length but here I am not sure how to identify the two vectors.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First off, if you want to extract count features and apply TF-IDF normalization and row-wise euclidean normalization you can do it in one operation with <code>TfidfVectorizer</code>:</p>\n<pre><code>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n&gt;&gt;&gt; from sklearn.datasets import fetch_20newsgroups\n&gt;&gt;&gt; twenty = fetch_20newsgroups()\n\n&gt;&gt;&gt; tfidf = TfidfVectorizer().fit_transform(twenty.data)\n&gt;&gt;&gt; tfidf\n&lt;11314x130088 sparse matrix of type '&lt;type 'numpy.float64'&gt;'\n    with 1787553 stored elements in Compressed Sparse Row format&gt;\n</code></pre>\n<p>Now to find the cosine distances of one document (e.g. the first in the dataset) and all of the others you just need to compute the dot products of the first vector with all of the others as the tfidf vectors are already row-normalized.  </p>\n<p>As explained by Chris Clark in comments and <a href=\"https://github.com/taki0112/Vector_Similarity\" rel=\"noreferrer\">here</a> Cosine Similarity does not take into account the magnitude of the vectors. Row-normalised have a magnitude of 1 and so the Linear Kernel is sufficient to calculate the similarity values.</p>\n<p>The scipy sparse matrix API is a bit weird (not as flexible as dense N-dimensional numpy arrays). To get the first vector you need to slice the matrix row-wise to get a submatrix with a single row:</p>\n<pre><code>&gt;&gt;&gt; tfidf[0:1]\n&lt;1x130088 sparse matrix of type '&lt;type 'numpy.float64'&gt;'\n    with 89 stored elements in Compressed Sparse Row format&gt;\n</code></pre>\n<p>scikit-learn already provides pairwise metrics (a.k.a. kernels in machine learning parlance) that work for both dense and sparse representations of vector collections. In this case we need a dot product that is also known as the linear kernel:</p>\n<pre><code>&gt;&gt;&gt; from sklearn.metrics.pairwise import linear_kernel\n&gt;&gt;&gt; cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n&gt;&gt;&gt; cosine_similarities\narray([ 1.        ,  0.04405952,  0.11016969, ...,  0.04433602,\n    0.04457106,  0.03293218])\n</code></pre>\n<p>Hence to find the top 5 related documents, we can use <code>argsort</code> and some negative array slicing (most related documents have highest cosine similarity values, hence at the end of the sorted indices array):</p>\n<pre><code>&gt;&gt;&gt; related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n&gt;&gt;&gt; related_docs_indices\narray([    0,   958, 10576,  3277])\n&gt;&gt;&gt; cosine_similarities[related_docs_indices]\narray([ 1.        ,  0.54967926,  0.32902194,  0.2825788 ])\n</code></pre>\n<p>The first result is a sanity check: we find the query document as the most similar document with a cosine similarity score of 1 which has the following text:</p>\n<pre><code>&gt;&gt;&gt; print twenty.data[0]\nFrom: <a class=\"__cf_email__\" data-cfemail=\"a8c4cddad0dbdce8dfc9c586ddc5cc86cdccdd\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a> (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is\nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n</code></pre>\n<p>The second most similar document is a reply that quotes the original message hence has many common words:</p>\n<pre><code>&gt;&gt;&gt; print twenty.data[958]\nFrom: <a class=\"__cf_email__\" data-cfemail=\"1d6f6e78647072686f5d6f78787933787968\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a> (Robert Seymour)\nSubject: Re: WHAT car is this!?\nArticle-I.D.: reed.1993Apr21.032905.29286\nReply-To: <a class=\"__cf_email__\" data-cfemail=\"3c4e4f59455153494e7c4e59595812595849\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\nOrganization: Reed College, Portland, OR\nLines: 26\n\nIn article &lt;<a class=\"__cf_email__\" data-cfemail=\"64555d5d5725141656544a5553505650524a5550575351241305094a1109004a010011\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>&gt; <a class=\"__cf_email__\" data-cfemail=\"58343d2a202b2c182f3935762d353c763d3c2d\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a> (where's my\nthing) writes:\n&gt;\n&gt;  I was wondering if anyone out there could enlighten me on this car I saw\n&gt; the other day. It was a 2-door sports car, looked to be from the late 60s/\n&gt; early 70s. It was called a Bricklin. The doors were really small. In\naddition,\n&gt; the front bumper was separate from the rest of the body. This is\n&gt; all I know. If anyone can tellme a model name, engine specs, years\n&gt; of production, where this car is made, history, or whatever info you\n&gt; have on this funky looking car, please e-mail.\n\nBricklins were manufactured in the 70s with engines from Ford. They are rather\nodd looking with the encased front bumper. There aren't a lot of them around,\nbut Hemmings (Motor News) ususally has ten or so listed. Basically, they are a\nperformance Ford with new styling slapped on top.\n\n&gt;    ---- brought to you by your neighborhood Lerxst ----\n\nRush fan?\n\n--\nRobert Seymour              <a class=\"__cf_email__\" data-cfemail=\"6d1f1e08140002181f2d1f08080943080918\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\nPhysics and Philosophy, Reed College    (NeXTmail accepted)\nArtificial Life Project         Reed College\nReed Solar Energy Project (SolTrain)    Portland, OR\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>WIth the Help of @excray's comment, I manage to figure it out the answer, What we need to do is actually write a simple for loop to iterate over the two arrays that represent the train data and test data. </p>\n<p>First implement a simple lambda function to hold formula for the cosine calculation:</p>\n<pre><code>cosine_function = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n</code></pre>\n<p>And then just write a simple for loop to iterate over the to vector, logic is for every \"For each vector in trainVectorizerArray, you have to find the cosine similarity with the vector in testVectorizerArray.\"</p>\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport numpy.linalg as LA\n\ntrain_set = [\"The sky is blue.\", \"The sun is bright.\"] #Documents\ntest_set = [\"The sun in the sky is bright.\"] #Query\nstopWords = stopwords.words('english')\n\nvectorizer = CountVectorizer(stop_words = stopWords)\n#print vectorizer\ntransformer = TfidfTransformer()\n#print transformer\n\ntrainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\ntestVectorizerArray = vectorizer.transform(test_set).toarray()\nprint 'Fit Vectorizer to train set', trainVectorizerArray\nprint 'Transform Vectorizer to test set', testVectorizerArray\ncx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n\nfor vector in trainVectorizerArray:\n    print vector\n    for testV in testVectorizerArray:\n        print testV\n        cosine = cx(vector, testV)\n        print cosine\n\ntransformer.fit(trainVectorizerArray)\nprint\nprint transformer.transform(trainVectorizerArray).toarray()\n\ntransformer.fit(testVectorizerArray)\nprint \ntfidf = transformer.transform(testVectorizerArray)\nprint tfidf.todense()\n</code></pre>\n<p>Here is the output:</p>\n<pre><code>Fit Vectorizer to train set [[1 0 1 0]\n [0 1 0 1]]\nTransform Vectorizer to test set [[0 1 1 1]]\n[1 0 1 0]\n[0 1 1 1]\n0.408\n[0 1 0 1]\n[0 1 1 1]\n0.816\n\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.70710678  0.          0.70710678]]\n\n[[ 0.          0.57735027  0.57735027  0.57735027]]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I know its an old post. but I tried the <a href=\"http://scikit-learn.sourceforge.net/stable/\">http://scikit-learn.sourceforge.net/stable/</a> package. here is my code to find the cosine similarity. The question was how will you calculate the cosine similarity with this package and here is my code for that</p>\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nf = open(\"/root/Myfolder/scoringDocuments/doc1\")\ndoc1 = str.decode(f.read(), \"UTF-8\", \"ignore\")\nf = open(\"/root/Myfolder/scoringDocuments/doc2\")\ndoc2 = str.decode(f.read(), \"UTF-8\", \"ignore\")\nf = open(\"/root/Myfolder/scoringDocuments/doc3\")\ndoc3 = str.decode(f.read(), \"UTF-8\", \"ignore\")\n\ntrain_set = [\"president of India\",doc1, doc2, doc3]\n\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)  #finds the tfidf score with normalization\nprint \"cosine scores ==&gt; \",cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train)  #here the first element of tfidf_matrix_train is matched with other three elements\n</code></pre>\n<p>Here suppose the query is the first element of train_set and doc1,doc2 and doc3 are the documents which I want to rank with the help of cosine similarity. then I can use this code. </p>\n<p>Also the tutorials provided in the question was very useful. Here are all the parts for it \n<a href=\"http://pyevolve.sourceforge.net/wordpress/?p=1589\">part-I</a>,<a href=\"http://pyevolve.sourceforge.net/wordpress/?p=1747\">part-II</a>,<a href=\"http://pyevolve.sourceforge.net/wordpress/?p=2497\">part-III</a></p>\n<p>the output will be as follows :</p>\n<pre><code>[[ 1.          0.07102631  0.02731343  0.06348799]]\n</code></pre>\n<p>here 1 represents that query is matched with itself and the other three are the scores for matching the query with the respective documents.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about <a href=\"/help/on-topic\">a specific programming problem, a software algorithm, or software tools primarily used by programmers</a>. If you believe the question would be on-topic on <a href=\"https://stackexchange.com/sites\">another Stack Exchange site</a>, you can leave a comment to explain where the question may be able to be answered.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-01-25 10:07:18Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/40761185/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>In an LSTM network (<a href=\"https://i.sstatic.net/jbBjY.png\" rel=\"noreferrer\">Understanding LSTMs</a>), why does the input gate and output gate use tanh?</p>\n<p>What is the intuition behind this?</p>\n<p>It is just a nonlinear transformation? If it is, can I change both to another activation function (e.g., ReLU)?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>Sigmoid</em> specifically, is used as the gating function for the three gates (in, out, and forget) in <a href=\"https://en.wikipedia.org/wiki/Long_short-term_memory\" rel=\"noreferrer\">LSTM</a>, since it outputs a value between 0 and 1, and it can either let no flow or complete flow of information throughout the gates.</p>\n<p>On the other hand, to overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero. <code>Tanh</code> is a good function with the above property.</p>\n<p>A good neuron unit should be bounded, easily differentiable, monotonic (good for convex optimization) and easy to handle. If you consider these qualities, then I believe you can use <code>ReLU</code> in place of the <code>tanh</code> function since they are very good alternatives of each other.</p>\n<p>But before making a choice for activation functions, you must know what the advantages and disadvantages of your choice over others are. I am shortly describing some of the activation functions and their advantages.</p>\n<p><strong>Sigmoid</strong></p>\n<p>Mathematical expression: <code>sigmoid(z) = 1 / (1 + exp(-z))</code></p>\n<p>First-order derivative: <code>sigmoid'(z) = -exp(-z) / 1 + exp(-z)^2</code></p>\n<p>Advantages:</p>\n<pre><code>(1) The sigmoid function has all the fundamental properties of a good activation function.\n</code></pre>\n<p><strong>Tanh</strong></p>\n<p>Mathematical expression: <code>tanh(z) = [exp(z) - exp(-z)] / [exp(z) + exp(-z)]</code></p>\n<p>First-order derivative: <code>tanh'(z) = 1 - ([exp(z) - exp(-z)] / [exp(z) + exp(-z)])^2 = 1 - tanh^2(z)</code></p>\n<p>Advantages:</p>\n<pre><code>(1) Often found to converge faster in practice\n(2) Gradient computation is less expensive\n</code></pre>\n<p><strong>Hard Tanh</strong></p>\n<p>Mathematical expression: <code>hardtanh(z) = -1 if z &lt; -1; z if -1 &lt;= z &lt;= 1; 1 if z &gt; 1</code></p>\n<p>First-order derivative: <code>hardtanh'(z) = 1 if -1 &lt;= z &lt;= 1; 0 otherwise</code></p>\n<p>Advantages:</p>\n<pre><code>(1) Computationally cheaper than Tanh\n(2) Saturate for magnitudes of z greater than 1\n</code></pre>\n<p><strong>ReLU</strong></p>\n<p>Mathematical expression: <code>relu(z) = max(z, 0)</code></p>\n<p>First-order derivative: <code>relu'(z) = 1 if z &gt; 0; 0 otherwise</code></p>\n<p>Advantages:</p>\n<pre><code>(1) Does not saturate even for large values of z\n(2) Found much success in computer vision applications\n</code></pre>\n<p><strong>Leaky ReLU</strong></p>\n<p>Mathematical expression: <code>leaky(z) = max(z, k dot z) where 0 &lt; k &lt; 1</code></p>\n<p>First-order derivative: <code>relu'(z) = 1 if z &gt; 0; k otherwise</code></p>\n<p>Advantages:</p>\n<pre><code>(1) Allows propagation of error for non-positive z which ReLU doesn't\n</code></pre>\n<p><a href=\"ftp://ftp.icsi.berkeley.edu/pub/ai/jagota/vol2_6.pdf\" rel=\"noreferrer\">This paper</a> explains some fun activation function. You may consider to read it.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>LSTMs manage an internal state vector whose values should be able to increase or decrease when we add the output of some function. Sigmoid output is always non-negative; values in the state would only increase. The output from tanh can be positive or negative, allowing for increases and decreases in the state.</p>\n<p>That's why tanh is used to determine candidate values to get added to the internal state. The GRU cousin of the LSTM doesn't have a second tanh, so in a sense the second one is not necessary. Check out the diagrams and explanations in Chris Olah's <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noreferrer\">Understanding LSTM Networks</a> for more.</p>\n<p>The related question, \"Why are sigmoids used in LSTMs where they are?\" is also answered based on the possible outputs of the function: \"gating\" is achieved by multiplying by a number between zero and one, and that's what sigmoids output.</p>\n<p>There aren't really meaningful differences between the derivatives of sigmoid and tanh; tanh is just a rescaled and shifted sigmoid: see Richard Socher's <a href=\"https://cs224d.stanford.edu/lectures/CS224d-Lecture6.pdf\" rel=\"noreferrer\">Neural Tips and Tricks</a>. If second derivatives are relevant, I'd like to know how.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My training set has 970 samples and validation set has 243 samples.</p>\n<p>How big should batch size and number of epochs be when fitting a model to optimize the val_acc? Is there any sort of rule of thumb to use based on data input size?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since you have a pretty small dataset (~ 1000 samples), you would probably be safe using a batch size of 32, which is pretty standard. It won't make a huge difference for your problem unless you're training on hundreds of thousands or millions of observations. </p>\n<p><strong>To answer your questions on Batch Size and Epochs:</strong></p>\n<p><em>In general</em>: Larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but <em>can</em> converge faster. It's definitely problem dependent. </p>\n<p><em>In general</em>, the models improve with more epochs of training, to a point. They'll start to plateau in accuracy as they converge. Try something like 50 and plot number of epochs (x axis) vs. accuracy (y axis). You'll see where it levels out. </p>\n<p>What is the type and/or shape of your data? Are these images, or just tabular data? This is an important detail. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Great answers above. Everyone gave good inputs.</p>\n<p>Ideally, this is the sequence of the batch sizes that should be used: </p>\n<pre><code>{1, 2, 4, 8, 16} - slow \n\n{ [32, 64],[ 128, 256] }- Good starters\n\n[32, 64] - CPU\n\n[128, 256] - GPU for more boost\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use Keras to perform non-linear regression on speech data. Each of my speech files gives me features that are 25000 rows in a text file, with each row containing 257 real valued numbers. I use a batch size of 100, epoch 50 to train  <code>Sequential</code> model in Keras with 1 hidden layer. After 50 epochs of training, it converges quite well to a low <code>val_loss</code>. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-09-30 09:44:03Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2021-09-30 09:48:15Z\">3 years ago</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/53580088/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>How do I calculate the output size in a convolution layer?</p>\n<p>For example, I have a 2D convolution layer that takes a 3x128x128 input and has 40 filters of size 5x5.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can use this formula <code>[(W−K+2P)/S]+1</code>.</p>\n<ul>\n<li>W is the input volume - in your case 128 </li>\n<li>K is the Kernel size - in your case 5</li>\n<li>P is the padding - in your case 0 i believe</li>\n<li>S is the stride - which you have not provided. </li>\n</ul>\n<p>So, we input into the formula:</p>\n<pre><code>Output_Shape = (128-5+0)/1+1\n\nOutput_Shape = (124,124,40)\n</code></pre>\n<p>NOTE: Stride defaults to 1 if not provided and the <code>40</code> in <code>(124, 124, 40)</code> is the number of filters provided by the user.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can find it in two ways:\nsimple method: input_size - (filter_size - 1)</p>\n<pre><code>W - (K-1)\nHere W = Input size\n            K = Filter size\n            S = Stride\n            P = Padding\n</code></pre>\n<p>But the second method is the standard to find the output size.</p>\n<pre><code>Second method: (((W - K + 2P)/S) + 1)\n        Here W = Input size\n        K = Filter size\n        S = Stride\n        P = Padding \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let me start simple; since you have square matrices for both input and filter let me get one dimension. Then you can apply the same for other dimension(s). Imagine your are building fences between trees, if there are N trees, you have to build N-1 fences. Now apply that analogy to convolution layers.</p>\n<p>Your output size will be: input size - filter size + 1</p>\n<p><em>Because your filter can only have n-1 steps as fences I mentioned.</em></p>\n<p>Let's calculate your output with that idea.\n128 - 5 + 1 = 124\nSame for other dimension too. So now you have a 124 x 124 image.</p>\n<p>That is for one filter.</p>\n<p>If you apply this 40 times you will have another dimension: 124 x 124 x 40</p>\n<p>Here is a great guide if you want to know more about advanced convolution arithmetic: <a href=\"https://arxiv.org/pdf/1603.07285.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/1603.07285.pdf</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the <a href=\"https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\" rel=\"noreferrer\">MNIST beginner tutorial</a>, there is the statement </p>\n<pre><code>accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n</code></pre>\n<p><code>tf.cast</code> basically changes the type of tensor the object is, but what is the difference between <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_mean\" rel=\"noreferrer\"><code>tf.reduce_mean</code></a> and <a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html\" rel=\"noreferrer\"><code>np.mean</code></a>? </p>\n<p>Here is the doc on <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_mean\" rel=\"noreferrer\"><code>tf.reduce_mean</code></a>:</p>\n<blockquote>\n<p><code>reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></p>\n<p><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</p>\n<p><code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the defaut), reduces all dimensions.</p>\n<pre><code># 'x' is [[1., 1. ]]\n#         [2., 2.]]\ntf.reduce_mean(x) ==&gt; 1.5\ntf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]\ntf.reduce_mean(x, 1) ==&gt; [1.,  2.]\n</code></pre>\n</blockquote>\n<p>For a 1D vector, it looks like <code>np.mean == tf.reduce_mean</code>, but I don't understand what's happening in <code>tf.reduce_mean(x, 1) ==&gt; [1.,  2.]</code>. <code>tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]</code> kind of makes sense, since mean of <code>[1, 2]</code> and <code>[1, 2]</code> is <code>[1.5, 1.5]</code>, but what's going on with <code>tf.reduce_mean(x, 1)</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The functionality of <code>numpy.mean</code> and <code>tensorflow.reduce_mean</code> are the same. They do the same thing. From the documentation, for <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\" rel=\"noreferrer\">numpy</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_mean\" rel=\"noreferrer\">tensorflow</a>, you can see that. Lets look at an example,</p>\n<pre><code>c = np.array([[3.,4], [5.,6], [6.,7]])\nprint(np.mean(c,1))\n\nMean = tf.reduce_mean(c,1)\nwith tf.Session() as sess:\n    result = sess.run(Mean)\n    print(result)\n</code></pre>\n<p>Output</p>\n<pre><code>[ 3.5  5.5  6.5]\n[ 3.5  5.5  6.5]\n</code></pre>\n<p>Here you can see that when <code>axis</code>(numpy) or <code>reduction_indices</code>(tensorflow) is 1, it computes mean across (3,4) and (5,6) and (6,7), so <code>1</code> defines across which axis the mean is computed. When it is 0, the mean is computed across(3,5,6) and (4,6,7), and so on. I hope you get the idea.</p>\n<p>Now what are the differences between them?</p>\n<p>You can compute the numpy operation anywhere on python. But in order to do a tensorflow operation, it must be done inside a tensorflow <code>Session</code>. You can read more about it <a href=\"https://www.tensorflow.org/get_started/get_started\" rel=\"noreferrer\">here</a>. So when you need to perform any computation for your tensorflow graph(or structure if you will), it must be done inside a tensorflow <code>Session</code>.</p>\n<p>Lets look at another example.</p>\n<pre><code>npMean = np.mean(c)\nprint(npMean+1)\n\ntfMean = tf.reduce_mean(c)\nAdd = tfMean + 1\nwith tf.Session() as sess:\n    result = sess.run(Add)\n    print(result)\n</code></pre>\n<p>We could increase mean by <code>1</code> in <code>numpy</code> as you would naturally, but in order to do it in tensorflow, you need to perform that in <code>Session</code>, without using <code>Session</code> you can't do that. In other words, when you are computing <code>tfMean = tf.reduce_mean(c)</code>, tensorflow doesn't compute it then. It only computes that in a <code>Session</code>. But numpy computes that instantly, when you write <code>np.mean()</code>.</p>\n<p>I hope it makes sense.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The key here is the word reduce, a concept from functional programming, which makes it possible for <code>reduce_mean</code> in TensorFlow to keep a running average of the results of computations from a batch of inputs.</p>\n<p>If you are not familiar with functional programming, this can seem mysterious. So first let us see what <code>reduce</code> does. If you were given a list like [1,2,5,4] and were told to compute the mean, that is easy - just pass the whole array to <code>np.mean</code> and you get the mean. However what if you had to compute the mean of a stream of numbers? In that case, you would have to first assemble the array by reading from the stream and then call <code>np.mean</code> on the resulting array - you would have to write some more code.</p>\n<p>An alternative is to use the reduce paradigm. As an example, look at how we can use reduce in python to calculate the sum of numbers:\n<code>reduce(lambda x,y: x+y, [1,2,5,4])</code>.</p>\n<p>It works like this:</p>\n<ol>\n<li>Step 1: Read 2 digits from the list - 1,2. Evaluate lambda 1,2. reduce stores the result 3. Note - this is the only step where 2 digits are read off the list</li>\n<li>Step 2: Read the next digit from the list - 5. Evaluate lambda 5, 3 (3 being the result from step 1, that reduce stored). reduce stores the result 8.</li>\n<li>Step 3: Read the next digit from the list - 4. Evaluate lambda 8,4 (8 being the result of step 2, that reduce stored). reduce stores the result 12</li>\n<li>Step 4: Read the next digit from the list - there are none, so return the stored result of 12.</li>\n</ol>\n<p>Read more here <a href=\"https://www.python-course.eu/lambda.php\" rel=\"noreferrer\">Functional Programming in Python</a></p>\n<p>To see how this applies to TensorFlow, look at the following block of code, which defines a simple graph, that takes in a float and computes the mean. The input to the graph however is not a single float but an array of floats. The <code>reduce_mean</code> computes the mean value over all those floats.</p>\n<pre><code>import tensorflow as tf\n\n\ninp = tf.placeholder(tf.float32)\nmean = tf.reduce_mean(inp)\n\nx = [1,2,3,4,5]\n\nwith tf.Session() as sess:\n    print(mean.eval(feed_dict={inp : x}))\n</code></pre>\n<p>This pattern comes in handy when computing values over batches of images. Look at <a href=\"https://www.tensorflow.org/get_started/mnist/pros\" rel=\"noreferrer\">The Deep MNIST Example</a> where you see code like:</p>\n<pre><code>correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The new documentation states that <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_mean\" rel=\"noreferrer\"><code>tf.reduce_mean()</code></a> produces the same results as np.mean:</p>\n<blockquote>\n<p>Equivalent to np.mean</p>\n</blockquote>\n<p>It also has absolutely the same parameters as <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\" rel=\"noreferrer\">np.mean</a>. But here is an important difference: they produce the same results <strong>only on float values</strong>:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom random import randint\n\nnum_dims = 10\nrand_dim = randint(0, num_dims - 1)\nc = np.random.randint(50, size=tuple([5] * num_dims)).astype(float)\n\nwith tf.Session() as sess:\n    r1 = sess.run(tf.reduce_mean(c, rand_dim))\n    r2 = np.mean(c, rand_dim)\n    is_equal = np.array_equal(r1, r2)\n    print is_equal\n    if not is_equal:\n        print r1\n        print r2\n</code></pre>\n<p>If you will remove type conversion, you will see different results</p>\n<hr/>\n<p>In additional to this, many other <code>tf.reduce_</code> functions such as <code>reduce_all</code>, <code>reduce_any</code>, <code>reduce_min</code>, <code>reduce_max</code>, <code>reduce_prod</code> produce the same values as there numpy analogs. Clearly because they are operations, they can be executed only from inside of the session.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-04-06 01:31:59Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/53033556/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>When I increase/decrease batch size of the mini-batch used in SGD, should I change learning rate? If so, then how?</p>\n<p>For reference, I was discussing with someone, and it was said that, when batch size is increased, the learning rate should be decreased by some extent. </p>\n<p>My understanding is when I increase batch size, computed average gradient will be less noisy and so I either keep same learning rate or increase it. </p>\n<p>Also, if I use an adaptive learning rate optimizer, like Adam or RMSProp, then I guess I can leave learning rate untouched.</p>\n<p>Please correct me if I am mistaken and give any insight on this.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Theory suggests that when multiplying the batch size by k, one should multiply the learning rate by sqrt(k) to keep the variance in the gradient expectation constant. See page 5 at <em>A. Krizhevsky.   One weird trick for parallelizing convolutional neural networks</em>: <a href=\"https://arxiv.org/abs/1404.5997\" rel=\"noreferrer\">https://arxiv.org/abs/1404.5997</a></p>\n<p>However, recent experiments with large mini-batches suggest for a simpler linear scaling rule, i.e multiply your learning rate by k when using mini-batch size of kN.\nSee <em>P.Goyal et al.: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</em> <a href=\"https://arxiv.org/abs/1706.02677\" rel=\"noreferrer\">https://arxiv.org/abs/1706.02677</a></p>\n<p>I would say that with using Adam, Adagrad and other adaptive optimizers, learning rate may remain the same if batch size does not change substantially.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>Learning Rate Scaling for Dummies</h1>\n<p>I've always found the heuristics which seem to vary somewhere between scale with the square root of the batch size and the batch size to be a bit hand-wavy and fluffy, as is often the case in Deep Learning. Hence I devised my own theoretical framework to answer this question.</p>\n<p>EDIT: Since the posting of this answer, my paper on this topic has been published at the journal of machine learning (<a href=\"https://www.jmlr.org/papers/volume23/20-1258/20-1258.pdf\" rel=\"noreferrer\">https://www.jmlr.org/papers/volume23/20-1258/20-1258.pdf</a>). I want to thank the stackoverflow community for believing in my ideas, engaging with and probing me, at a time where the research community dismissed me out of hand.</p>\n<h3>Learning Rate is a function of the Largest Eigenvalue</h3>\n<p>Let me start with two small sub-questions, which answer the main question</p>\n<ul>\n<li>Are there any cases where we can a priori know the optimal learning rate?</li>\n</ul>\n<p><em>Yes, for the convex quadratic, the optimal learning rate is given as 2/(λ+μ), where λ,μ represent the largest and smallest eigenvalues of the Hessian (Hessian = the second derivative of the loss ∇∇L, which is a matrix) respectively.</em></p>\n<ul>\n<li>How do we expect these eigenvalues (which represent how much the loss changes along a infinitesimal move in the direction of the eigenvectors) to change as a function of batch size?</li>\n</ul>\n<p>This is actually a little more tricky to answer (it is what I made the theory for in the first place), but it goes something like this.</p>\n<p>Let us imagine that we have all the data and that would give us the full Hessian H. But now instead we only sub-sample this Hessian so we use a batch Hessian B. We can simply re-write B=H+(B-H)=H+E. Where E is now some error or fluctuation matrix.</p>\n<p>Under some technical assumptions on the nature of the elements of E, we can assume this fluctations to be a zero mean random matrix and so the Batch Hessian becomes a fixed matrix + a random matrix.</p>\n<p>For this model, the change in eigenvalues (which determines how large the learning rate can be) is known. <em>In my paper there is another more fancy model, but the answer is more or less the same.</em></p>\n<h3>What actually happens? Experiments and Scaling Rules</h3>\n<p>I attach a plot of what happens in the case that the largest eigenvalue from the full data matrix is far outside that of the noise matrix (usually the case). As we increase the mini-batch size, the size of the noise matrix decreases and so the largest eigenvalue also decreases in size, hence larger learning rates can be used. This effect is initially proportional and continues to be approximately proportional until a threshold after which no appreciable decrease happens.</p>\n<p><a href=\"https://i.sstatic.net/THQwJ.png\" rel=\"noreferrer\"><img alt=\"largest eigenvalue change with minibatching\" src=\"https://i.sstatic.net/THQwJ.png\"/></a></p>\n<p>How well does this hold in practice? The answer as shown below in my plot on the VGG-16 without batch norm (see paper for batch normalisation and resnets), is very well.</p>\n<p><a href=\"https://i.sstatic.net/gp9em.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/gp9em.png\"/></a></p>\n<p>I would hasten to add that for adaptive order methods, if you use a small numerical stability constant (epsilon for Adam) the argument is a little different because you have an interplay of the eigenvalues, the estimated eigenvalues and your stability constant! so you actually end up getting a square root rule up till a threshold. Quite why nobody is discussing this or has published this result is honestly a little beyond me.</p>\n<p><a href=\"https://i.sstatic.net/b1MyO.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/b1MyO.png\"/></a></p>\n<p>But if you want my practical advice, stick with SGD and just go proportional to the increase in batch size if your batch size is small and then don't increase it beyond a certain point.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Apart from the papers mentioned in Dmytro's answer, you can refer to the article of: <a href=\"https://www.research.ed.ac.uk/portal/files/75846467/width_of_minima_reached_by_stochastic_gradient_descent_is_influenced_by_learning_rate_to_batch_size_ratio.pdf\" rel=\"noreferrer\">Jastrzębski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., &amp; Storkey, A. (2018, October). Width of Minima Reached by Stochastic Gradient Descent is Influenced by Learning Rate to Batch Size Ratio</a>. The authors give the mathematical and empirical foundation to the idea that the ratio of learning rate to batch size influences the generalization capacity of DNN. They show that this ratio plays a major role in the width of the minima found by SGD. The higher ratio the wider is minima and better generalization.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose I have a Tensorflow tensor. How do I get the dimensions (shape) of the tensor as integer values? I know there are two methods, <code>tensor.get_shape()</code> and <code>tf.shape(tensor)</code>, but I can't get the shape values as integer <code>int32</code> values.</p>\n<p>For example, below I've created a 2-D tensor, and I need to get the number of rows and columns as <code>int32</code> so that I can call <code>reshape()</code> to create a tensor of shape <code>(num_rows * num_cols, 1)</code>. However, the method <code>tensor.get_shape()</code> returns values as <code>Dimension</code> type, not <code>int32</code>.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nsess = tf.Session()    \ntensor = tf.convert_to_tensor(np.array([[1001,1002,1003],[3,4,5]]), dtype=tf.float32)\n\nsess.run(tensor)    \n# array([[ 1001.,  1002.,  1003.],\n#        [    3.,     4.,     5.]], dtype=float32)\n\ntensor_shape = tensor.get_shape()    \ntensor_shape\n# TensorShape([Dimension(2), Dimension(3)])    \nprint tensor_shape    \n# (2, 3)\n\nnum_rows = tensor_shape[0] # ???\nnum_cols = tensor_shape[1] # ???\n\ntensor2 = tf.reshape(tensor, (num_rows*num_cols, 1))    \n# Traceback (most recent call last):\n#   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1750, in reshape\n#     name=name)\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 454, in apply_op\n#     as_ref=input_arg.is_ref)\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 621, in convert_to_tensor\n#     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 180, in _constant_tensor_conversion_function\n#     return constant(v, dtype=dtype, name=name)\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 163, in constant\n#     tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 353, in make_tensor_proto\n#     _AssertCompatible(values, dtype)\n#   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 290, in _AssertCompatible\n#     (dtype.name, repr(mismatch), type(mismatch).__name__))\n# TypeError: Expected int32, got Dimension(6) of type 'Dimension' instead.\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To get the shape as a list of ints, do <code>tensor.get_shape().as_list()</code>.</p>\n<p>To complete your <code>tf.shape()</code> call, try <code>tensor2 = tf.reshape(tensor, tf.TensorShape([num_rows*num_cols, 1]))</code>. Or you can directly do <code>tensor2 = tf.reshape(tensor, tf.TensorShape([-1, 1]))</code> where its first dimension can be inferred.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Another way to solve this is like this:</p>\n<pre><code>tensor_shape[0].value\n</code></pre>\n<p>This will return the int value of the Dimension object.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>2.0 Compatible Answer</strong>: In <strong><code>Tensorflow 2.x (2.1)</code></strong>, you can get the dimensions (shape) of the tensor as integer values, as shown in the Code below:</p>\n<p><strong>Method 1 (using <code>tf.shape</code>)</strong>:</p>\n<pre><code>import tensorflow as tf\nc = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nShape = c.shape.as_list()\nprint(Shape)   # [2,3]\n</code></pre>\n<p><strong>Method 2 (using <code>tf.get_shape()</code>)</strong>:</p>\n<pre><code>import tensorflow as tf\nc = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nShape = c.get_shape().as_list()\nprint(Shape)   # [2,3]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have noticed that <em>weight_regularizer</em> is no more available in Keras and that, in its place, there are <em>activity</em> and <em>kernel</em> regularizer. \nI would like to know:</p>\n<ul>\n<li>What are the main differences between <em>kernel</em> and <em>activity</em> regularizers?</li>\n<li>Could I use <em>activity_regularizer</em> in place of <em>weight_regularizer</em>?</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The activity regularizer works as a function of the output of the net, and is mostly used to regularize hidden units, while weight_regularizer, as the name says, works on the weights (e.g. making them decay). Basically you can express the regularization loss as a function of the output (<code>activity_regularizer</code>) or of the weights (<code>weight_regularizer</code>).</p>\n<p>The new <code>kernel_regularizer</code> replaces <code>weight_regularizer</code> - although it's not very clear from the documentation.</p>\n<p>From the definition of <code>kernel_regularizer</code>:</p>\n<blockquote>\n<p>kernel_regularizer: Regularizer function applied to\nthe <code>kernel</code> weights matrix\n(see regularizer).</p>\n</blockquote>\n<p>And <code>activity_regularizer</code>:</p>\n<blockquote>\n<p>activity_regularizer: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see regularizer).</p>\n</blockquote>\n<p><strong>Important Edit</strong>: Note that there is a bug in the <em>activity_regularizer</em> that was <strong>only fixed in version 2.1.4 of Keras</strong> (at least with Tensorflow backend). Indeed, in the older versions, the activity regularizer function is applied to the input of the layer, instead of being applied to the output (the actual activations of the layer, as intended). So beware if you are using an older version of Keras (before 2.1.4), activity regularization may probably not work as intended.</p>\n<p>You can see the commit on <a href=\"https://github.com/keras-team/keras/commits/master?after=75114feeac5ee6aa7679802ce7e5172c63565e2c+279\" rel=\"noreferrer\">GitHub</a></p>\n<p><a href=\"https://i.sstatic.net/EQSyd.png\" rel=\"noreferrer\">Five months ago François Chollet provided a fix to the activity regularizer, that was then included in Keras 2.1.4</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This answer is a bit late, but is useful for the future readers.\nSo, necessity is the mother of invention as they say. I only understood it when I needed it.<br/>\nThe above answer doesn't really state the difference cause both of them end up affecting the weights, so what's the difference between punishing for the weights themselves or the output of the layer?<br/>\nHere is the answer: I encountered a case where the weights of the net are small and nice, ranging between [-0.3] to [+0.3].<br/>\nSo, I really can't punish them, there is nothing wrong with them. A kernel regularizer is useless. However, the output of the layer is HUGE, in 100's.<br/>\nKeep in mind that the input to the layer is also small, always less than one. But those small values interact with the weights in such a way that produces those massive outputs. Here I realized that what I need is an activity regularizer, rather than kernel regularizer. With this, I'm punishing the layer for those large outputs, I don't care if the weights themselves are small, I just want to deter it from reaching such state cause this saturates my sigmoid activation and causes tons of other troubles like vanishing gradient and stagnation.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Helpful answer above. I want to add another use-case I have encountered, and which I've used it for.</p>\n<p>The use-case is in the context of autoencoders, where you want to limit the size of the encodings to a small number (say 10 units, for example).</p>\n<p>One way of forcing an encoding size of 10 units is to simply make an autoencoder whose coding layer is limited to 10 units. This is a hard limit, but has some drawbacks.</p>\n<p>An alternative way of getting small-sized encodings is to impose a 'soft limit' using <em>activity regularisation</em>. You create a large coding layer (e.g. 100 units), but you penalize the net whenever it activates more than 10 units from the 100 available units. You don't care about the weights/kernel of the encoding layer (hence no kernel regularisation); you instead penalise the output/activity of that layer such that you push it towards more sparse outputs/encodings. This results in a \"sparse autoencoder\" as you've encouraged the net to only use 10% of its available coding size on average.</p>\n<p>A sketch of my training loop in PyTorch:</p>\n<pre><code>autoencoder = torch.nn.Sequential(encoder, decoder)\n\nfor epoch in range(n_epochs):\n  for X_in in train_data:\n    X_recon = autoencoder(X_in)\n    \n    #For a basic autoencoder, the loss is simply MSE:\n    mse_loss = torch.nn.MSELoss()(X_recon, X_in)\n    \n    #For a sparse autoencoder using activity regularisation,\n    #  add a loss term penalising too many active outputs:\n    codings = encoder(X) #get the activations from the coding layer\n    activity_regularisation_l1_loss = codings.abs().sum()\n\n    total_loss = mse_loss + 1e-3 * activity_regularisation_l1_loss\n\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n  \n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can someone please explain this? I know bidirectional LSTMs have a forward and backward pass but what is the advantage of this over a unidirectional LSTM?</p>\n<p>What is each of them better suited for?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>LSTM in its core, preserves information from inputs that has already passed through it using the hidden state.</p>\n<p>Unidirectional LSTM only preserves information of the <strong>past</strong> because the only inputs it has seen are from the past.</p>\n<p>Using bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the <strong>future</strong> and using the two hidden states combined you are able in any point in time to preserve information from <strong>both past and future</strong>.</p>\n<p>What they are suited for is a very complicated question but BiLSTMs show very good results as they can understand context better, I will try to explain through an example.</p>\n<p>Lets say we try to predict the next word in a sentence, on a high level what a unidirectional LSTM will see is</p>\n<blockquote>\n<p>The boys went to ....</p>\n</blockquote>\n<p>And will try to predict the next word only by this context, with bidirectional LSTM you will be able to see information further down the road for example</p>\n<p>Forward LSTM:</p>\n<blockquote>\n<p>The boys went to ...</p>\n</blockquote>\n<p>Backward LSTM:</p>\n<blockquote>\n<p>... and then they got out of the pool</p>\n</blockquote>\n<p>You can see that using the information from the future it could be easier for the network to understand what the next word is.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Adding to Bluesummer's answer, here is how you would implement Bidirectional LSTM from scratch without calling <code>BiLSTM</code> module. This might better contrast the difference between a uni-directional and bi-directional LSTMs. As you see, we merge two LSTMs to create a bidirectional LSTM.</p>\n<p>You can merge outputs of the forward and backward LSTMs by using either <code>{'sum', 'mul', 'concat', 'ave'}</code>.</p>\n<pre><code>left = Sequential()\nleft.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n               forget_bias_init='one', return_sequences=True, activation='tanh',\n               inner_activation='sigmoid', input_shape=(99, 13)))\nright = Sequential()\nright.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n               forget_bias_init='one', return_sequences=True, activation='tanh',\n               inner_activation='sigmoid', input_shape=(99, 13), go_backwards=True))\n\nmodel = Sequential()\nmodel.add(Merge([left, right], mode='sum'))\n\nmodel.add(TimeDistributedDense(nb_classes))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-5, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\nprint(\"Train...\")\nmodel.fit([X_train, X_train], Y_train, batch_size=1, nb_epoch=nb_epoches, validation_data=([X_test, X_test], Y_test), verbose=1, show_accuracy=True)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In comparison to <code>LSTM</code>, <code>BLSTM</code> or <code>BiLSTM</code> has two networks, one access <code>past</code>information in <code>forward</code> direction and another access <code>future</code> in the <code>reverse</code> direction. <a href=\"https://en.wikipedia.org/wiki/File:RNN_BRNN.png\" rel=\"noreferrer\">wiki</a></p>\n<p>A new class <code>Bidirectional</code> is added as per official doc here: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\" rel=\"noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional</a></p>\n<pre><code>model = Sequential()\nmodel.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5,\n10)))\n</code></pre>\n<p>and  activation function can be added like this:</p>\n<pre><code>model = Sequential()\nmodel.add(Bidirectional(LSTM(num_channels, \n        implementation = 2, recurrent_activation = 'sigmoid'),\n        input_shape=(input_length, input_dim)))\n</code></pre>\n<p>Complete example using IMDB data will be like this.The result after 4 epoch.</p>\n<pre><code>Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n17465344/17464789 [==============================] - 4s 0us/step\nTrain...\nTrain on 25000 samples, validate on 25000 samples\nEpoch 1/4\n25000/25000 [==============================] - 78s 3ms/step - loss: 0.4219 - acc: 0.8033 - val_loss: 0.2992 - val_acc: 0.8732\nEpoch 2/4\n25000/25000 [==============================] - 82s 3ms/step - loss: 0.2315 - acc: 0.9106 - val_loss: 0.3183 - val_acc: 0.8664\nEpoch 3/4\n25000/25000 [==============================] - 91s 4ms/step - loss: 0.1802 - acc: 0.9338 - val_loss: 0.3645 - val_acc: 0.8568\nEpoch 4/4\n25000/25000 [==============================] - 92s 4ms/step - loss: 0.1398 - acc: 0.9509 - val_loss: 0.3562 - val_acc: 0.8606\n</code></pre>\n<p><strong>BiLSTM</strong> or <strong>BLSTM</strong></p>\n<pre><code>import numpy as np\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\nfrom keras.datasets import imdb\n\n\nn_unique_words = 10000 # cut texts after this number of words\nmaxlen = 200\nbatch_size = 128\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=n_unique_words)\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\nmodel.add(Embedding(n_unique_words, 128, input_length=maxlen))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=4,\n          validation_data=[x_test, y_test])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>After Training, I saved Both Keras whole Model and Only Weights using </p>\n<pre><code>model.save_weights(MODEL_WEIGHTS) and model.save(MODEL_NAME)\n</code></pre>\n<p>Models and Weights were saved successfully and there was no error.\nI can successfully load the weights simply using model.load_weights and they are good to go, but when i try to load the save model via load_model, i am getting an error.</p>\n<pre><code>File \"C:/Users/Rizwan/model_testing/model_performance.py\", line 46, in &lt;module&gt;\nModel2 = load_model('nasnet_RS2.h5',custom_objects={'euc_dist_keras': euc_dist_keras})\nFile \"C:\\Users\\Rizwan\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py\", line 419, in load_model\nmodel = _deserialize_model(f, custom_objects, compile)\nFile \"C:\\Users\\Rizwan\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py\", line 321, in _deserialize_model\noptimizer_weights_group['weight_names']]\nFile \"C:\\Users\\Rizwan\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py\", line 320, in &lt;listcomp&gt;\nn.decode('utf8') for n in\nAttributeError: 'str' object has no attribute 'decode'\n</code></pre>\n<p>I never received this error and i used to load any models successfully. I am using Keras 2.2.4 with tensorflow backend. Python 3.6.\nMy Code for training is :</p>\n<pre><code>from keras_preprocessing.image import ImageDataGenerator\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom keras.callbacks import ReduceLROnPlateau, TensorBoard, \nModelCheckpoint,EarlyStopping\nimport pandas as pd\n\nMODEL_NAME = \"nasnet_RS2.h5\"\nMODEL_WEIGHTS = \"nasnet_RS2_weights.h5\"\ndef euc_dist_keras(y_true, y_pred):\nreturn K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))\ndef main():\n\n# Here, we initialize the \"NASNetMobile\" model type and customize the final \n#feature regressor layer.\n# NASNet is a neural network architecture developed by Google.\n# This architecture is specialized for transfer learning, and was discovered via Neural Architecture Search.\n# NASNetMobile is a smaller version of NASNet.\nmodel = NASNetMobile()\nmodel = Model(model.input, Dense(1, activation='linear', kernel_initializer='normal')(model.layers[-2].output))\n\n#    model = load_model('current_best.hdf5', custom_objects={'euc_dist_keras': euc_dist_keras})\n\n# This model will use the \"Adam\" optimizer.\nmodel.compile(\"adam\", euc_dist_keras)\nlr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.003)\n# This callback will log model stats to Tensorboard.\ntb_callback = TensorBoard()\n# This callback will checkpoint the best model at every epoch.\nmc_callback = ModelCheckpoint(filepath='current_best_mem3.h5', verbose=1, save_best_only=True)\nes_callback=EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n\n# This is the train DataSequence.\n# These are the callbacks.\n#callbacks = [lr_callback, tb_callback,mc_callback]\ncallbacks = [lr_callback, tb_callback,es_callback]\n\ntrain_pd = pd.read_csv(\"./train3.txt\", delimiter=\" \", names=[\"id\", \"label\"], index_col=None)\ntest_pd = pd.read_csv(\"./val3.txt\", delimiter=\" \", names=[\"id\", \"label\"], index_col=None)\n\n #    train_pd = pd.read_csv(\"./train2.txt\",delimiter=\" \",header=None,index_col=None)\n #    test_pd = pd.read_csv(\"./val2.txt\",delimiter=\" \",header=None,index_col=None)\n#model.summary()\nbatch_size=32\ndatagen = ImageDataGenerator(rescale=1. / 255)\ntrain_generator = datagen.flow_from_dataframe(dataframe=train_pd, \ndirectory=\"./images\", x_col=\"id\", y_col=\"label\",\n                                              has_ext=True, \nclass_mode=\"other\", target_size=(224, 224),\n                                              batch_size=batch_size)\nvalid_generator = datagen.flow_from_dataframe(dataframe=test_pd, directory=\"./images\", x_col=\"id\", y_col=\"label\",\n                                              has_ext=True, class_mode=\"other\", target_size=(224, 224),\n                                              batch_size=batch_size)\n\nSTEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size\nmodel.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    callbacks=callbacks,\n                    epochs=20)\n\n# we save the model.\nmodel.save_weights(MODEL_WEIGHTS)\nmodel.save(MODEL_NAME)\nif __name__ == '__main__':\n   # freeze_support() here if program needs to be frozen\n    main()\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For me the solution was downgrading the <code>h5py</code> package (in my case to 2.10.0), apparently putting back only Keras and Tensorflow to the correct versions was not enough.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I downgraded my h5py package with the following command,</p>\n<pre><code>pip install 'h5py==2.10.0' --force-reinstall\n</code></pre>\n<p>Restarted my ipython kernel and it worked.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For me it was the version of <strong>h5py</strong> that was superior to my previous build.<br/>\nFixed it by setting to <code>2.10.0</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to recover from a PCA done with scikit-learn, <strong>which</strong> features are selected as <em>relevant</em>.</p>\n<p>A classic example with IRIS dataset.</p>\n<pre><code>import pandas as pd\nimport pylab as pl\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# load dataset\niris = datasets.load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# normalize data\ndf_norm = (df - df.mean()) / df.std()\n\n# PCA\npca = PCA(n_components=2)\npca.fit_transform(df_norm.values)\nprint pca.explained_variance_ratio_\n</code></pre>\n<p>This returns</p>\n<pre><code>In [42]: pca.explained_variance_ratio_\nOut[42]: array([ 0.72770452,  0.23030523])\n</code></pre>\n<p><strong>How can I recover which two features allow these two explained variance among the dataset ?</strong>\nSaid diferently, how can i get the index of this features in iris.feature_names ?</p>\n<pre><code>In [47]: print iris.feature_names\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This information is included in the <code>pca</code> attribute: <code>components_</code>. As described in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" rel=\"noreferrer\">documentation</a>, <code>pca.components_</code> outputs an array of <code>[n_components, n_features]</code>, so to get how components are linearly related with the different features you have to:</p>\n<p><strong>Note</strong>: each coefficient represents the correlation between a particular pair of component and feature</p>\n<pre><code>import pandas as pd\nimport pylab as pl\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# load dataset\niris = datasets.load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# normalize data\nfrom sklearn import preprocessing\ndata_scaled = pd.DataFrame(preprocessing.scale(df),columns = df.columns) \n\n# PCA\npca = PCA(n_components=2)\npca.fit_transform(data_scaled)\n\n# Dump components relations with features:\nprint(pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC-1','PC-2']))\n\n      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\nPC-1           0.522372         -0.263355           0.581254          0.565611\nPC-2          -0.372318         -0.925556          -0.021095         -0.065416\n</code></pre>\n<p><strong>IMPORTANT:</strong> As a side comment, note the PCA sign does not affect its interpretation since the sign does not affect the variance contained in each component. Only the relative signs of features forming the PCA dimension are important. In fact, if you run the PCA code again, you might get the PCA dimensions with the signs inverted. For an intuition about this, think about a vector and its negative in 3-D space - both are essentially representing the same direction in space.\nCheck <a href=\"https://stats.stackexchange.com/questions/88880/does-the-sign-of-scores-or-of-loadings-in-pca-or-fa-have-a-meaning-may-i-revers\">this post</a> for further reference.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Edit: as others have commented, you may get same values from <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" rel=\"noreferrer\"><code>.components_</code></a> attribute.</p>\n<hr/>\n<p>Each principal component is a linear combination of the original variables:</p>\n<p><img alt=\"pca-coef\" src=\"https://i.sstatic.net/RQKn6.png\"/></p>\n<p>where <code>X_i</code>s are the original variables, and <code>Beta_i</code>s are the corresponding weights or so called coefficients.</p>\n<p>To obtain the weights, you may simply pass identity matrix to the <code>transform</code> method:</p>\n<pre><code>&gt;&gt;&gt; i = np.identity(df.shape[1])  # identity matrix\n&gt;&gt;&gt; i\narray([[ 1.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  1.]])\n\n&gt;&gt;&gt; coef = pca.transform(i)\n&gt;&gt;&gt; coef\narray([[ 0.5224, -0.3723],\n       [-0.2634, -0.9256],\n       [ 0.5813, -0.0211],\n       [ 0.5656, -0.0654]])\n</code></pre>\n<p>Each column of the <code>coef</code> matrix above shows the weights in the linear combination which obtains corresponding principal component:</p>\n<pre><code>&gt;&gt;&gt; pd.DataFrame(coef, columns=['PC-1', 'PC-2'], index=df.columns)\n                    PC-1   PC-2\nsepal length (cm)  0.522 -0.372\nsepal width (cm)  -0.263 -0.926\npetal length (cm)  0.581 -0.021\npetal width (cm)   0.566 -0.065\n\n[4 rows x 2 columns]\n</code></pre>\n<p>For example, above shows that the second principal component (<code>PC-2</code>) is mostly aligned with <code>sepal width</code>, which has the highest weight of <code>0.926</code> in absolute value;</p>\n<p>Since the data were normalized, you can confirm that the principal components have variance <code>1.0</code> which is equivalent to each coefficient vector having norm <code>1.0</code>:</p>\n<pre><code>&gt;&gt;&gt; np.linalg.norm(coef,axis=0)\narray([ 1.,  1.])\n</code></pre>\n<p>One may also confirm that the principal components can be calculated as the dot product of the above coefficients and the original variables:</p>\n<pre><code>&gt;&gt;&gt; np.allclose(df_norm.values.dot(coef), pca.fit_transform(df_norm.values))\nTrue\n</code></pre>\n<p>Note that we need to use <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html\" rel=\"noreferrer\"><code>numpy.allclose</code></a> instead of regular equality operator, because of floating point precision error.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The way this question is phrased reminds me of a misunderstanding of Principle Component Analysis when I was first trying to figure it out. I’d like to go through it here in the hope that others won’t spend as much time on a road-to-nowhere as I did before the penny finally dropped.</p>\n<p>The notion of “recovering” feature names suggests that PCA identifies those features that are most important in a dataset. That’s not strictly true.</p>\n<p>PCA, as I understand it, identifies the features with the greatest variance in a dataset, and can then use this quality of the dataset to create a smaller dataset with a minimal loss of descriptive power. The advantages of a smaller dataset is that it requires less processing power and should have less noise in the data. But the features of greatest variance are not the \"best\" or \"most important\" features of a dataset, insofar as such concepts can be said to exist at all.</p>\n<p>To bring that theory into the practicalities of @Rafa’s sample code above:\n</p>\n<pre><code># load dataset\niris = datasets.load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# normalize data\nfrom sklearn import preprocessing\ndata_scaled = pd.DataFrame(preprocessing.scale(df),columns = df.columns) \n\n# PCA\npca = PCA(n_components=2)\npca.fit_transform(data_scaled)\n</code></pre>\n<p>consider the following:\n</p>\n<pre><code>post_pca_array = pca.fit_transform(data_scaled)\n\nprint data_scaled.shape\n(150, 4)\n\nprint post_pca_array.shape\n(150, 2)\n</code></pre>\n<p>In this case, <code>post_pca_array</code> has the same 150 rows of data as <code>data_scaled</code>, but <code>data_scaled</code>’s four columns have been reduced from four to two.</p>\n<p>The critical point here is that the two columns – or components, to be terminologically consistent – of <code>post_pca_array</code> are not the two “best” columns of <code>data_scaled</code>. They are two new columns, determined by the algorithm behind <code>sklearn.decomposition</code>’s <code>PCA</code> module. The second column, <code>PC-2</code> in @Rafa’s example, is informed by <code>sepal_width</code> more than any other column, but the values in <code>PC-2</code> and <code>data_scaled['sepal_width']</code> are not the same.</p>\n<p>As such, while it’s interesting to find out how much each column in original data contributed to the components of a post-PCA dataset, the notion of “recovering” column names is a little misleading, and certainly misled me for a long time. The only situation where there would be a match between post-PCA and original columns would be if the number of principle components were set at the same number as columns in the original. However, there would be no point in using the same number of columns because the data would not have changed. You would only have gone there to come back again, as it were.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to make a simple neural network which uses the ReLU function. Can someone give me a clue of how can I implement the function using numpy.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are a couple of ways.</p>\n<pre><code>&gt;&gt;&gt; x = np.random.random((3, 2)) - 0.5\n&gt;&gt;&gt; x\narray([[-0.00590765,  0.18932873],\n       [-0.32396051,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n&gt;&gt;&gt; np.maximum(x, 0)\narray([[ 0.        ,  0.18932873],\n       [ 0.        ,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n&gt;&gt;&gt; x * (x &gt; 0)\narray([[-0.        ,  0.18932873],\n       [-0.        ,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n&gt;&gt;&gt; (abs(x) + x) / 2\narray([[ 0.        ,  0.18932873],\n       [ 0.        ,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n</code></pre>\n<p>If timing the results with the following code:</p>\n<pre><code>import numpy as np\n\nx = np.random.random((5000, 5000)) - 0.5\nprint(\"max method:\")\n%timeit -n10 np.maximum(x, 0)\n\nprint(\"multiplication method:\")\n%timeit -n10 x * (x &gt; 0)\n\nprint(\"abs method:\")\n%timeit -n10 (abs(x) + x) / 2\n</code></pre>\n<p>We get:</p>\n<pre><code>max method:\n10 loops, best of 3: 239 ms per loop\nmultiplication method:\n10 loops, best of 3: 145 ms per loop\nabs method:\n10 loops, best of 3: 288 ms per loop\n</code></pre>\n<p>So the multiplication seems to be the fastest.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can do it in much easier way:</p>\n<pre><code>def ReLU(x):\n    return x * (x &gt; 0)\n\ndef dReLU(x):\n    return 1. * (x &gt; 0)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm completely revising my original answer because of points raised in the other questions and comments. Here is the new benchmark script:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import time\nimport numpy as np\n\n\ndef fancy_index_relu(m):\n    m[m &lt; 0] = 0\n\n\nrelus = {\n    \"max\": lambda x: np.maximum(x, 0),\n    \"in-place max\": lambda x: np.maximum(x, 0, x),\n    \"mul\": lambda x: x * (x &gt; 0),\n    \"abs\": lambda x: (abs(x) + x) / 2,\n    \"fancy index\": fancy_index_relu,\n}\n\nfor name, relu in relus.items():\n    n_iter = 20\n    x = np.random.random((n_iter, 5000, 5000)) - 0.5\n\n    t1 = time.time()\n    for i in range(n_iter):\n        relu(x[i])\n    t2 = time.time()\n\n    print(\"{:&gt;12s}  {:3.0f} ms\".format(name, (t2 - t1) / n_iter * 1000))\n</code></pre>\n<p>It takes care to use a different ndarray for each implementation and iteration. Here are the results:</p>\n<pre><code>         max  126 ms\nin-place max  107 ms\n         mul  136 ms\n         abs   86 ms\n fancy index  132 ms\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is <a href=\"/help/closed-questions\">off-topic</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> <a href=\"/posts/2620343/edit\">Update the question</a> so it's <a href=\"/help/on-topic\">on-topic</a> for Stack Overflow.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2012-12-23 10:57:11Z\">11 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/2620343/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<ul>\n<li>What is <strong>machine learning</strong> ? </li>\n<li>What does machine learning <strong>code</strong> do ?</li>\n<li>When we say that the machine learns, does it modify the code of <strong>itself</strong> or it modifies history (database) which will contain the experience of code for given set of inputs?</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>What is a machine learning ?</p>\n</blockquote>\n<p>Essentially, it is a method of teaching computers to make and improve predictions or behaviors based on some data. What is this \"data\"? Well, that depends entirely on the problem. It could be readings from a robot's sensors as it learns to walk, or the correct output of a program for certain input.</p>\n<p>Another way to think about machine learning is that it is \"pattern recognition\" - the act of teaching a program to react to or recognize patterns.</p>\n<blockquote>\n<p>What does machine learning code do ?</p>\n</blockquote>\n<p>Depends on the <em>type</em> of machine learning you're talking about. Machine learning is a <em>huge</em> field, with hundreds of different algorithms for solving myriad different problems - see <a href=\"http://en.wikipedia.org/wiki/Machine_learning\" rel=\"noreferrer\">Wikipedia</a> for more information; specifically, look under <a href=\"http://en.wikipedia.org/wiki/Machine_learning#Algorithm_types\" rel=\"noreferrer\">Algorithm Types</a>.</p>\n<blockquote>\n<p>When we say machine learns, does it modify the code of itself or it modifies history (Data Base) which will contain the experience of code for given set of inputs ?</p>\n</blockquote>\n<p>Once again, <em>it depends</em>.</p>\n<p>One example of code actually being modified is <a href=\"http://en.wikipedia.org/wiki/Genetic_programming\" rel=\"noreferrer\">Genetic Programming</a>, where you essentially evolve a program to complete a task (of course, the program doesn't modify itself - but it does modify another computer program).</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Artificial_neural_network\" rel=\"noreferrer\">Neural networks</a>, on the other hand, modify their parameters automatically in response to prepared stimuli and expected response. This allows them to produce many behaviors (theoretically, they can produce <em>any</em> behavior because they can approximate any function to an arbitrary precision, given enough time).</p>\n<hr/>\n<p>I should note that your use of the term \"database\" implies that machine learning algorithms work by \"remembering\" information, events, or experiences. This is not necessarily (or even often!) the case.</p>\n<p>Neural networks, which I already mentioned, only keep the current \"state\" of the approximation, which is updated as learning occurs. Rather than remembering what happened and how to react to it, neural networks build a sort of \"model\" of their \"world.\" The model tells them how to react to certain inputs, even if the inputs are something that it has never seen before.</p>\n<p>This last ability - the ability to react to inputs that have never been seen before - is one of the core tenets of many machine learning algorithms. Imagine trying to teach a computer driver to navigate highways in traffic. Using your \"database\" metaphor, you would have to teach the computer exactly what to do in <em>millions</em> of possible situations. An effective machine learning algorithm would (hopefully!) be able to learn similarities between different states and react to them similarly.</p>\n<p>The similarities between states can be anything - even things we might think of as \"mundane\" can really trip up a computer! For example, let's say that the computer driver learned that when a car in front of it slowed down, it had to slow down to. For a human, replacing the car with a motorcycle doesn't change anything - we recognize that the motorcycle is also a vehicle. For a machine learning algorithm, this can actually be surprisingly difficult! A database would have to store information separately about the case where a car is in front and where a motorcycle is in front. A machine learning algorithm, on the other hand, would \"learn\" from the car example and be able to generalize to the motorcycle example automatically.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Machine learning is a field of computer science, probability theory, and optimization theory which allows complex tasks to be solved for which a logical/procedural approach would not be possible or feasible.</p>\n<p>There are several different categories of machine learning, including (but not limited to):</p>\n<ul>\n<li>Supervised learning</li>\n<li>Reinforcement learning</li>\n</ul>\n<p><b>Supervised Learning</b><br/>\nIn supervised learning, you have some really complex function (mapping) from inputs to outputs, you have lots of examples of input/output pairs, but you don't know what that complicated function is. A supervised learning algorithm makes it possible, given a large data set of input/output pairs, to predict the output value for some new input value that you may not have seen before. The basic method is that you break the data set down into a training set and a test set. You have some model with an associated error function which you try to minimize over the training set, and then you make sure that your solution works on the test set. Once you have repeated this with different machine learning algorithms and/or parameters until the model performs reasonably well on the test set, then you can attempt to use the result on new inputs. Note that in this case, the program does not change, only the model (data) is changed. Although one could, theoretically, output a different program, but that is not done in practice, as far as I am aware. An example of supervised learning would be the digit recognition system used by the post office, where it maps the pixels to labels in the set 0...9,  using a large set of pictures of digits that were labeled by hand as being in 0...9.</p>\n<p><b>Reinforcement Learning</b><br/>\nIn reinforcement learning, the program is responsible for making decisions, and it periodically receives some sort of award/utility for its actions. However, unlike in the supervised learning case, the results are not immediate; the algorithm could prescribe a large sequence of actions and only receive feedback at the very end. In reinforcement learning, the goal is to build up a good model such that the algorithm will generate the sequence of decisions that lead to the highest long term utility/reward. A good example of reinforcement learning is teaching a robot how to navigate by giving a negative penalty whenever its bump sensor detects that it has bumped into an object. If coded correctly, it is possible for the robot to eventually correlate its range finder sensor data with its bumper sensor data and the directions that sends to the wheels, and ultimately choose a form of navigation that results in it not bumping into objects.</p>\n<p><b>More Info</b><br/>\nIf you are interested in learning more, I strongly recommend that you read <a href=\"https://rads.stackoverflow.com/amzn/click/com/0387310738\" rel=\"nofollow noreferrer\">Pattern Recognition and Machine Learning by Christopher M. Bishop</a> or take a machine learning course. You may also be interested in reading, for free, the <a href=\"http://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=Lectures.Lectures\" rel=\"noreferrer\">lecture notes from CIS 520: Machine Learning at Penn</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ul>\n<li><p><strong>Machine learning</strong> is a scientific discipline that is concerned with the design and development of algorithms  that allow computers to evolve behaviors based on empirical data, such as from sensor  data or databases. Read more on <a href=\"http://en.wikipedia.org/wiki/Machine_learning\" rel=\"noreferrer\">Wikipedia</a></p></li>\n<li><p>Machine learning <strong>code</strong> records \"facts\" or approximations in some sort of storage, and with the algorithms calculates different probabilities. </p></li>\n<li><p>The code <strong>itself</strong> will not be modified when a machine learns, only the database of what \"it knows\".</p></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have an assignment to make an AI Agent that will learn to play a video game using ML. I want to create a new environment using OpenAI Gym because I don't want to use an existing environment. How can I create a new, custom Environment?</p>\n<p>Also, is there any other way I can start to develop making AI Agent to play a specific video game without the help of OpenAI Gym?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>See my <a href=\"https://github.com/MartinThoma/banana-gym\" rel=\"noreferrer\"><code>banana-gym</code></a> for an extremely small environment.</p>\n<h1>Create new environments</h1>\n<p>See the main page of the repository:</p>\n<p><a href=\"https://github.com/openai/gym/blob/master/docs/creating_environments.md\" rel=\"noreferrer\">https://github.com/openai/gym/blob/master/docs/creating_environments.md</a></p>\n<p>The steps are:</p>\n<ol>\n<li>Create a new repository with a PIP-package structure</li>\n</ol>\n<p>It should look like this</p>\n<pre><code>gym-foo/\n  README.md\n  setup.py\n  gym_foo/\n    __init__.py\n    envs/\n      __init__.py\n      foo_env.py\n      foo_extrahard_env.py\n</code></pre>\n<p>For the contents of it, follow the link above. Details which are not mentioned there are especially how some functions in <code>foo_env.py</code> should look like. Looking at examples and at <a href=\"https://gym.openai.com/docs/\" rel=\"noreferrer\">gym.openai.com/docs/</a> helps. Here is an example:</p>\n<pre><code>class FooEnv(gym.Env):\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self):\n        pass\n\n    def _step(self, action):\n        \"\"\"\n\n        Parameters\n        ----------\n        action :\n\n        Returns\n        -------\n        ob, reward, episode_over, info : tuple\n            ob (object) :\n                an environment-specific object representing your observation of\n                the environment.\n            reward (float) :\n                amount of reward achieved by the previous action. The scale\n                varies between environments, but the goal is always to increase\n                your total reward.\n            episode_over (bool) :\n                whether it's time to reset the environment again. Most (but not\n                all) tasks are divided up into well-defined episodes, and done\n                being True indicates the episode has terminated. (For example,\n                perhaps the pole tipped too far, or you lost your last life.)\n            info (dict) :\n                 diagnostic information useful for debugging. It can sometimes\n                 be useful for learning (for example, it might contain the raw\n                 probabilities behind the environment's last state change).\n                 However, official evaluations of your agent are not allowed to\n                 use this for learning.\n        \"\"\"\n        self._take_action(action)\n        self.status = self.env.step()\n        reward = self._get_reward()\n        ob = self.env.getState()\n        episode_over = self.status != hfo_py.IN_GAME\n        return ob, reward, episode_over, {}\n\n    def _reset(self):\n        pass\n\n    def _render(self, mode='human', close=False):\n        pass\n\n    def _take_action(self, action):\n        pass\n\n    def _get_reward(self):\n        \"\"\" Reward is given for XY. \"\"\"\n        if self.status == FOOBAR:\n            return 1\n        elif self.status == ABC:\n            return self.somestate ** 2\n        else:\n            return 0\n</code></pre>\n<h1>Use your environment</h1>\n<pre><code>import gym\nimport gym_foo\nenv = gym.make('MyEnv-v0')\n</code></pre>\n<h1>Examples</h1>\n<ol>\n<li><a href=\"https://github.com/openai/gym-soccer\" rel=\"noreferrer\">https://github.com/openai/gym-soccer</a></li>\n<li><a href=\"https://github.com/openai/gym-wikinav\" rel=\"noreferrer\">https://github.com/openai/gym-wikinav</a></li>\n<li><a href=\"https://github.com/alibaba/gym-starcraft\" rel=\"noreferrer\">https://github.com/alibaba/gym-starcraft</a></li>\n<li><a href=\"https://github.com/endgameinc/gym-malware\" rel=\"noreferrer\">https://github.com/endgameinc/gym-malware</a></li>\n<li><a href=\"https://github.com/hackthemarket/gym-trading\" rel=\"noreferrer\">https://github.com/hackthemarket/gym-trading</a></li>\n<li><a href=\"https://github.com/tambetm/gym-minecraft\" rel=\"noreferrer\">https://github.com/tambetm/gym-minecraft</a></li>\n<li><a href=\"https://github.com/ppaquette/gym-doom\" rel=\"noreferrer\">https://github.com/ppaquette/gym-doom</a></li>\n<li><a href=\"https://github.com/ppaquette/gym-super-mario\" rel=\"noreferrer\">https://github.com/ppaquette/gym-super-mario</a></li>\n<li><a href=\"https://github.com/tuzzer/gym-maze\" rel=\"noreferrer\">https://github.com/tuzzer/gym-maze</a></li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Its definitely possible. They say so in the Documentation page, close to the end.</p>\n<p><a href=\"https://gym.openai.com/docs\" rel=\"noreferrer\">https://gym.openai.com/docs</a></p>\n<p>As to how to do it, you should look at the source code of the existing environments for inspiration. Its available in github:</p>\n<p><a href=\"https://github.com/openai/gym#installation\" rel=\"noreferrer\">https://github.com/openai/gym#installation</a></p>\n<p>Most of their environments they did not implement from scratch, but rather created a wrapper around existing environments and gave it all an interface that is convenient for reinforcement learning.</p>\n<p>If you want to make your own, you should probably go in this direction and try to adapt something that already exists to the gym interface. Although there is a good chance that this is very time consuming.</p>\n<p><strong>There is another option that may be interesting for your purpose. It's OpenAI's Universe</strong></p>\n<p><a href=\"https://universe.openai.com/\" rel=\"noreferrer\">https://universe.openai.com/</a></p>\n<p>It can integrate with websites so that you train your models on kongregate games, for example. But Universe is not as easy to use as Gym.</p>\n<p>If you are a beginner, my recommendation is that you start with a vanilla implementation on a standard environment. After you get passed the problems with the basics, go on to increment...</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to do some timing comparisons between CPU &amp; GPU as well as some profiling and would like to know if there's a way to tell <a class=\"post-tag\" href=\"/questions/tagged/pytorch\" rel=\"tag\" title=\"show questions tagged 'pytorch'\">pytorch</a> to not use the GPU and instead use the CPU only? I realize I could install another CPU-only <a class=\"post-tag\" href=\"/questions/tagged/pytorch\" rel=\"tag\" title=\"show questions tagged 'pytorch'\">pytorch</a>, but hoping there's an easier way.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Before running your code, run this shell command to tell torch that there are no GPUs:</p>\n<pre><code>export CUDA_VISIBLE_DEVICES=\"\"\n</code></pre>\n<hr/>\n<p>This will tell it to use only one GPU (the one with id 0) and so on:</p>\n<pre><code>export CUDA_VISIBLE_DEVICES=\"0\"\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I just wanted to add that it is also possible to do so within the PyTorch Code:</p>\n<p>Here is a small example taken from the <a href=\"https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#writing-device-agnostic-code\" rel=\"noreferrer\">PyTorch Migration Guide for 0.4.0</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code># at beginning of the script\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n...\n\n# then whenever you get a new Tensor or Module\n# this won't copy if they are already on the desired device\ninput = data.to(device)\nmodel = MyModule(...).to(device)\n</code></pre>\n<p><em>I think the example is pretty self-explaining. But if there are any questions just ask! <br/>One big advantage is when using this syntax like in the example above is, that you can create code which runs on CPU if no GPU is available but also on GPU without changing a single line.</em></p>\n<p>Instead of using the <em>if-statement</em> with <code>torch.cuda.is_available()</code> you can also just set the device to <em>CPU</em> like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>device = torch.device(\"cpu\")\n</code></pre>\n<p>Further you can create tensors on the desired <em>device</em> using the <code>device</code> flag:</p>\n<pre class=\"lang-py prettyprint-override\"><code>mytensor = torch.rand(5, 5, device=device)\n</code></pre>\n<p>This will create a tensor directly on the <code>device</code> you specified previously.</p>\n<p>I want to point out, that you can switch between <em>CPU</em> and <em>GPU</em>  using this syntax, but also between different <em>GPUs</em>.</p>\n<p>I hope this is helpful!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Simplest way using Python is:</p>\n<pre><code>os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a large set of vectors in 3 dimensions. I need to cluster these based on Euclidean distance such that all the vectors in any particular cluster have a Euclidean distance between each other less than a threshold \"T\".</p>\n<p>I do not know how many clusters exist. At the end, there may be individual vectors existing that are not part of any cluster because its euclidean distance is not less than \"T\" with any of the vectors in the space.</p>\n<p>What existing algorithms / approach should be used here?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"http://en.wikipedia.org/wiki/Hierarchical_clustering\" rel=\"noreferrer\">hierarchical clustering</a>. It is a rather basic approach, so there are lots of implementations available. It is for example included in Python's <a href=\"http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\" rel=\"noreferrer\">scipy</a>. </p>\n<p>See for example the following script:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy\nimport scipy.cluster.hierarchy as hcluster\n\n# generate 3 clusters of each around 100 points and one orphan point\nN=100\ndata = numpy.random.randn(3*N,2)\ndata[:N] += 5\ndata[-N:] += 10\ndata[-1:] -= 20\n\n# clustering\nthresh = 1.5\nclusters = hcluster.fclusterdata(data, thresh, criterion=\"distance\")\n\n# plotting\nplt.scatter(*numpy.transpose(data), c=clusters)\nplt.axis(\"equal\")\ntitle = \"threshold: %f, number of clusters: %d\" % (thresh, len(set(clusters)))\nplt.title(title)\nplt.show()\n</code></pre>\n<p>Which produces a result similar to the following image. \n<img alt=\"clusters\" src=\"https://i.sstatic.net/2jgjf.png\"/></p>\n<p>The threshold given as a parameter is a distance value on which basis the decision is made whether points/clusters will be merged into another cluster. The distance metric being used can also be specified.</p>\n<p>Note that there are various methods for how to compute the intra-/inter-cluster similarity, e.g. distance between the closest points, distance between the furthest points, distance to the cluster centers and so on. Some of these methods are also supported by scipys hierarchical clustering module (<a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\" rel=\"noreferrer\">single/complete/average... linkage</a>). According to your post I think you would want to use <a href=\"http://en.wikipedia.org/wiki/Complete-linkage_clustering\" rel=\"noreferrer\">complete linkage</a>. </p>\n<p>Note that this approach also allows small (single point) clusters if they don't meet the similarity criterion of the other clusters, i.e. the distance threshold.</p>\n<hr/>\n<p>There are other algorithms that will perform better, which will become relevant in situations with lots of data points. As other answers/comments suggest you might also want to have a look at the DBSCAN algorithm:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/DBSCAN\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/DBSCAN</a></li>\n<li><a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\" rel=\"noreferrer\">http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html</a></li>\n<li><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN</a></li>\n</ul>\n<hr/>\n<p>For a nice overview on these and other clustering algorithms, also have a look at this demo page (of Python's scikit-learn library):</p>\n<ul>\n<li><a href=\"http://scikit-learn.org/stable/modules/clustering.html\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/clustering.html</a></li>\n</ul>\n<p>Image copied from that place: </p>\n<p><a href=\"https://i.sstatic.net/us6N2.jpg\" rel=\"noreferrer\"><img alt=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\" src=\"https://i.sstatic.net/us6N2.jpg\"/></a></p>\n<p>As you can see, each algorithm makes some assumptions about the number and shape of the clusters that need to be taken into account. Be it implicit assumptions imposed by the algorithm or explicit assumptions specified by parameterization. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The answer by moooeeeep recommended using hierarchical clustering.  I wanted to elaborate on how to <em>choose</em> the treshold of the clustering.</p>\n<p>One way is to compute clusterings based on different thresholds <em>t1</em>, <em>t2</em>, <em>t3</em>,... and then compute a metric for the \"quality\" of the clustering.  The premise is that the quality of a clustering with the <em>optimal</em> number of clusters will have the maximum value of the quality metric.</p>\n<p>An example of a good quality metric I've used in the past is Calinski-Harabasz.  Briefly: you  compute the average inter-cluster distances and divide them by the within-cluster distances.  The optimal clustering assignment will have clusters that are separated from each other the most, and clusters that are \"tightest\".</p>\n<p>By the way, you don't have to use hierarchical clustering.  You can also use something like <em>k</em>-means, precompute it for each <em>k</em>, and then pick the <em>k</em> that has the highest Calinski-Harabasz score.</p>\n<p>Let me know if you need more references, and I'll scour my hard disk for some papers.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Check out the <a href=\"https://en.wikipedia.org/wiki/DBSCAN\" rel=\"noreferrer\">DBSCAN</a> algorithm. It clusters based on local density of vectors, i.e. they must not be more than some <em>ε</em> distance apart, and can determine the number of clusters automatically. It also considers outliers, i.e. points with an unsufficient number of <em>ε</em>-neighbors, to not be part of a cluster. The Wikipedia page links to a few implementations.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-08-16 00:27:59Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/35655267/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is inductive bias in machine learning? Why is it necessary?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Every machine learning algorithm with any ability to generalize beyond the training data that it sees has some type of inductive bias, which are the assumptions made by the model to learn the target function and to generalize beyond training data.</p>\n<p>For example, in linear regression, the model assumes that the output or dependent variable is related to independent variable linearly (in the weights). This is an inductive bias of the model.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>What is inductive bias?</h1>\n<p>Pretty much every design choice in machine learning signifies some sort of inductive bias. <a href=\"https://arxiv.org/pdf/1806.01261.pdf\" rel=\"noreferrer\">\"Relational inductive biases, deep learning, and graph networks\" (Battaglia et. al, 2018)</a> is an amazing 🙌 read, which I will be referring to throughout this answer.</p>\n<blockquote>\n<p>An <strong>inductive bias</strong> allows a learning algorithm to <strong>prioritize one solution (or interpretation) over another</strong>, independent of the observed data. [...] Inductive biases can express assumptions about either the data-generating process or the space of solutions.</p>\n</blockquote>\n<h2>Examples in deep learning</h2>\n<p>Concretely speaking, the very <strong>composition of layers</strong> 🍰 in deep learning provides a  type of relational inductive bias: <em>hierarchical processing</em>. The <strong>type of layer</strong> imposes further relational inductive biases:</p>\n<p><a href=\"https://i.sstatic.net/QbD58.png\" rel=\"noreferrer\"><img alt=\"Various relational inductive biases in standard deep learning components (Battaglia et. al, 2018)\" src=\"https://i.sstatic.net/QbD58.png\"/></a></p>\n<p>More generally, non-relational inductive biases used in deep learning include:</p>\n<ul>\n<li>activation non-linearities,</li>\n<li>weight decay,</li>\n<li>dropout,</li>\n<li>batch and layer normalization,</li>\n<li>data augmentation,</li>\n<li>training curricula,</li>\n<li>optimization algorithms,</li>\n<li><em>anything that imposes constraints on the learning trajectory</em>.</li>\n</ul>\n<h2>Examples outside of deep learning</h2>\n<p>In a Bayesian model, inductive biases are typically expressed through the choice and parameterization of the prior distribution. Adding a Tikhonov regularization penalty to your loss function implies assuming that simpler hypotheses are more likely.</p>\n<h1>Conclusion</h1>\n<p>The stronger the inductive bias, the better the sample efficiency--this can be understood in terms of the <strong>bias-variance tradeoff</strong>. Many modern deep learning methods follow an “end-to-end” design philosophy which emphasizes minimal <em>a priori</em> representational and computational assumptions, which explains why they tend to be so <strong>data-intensive</strong>. On the other hand, there is a lot of research into baking stronger relational inductive biases into deep learning architectures, e.g. with graph networks.</p>\n<h2>An aside about the word \"inductive\"</h2>\n<p>In philosophy, inductive reasoning refers to <strong>generalization</strong> from specific observations to a conclusion. This is a counterpoint to deductive reasoning, which refers to <strong>specialization</strong> from general ideas to a conclusion.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>Inductive bias</em> is the set of assumptions a learner uses to predict results given inputs it has not yet encountered.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is <a href=\"/help/closed-questions\">not about programming or software development</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about <a href=\"/help/on-topic\">a specific programming problem, a software algorithm, or software tools primarily used by programmers</a>. If you believe the question would be on-topic on <a href=\"https://stackexchange.com/sites\">another Stack Exchange site</a>, you can leave a comment to explain where the question may be able to be answered.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2024-02-21 03:02:54Z\">8 months ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2024-03-02 03:13:00Z\">7 months ago</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/43979449/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm trying to use deep learning to predict income from 15 self reported attributes from a dating site.</p>\n<p>We're getting rather odd results, where our validation data is getting better accuracy and lower loss, than our training data. And this is consistent across different sizes of hidden layers.\nThis is our model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>for hl1 in [250, 200, 150, 100, 75, 50, 25, 15, 10, 7]:\n    def baseline_model():\n        model = Sequential()\n        model.add(Dense(hl1, input_dim=299, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l1_l2(0.001)))\n        model.add(Dropout(0.5, seed=seed))\n        model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))\n\n        model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n        return model\n\n    history_logs = LossHistory()\n    model = baseline_model()\n    history = model.fit(X, Y, validation_split=0.3, shuffle=False, epochs=50, batch_size=10, verbose=2, callbacks=[history_logs])\n</code></pre>\n<p>And this is an example of the accuracy and losses:\n<img alt=\"Accuracy with hidden layer of 250 neurons\" src=\"https://i.sstatic.net/0TfSp.png\"/> and <img alt=\"the loss\" src=\"https://i.sstatic.net/TH6Nr.png\"/>.</p>\n<p>We've tried to remove regularization and dropout, which, as expected, ended in overfitting (training acc: ~85%). We've even tried to decrease the learning rate drastically, with similiar results.</p>\n<p>Has anyone seen similar results?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This happens when you use <code>Dropout</code>, since the behaviour when training and testing are different. </p>\n<p>When training, a percentage of the features are set to zero (50% in your case since you are using <code>Dropout(0.5)</code>). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can check the <a href=\"https://keras.io/getting-started/faq/\" rel=\"noreferrer\">Keras FAQ</a> and especially the section <em>\"Why is the training loss much higher than the testing loss?\"</em>. </p>\n<p>I would also suggest you to take some time and read this <em>very good</em> <a href=\"http://cs231n.github.io/neural-networks-3/#sanitycheck\" rel=\"noreferrer\">article</a>  regarding some \"sanity checks\" you should always take into consideration when building a NN. </p>\n<p>In addition, whenever possible, check if your results make sense. For example, in case of a n-class classification with categorical cross entropy the loss on the first epoch should be <code>-ln(1/n)</code>.</p>\n<p>Apart your specific case, I believe that apart from the <code>Dropout</code> the dataset split may sometimes result in this situation. Especially if the dataset split is not random (in case where temporal or spatial patterns exist) the validation set may be fundamentally different, i.e less noise or less variance, from the train and thus easier to to predict leading to higher accuracy on the validation set than on training. </p>\n<p>Moreover, if the validation set is very small compared to the training then by random the model fits better the validation set than the training.]</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This indicates the presence of high bias in your dataset. It is underfitting. The solutions to issue are:- </p>\n<ol>\n<li><p>Probably the network is struggling to fit the training data. Hence, try a \nlittle bit bigger network.</p></li>\n<li><p>Try a different Deep Neural Network. I mean to say change the architecture\na bit.</p></li>\n<li><p>Train for longer time.</p></li>\n<li><p>Try using advanced optimization algorithms.</p></li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/47302085/what-is-metrics-in-keras\">What is \"metrics\" in Keras?</a>\n<span class=\"question-originals-answer-count\">\n                                (5 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2019-02-18 10:14:17Z\">5 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>It is not clear for  me the difference between loss function and metrics in Keras. The documentation was not helpful for me.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The loss function is used to optimize your model. This is the function that will get minimized by the optimizer.</p>\n<p>A metric is used to judge the performance of your model. This is only for you to look at and has nothing to do with the optimization process.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The loss function is that parameter one passes to Keras <strong>model.compile</strong> which is actually optimized while training the model . This loss function is generally minimized by the model.</p>\n<p>Unlike the loss function , the metric is another list of parameters passed to Keras <strong>model.compile</strong> which is actually used for judging the performance of the model.</p>\n<p>For example :   In classification problems, we want to minimize the cross-entropy loss, while also want to assess the model performance with the AUC. In this case, cross-entropy is the loss function and AUC is the metric. Metric is the model performance parameter that one can see while the model is judging itself on the validation set after each epoch of training. It is important to note that the metric is important for few Keras callbacks like EarlyStopping when one wants to stop training the model in case the metric isn't improving for a certaining no. of epochs.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a contrived example in mind: Let's think about linear regression on a 2D-plane. In this case, loss function would be the mean squared error, the fitted line would minimize this error. </p>\n<p>However, for some reason we are very very interested in the area under the curve from 0 to 1 of our fitted line, and thus this can be one of the metrics. And we monitor this metric while the model minimizes the mean squared error loss function.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset containing grayscale images and I want to train a state-of-the-art CNN on them. I'd very much like to fine-tune a pre-trained model (like the ones <a href=\"https://github.com/tensorflow/models/tree/master/research/slim#Pretrained\" rel=\"noreferrer\">here</a>).</p>\n<p>The problem is that almost all models I can find the weights for have been trained on the ImageNet dataset, which contains RGB images.</p>\n<p>I can't use one of those models because their input layer expects a batch of shape <code>(batch_size, height, width, 3)</code> or <code>(64, 224, 224, 3)</code> in my case, but my images batches are <code>(64, 224, 224)</code>.</p>\n<p>Is there any way that I can use one of those models? I've thought of dropping the input layer after I've loaded the weights and adding my own (like we do for the top layers). Is this approach correct?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The model's architecture <strong>cannot</strong> be changed because the weights have been trained for a specific input configuration. Replacing the first layer with your own would pretty much render the rest of the weights useless. </p>\n<p>-- Edit: elaboration suggested by Prune--<br/>\nCNNs are built so that as they go deeper, they can extract high-level features derived from the lower-level features that the previous layers extracted. By removing the initial layers of a CNN, you are destroying that hierarchy of features because the subsequent layers won't receive the features that they are supposed to as their input. In your case the second layer has been trained to <strong>expect</strong> the features of the first layer. By replacing your first layer with random weights, you are essentially throwing away any training that has been done on the subsequent layers, as they would need to be retrained. I doubt that they could retain any of the knowledge learned during the initial training.<br/>\n--- end edit ---</p>\n<p>There is an easy way, though, which you can make your model work with grayscale images. You just need to make the image to <strong>appear</strong> to be RGB. The easiest way to do so is to <em>repeat</em> the image array 3 times on a new dimension. Because you will have the <em>same image</em> over all 3 channels, the performance of the model should be the same as it was on RGB images.</p>\n<p>In <em>numpy</em> this can be easily done like this:</p>\n<pre><code>print(grayscale_batch.shape)  # (64, 224, 224)\nrgb_batch = np.repeat(grayscale_batch[..., np.newaxis], 3, -1)\nprint(rgb_batch.shape)  # (64, 224, 224, 3)\n</code></pre>\n<p>The way this works is that it first creates a new dimension (to place the channels) and then it repeats the existing array 3 times on this new dimension.</p>\n<p>I'm also pretty sure that keras' <a href=\"https://keras.io/preprocessing/image/\" rel=\"noreferrer\">ImageDataGenerator</a> can load grayscale images as RGB.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Converting grayscale images to RGB as per the currently accepted answer is one approach to this problem, but not the most efficient. You most certainly can modify the weights of the model's first convolutional layer and achieve the stated goal. The modified model will both work out of the box (with reduced accuracy) and be finetunable. Modifying the weights of the first layer does not render the rest of the weights useless as suggested by others.</p>\n<p>To do this, you'll have to add some code where the pretrained weights are loaded. In your framework of choice, you need to figure out how to grab the weights of the first convolutional layer in your network and modify them before assigning to your 1-channel model. The required modification is to sum the weight tensor over the dimension of the input channels. The way the weights tensor is organized varies from framework to framework. The PyTorch default is [out_channels, in_channels, kernel_height, kernel_width]. In Tensorflow I believe it is [kernel_height, kernel_width, in_channels, out_channels].</p>\n<p>Using PyTorch as an example, in a ResNet50 model from Torchvision (<a href=\"https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\" rel=\"noreferrer\">https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py</a>), the shape of the weights for conv1 is [64, 3, 7, 7]. Summing over dimension 1 results in a tensor of shape [64, 1, 7, 7]. At the bottom I've included a snippet of code that would work with the ResNet models in Torchvision assuming that an argument (inchans) was added to specify a different number of input channels for the model.</p>\n<p>To prove this works I did three runs of ImageNet validation on ResNet50 with pretrained weights. There is a slight difference in the numbers for run 2 &amp; 3, but it's minimal and should be irrelevant once finetuned.</p>\n<ol>\n<li>Unmodified ResNet50 w/ RGB Images : Prec @1: 75.6, Prec @5: 92.8</li>\n<li>Unmodified ResNet50 w/ 3-chan Grayscale Images: Prec @1: 64.6, Prec @5: 86.4</li>\n<li>Modified 1-chan ResNet50 w/ 1-chan Grayscale Images: Prec @1: 63.8, Prec @5: 86.1</li>\n</ol>\n<pre><code>def _load_pretrained(model, url, inchans=3):\n    state_dict = model_zoo.load_url(url)\n    if inchans == 1:\n        conv1_weight = state_dict['conv1.weight']\n        state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n    elif inchans != 3:\n        assert False, \"Invalid number of inchans for pretrained weights\"\n    model.load_state_dict(state_dict)\n\ndef resnet50(pretrained=False, inchans=3):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], inchans=inchans)\n    if pretrained:\n        _load_pretrained(model, model_urls['resnet50'], inchans=inchans)\n    return model\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A simple way to do this is to add a convolution layer before the base model and then feed the output to the base model. Like this:</p>\n<pre><code>from keras.models import Model\nfrom keras.layers import Input \n\nresnet = Resnet50(weights='imagenet',include_top= 'TRUE') \n\ninput_tensor = Input(shape=(IMG_SIZE,IMG_SIZE,1) )\nx = Conv2D(3,(3,3),padding='same')(input_tensor)    # x has a dimension of (IMG_SIZE,IMG_SIZE,3)\nout = resnet (x) \n\nmodel = Model(inputs=input_tensor,outputs=out)\n\n\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/43691380/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-12-25 20:03:45Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2023-11-22 20:17:18Z\">11 months ago</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/43691380/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>From the <a href=\"https://xgboost.readthedocs.io/en/latest/python/python_intro.html#training\" rel=\"noreferrer\">XGBoost guide</a>:</p>\n<blockquote>\n<p>After training, the model can be saved.</p>\n<pre><code>bst.save_model('0001.model')\n</code></pre>\n<p>The model and its feature map can also be dumped to a text file.</p>\n<pre><code># dump model\nbst.dump_model('dump.raw.txt')\n# dump model with feature map\nbst.dump_model('dump.raw.txt', 'featmap.txt')\n</code></pre>\n<p>A saved model can be loaded as follows:</p>\n<pre><code>bst = xgb.Booster({'nthread': 4})  # init model\nbst.load_model('model.bin')  # load data\n</code></pre>\n</blockquote>\n<p>My questions are following.</p>\n<ol>\n<li>What's the difference between <code>save_model</code> &amp; <code>dump_model</code>?</li>\n<li>What's the difference between saving <code>'0001.model'</code> and <code>'dump.raw.txt','featmap.txt'</code>?</li>\n<li>Why the model name for loading <code>model.bin</code> is different from the name to be saved <code>0001.model</code>?</li>\n<li>Suppose that I trained two models: <code>model_A</code> and <code>model_B</code>. I wanted to save both models for future use. Which <code>save</code> &amp; <code>load</code> function should I use? Could you help show the clear process?</li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is how I solved the problem:</p>\n<pre><code>import pickle\nfile_name = \"xgb_reg.pkl\"\n\n# save\npickle.dump(xgb_model, open(file_name, \"wb\"))\n\n# load\nxgb_model_loaded = pickle.load(open(file_name, \"rb\"))\n\n# test\nind = 1\ntest = X_val[ind]\nxgb_model_loaded.predict(test)[0] == xgb_model.predict(test)[0]\n\nOut[1]: True\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Both functions <code>save_model</code> and <code>dump_model</code> save the model, the difference is that in <code>dump_model</code> you can save feature name and save tree in text format.</p>\n<p>The <code>load_model</code> will work with model from <code>save_model</code>. The model from <code>dump_model</code> can be used  for example with <a href=\"https://github.com/Far0n/xgbfi\" rel=\"noreferrer\">xgbfi</a>.</p>\n<p>During loading the model, you need to specify the path where your models is saved. In the example <code>bst.load_model(\"model.bin\")</code> model is loaded from file <code>model.bin</code> - it is just a name of file with model. Good luck!</p>\n<p><strong>EDIT</strong>: From Xgboost documentation (for version <code>1.3.3</code>), the <code>dump_model()</code> should be used for saving the model for further interpretation. For saving and loading the model the <code>save_model()</code> and <code>load_model()</code> should be used. Please check the <a href=\"https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html#difference-between-saving-model-and-dumping-model\" rel=\"noreferrer\">docs</a> for more details.</p>\n<p>There is also a difference between <code>Learning API</code> and <code>Scikit-Learn API</code> of Xgboost. The latter saves the <code>best_ntree_limit</code> variable which is set during the training with early stopping. You can read details in my article <a href=\"https://mljar.com/blog/xgboost-save-load-python/\" rel=\"noreferrer\">How to save and load Xgboost in Python?</a></p>\n<p>The <code>save_model()</code> method recognize the format of the file name, if <code>*.json</code> is specified, then model is saved in JSON, otherwise it is text file.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>An easy way of saving and loading a xgboost model is with joblib library.</p>\n<pre><code>import joblib\n#save model\njoblib.dump(xgb, filename) \n\n#load saved model\nxgb = joblib.load(filename)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Currently I use the following code:</p>\n<pre><code>callbacks = [\n    EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n]\nmodel.fit(X_train.astype('float32'), Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n      shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n      callbacks=callbacks)\n</code></pre>\n<p>It tells Keras to stop training when loss didn't improve for 2 epochs. But I want to stop training after loss became smaller than some constant \"THR\":</p>\n<pre><code>if val_loss &lt; THR:\n    break\n</code></pre>\n<p>I've seen in documentation there are possibility to make your own callback:\n<a href=\"http://keras.io/callbacks/\">http://keras.io/callbacks/</a>\nBut nothing found how to stop training process. I need an advice.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I found the answer. I looked into Keras sources and find out code for EarlyStopping. I made my own callback, based on it:</p>\n<pre><code>class EarlyStoppingByLossVal(Callback):\n    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n        if current &lt; self.value:\n            if self.verbose &gt; 0:\n                print(\"Epoch %05d: early stopping THR\" % epoch)\n            self.model.stop_training = True\n</code></pre>\n<p>And usage:</p>\n<pre><code>callbacks = [\n    EarlyStoppingByLossVal(monitor='val_loss', value=0.00001, verbose=1),\n    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n]\nmodel.fit(X_train.astype('float32'), Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n      shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n      callbacks=callbacks)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The keras.callbacks.EarlyStopping callback does have a min_delta argument. From Keras documentation:</p>\n<blockquote>\n<p>min_delta: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One solution is to call <code>model.fit(nb_epoch=1, ...)</code> inside a for loop, then you can put a break statement inside the for loop and do whatever other custom control flow you want.  </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In a slide within the introductory lecture on machine learning by Stanford's Andrew Ng at Coursera, he gives the following one line Octave solution to the cocktail party problem given the audio sources are recorded by two spatially separated microphones:</p>\n<pre><code>[W,s,v]=svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');\n</code></pre>\n<p>At the bottom of the slide is \"source: Sam Roweis, Yair Weiss, Eero Simoncelli\" and at the bottom of an earlier slide is \"Audio clips courtesy of Te-Won Lee\". In the video, Professor Ng says,</p>\n<blockquote>\n<p>\"So you might look at unsupervised learning like this and ask, 'How complicated is it to implement this?' It seems like in order to build this application, it seems like to do this audio processing, you would write a ton of code, or maybe link  into a bunch of C++ or Java libraries that process audio. It seems like it would be a really complicated program to do this audio: separating out audio and so on. It turns out the algorithm to do what you just heard, that can be done with just one line of code ... shown right here. It did take researchers a long time to come up with this line of code. So I'm not saying this is an easy problem. But it turns out that when you use the right programming environment many learning algorithms will be really short programs.\"</p>\n</blockquote>\n<p>The separated audio results played in the video lecture are not perfect but, in my opinion, amazing. Does anyone have any insight on how that one line of code performs so well? In particular, does anyone know of a reference that explains the work of Te-Won Lee, Sam Roweis, Yair Weiss, and Eero Simoncelli with respect to that one line of code?</p>\n<p><strong>UPDATE</strong></p>\n<p>To demonstrate the algorithm's sensitivity to microphone separation distance, the following simulation (in Octave) separates the tones from two spatially separated tone generators.</p>\n<pre><code>% define model \nf1 = 1100;              % frequency of tone generator 1; unit: Hz \nf2 = 2900;              % frequency of tone generator 2; unit: Hz \nTs = 1/(40*max(f1,f2)); % sampling period; unit: s \ndMic = 1;               % distance between microphones centered about origin; unit: m \ndSrc = 10;              % distance between tone generators centered about origin; unit: m \nc = 340.29;             % speed of sound; unit: m / s \n\n% generate tones\nfigure(1);\nt = [0:Ts:0.025];\ntone1 = sin(2*pi*f1*t);\ntone2 = sin(2*pi*f2*t);\nplot(t,tone1); \nhold on;\nplot(t,tone2,'r'); xlabel('time'); ylabel('amplitude'); axis([0 0.005 -1 1]); legend('tone 1', 'tone 2');\nhold off;\n\n% mix tones at microphones\n% assume inverse square attenuation of sound intensity (i.e., inverse linear attenuation of sound amplitude)\nfigure(2);\ndNear = (dSrc - dMic)/2;\ndFar = (dSrc + dMic)/2;\nmic1 = 1/dNear*sin(2*pi*f1*(t-dNear/c)) + \\\n       1/dFar*sin(2*pi*f2*(t-dFar/c));\nmic2 = 1/dNear*sin(2*pi*f2*(t-dNear/c)) + \\\n       1/dFar*sin(2*pi*f1*(t-dFar/c));\nplot(t,mic1);\nhold on;\nplot(t,mic2,'r'); xlabel('time'); ylabel('amplitude'); axis([0 0.005 -1 1]); legend('mic 1', 'mic 2');\nhold off;\n\n% use svd to isolate sound sources\nfigure(3);\nx = [mic1' mic2'];\n[W,s,v]=svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');\nplot(t,v(:,1));\nhold on;\nmaxAmp = max(v(:,1));\nplot(t,v(:,2),'r'); xlabel('time'); ylabel('amplitude'); axis([0 0.005 -maxAmp maxAmp]); legend('isolated tone 1', 'isolated tone 2');\nhold off;\n</code></pre>\n<p>After about 10 minutes of execution on my laptop computer, the simulation generates the following three figures illustrating the two isolated tones have the correct frequencies.</p>\n<p><img alt=\"Figure 1\" src=\"https://i.sstatic.net/5FOS9.png\"/>\n<img alt=\"Figure 2\" src=\"https://i.sstatic.net/TyU8G.png\"/>\n<img alt=\"Figure 3\" src=\"https://i.sstatic.net/83RI2.png\"/></p>\n<p>However, setting the microphone separation distance to zero (i.e., dMic = 0) causes the simulation to instead generate the following three figures illustrating the simulation could not isolate a second tone (confirmed by the single significant diagonal term returned in svd's s matrix).</p>\n<p><img alt=\"Figure 1 with no mic separation\" src=\"https://i.sstatic.net/q9QbR.png\"/>\n<img alt=\"Figure 2 with no mic separation\" src=\"https://i.sstatic.net/qpaGo.png\"/>\n<img alt=\"Figure 3 with no mic separation\" src=\"https://i.sstatic.net/KKUp1.png\"/></p>\n<p>I was hoping the microphone separation distance on a smartphone would be large enough to produce good results but setting the microphone separation distance to 5.25 inches (i.e., dMic = 0.1333 meters) causes the simulation to generate the following, less than encouraging, figures illustrating higher frequency components in the first isolated tone.</p>\n<p><img alt=\"Figure 1 on smartphone\" src=\"https://i.sstatic.net/Y62sE.png\"/>\n<img alt=\"Figure 2 on smartphone\" src=\"https://i.sstatic.net/auJGT.png\"/>\n<img alt=\"Figure 3 on smartphone\" src=\"https://i.sstatic.net/vslRE.png\"/></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was trying to figure this out as well, 2 years later. But I got my answers; hopefully it'll help someone. </p>\n<p>You need 2 audio recordings. You can get audio examples from <a href=\"http://research.ics.aalto.fi/ica/cocktail/cocktail_en.cgi\">http://research.ics.aalto.fi/ica/cocktail/cocktail_en.cgi</a>. </p>\n<p>reference for implementation is <a href=\"http://www.cs.nyu.edu/~roweis/kica.html\">http://www.cs.nyu.edu/~roweis/kica.html</a></p>\n<p>ok, here's code -</p>\n<pre><code>[x1, Fs1] = audioread('mix1.wav');\n[x2, Fs2] = audioread('mix2.wav');\nxx = [x1, x2]';\nyy = sqrtm(inv(cov(xx')))*(xx-repmat(mean(xx,2),1,size(xx,2)));\n[W,s,v] = svd((repmat(sum(yy.*yy,1),size(yy,1),1).*yy)*yy');\n\na = W*xx; %W is unmixing matrix\nsubplot(2,2,1); plot(x1); title('mixed audio - mic 1');\nsubplot(2,2,2); plot(x2); title('mixed audio - mic 2');\nsubplot(2,2,3); plot(a(1,:), 'g'); title('unmixed wave 1');\nsubplot(2,2,4); plot(a(2,:),'r'); title('unmixed wave 2');\n\naudiowrite('unmixed1.wav', a(1,:), Fs1);\naudiowrite('unmixed2.wav', a(2,:), Fs1);\n</code></pre>\n<p><a href=\"https://i.sstatic.net/8h4d2.png\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/8h4d2.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>x(t)</code> is the original voice from one channel/microphone. </p>\n<p><code>X = repmat(sum(x.*x,1),size(x,1),1).*x)*x'</code> is an estimation of the power spectrum of <code>x(t)</code>. Although <code>X' = X</code>, the intervals between rows and columns are not the same at all. Each row represents the time of the signal, while each column is frequency. I guess this is an estimation and simplification of a more strict expression called <a href=\"http://www.mathworks.com/help/signal/ref/spectrogram.html\">spectrogram</a>. </p>\n<p><a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular Value Decomposition</a> on spectrogram is used to factorize the signal into different components based on spectrum information. Diagonal values in <code>s</code> are the magnitude of different spectrum components. The rows in <code>u</code> and columns in <code>v'</code> are the orthogonal vectors that map the frequency component with the corresponding magnitude to <code>X</code> space.</p>\n<p>I don't have voice data to test, but in my understanding, by means of SVD, the components fall into the similar orthogonal vectors are hopefully be clustered with the help of unsupervised learning. Say, if the first 2 diagonal magnitudes from s are clustered, then <code>u*s_new*v'</code> will form the one-person-voice, where <code>s_new</code> is the same of <code>s</code> except all the elements at <code>(3:end,3:end)</code> are eliminated.</p>\n<p>Two articles about the <a href=\"http://orbit.dtu.dk/fedora/objects/orbit:82321/datastreams/file_4737044/content\">sound-formed matrix</a> and <a href=\"http://airccse.org/journal/ijnlc/papers/2113ijnlc02.pdf\">SVD</a> are for your reference.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am newbie in convolutional neural networks and just have idea about feature maps and how convolution is done on images to extract features. I would be glad to know some details on applying batch normalisation in CNN.</p>\n<p>I read this paper <a href=\"https://arxiv.org/pdf/1502.03167v3.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1502.03167v3.pdf</a> and could understand the BN algorithm applied on a data but in the end they mentioned that a slight modification is required when applied to CNN:</p>\n<blockquote>\n<p>For convolutional layers, we additionally want the normalization to obey the convolutional property – so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a mini- batch, over all locations. In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations – so for a mini-batch of size m and feature maps of size p × q, we use the effec- tive mini-batch of size m′ = |B| = m · pq. We learn a pair of parameters γ(k) and β(k) per feature map, rather than per activation. Alg. 2 is modified similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map.</p>\n</blockquote>\n<p>I am total confused when they say\n<strong>\"so that different elements of the same feature map, at different locations, are normalized in the same way\"</strong></p>\n<p>I know what feature maps mean and different elements are the weights in every feature map. But I could not understand what location or spatial location means.</p>\n<p>I could not understand the below sentence at all\n<strong>\"In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations\"</strong></p>\n<p>I would be glad if someone cold elaborate and explain me in much simpler terms </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's start with the terms. Remember that the output of the convolutional layer is a 4-rank tensor <code>[B, H, W, C]</code>, where <code>B</code> is the batch size, <code>(H, W)</code> is the <em>feature map</em> size, <code>C</code> is the number of channels. An index <code>(x, y)</code> where <code>0 &lt;= x &lt; H</code> and <code>0 &lt;= y &lt; W</code> is a <em>spatial location</em>.</p>\n<h2>Usual batchnorm</h2>\n<p>Now, here's how the batchnorm is applied in a usual way (in pseudo-code):</p>\n<pre class=\"lang-py prettyprint-override\"><code># t is the incoming tensor of shape [B, H, W, C]\n# mean and stddev are computed along 0 axis and have shape [H, W, C]\nmean = mean(t, axis=0)\nstddev = stddev(t, axis=0)\nfor i in 0..B-1:\n  out[i,:,:,:] = norm(t[i,:,:,:], mean, stddev)\n</code></pre>\n<p>Basically, it computes <code>H*W*C</code> means and <code>H*W*C</code> standard deviations across <code>B</code> elements. You may notice that different elements at different spatial locations have their own mean and variance and gather only <code>B</code> values.</p>\n<h2>Batchnorm in conv layer</h2>\n<p>This way is totally possible. But the convolutional layer has a special property: filter weights are shared across the input image (you can read it in detail in <a href=\"http://cs231n.github.io/convolutional-networks/#conv\" rel=\"noreferrer\">this post</a>). That's why it's reasonable to normalize the output in the same way, so that each output value takes the mean and variance of <code>B*H*W</code> values, at different locations.</p>\n<p>Here's how the code looks like in this case (again pseudo-code):</p>\n<pre class=\"lang-py prettyprint-override\"><code># t is still the incoming tensor of shape [B, H, W, C]\n# but mean and stddev are computed along (0, 1, 2) axes and have just [C] shape\nmean = mean(t, axis=(0, 1, 2))\nstddev = stddev(t, axis=(0, 1, 2))\nfor i in 0..B-1, x in 0..H-1, y in 0..W-1:\n  out[i,x,y,:] = norm(t[i,x,y,:], mean, stddev)\n</code></pre>\n<p>In total, there are only <code>C</code> means and standard deviations and each one of them is computed over <code>B*H*W</code> values. That's what they mean when they say \"effective mini-batch\": the difference between the two is only in axis selection (or equivalently \"mini-batch selection\").</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Some clarification on Maxim's answer. </p>\n<p>I was puzzled by seeing in Keras that the axis you specify is the channels axis, as it doesn't make sense to normalize over the channels - as every channel in a conv-net is considered a different \"feature\". I.e. normalizing over all channels is equivalent to normalizing number of bedrooms with size in square feet (multivariate regression example from Andrew's ML course). This is usually not what you want - what you do is normalize every feature by itself. I.e. you normalize the number of bedrooms across all examples to be with mu=0 and std=1, and you normalize the the square feet across all examples to be with mu=0 and std=1.</p>\n<p>This is why you want C means and stds, because you want a mean and std per channel/feature.</p>\n<p>After checking and testing it myself I realized the issue: there's a bit of a confusion/misconception here. The axis you specify in Keras is actually the axis which is <strong>not</strong> in the calculations. i.e. you get average over every axis except the one specified by this argument. This is confusing, as it is exactly the opposite behavior of how NumPy works, where the specified axis is the one you do the operation on (e.g. np.mean, np.std, etc.).</p>\n<p>I actually built a toy model with only BN, and then calculated the BN manually - took the mean, std across all the 3 first dimensions [m, n_W, n_H] and got n_C results, calculated (X-mu)/std (using broadcasting) and got identical results to the Keras results.</p>\n<p>Hope this helps anyone who was confused as I was.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm only 70% sure of what I say, so if it does not make sense, please edit or mention it before downvoting. </p>\n<p>About <code>location</code> or <code>spatial location</code>: they mean the position of pixels in an image or feature map. A feature map is comparable to a sparse modified version of image where concepts are represented. </p>\n<p>About <code>so that different elements of the same feature map, at different locations, are normalized in the same way</code>: \nsome normalisation algorithms are local, so they are dependent of their close surrounding (location) and not the things far apart in the image. They probably mean that every pixel, regardless of their location, is treated just like the element of a set, independently of it's direct special surrounding. </p>\n<p>About <code>In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations</code>: They get a flat list of every values of every training example in the minibatch, and this list combines things whatever their location is on the feature map. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I understand that Batch Normalisation helps in faster training by turning the activation towards unit Gaussian distribution and thus tackling vanishing gradients problem. Batch norm acts is applied differently at training(use mean/var from each batch) and test time (use finalized running mean/var from training phase).</p>\n<p>Instance normalisation, on the other hand, acts as contrast normalisation as mentioned in this paper <a href=\"https://arxiv.org/abs/1607.08022\" rel=\"noreferrer\">https://arxiv.org/abs/1607.08022</a> . The authors mention that the output stylised images should be not depend on the contrast of the input content image and hence Instance normalisation helps. </p>\n<p>But then should we not also use instance normalisation for image classification where class label should not depend on the contrast of input image. I have not seen any paper using instance normalisation in-place of batch normalisation for classification. What is the reason for that? Also, can and should batch and instance normalisation be used together. I am eager to get an intuitive as well as theoretical understanding of when to use which normalisation. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Definition</h2>\n<p>Let's begin with the strict definition of both:</p>\n<p><strong>Batch normalization</strong>\n<a href=\"https://i.sstatic.net/VDqKY.jpg\" rel=\"noreferrer\"><img alt=\"batch-norm-formula\" src=\"https://i.sstatic.net/VDqKY.jpg\"/></a></p>\n<p><strong>Instance normalization</strong>\n<a href=\"https://i.sstatic.net/X5z48.jpg\" rel=\"noreferrer\"><img alt=\"instance-norm-formula\" src=\"https://i.sstatic.net/X5z48.jpg\"/></a></p>\n<p>As you can notice, they are doing the same thing, except for the number of input tensors that are normalized jointly. Batch version normalizes all images <em>across the batch and spatial locations</em> (in the CNN case, in the ordinary case <a href=\"https://stackoverflow.com/q/38553927/712995\">it's different</a>); instance version normalizes each element of the batch independently, i.e., across <em>spatial locations</em> only.</p>\n<p>In other words, where batch norm computes one mean and std dev (thus making the distribution of the whole layer Gaussian), instance norm computes <code>T</code> of them, making each individual image distribution look Gaussian, but not jointly.</p>\n<p>A simple analogy: during data pre-processing step, it's possible to normalize the data on per-image basis or normalize the whole data set.</p>\n<p><sup>Credit: the formulas are from <a href=\"https://github.com/aleju/papers/blob/master/neural-nets/Instance_Normalization_The_Missing_Ingredient_for_Fast_Stylization.md\" rel=\"noreferrer\">here</a>.</sup></p>\n<h2>Which normalization is better?</h2>\n<p>The answer depends on the network architecture, in particular on what is done <em>after</em> the normalization layer. Image classification networks usually stack the feature maps together and wire them to the FC layer, which <strong>share weights across the batch</strong> (the modern way is to use the CONV layer instead of FC, but the argument still applies).</p>\n<p>This is where the distribution nuances start to matter: the same neuron is going to receive the input from all images. If the variance across the batch is high, the gradient from the small activations will be completely suppressed by the high activations, which is exactly the problem that batch norm tries to solve. That's why it's fairly possible that per-instance normalization won't improve network convergence at all.</p>\n<p>On the other hand, batch normalization adds extra noise to the training, because the result for a particular instance depends on the neighbor instances. As it turns out, this kind of noise may be either good and bad for the network. This is well explained in the <a href=\"https://arxiv.org/pdf/1602.07868.pdf\" rel=\"noreferrer\">\"Weight Normalization\"</a> paper by Tim Salimans at al, which name recurrent neural networks and reinforcement learning DQNs as <em>noise-sensitive applications</em>. I'm not entirely sure, but I think that the same noise-sensitivity was the main issue in stylization task, which instance norm tried to fight. It would be interesting to check if weight norm performs better for this particular task.</p>\n<h2>Can you combine batch and instance normalization?</h2>\n<p>Though it makes a valid neural network, there's no practical use for it. Batch normalization noise is either helping the learning process (in this case it's preferable) or hurting it (in this case it's better to omit it). In both cases, leaving the network with one type of normalization is likely to improve the performance.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Great question and already answered nicely. Just to add: I found this visualisation From Kaiming He's Group Norm paper helpful. <a href=\"https://i.sstatic.net/1JdN6.png\" rel=\"noreferrer\"><img alt=\"\" src=\"https://i.sstatic.net/1JdN6.png\"/></a></p>\n<p>Source: <a href=\"https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\" rel=\"noreferrer\" title=\"link to article on Medium contrasting the Norms\">link to article on Medium contrasting the Norms</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I wanted to add more information to this question since there are some more recent works in this area. Your intuition</p>\n<blockquote>\n<p>use instance normalisation for image classification where class label\n  should not depend on the contrast of input image</p>\n</blockquote>\n<p>is partly correct. I would say that a pig in broad daylight is still a pig when the image is taken at night or at dawn. However, this does not mean using instance normalization across the network will give you better result. Here are some reasons:</p>\n<ol>\n<li>Color distribution still play a role. It is more likely to be a apple than an orange if  it has a lot of red.</li>\n<li>At later layers, you can no longer imagine instance normalization acts as contrast normalization. Class specific details will emerge in deeper layers and normalizing them by instance will hurt the model's performance greatly.</li>\n</ol>\n<p><a href=\"https://arxiv.org/abs/1807.09441\" rel=\"noreferrer\">IBN-Net</a> uses both batch normalization and instance normalization in their model. They only put instance normalization in early layers and have achieved improvement in both accuracy and ability to generalize. They have open sourced code <a href=\"https://github.com/XingangPan/IBN-Net\" rel=\"noreferrer\">here</a>.</p>\n<p><a href=\"https://i.sstatic.net/S74SW.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/S74SW.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I got this from the <code>sklearn</code> webpage:</p>\n<ul>\n<li><p><strong>Pipeline</strong>: Pipeline of transforms with a final estimator</p>\n</li>\n<li><p><strong>Make_pipeline</strong>: Construct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor.</p>\n</li>\n</ul>\n<p>But I still do not understand when I have to use each one. Can anyone give me an example?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The only difference is that <code>make_pipeline</code> generates names for steps automatically.</p>\n<p>Step names are needed e.g. if you want to use a pipeline with model selection utilities (e.g. GridSearchCV). With grid search you need to specify parameters for various steps of a pipeline:</p>\n<pre><code>pipe = Pipeline([('vec', CountVectorizer()), ('clf', LogisticRegression()])\nparam_grid = [{'clf__C': [1, 10, 100, 1000]}\ngs = GridSearchCV(pipe, param_grid)\ngs.fit(X, y)\n</code></pre>\n<p>compare it with make_pipeline:</p>\n<pre><code>pipe = make_pipeline(CountVectorizer(), LogisticRegression())     \nparam_grid = [{'logisticregression__C': [1, 10, 100, 1000]}\ngs = GridSearchCV(pipe, param_grid)\ngs.fit(X, y)\n</code></pre>\n<p>So, with <code>Pipeline</code>:</p>\n<ul>\n<li>names are explicit, you don't have to figure them out if you need them;</li>\n<li>name doesn't change if you change estimator/transformer used in a step, e.g. if you replace LogisticRegression() with LinearSVC() you can still use <code>clf__C</code>.</li>\n</ul>\n<p><code>make_pipeline</code>:</p>\n<ul>\n<li>shorter and arguably more readable notation;</li>\n<li>names are auto-generated using a straightforward rule (lowercase name of an estimator).</li>\n</ul>\n<p>When to use them is up to you :) I prefer make_pipeline for quick experiments and Pipeline for more stable code; a rule of thumb: IPython Notebook -&gt; make_pipeline; Python module in a larger project -&gt; Pipeline. But it is certainly not a big deal to use make_pipeline in a module or Pipeline in a short script or a notebook. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If we look at <a href=\"https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/pipeline.py#L872\" rel=\"nofollow noreferrer\">the source code</a>, <code>make_pipeline()</code> creates a <code>Pipeline</code> object, so they are equivalent. As mentioned by @Mikhail Korobov, the only difference is that <code>make_pipeline()</code> doesn't admit estimator names and they are set to the lowercase of their types. In other words, <code>type(estimator).__name__.lower()</code> is used to create estimator names (<a href=\"https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/pipeline.py#L807\" rel=\"nofollow noreferrer\">source</a>). So it's really a simpler form of building a pipeline.</p>\n<p>On a related note, to get parameter names you can use <code>get_params()</code>. This is useful if you want to know the parameter names for <code>GridSearch()</code>. The parameter names are <a href=\"https://github.com/scikit-learn/scikit-learn/blob/2a2772a87b6c772dc3b8292bcffb990ce27515a8/sklearn/utils/metaestimators.py#L49\" rel=\"nofollow noreferrer\">created by</a> concatenating the estimator names with their kwargs recursively (e.g. <code>max_iter</code> of a <code>LogisticRegression()</code> is stored as <code>'logisticregression__max_iter'</code> or <code>C</code> parameter in <code>OneVsRestClassifier(LogisticRegression())</code> as <code>'onevsrestclassifier__estimator__C'</code>; the latter because when written using kwargs, it is <code>OneVsRestClassifier(estimator=LogisticRegression())</code>).</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification()\npipe = make_pipeline(PCA(), LogisticRegression())\n\nprint(pipe.get_params())\n\n# {'memory': None,\n#  'steps': [('pca', PCA()), ('logisticregression', LogisticRegression())],\n#  'verbose': False,\n#  'pca': PCA(),\n#  'logisticregression': LogisticRegression(),\n#  'pca__copy': True,\n#  'pca__iterated_power': 'auto',\n#  'pca__n_components': None,\n#  'pca__n_oversamples': 10,\n#  'pca__power_iteration_normalizer': 'auto',\n#  'pca__random_state': None,\n#  'pca__svd_solver': 'auto',\n#  'pca__tol': 0.0,\n#  'pca__whiten': False,\n#  'logisticregression__C': 1.0,\n#  'logisticregression__class_weight': None,\n#  'logisticregression__dual': False,\n#  'logisticregression__fit_intercept': True,\n#  'logisticregression__intercept_scaling': 1,\n#  'logisticregression__l1_ratio': None,\n#  'logisticregression__max_iter': 100,\n#  'logisticregression__multi_class': 'auto',\n#  'logisticregression__n_jobs': None,\n#  'logisticregression__penalty': 'l2',\n#  'logisticregression__random_state': None,\n#  'logisticregression__solver': 'lbfgs',\n#  'logisticregression__tol': 0.0001,\n#  'logisticregression__verbose': 0,\n#  'logisticregression__warm_start': False}\n\n# use the params from above to construct param_grid\nparam_grid = {'pca__n_components': [2, None], 'logisticregression__C': [0.1, 1]}\ngs = GridSearchCV(pipe, param_grid)\ngs.fit(X, y)\n\nbest_score = gs.score(X, y)\n</code></pre>\n<p>Circling back to <code>Pipeline</code> vs <code>make_pipeline</code>; <code>Pipeline</code> gives you more flexibility in naming parameters but if you name each estimator using lowercase of its type, then <code>Pipeline</code> and <code>make_pipeline</code> they will both have the same params and steps attributes.</p>\n<pre class=\"lang-py prettyprint-override\"><code>pca = PCA()\nlr = LogisticRegression()\nmake_pipe = make_pipeline(pca, lr)\npipe = Pipeline([('pca', pca), ('logisticregression', lr)])\n\nmake_pipe.get_params() == pipe.get_params()   # True\nmake_pipe.steps == pipe.steps                 # True\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Could someone please explain to me how to update the bias throughout backpropagation? </p>\n<p>I've read quite a few books, but can't find bias updating!</p>\n<p>I understand that bias is an extra input of 1 with a weight attached to it (for each neuron). There must be a formula.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Following the notation of <a href=\"http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf\" rel=\"noreferrer\">Rojas 1996, chapter 7</a>, backpropagation computes partial derivatives of the error function <code>E</code> (aka cost, aka loss)</p>\n<pre><code>∂E/∂w[i,j] = delta[j] * o[i]\n</code></pre>\n<p>where <code>w[i,j]</code> is the weight of the connection between neurons <code>i</code> and <code>j</code>, <code>j</code> being one layer higher in the network than <code>i</code>, and <code>o[i]</code> is the output (activation) of <code>i</code> (in the case of the \"input layer\", that's just the value of feature <code>i</code> in the training sample under consideration). How to determine <code>delta</code> is given in any textbook and depends on the activation function, so I won't repeat it here.</p>\n<p>These values can then be used in weight updates, e.g.</p>\n<pre><code>// update rule for vanilla online gradient descent\nw[i,j] -= gamma * o[i] * delta[j]\n</code></pre>\n<p>where <code>gamma</code> is the learning rate.</p>\n<p><strong>The rule for bias weights</strong> is very similar, except that there's no input from a previous layer. Instead, bias is (conceptually) caused by input from a neuron with a fixed activation of 1. So, the update rule for bias weights is</p>\n<pre><code>bias[j] -= gamma_bias * 1 * delta[j]\n</code></pre>\n<p>where <code>bias[j]</code> is the weight of the bias on neuron <code>j</code>, the multiplication with 1 can obviously be omitted, and <code>gamma_bias</code> may be set to <code>gamma</code> or to a different value. If I recall correctly, lower values are preferred, though I'm not sure about the theoretical justification of that.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The amount you change each individual weight and bias will be the partial derivative of your cost function in relation to each individual weight and each individual bias. </p>\n<pre><code>∂C/∂(index of bias in network)\n</code></pre>\n<p>Since your cost function probably doesn't explicitly depend on individual weights and values (Cost might equal (network output - expected output)^2, for example), you'll need to relate the partial derivatives of each weight and bias to something you know, i.e. the activation values (outputs) of neurons. Here's a great guide to doing this:</p>\n<p><a href=\"https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d\" rel=\"noreferrer\">https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d</a></p>\n<p>This guide states how to do these things clearly, but can sometimes be lacking on explanation. I found it very helpful to read chapters 1 and 2 of this book as I read the guide linked above:</p>\n<p><a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\" rel=\"noreferrer\">http://neuralnetworksanddeeplearning.com/chap1.html</a>\n(provides essential background for the answer to your question)</p>\n<p><a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"noreferrer\">http://neuralnetworksanddeeplearning.com/chap2.html</a>\n(answers your question)</p>\n<p>Basically, biases are updated in the same way that weights are updated: a change is determined based on the gradient of the cost function at a multi-dimensional point. </p>\n<p>Think of the problem your network is trying to solve as being a landscape of multi-dimensional hills and valleys (gradients). This landscape is a graphical representation of how your cost changes with changing weights and biases. The goal of a neural network is to reach the lowest point in this landscape, thereby finding the smallest cost and minimizing error. If you imagine your network as a traveler trying to reach the bottom of these gradients (i.e. Gradient Descent), then the amount you will change each weight (and bias) by is related to the the slope of the incline (gradient of the function) that the traveler is currently climbing down. The exact location of the traveler is given by a multi-dimensional coordinate point (weight1, weight2, weight3, ... weight_n), where the bias can be thought of as another kind of weight. Thinking of the weights/biases of a network as the variables for the network's cost function make it clear that ∂C/∂(index of bias in network) must be used.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I understand that the function of bias is to make level adjust of the\ninput values. Below is what happens inside the neuron. The activation function of course\nwill make the final output, but it is left out for clarity.</p>\n<ul>\n<li><em>O = W1 I1 + W2 I2 + W3 I3</em></li>\n</ul>\n<p>In real neuron something happens already at synapses, the input data is level adjusted with average of samples and scaled with deviation of samples. Thus the input data is normalized and with equal weights they will make the same effect. The normalized <em>In</em> is calculated from raw data <em>in</em> (n is the index).</p>\n<ul>\n<li><em>Bn</em> = average(<em>in</em>); <em>Sn</em> = 1/stdev((<em>in</em>);  <em>In</em>= (<em>in</em>+<em>Bn</em>)<em>Sn</em></li>\n</ul>\n<p>However this is not necessary to be performed separately, because the neuron weights and bias can do the same function. When you subsitute <em>In</em> with the  <em>in</em>, you get new formula</p>\n<ul>\n<li><em>O</em> = <em>w1 i1</em> + <em>w2 i2</em> + <em>w3 i3</em>+ <em>wbs</em></li>\n</ul>\n<p>The last <em>wbs</em> is the bias and new weights <em>wn</em>  as well</p>\n<ul>\n<li><em>wbs = W1 B1 S1 + W2 B2 S2 + W3 B3 S3</em></li>\n<li><em>wn =W1 (in+Bn) Sn</em></li>\n</ul>\n<p>So there exists a bias and it will/should be adjusted automagically with the backpropagation</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2015-02-01 18:59:32Z\">9 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/573768/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm looking for an open source implementation, preferably in python, of <strong>Textual Sentiment Analysis</strong> (<a href=\"http://en.wikipedia.org/wiki/Sentiment_analysis\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). Is anyone familiar with such open source implementation I can use?</p>\n<p>I'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.</p>\n<p>Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.</p>\n<p>BTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.</p>\n<p>Thanks!</p>\n<p>BTW, an early demo is <a href=\"http://twitgraph.appspot.com/?show_inputs=1&amp;duration=30&amp;q=youtube+annotations\" rel=\"noreferrer\">here</a> and the code I have so far is <a href=\"http://code.google.com/p/twitgraph/\" rel=\"noreferrer\">here</a> and I'd love to opensource it with any interested developer.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Good luck with that.</p>\n<p>Sentiment is enormously contextual, and tweeting culture makes the problem worse because <em>you aren't given the context</em> for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message.</p>\n<p>If they say the video is bad, does that mean bad, or <em>bad</em>?</p>\n<blockquote>\n<p>A linguistics professor was lecturing\n  to her class one day. \"In English,\"\n  she said, \"A double negative forms a\n  positive. In some languages, though,\n  such as Russian, a double negative is\n  still a negative. However, there is no\n  language wherein a double positive can\n  form a negative.\"</p>\n<p>A voice from the back of the room\n  piped up, \"Yeah . . .right.\"</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.</p>\n<p>You may be interested in <a href=\"http://tadm.sf.net\" rel=\"noreferrer\">The Toolkit for Advanced Discriminative Modeling</a>, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.</p>\n<p>The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for <a href=\"http://comp.ling.utexas.edu/jbaldrid/courses/2006/cl2/\" rel=\"noreferrer\">Computational Linguistics II</a> to get an idea of how to make it work and what previous applications it has served.</p>\n<p>Another great tool which works in the same vein is <a href=\"http://mallet.cs.umass.edu/\" rel=\"noreferrer\">Mallet</a>.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  <a href=\"http://www.cs.waikato.ac.nz/ml/weka/\" rel=\"noreferrer\">Weka</a> is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.</p>\n<p>Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Thanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from <a href=\"http://www.divmod.org/trac/wiki/DivmodReverend\" rel=\"noreferrer\">here</a>. \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice.</p>\n<p>The full details of my work as in a <a href=\"http://www.gbsheli.com/2009/03/twitgraph-en.html\" rel=\"noreferrer\">blog post</a>.</p>\n<p>Again, your help was very useful, so thank you!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was wondering if there are classifiers that handle nan/null values in scikit-learn.  I thought random forest regressor handles this but I got an error when I call <code>predict</code>.</p>\n<pre><code>X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n</code></pre>\n<p>Can I not call predict with any scikit-learn algorithm with missing values?</p>\n<p><strong>Edit.</strong>\nNow that I think about this, it makes sense.  It's not an issue during training but when you predict how do you branch when the variable is null?  maybe you could just split both ways and average the result?  It seems like k-NN should work fine as long as the distance function ignores nulls though.</p>\n<p><strong>Edit 2 (older and wiser me)</strong>\nSome gbm libraries (such as xgboost) use a ternary tree instead of a binary tree precisely for this purpose: 2 children for the yes/no decision and 1 child for the missing decision. sklearn is <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L70-L74\" rel=\"noreferrer\">using a binary tree</a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Short answer</h2>\n<p>Sometimes missing values are simply not applicable. Imputing them is meaningless. In these cases you should use a model that can handle missing values. Scitkit-learn's models cannot handle missing values. XGBoost can.</p>\n<hr/>\n<h2>More on scikit-learn and XGBoost</h2>\n<p>As mentioned in <a href=\"https://machinelearningmastery.com/handle-missing-data-python/\" rel=\"noreferrer\">this article</a>, scikit-learn's decision trees and KNN algorithms are <strong>not (<a href=\"https://github.com/scikit-learn/scikit-learn/issues/5870\" rel=\"noreferrer\">yet</a>) robust enough</strong> to work with missing values. If imputation doesn't make sense, don't do it.</p>\n<h2>Consider situtations when imputation doesn't make sense.</h2>\n<blockquote>\n<p>keep in mind this is a made-up example</p>\n</blockquote>\n<p>Consider a dataset with <strong>rows of cars</strong> (\"Danho Diesel\", \"Estal Electric\", \"Hesproc Hybrid\") and <strong>columns with their properties</strong> (Weight, Top speed, Acceleration, Power output, Sulfur Dioxide Emission, Range).</p>\n<p>Electric cars do not produce exhaust fumes - so the <strong>Sulfur dioxide emission of the <em>Estal Electric</em> should be a <code>NaN</code>-value (missing)</strong>. You could argue that it should be set to 0 - but electric cars cannot produce sulfur dioxide. Imputing the value will ruin your predictions.</p>\n<p>As mentioned in <a href=\"https://machinelearningmastery.com/handle-missing-data-python/\" rel=\"noreferrer\">this article</a>, scikit-learn's decision trees and KNN algorithms are <strong>not (<a href=\"https://github.com/scikit-learn/scikit-learn/issues/5870\" rel=\"noreferrer\">yet</a>) robust enough</strong> to work with missing values. If imputation doesn't make sense, don't do it.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I made an example that contains both missing values in training and the test sets</p>\n<p>I just picked a strategy to replace missing data with the mean, using the <code>SimpleImputer</code> class. There are other strategies.</p>\n<pre><code>from __future__ import print_function\n\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n\n\nX_train = [[0, 0, np.nan], [np.nan, 1, 1]]\nY_train = [0, 1]\nX_test_1 = [0, 0, np.nan]\nX_test_2 = [0, np.nan, np.nan]\nX_test_3 = [np.nan, 1, 1]\n\n# Create our imputer to replace missing values with the mean e.g.\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp = imp.fit(X_train)\n\n# Impute our data, then train\nX_train_imp = imp.transform(X_train)\nclf = RandomForestClassifier(n_estimators=10)\nclf = clf.fit(X_train_imp, Y_train)\n\nfor X_test in [X_test_1, X_test_2, X_test_3]:\n    # Impute each test item, then predict\n    X_test_imp = imp.transform(X_test)\n    print(X_test, '-&gt;', clf.predict(X_test_imp))\n\n# Results\n[0, 0, nan] -&gt; [0]\n[0, nan, nan] -&gt; [0]\n[nan, 1, 1] -&gt; [1]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you are using DataFrame, you could use <code>fillna</code>. Here I replaced the missing data with the mean of that column.</p>\n<pre><code>df.fillna(df.mean(), inplace=True)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use KerasClassifier to train the classifier.</p>\n<p>The code is below:</p>\n<pre><code>import numpy\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataframe = read_csv(\"iris.csv\", header=None)\ndataset = dataframe.values\nX = dataset[:,0:4].astype(float)\nY = dataset[:,4]\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n#print(\"encoded_Y\")\n#print(encoded_Y)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)\n#print(\"dummy_y\")\n#print(dummy_y)\n# define baseline model\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(4, input_dim=4, init='normal', activation='relu'))\n    #model.add(Dense(4, init='normal', activation='relu'))\n    model.add(Dense(3, init='normal', activation='softmax'))\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n\nestimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)\n#global_model = baseline_model()\nkfold = KFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, dummy_y, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n</code></pre>\n<p>But How to save the final model for future prediction?</p>\n<p>I usually use below code to save model:</p>\n<pre><code># serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n</code></pre>\n<p>But I don't know how to insert the saving model's code into KerasClassifier's code.</p>\n<p>Thank you.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The model has a <code>save</code> method, which saves all the details necessary to reconstitute the model. An example from the <a href=\"https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\" rel=\"noreferrer\">keras documentation</a>:</p>\n<pre><code>from keras.models import load_model\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can save the model in <strong>json</strong> and weights in a <strong>hdf5</strong> file format. </p>\n<pre><code># keras library import  for Saving and loading model and weights\n\nfrom keras.models import model_from_json\nfrom keras.models import load_model\n\n# serialize model to JSON\n#  the keras model which is trained is defined as 'model' in this example\nmodel_json = model.to_json()\n\n\nwith open(\"model_num.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\n# serialize weights to HDF5\nmodel.save_weights(\"model_num.h5\")\n</code></pre>\n<p>files \"model_num.h5\" and \"model_num.json\" are created which contain our model and weights </p>\n<p>To use the same trained model for further testing you can simply load the hdf5 file and use it for the prediction of different data. \nhere's how to load the model from saved files.</p>\n<pre><code># load json and create model\njson_file = open('model_num.json', 'r')\n\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"model_num.h5\")\nprint(\"Loaded model from disk\")\n\nloaded_model.save('model_num.hdf5')\nloaded_model=load_model('model_num.hdf5')\n</code></pre>\n<p>To predict for different data you can use this</p>\n<pre><code>loaded_model.predict_classes(\"your_test_data here\")\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <code>model.save(filepath)</code> to save a Keras model into a single HDF5 file which will contain:</p>\n<ul>\n<li>the architecture of the model, allowing to re-create the model.</li>\n<li>the weights of the model.</li>\n<li>the training configuration (loss, optimizer)</li>\n<li>the state of the optimizer, allowing to resume training exactly where you left off.</li>\n</ul>\n<p>In your Python code probable the last line should be:</p>\n<pre><code>model.save(\"m.hdf5\")\n</code></pre>\n<p>This allows you to save the entirety of the state of a model in a single file.\nSaved models can be reinstantiated via <code>keras.models.load_model()</code>.</p>\n<p>The model returned by <code>load_model()</code> is a compiled model ready to be used (unless the saved model was never compiled in the first place).</p>\n<p><code>model.save()</code> arguments:</p>\n<ul>\n<li>filepath: String, path to the file to save the weights to.</li>\n<li>overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt.</li>\n<li>include_optimizer: If True, save optimizer's state together.</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have trouble understanding the difference (if there is one) between <code>roc_auc_score()</code> and <code>auc()</code> in scikit-learn.</p>\n<p>Im tying to predict a binary output with imbalanced classes (around 1.5% for Y=1).</p>\n<h3>Classifier</h3>\n<pre><code>model_logit = LogisticRegression(class_weight='auto')\nmodel_logit.fit(X_train_ridge, Y_train)\n</code></pre>\n<h3>Roc curve</h3>\n<pre><code>false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, clf.predict_proba(xtest)[:,1])\n</code></pre>\n<h3>AUC's</h3>\n<pre><code>auc(false_positive_rate, true_positive_rate)\nOut[490]: 0.82338034042531527\n</code></pre>\n<p>and</p>\n<pre><code>roc_auc_score(Y_test, clf.predict(xtest))\nOut[493]: 0.75944737191205602\n</code></pre>\n<p>Somebody can explain this difference ? I thought both were just calculating the area under the ROC curve. Might be because of the imbalanced dataset but I could not figure out why.</p>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>AUC</code> is not always <strong>area under the curve</strong> of a <code>ROC</code> curve. Area Under the Curve is an (abstract) area under <strong>some</strong> curve, so it is a more <strong>general</strong> thing than <code>AUROC</code>. With imbalanced classes, it may be better to find AUC for a precision-recall curve.</p>\n<p>See sklearn source for <code>roc_auc_score</code>:</p>\n<pre><code>def roc_auc_score(y_true, y_score, average=\"macro\", sample_weight=None):\n    # &lt;...&gt; docstring &lt;...&gt;\n    def _binary_roc_auc_score(y_true, y_score, sample_weight=None):\n            # &lt;...&gt; bla-bla &lt;...&gt;\n    \n            fpr, tpr, tresholds = roc_curve(y_true, y_score,\n                                            sample_weight=sample_weight)\n            return auc(fpr, tpr, reorder=True)\n    \n    return _average_binary_score(\n        _binary_roc_auc_score, y_true, y_score, average,\n        sample_weight=sample_weight) \n</code></pre>\n<p>As you can see, this first gets a roc curve, and then calls <code>auc()</code> to get the area.</p>\n<p>I guess your problem is the <code>predict_proba()</code> call. For a normal <code>predict()</code> the outputs are always the same:</p>\n<pre><code>import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nest = LogisticRegression(class_weight='auto')\nX = np.random.rand(10, 2)\ny = np.random.randint(2, size=10)\nest.fit(X, y)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y, est.predict(X))\nprint auc(false_positive_rate, true_positive_rate)\n# 0.857142857143\nprint roc_auc_score(y, est.predict(X))\n# 0.857142857143\n</code></pre>\n<p>If you change the above for this, you'll sometimes get different outputs:</p>\n<pre><code>false_positive_rate, true_positive_rate, thresholds = roc_curve(y, est.predict_proba(X)[:,1])\n# may differ\nprint auc(false_positive_rate, true_positive_rate)\nprint roc_auc_score(y, est.predict(X))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>predict</code> returns only one class or the other. Then you compute a ROC with the results of <code>predict</code> on a classifier, there are only three thresholds (trial all one class, trivial all the other class, and in between).  Your ROC curve looks like this:</p>\n<pre><code>      ..............................\n      |\n      |\n      |\n......|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n</code></pre>\n<p>Meanwhile, <code>predict_proba()</code> returns an entire range of probabilities, so now you can put more than three thresholds on your data.</p>\n<pre><code>             .......................\n             |\n             |\n             |\n          ...|\n          |\n          |\n     .....|\n     |\n     |\n ....|\n.|\n|\n|\n|\n|\n</code></pre>\n<p>Hence different areas.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you use the y_pred (class labels), you already decided on\nthe threshold. When you use y_prob (positive class probability)\nyou are open to the threshold, and the ROC Curve should help\nyou decide the threshold.</p>\n<p>For the first case you are using the probabilities:</p>\n<pre><code>y_probs = clf.predict_proba(xtest)[:,1]\nfp_rate, tp_rate, thresholds = roc_curve(y_true, y_probs)\nauc(fp_rate, tp_rate)\n</code></pre>\n<p>When you do that, you're considering the AUC 'before' taking\na decision on the threshold you'll be using.</p>\n<p>In the second case, you are using the prediction (not the probabilities),\nin that case, use 'predict' instead of 'predict_proba' for both and you\nshould get the same result.</p>\n<pre><code>y_pred = clf.predict(xtest)\nfp_rate, tp_rate, thresholds = roc_curve(y_true, y_pred)\nprint auc(fp_rate, tp_rate)\n# 0.857142857143\n\nprint roc_auc_score(y, y_pred)\n# 0.857142857143\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>See below for 50 tweets about \"apple.\" I have hand labeled the positive matches about Apple Inc. They are marked as 1 below.</p>\n<p>Here are a couple of lines:</p>\n<pre><code>1|“@chrisgilmer: Apple targets big business with new iOS 7 features http://bit.ly/15F9JeF ”. Finally.. A corp iTunes account!\n0|“@Zach_Paull: When did green skittles change from lime to green apple? #notafan” @Skittles\n1|@dtfcdvEric: @MaroneyFan11 apple inc is searching for people to help and tryout all their upcoming tablet within our own net page No.\n0|@STFUTimothy have you tried apple pie shine?\n1|#SuryaRay #India Microsoft to bring Xbox and PC games to Apple, Android phones: Report: Microsoft Corp... http://dlvr.it/3YvbQx  @SuryaRay\n</code></pre>\n<p>Here is the total data set: <a href=\"http://pastebin.com/eJuEb4eB\" rel=\"noreferrer\">http://pastebin.com/eJuEb4eB</a></p>\n<p>I need to build a model that classifies \"Apple\" (Inc). from the rest.</p>\n<p>I'm not looking for a general overview of machine learning, rather I'm looking for actual model in code (<a href=\"http://en.wikipedia.org/wiki/Python_%28programming_language%29\" rel=\"noreferrer\">Python</a> preferred).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What you are looking for is called <a href=\"http://en.wikipedia.org/wiki/Named-entity_recognition\" rel=\"noreferrer\">Named Entity Recognition</a>. It is a statistical technique that (most commonly) uses <a href=\"http://en.wikipedia.org/wiki/Conditional_random_field\" rel=\"noreferrer\">Conditional Random Fields</a> to find named entities, based on having been trained to learn things about named entities.</p>\n<p>Essentially, it looks at the content and <em>context</em> of the word, (looking back and forward a few words), to estimate the probability that the word is a named entity. </p>\n<p>Good software can look at other features of words, such as their length or shape (like \"Vcv\" if it starts with \"Vowel-consonant-vowel\")</p>\n<p>A very good library (GPL) is <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"noreferrer\">Stanford's NER</a></p>\n<p>Here's the demo: <a href=\"http://nlp.stanford.edu:8080/ner/\" rel=\"noreferrer\">http://nlp.stanford.edu:8080/ner/</a></p>\n<p>Some sample text to try:</p>\n<blockquote>\n<p>I was eating an apple over at Apple headquarters and I thought about\n  Apple Martin, the daughter of the Coldplay guy</p>\n</blockquote>\n<p>(the 3class and 4class classifiers get it right)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would do it as follows:</p>\n<ol>\n<li>Split the sentence into words, normalise them, build a dictionary</li>\n<li>With each word, store how many times they occurred in tweets about the company, and how many times they appeared in tweets about the fruit - these tweets must be confirmed by a human</li>\n<li>When a new tweet comes in, find every word in the tweet in the dictionary, calculate a weighted score - words that are used frequently in relation to the company would get a high company score, and vice versa; words used rarely, or used with both the company and the fruit, would not have much of a score.</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a semi-working system that solves this problem, open sourced using scikit-learn, with a series of blog posts describing what I'm doing. The problem I'm tackling is word-sense disambiguation (choosing one of multiple <a href=\"https://en.wikipedia.org/wiki/Word_sense\" rel=\"noreferrer\">word sense</a> options), which is not the same as Named Entity Recognition. My basic approach is somewhat-competitive with existing solutions and (crucially) is customisable.</p>\n<p>There are some existing commercial NER tools (OpenCalais, DBPedia Spotlight, and AlchemyAPI) that might give you a good enough commercial result - do try these first!</p>\n<p>I used some of these for a client project (I consult using NLP/ML in London), but I wasn't happy with their recall (<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">precision and recall</a>). Basically they can be precise (when they say \"This is Apple Inc\" they're typically correct), but with low recall (they rarely say \"This is Apple Inc\" even though to a human the tweet is obviously about Apple Inc). I figured it'd be an intellectually interesting exercise to build an open source version tailored to tweets. Here's the current code:\n<a href=\"https://github.com/ianozsvald/social_media_brand_disambiguator\" rel=\"noreferrer\">https://github.com/ianozsvald/social_media_brand_disambiguator</a></p>\n<p>I'll note - I'm not trying to solve the generalised word-sense disambiguation problem with this approach, just <strong>brand</strong> disambiguation (companies, people, etc.) when you already have their name. That's why I believe that this straightforward approach will work.</p>\n<p>I started this six weeks ago, and it is written in Python 2.7 using scikit-learn. It uses a very basic approach. I vectorize using a binary count vectorizer (I only count whether a word appears, not how many times) with 1-3 <a href=\"https://en.wikipedia.org/wiki/N-gram\" rel=\"noreferrer\">n-grams</a>. I don't scale with TF-IDF (TF-IDF is good when you have a variable document length; for me the tweets are only one or two sentences, and my testing results didn't show improvement with TF-IDF).</p>\n<p>I use the basic tokenizer which is very basic but surprisingly useful. It ignores @ # (so you lose some context) and of course doesn't expand a URL. I then train using <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" rel=\"noreferrer\">logistic regression</a>, and it seems that this problem is somewhat linearly separable (lots of terms for one class don't exist for the other). Currently I'm avoiding any stemming/cleaning (I'm trying The Simplest Possible Thing That Might Work).</p>\n<p>The code has a full README, and you should be able to ingest your tweets relatively easily and then follow my suggestions for testing.</p>\n<p>This works for Apple as people don't eat or drink Apple computers, nor do we type or play with fruit, so the words are easily split to one category or the other. This condition may not hold when considering something like #definance for the TV show (where people also use #definance in relation to the Arab Spring, cricket matches, exam revision and a music band). Cleverer approaches may well be required here.</p>\n<p>I have <a href=\"http://ianozsvald.com/category/socialmediabranddisambiguator/\" rel=\"noreferrer\">a series of blog posts</a> describing this project including a one-hour presentation I gave at the BrightonPython usergroup (which turned into a shorter presentation for 140 people at DataScienceLondon).</p>\n<p>If you use something like LogisticRegression (where you get a probability for each classification) you can pick only the confident classifications, and that way you can force high precision by trading against recall (so you get correct results, but fewer of them). You'll have to tune this to your system.</p>\n<p>Here's a possible algorithmic approach using scikit-learn:</p>\n<ul>\n<li>Use a Binary CountVectorizer (I don't think term-counts in short messages add much information as most words occur only once)</li>\n<li>Start with a Decision Tree classifier. It'll have explainable performance (see <em><a href=\"http://ianozsvald.com/2013/07/07/overfitting-with-a-decision-tree/\" rel=\"noreferrer\">Overfitting with a Decision Tree</a></em> for an example).</li>\n<li>Move to logistic regression</li>\n<li>Investigate the errors generated by the classifiers (read the DecisionTree's exported output or look at the coefficients in LogisticRegression, work the mis-classified tweets back through the Vectorizer to see what the underlying Bag of Words representation looks like - there will be fewer tokens there than you started with in the raw tweet - are there enough for a classification?)</li>\n<li>Look at my example code in <a href=\"https://github.com/ianozsvald/social_media_brand_disambiguator/blob/master/learn1.py\" rel=\"noreferrer\">https://github.com/ianozsvald/social_media_brand_disambiguator/blob/master/learn1.py</a> for a worked version of this approach</li>\n</ul>\n<p>Things to consider:</p>\n<ul>\n<li>You need a larger dataset. I'm using 2000 labelled tweets (it took me five hours), and as a minimum you want a balanced set with &gt;100 per class (see the overfitting note below)</li>\n<li>Improve the tokeniser (very easy with scikit-learn) to keep # @ in tokens, and maybe add a capitalised-brand detector (as user @user2425429 notes)</li>\n<li>Consider a non-linear classifier (like @oiez's suggestion above) when things get harder. Personally I found LinearSVC to do worse than logistic regression (but that may be due to the high-dimensional feature space that I've yet to reduce).</li>\n<li>A tweet-specific part of speech tagger (in my humble opinion not Standford's as @Neil suggests - it performs poorly on poor Twitter grammar in my experience)</li>\n<li>Once you have lots of tokens you'll probably want to do some dimensionality reduction (I've not tried this yet - see my blog post on LogisticRegression l1 l2 penalisation)</li>\n</ul>\n<p>Re. overfitting. In my dataset with 2000 items I have a 10 minute snapshot from Twitter of 'apple' tweets. About 2/3 of the tweets are for Apple Inc, 1/3 for other-apple-uses. I pull out a balanced subset (about 584 rows I think) of each class and do five-fold cross validation for training.</p>\n<p>Since I only have a 10 minute time-window I have many tweets about the same topic, and this is probably why my classifier does so well relative to existing tools - it will have overfit to the training features without generalising well (whereas the existing commercial tools perform worse on this snapshop, but more reliably across a wider set of data). I'll be expanding my time window to test this as a subsequent piece of work.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Shamelessly jumping on the bandwagon :-)</p>\n<p>Inspired by <a href=\"https://stackoverflow.com/questions/8479058/how-do-i-find-waldo-with-mathematica\">How do I find Waldo with Mathematica</a> and the followup <a href=\"https://stackoverflow.com/questions/8563604/how-to-find-waldo-with-r\">How to find Waldo with R</a>, as a new python user I'd love to see how this could be done. It seems that python would be better suited to this than R, and we don't have to worry about licenses as we would with Mathematica or Matlab.</p>\n<p>In an example like the one below obviously simply using stripes wouldn't work. It would be interesting if a simple rule based approach could be made to work for difficult examples such as this.</p>\n<p><img alt=\"At the beach\" src=\"https://i.sstatic.net/WEGrv.jpg\"/></p>\n<p>I've added the [machine-learning] tag as I believe the correct answer will have to use ML techniques, such as the Restricted Boltzmann Machine (RBM) approach advocated by Gregory Klopper in the original thread. There is some <a href=\"http://imonad.com/rbm/restricted-boltzmann-machine/\" rel=\"noreferrer\">RBM code available in python</a> which might be a good place to start, but obviously training data is needed for that approach. </p>\n<p>At the <a href=\"http://mlsp2009.conwiz.dk/\" rel=\"noreferrer\">2009 IEEE International Workshop on MACHINE LEARNING FOR SIGNAL PROCESSING (MLSP 2009)</a> they ran a <a href=\"http://mlsp2009.conwiz.dk/index.php?id=43\" rel=\"noreferrer\">Data Analysis Competition: Where's Wally?</a>. Training data is provided in matlab format. Note that the links on that website are dead, but the data (along with the source of an approach taken by <a href=\"http://www.eeng.nuim.ie/~semcloone/homepage.html\" rel=\"noreferrer\">Sean McLoone</a> and colleagues can be found <a href=\"https://labanquise.insa-rouen.fr/projects/l1pca/\" rel=\"noreferrer\">here</a> (see SCM link). Seems like one place to start.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's an implementation with <a href=\"http://luispedro.org/software/mahotas\" rel=\"noreferrer\" title=\"mahotas homepage\">mahotas</a></p>\n<pre><code>from pylab import imshow\nimport numpy as np\nimport mahotas\nwally = mahotas.imread('DepartmentStore.jpg')\n\nwfloat = wally.astype(float)\nr,g,b = wfloat.transpose((2,0,1))\n</code></pre>\n<p>Split into red, green, and blue channels. It's better to use floating point arithmetic below, so we convert at the top.</p>\n<pre><code>w = wfloat.mean(2)\n</code></pre>\n<p><code>w</code> is the white channel.</p>\n<pre><code>pattern = np.ones((24,16), float)\nfor i in xrange(2):\n    pattern[i::4] = -1\n</code></pre>\n<p>Build up a pattern of +1,+1,-1,-1 on the vertical axis. This is wally's shirt.</p>\n<pre><code>v = mahotas.convolve(r-w, pattern)\n</code></pre>\n<p>Convolve with red minus white. This will give a strong response where the shirt is.</p>\n<pre><code>mask = (v == v.max())\nmask = mahotas.dilate(mask, np.ones((48,24)))\n</code></pre>\n<p>Look for the maximum value and dilate it to make it visible. Now, we tone down the whole image, except the region or interest:</p>\n<pre><code>wally -= .8*wally * ~mask[:,:,None]\nimshow(wally)\n</code></pre>\n<p>And we get <img alt=\"waldo\" src=\"https://i.sstatic.net/5O2sQ.png\"/>!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could try template matching, and then taking down which produced the highest resemblance, and then using machine learning to narrow it more. That is also very difficult, and with the accuracy of template matching, it may just return every face or face-like image. I am thinking you will need more than just machine learning if you hope to do this consistently.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>maybe you should start with breaking the problem into two smaller ones:</p>\n<ol>\n<li>create an algorithm that separates people from the background.</li>\n<li>train a neural network classifier with as many positive and negative examples as possible.</li>\n</ol>\n<p>those are still two very big problems to tackle...</p>\n<p>BTW, I would choose c++ and open CV, it seems much more suited for this.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to do a transfer learning; for that purpose I want to remove the last two layers of the neural network and add another two layers. This is an example code which also output the same error.</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Input,Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.layers.core import Dropout, Activation\nfrom keras.layers.pooling import GlobalAveragePooling2D\nfrom keras.models import Model\n\nin_img = Input(shape=(3, 32, 32))\nx = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)\nx = Activation('relu', name='relu_conv1')(x)\nx = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\nx = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)\nx = Activation('relu', name='relu_conv2')(x)\nx = GlobalAveragePooling2D()(x)\no = Activation('softmax', name='loss')(x)\nmodel = Model(input=in_img, output=[o])\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n#model.load_weights('model_weights.h5', by_name=True)\nmodel.summary()\n\nmodel.layers.pop()\nmodel.layers.pop()\nmodel.summary()\nmodel.add(MaxPooling2D())\nmodel.add(Activation('sigmoid', name='loss'))\n</code></pre>\n<p>I removed the layer using <code>pop()</code> but when I tried to add its outputting this error</p>\n<pre><code>AttributeError: 'Model' object has no attribute 'add'\n</code></pre>\n<p>I know the most probable reason for the error is improper use of <code>model.add()</code>. what other syntax should I use?</p>\n<p><strong>EDIT:</strong></p>\n<p>I tried to remove/add layers in keras but its not  allowing it to be added after loading external weights.</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Input,Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.layers.core import Dropout, Activation\nfrom keras.layers.pooling import GlobalAveragePooling2D\nfrom keras.models import Model\nin_img = Input(shape=(3, 32, 32))\n\ndef gen_model():\n    in_img = Input(shape=(3, 32, 32))\n    x = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)\n    x = Activation('relu', name='relu_conv1')(x)\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n    x = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)\n    x = Activation('relu', name='relu_conv2')(x)\n    x = GlobalAveragePooling2D()(x)\n    o = Activation('softmax', name='loss')(x)\n    model = Model(input=in_img, output=[o])\n    return model\n\n#parent model\nmodel=gen_model()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\nmodel.summary()\n\n#saving model weights\nmodel.save('model_weights.h5')\n\n#loading weights to second model\nmodel2=gen_model()\nmodel2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\nmodel2.load_weights('model_weights.h5', by_name=True)\n\nmodel2.layers.pop()\nmodel2.layers.pop()\nmodel2.summary()\n\n#editing layers in the second model and saving as third model\nx = MaxPooling2D()(model2.layers[-1].output)\no = Activation('sigmoid', name='loss')(x)\nmodel3 = Model(input=in_img, output=[o])\n</code></pre>\n<p>its showing this error</p>\n<pre><code>RuntimeError: Graph disconnected: cannot obtain value for tensor input_4 at layer \"input_4\". The following previous layers were accessed without issue: []\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can take the <code>output</code> of the last model and create a new model. The lower layers remains the same.</p>\n<pre><code>model.summary()\nmodel.layers.pop()\nmodel.layers.pop()\nmodel.summary()\n\nx = MaxPooling2D()(model.layers[-1].output)\no = Activation('sigmoid', name='loss')(x)\n\nmodel2 = Model(inputs=in_img, outputs=[o])\nmodel2.summary()\n</code></pre>\n<p>Check <a href=\"https://stackoverflow.com/questions/41378461/how-to-use-models-from-keras-applications-for-transfer-learnig/41386444#41386444\">How to use models from keras.applications for transfer learnig?</a></p>\n<p><strong>Update on Edit:</strong></p>\n<p>The new error is because you are trying to create the new model on global <code>in_img</code> which is actually not used in the previous model creation.. there you are actually defining a local <code>in_img</code>. So the global <code>in_img</code> is obviously not connected to the upper layers in the symbolic graph. And it has nothing to do with loading weights.</p>\n<p>To better resolve this problem you should instead use <code>model.input</code> to reference to the input.</p>\n<pre><code>model3 = Model(input=model2.input, output=[o])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Another way to do it</p>\n<pre><code>from keras.models import Model\n\nlayer_name = 'relu_conv2'\nmodel2= Model(inputs=model1.input, outputs=model1.get_layer(layer_name).output)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As of Keras 2.3.1 and TensorFlow 2.0, <code>model.layers.pop()</code> is not working as intended (see issue <a href=\"https://github.com/tensorflow/tensorflow/issues/22479#issuecomment-424577448\" rel=\"noreferrer\">here</a>). They suggested two options to do this.</p>\n<p>One option is to recreate the model and copy the layers. For instance, if you want to remove the last layer and add another one, you can do:</p>\n<pre><code>model = Sequential()\nfor layer in source_model.layers[:-1]: # go through until last layer\n    model.add(layer)\nmodel.add(Dense(3, activation='softmax'))\nmodel.summary()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n</code></pre>\n<p>Another option is to use the functional model:</p>\n<pre><code>predictions = Dense(3, activation='softmax')(source_model.layers[-2].output)\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n</code></pre>\n<p><code>model.layers[-1].output</code> means the last layer's output which is the final output, so in your code, you actually didn't remove any layers, you added another head/path.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to <strong>extract a specific type of information from web pages</strong> in Python. Let's say postal address. It has thousands of forms, but still, it is somehow recognizable. As there is a large number of forms, it would be probably very difficult to write <strong>regular expression</strong> or even something like a <strong>grammar</strong> and to use a <strong>parser generator</strong> for parsing it out.</p>\n<p>So I think the way I should go is <strong>machine learning</strong>. If I understand it well, I should be able to make a sample of data where I will point out what should be the result and then I have something which can learn from this how to recognize the result by itself. This is all I know about machine learning. Maybe I could use some <strong>natural language</strong> processing, but probably not much as all the libraries work with English mostly and I need this for Czech.</p>\n<p>Questions:</p>\n<ol>\n<li>Can I solve this problem easily by machine learning? Is it a good way to go?</li>\n<li>Are there any <strong>simple</strong> examples which would allow me to start? I am machine learning noob and I need something practical for start; closer to my problem is better; simpler is better.</li>\n<li>There are plenty of Python libraries for machine learning. Which one would suit my problem best?</li>\n<li>Lots of such libs have not very easy-to-use docs as they come from scientific environment. Are there any good sources (books, articles, quickstarts) bridging the gap, i.e. focused on newbies who know totally nothing about machine learning? Every docs I open start with terms I don't understand such as <em>network</em>, <em>classification</em>, <em>datasets</em>, etc.</li>\n</ol>\n<p>Update:</p>\n<p>As you all mentioned I should show a piece of data I am trying to get out of the web, here is an example. I am interested in cinema <em>showtimes</em>. They look like this (three of them):</p>\n<pre><code>&lt;div class=\"Datum\" rel=\"d_0\"&gt;27. června – středa, 20.00\n&lt;/div&gt;&lt;input class=\"Datum_cas\" id=\"2012-06-27\" readonly=\"\"&gt;&lt;a href=\"index.php?den=0\" rel=\"0\" class=\"Nazev\"&gt;Zahájení letního kina \n&lt;/a&gt;&lt;div style=\"display: block;\" class=\"ajax_box d-0\"&gt;\n&lt;span class=\"ajax_box Orig_nazev\"&gt;zábava • hudba • film • letní bar\n&lt;/span&gt;\n&lt;span class=\"Tech_info\"&gt;Svět podle Fagi\n&lt;/span&gt;\n&lt;span class=\"Popis\"&gt;Facebooková  komiksová Fagi v podání divadla DNO. Divoké písně, co nezařadíte, ale slušně si na ně zařádíte. Slovní smyčky, co se na nich jde oběsit. Kabaret, improvizace, písně, humor, zběsilost i v srdci.&lt;br&gt;Koncert Tres Quatros Kvintet. Instrumentální muzika s pevným funkovým groovem, jazzovými standardy a neodmyslitelnými improvizacemi.\n&lt;/span&gt;\n&lt;input class=\"Datum_cas\" id=\"ajax_0\" type=\"text\"&gt;\n&lt;/div&gt;\n\n&lt;div class=\"Datum\" rel=\"d_1\"&gt;27. června – středa, 21.30\n&lt;/div&gt;&lt;input class=\"Datum_cas\" id=\"2012-06-27\" readonly=\"\"&gt;&lt;a href=\"index.php?den=1\" rel=\"1\" class=\"Nazev\"&gt;Soul Kitchen\n&lt;/a&gt;&lt;div style=\"display: block;\" class=\"ajax_box d-1\"&gt;\n&lt;span class=\"ajax_box Orig_nazev\"&gt;Soul Kitchen\n&lt;/span&gt;\n&lt;span class=\"Tech_info\"&gt;Komedie, Německo, 2009, 99 min., čes. a angl. tit.\n&lt;/span&gt;\n&lt;span class=\"Rezie\"&gt;REŽIE: Fatih Akin \n&lt;/span&gt;\n&lt;span class=\"Hraji\"&gt;HRAJÍ: Adam Bousdoukos, Moritz Bleibtreu, Birol Ünel, Wotan Wilke Möhring\n&lt;/span&gt;\n&lt;span class=\"Popis\"&gt;Poslední film miláčka publika Fatiho Akina, je turbulentním vyznáním lásky multikulturnímu Hamburku. S humorem zde Akin vykresluje příběh Řeka žijícího v Německu, který z malého bufetu vytvoří originální restauraci, jež se brzy stane oblíbenou hudební scénou. \"Soul Kitchen\" je skvělá komedie o přátelství, lásce, rozchodu a boji o domov, který je třeba v dnešním nevypočitatelném světě chránit víc než kdykoliv předtím. Zvláštní cena poroty na festivalu v Benátkách\n&lt;/span&gt;\n&lt;input class=\"Datum_cas\" id=\"ajax_1\" type=\"text\"&gt;\n&lt;/div&gt;\n\n&lt;div class=\"Datum\" rel=\"d_2\"&gt;28. června – čtvrtek, 21:30\n&lt;/div&gt;&lt;input class=\"Datum_cas\" id=\"2012-06-28\" readonly=\"\"&gt;&lt;a href=\"index.php?den=2\" rel=\"2\" class=\"Nazev\"&gt;Rodina je základ státu\n&lt;/a&gt;&lt;div style=\"display: block;\" class=\"ajax_box d-2\"&gt;\n&lt;span class=\"Tech_info\"&gt;Drama, Česko, 2011, 103 min.\n&lt;/span&gt;\n&lt;span class=\"Rezie\"&gt;REŽIE: Robert Sedláček\n&lt;/span&gt;\n&lt;span class=\"Hraji\"&gt;HRAJÍ: Igor Chmela, Eva Vrbková, Martin Finger, Monika A. Fingerová, Simona Babčáková, Jiří Vyorálek, Jan Fišar, Jan Budař, Marek Taclík, Marek Daniel\n&lt;/span&gt;\n&lt;span class=\"Popis\"&gt;Když vám hoří půda pod nohama, není nad rodinný výlet. Bývalý učitel dějepisu, který dosáhl vysokého manažerského postu ve významném finančním ústavu, si řadu let spokojeně žije společně se svou rodinou v luxusní vile na okraji Prahy. Bezstarostný život ale netrvá věčně a na povrch začnou vyplouvat machinace s penězi klientů týkající se celého vedení banky. Libor se následně ocitá pod dohledem policejních vyšetřovatelů, kteří mu začnou tvrdě šlapat na paty. Snaží se uniknout před hrozícím vězením a oddálit osvětlení celé situace své nic netušící manželce. Rozhodne se tak pro netradiční útěk, kdy pod záminkou společné dovolené odveze celou rodinu na jižní Moravu…  Rodinný výlet nebo zoufalý úprk před spravedlností? Igor Chmela, Eva Vrbková a Simona Babčáková v rodinném dramatu a neobyčejné road-movie inspirované skutečností.\n&lt;/span&gt;\n</code></pre>\n<p>Or like this:</p>\n<pre><code>&lt;strong&gt;POSEL&amp;nbsp;&amp;nbsp; 18.10.-22.10 v 18:30 &lt;/strong&gt;&lt;br&gt;Drama. ČR/90´. Režie: Vladimír Michálek Hrají: Matěj Hádek, Eva Leinbergerová, Jiří Vyorávek&lt;br&gt;Třicátník Petr miluje kolo a své vášni podřizuje celý svůj život. Neplánuje, neplatí účty, neřeší nic, co může&lt;br&gt;počkat  do zítra. Budování společného života s přételkyní je mu proti srsti  stejně jako dělat kariéru. Aby mohl jezdit na kole, raději pracuje jako  poslíček. Jeho život je neřízená střela, ve které neplatí žádná  pravidla. Ale problémy se na sebe na kupí a je stále těžší před nimi  ujet …&lt;br&gt; &lt;br&gt;\n\n&lt;strong&gt;VE STÍNU&amp;nbsp; 18.10.-24.10. ve 20:30 a 20.10.-22.10. též v 16:15&lt;/strong&gt;&lt;br&gt;Krimi. ČR/98´. Režie: D.Vondříček Hrají: I.Trojan, S.Koch, S.Norisová, J.Štěpnička, M.Taclík&lt;br&gt;Kapitán  Hakl (Ivan Trojan) vyšetřuje krádež v klenotnictví. Z běžné vloupačky  se ale vlivem zákulisních intrik tajné policie začíná stávat politická  kauza. Z nařízení Státní bezpečnosti přebírá Haklovo vyšetřování major  Zenke (Sebastian Koch), policejní specialista z NDR, pod jehož vedením  se vyšetřování ubírá jiným směrem, než Haklovi napovídá instinkt  zkušeného kriminalisty. Na vlastní pěst pokračuje ve vyšetřování. Může  jediný spravedlivý obstát v boji s dobře propojenou sítí komunistické  policie?&amp;nbsp; Protivník je silný a Hakl se brzy přesvědčuje, že věřit nelze  nikomu a ničemu. Každý má svůj stín minulosti, své slabé místo, které  dokáže z obětí udělat viníky a z viníků hrdiny. &lt;br&gt;&lt;br&gt;\n\n&lt;strong&gt;ASTERIX A OBELIX VE SLUŽBÁCH JEJÍHO VELIČENSTVA&amp;nbsp; ve 3D&amp;nbsp;&amp;nbsp;&amp;nbsp; 20.10.-21.10. ve 13:45 &lt;/strong&gt;&lt;br&gt;Dobrodružná fantazy. Fr./124´. ČESKÝ DABING. Režie: Laurent Tirard&lt;br&gt;Hrají: Gérard Depardieu, Edouard Baer, Fabrice Luchini&lt;br&gt;Pod  vedením Julia Caesara napadly proslulé římské legie Británii. Jedné  malé vesničce se však daří statečně odolávat, ale každým dnem je slabší a  slabší. Britská královna proto vyslala svého věrného důstojníka  Anticlimaxe, aby vyhledal pomoc u Galů v druhé malinké vesničce ve  Francii vyhlášené svým důmyslným bojem proti Římanům… Když Anticlimax  popsal zoufalou situaci svých lidí, Galové mu darovali barel svého  kouzelného lektvaru a Astérix a Obélix jsou pověřeni doprovodit ho domů.  Jakmile dorazí do Británie, Anticlimax jim představí místní zvyky ve  vší parádě a všichni to pořádně roztočí! Vytočený Caesar se však  rozhodne naverbovat Normanďany, hrůzu nahánějící bojovníky Severu, aby  jednou provždy skoncovali s Brity. &lt;br&gt;&lt;br&gt;\n</code></pre>\n<p>Or it can look like anything similar to this. No special rules in HTML markup, no special rules in order, etc.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First, your task fits into the <strong>information extraction</strong> area of research. There are mainly 2 levels of complexity for this task:</p>\n<ul>\n<li>extract from a given html page or a website with the fixed template\n(like Amazon). In this case the best way is to look at the HTML code\nof the pages and craft the corresponding XPath or DOM selectors to\nget to the right info. The disadvantage with this approach is that it\nis not generalizable to new websites, since you have to do it for\neach website one by one.</li>\n<li>create a model that extracts same\ninformation from many websites within one domain (having an\nassumption that there is some inherent regularity in the way web\ndesigners present the corresponding attribute, like zip or phone or whatever else). In this   case you should create some features (to use ML approach and let IE algorithm to \"understand the content of pages\"). The most common features are: DOM path, the format of the value (attribute) to be extracted, layout (like bold, italic and etc.), and surrounding context words. You label some values (you need at least 100-300 pages depending on domain to do it with some sort of reasonable quality). Then you train a model on the labelled pages. There is also an alternative to it - to do IE in unsupervised manner (leveraging the idea of information regularity across pages). In this case you/your algorith tries to find repetitive patterns across pages (without labelling) and consider as valid those, that are the most frequent. </li>\n</ul>\n<p>The most challenging part overall will be to work with DOM tree and generate the right features. Also data labelling in the right way is a tedious task. For ML models - have a look at <strong>CRF, 2DCRF, semi-markov CRF</strong>. </p>\n<p>And finally, this is in the general case a cutting edge in IE research and not a hack that you can do it a few evenings. </p>\n<p>p.s. also I think NLTK will not be very helpful - it is an NLP, not Web-IE library.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>tl;dr: The problem <em>might</em> solvable using ML, but it's not straightforward if you're new to the topic</p>\n<hr/>\n<p>There's a lot of machine learning libraries for python:</p>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/\" rel=\"noreferrer\">Scikit-learn</a> is very popular general-purpose for beginners and great for simple problems with smallish datasets. </li>\n<li><a href=\"http://nltk.org/\" rel=\"noreferrer\">Natural Language Toolkit</a> has implementations for lots of algorithms, many of which are language agnostic (say, n-grams)</li>\n<li><a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\">Gensim</a> is great for text topic modelling</li>\n<li><a href=\"http://docs.opencv.org/modules/ml/doc/ml.html\" rel=\"noreferrer\">Opencv</a> implements some common algorithms (but is usually used for images)</li>\n<li><a href=\"https://spacy.io/\" rel=\"noreferrer\">Spacy</a> and <a href=\"https://github.com/huggingface/transformers\" rel=\"noreferrer\">Transformers</a> implement modern (state-of-the-art, as of 2020) text NLU (Natural Language Understanding) techniques, but require more familiarity with the complex techniques</li>\n</ul>\n<p>Usually you pick a library that suits your problem and the technique you want to use.</p>\n<p>Machine learning is a <strong>very</strong> vast area. Just for the supervised-learning classification subproblem, and considering only \"simple\" classifiers, there's Naive Bayes, KNN, Decision Trees, Support Vector Machines, feed-forward neural networks... The list goes on and on. This is why, as you say, there are no \"quickstarts\" or tutorials for machine learning in general. My advice here is, firstly, to <strong>understand the basic ML terminology</strong>, secondly, <strong>understand a subproblem</strong> (I'd advise classification within supervised-learning), and thirdly, <strong>study a simple algorithm that solves this subproblem</strong> (<a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" rel=\"noreferrer\">KNN</a> relies on highschool-level math).</p>\n<p>About your problem in particular: it seems you want detect the existence of a piece of data (postal code) inside an huge dataset (text). A classic classification algorithm expects a relatively small <a href=\"https://en.wikipedia.org/wiki/Feature_vector\" rel=\"noreferrer\">feature vector</a>. To obtain that, you will need to do what's called a <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction\" rel=\"noreferrer\">dimensionality reduction</a>: this means, isolate the parts that <strong>look</strong> like potential postal codes. Only then does the classification algorithm classify it (as \"postal code\" or \"not postal code\", for example).</p>\n<p>Thus, <strong>you need to find a way to isolate potential matches before you even think about using ML to approach this problem</strong>. This will most certainly entail natural language processing, as you said, if you don't or can't use regex or parsing.</p>\n<p>More advanced models in NLU could potentially parse your whole text, but they might require very large amounts of pre-classified data, and explaining them is outside of the scope of this question. The libraries I've mentioned earlier are a good start.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As per i know there are two ways to do this task using machine learning approach.</p>\n<p>1.Using computer vision to train the model and then extract the content based on your use case, this has already been implemented by <a href=\"https://www.diffbot.com/\" rel=\"noreferrer\">diffbot.com.</a>\nand they have not open sourced their solution.</p>\n<p>2.The other way to go around this problem is using supervised machine learning to train binary classifier to classify content vs boilerplate and then extract the content. This approach is used in <a href=\"https://moz.com/devblog/dragnet-content-extraction-from-diverse-feature-sets/\" rel=\"noreferrer\">dragnet.</a>\nand other research around this area. You can have a look at <a href=\"https://medium.com/skim-technologies/a-benchmark-comparison-of-extraction-from-html-pages-98d7c1229f51\" rel=\"noreferrer\">benchmark comparison</a> among different content extraction techniques.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to read an image from an electrocardiography and detect each one of the main waves in it (P wave, QRS complex and T wave). I can read the image and get a vector (like <code>(4.2; 4.4; 4.9; 4.7; ...)</code>). I need an algorithm that can walk through this vector and detect when each of these waves start and end. An example:</p>\n<p><img alt=\"alt text\" src=\"https://i.sstatic.net/U4t3t.jpg\"/></p>\n<p>Would be easy if they always had the same size, or if I knew how many waves the ECG has in advance. Given the wave:</p>\n<p><img alt=\"alt text\" src=\"https://i.sstatic.net/BHOaG.jpg\"/></p>\n<p>I extract the vector:</p>\n<pre><code>[0; 0; 20; 20; 20; 19; 18; 17; 17; 17; 17; 17; 16; 16; 16; 16; 16; 16; 16; 17; 17; 18; 19; 20; 21; 22; 23; 23; 23; 25; 25; 23; 22; 20; 19; 17; 16; 16; 14; 13; 14; 13; 13; 12; 12; 12; 12; 12; 11; 11; 10; 12; 16; 22; 31; 38; 45; 51; 47; 41; 33; 26; 21; 17; 17; 16; 16; 15; 16; 17; 17; 18; 18; 17; 18; 18; 18; 18; 18; 18; 18; 17; 17; 18; 19; 18; 18; 19; 19; 19; 19; 20; 20; 19; 20; 22; 24; 24; 25; 26; 27; 28; 29; 30; 31; 31; 31; 32; 32; 32; 31; 29; 28; 26; 24; 22; 20; 20; 19; 18; 18; 17; 17; 16; 16; 15; 15; 16; 15; 15; 15; 15; 15; 15; 15; 15; 15; 14; 15; 16; 16; 16; 16; 16; 16; 16; 16; 16; 15; 16; 15; 15; 15; 16; 16; 16; 16; 16; 16; 16; 16; 15; 16; 16; 16; 16; 16; 15; 15; 15; 15; 15; 16; 16; 17; 18; 18; 19; 19; 19; 20; 21; 22; 22; 22; 22; 21; 20; 18; 17; 17; 15; 15; 14; 14; 13; 13; 14; 13; 13; 13; 12; 12; 12; 12; 13; 18; 23; 30; 38; 47; 51; 44; 39; 31; 24; 18; 16; 15; 15; 15; 15; 15; 15; 16; 16; 16; 17; 16; 16; 17; 17; 16; 17; 17; 17; 17; 18; 18; 18; 18; 19; 19; 20; 20; 20; 20; 21; 22; 22; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 32; 33; 33; 33; 32; 30; 28; 26; 24; 23; 23; 22; 20; 19; 19; 18; 17; 17; 18; 17; 18; 18; 17; 18; 17; 18; 18; 17; 17; 17; 17; 16; 17; 17; 17; 18; 18; 17; 17; 18; 18; 18; 19; 18; 18; 17; 18; 18; 17; 17; 17; 17; 17; 18; 17; 17; 18; 17; 17; 17; 17; 17; 17; 17; 18; 17; 17; 18; 18; 18; 20; 20; 21; 21; 22; 23; 24; 23; 23; 21; 21; 20; 18; 18; 17; 16; 14; 13; 13; 13; 13; 13; 13; 13; 13; 13; 12; 12; 12; 16; 19; 28; 36; 47; 51; 46; 40; 32; 24; 20; 18; 16; 16; 16; 16; 15; 16; 16; 16; 17; 17; 17; 18; 17; 17; 18; 18; 18; 18; 19; 18; 18; 19; 20; 20; 20; 20; 20; 21; 21; 22; 22; 23; 25; 26; 27; 29; 29; 30; 31; 32; 33; 33; 33; 34; 35; 35; 35; 0; 0; 0; 0;]\n</code></pre>\n<p>I would like to detect, for example:</p>\n<ul>\n<li>P wave in <code>[19 - 37]</code>.</li>\n<li>QRS complex in <code>[51 - 64]</code>.</li>\n<li>etc.</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The first thing that <em>I</em> would do is <strong>see what is already out there</strong>. Indeed, this specific problem has already been heavily researched. Here is a brief overview of some really simple methods: <a href=\"http://www.physik.uni-freiburg.de/%7Eseverin/A_comparison_of_the_noise_sensitivity_of_nine_QRS_detection_Algorithms.pdf\" rel=\"noreferrer\">link</a>.</p>\n<p>I must respond to another answer, as well. I do research in signal processing and music information retrieval. On the surface, this problem does appear similar to onset detection, but the problem context is not the same. This type of biological signal processing, i.e., detection of the P, QRS, and T phases, can exploit knowledge of <em>specific time-domain characteristics</em> of each of these waveforms. Onset detection in MIR doesn't, really. (Not reliably, at least.)</p>\n<p>One approach that would work well for QRS detection (but not necessarily for note onset detection) is dynamic time warping. When time-domain characteristics remain invariant, DTW can work remarkably well. Here is a short IEEE paper that uses DTW for this problem: <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.6782&amp;rep=rep1&amp;type=pdf\" rel=\"noreferrer\">link</a>.</p>\n<p>This is a nice IEEE magazine article that compares many methods: <a href=\"http://www.sm.luth.se/csee/courses/sms/046/2004/QRS_tutorial.pdf\" rel=\"noreferrer\">link</a>. You'll see that many common signal processing models have been tried. Skim the paper, and try one that you understand at a basic level.</p>\n<p>EDIT: After browsing these articles, a wavelet-based approach seems most intuitive to me. DTW will work well, too, and there exist DTW modules out there, but the wavelet approach seems best to me. Someone else answered by exploiting derivatives of the signal. My first link examines methods from before 1990 that do that, but I suspect that they are not as robust as more modern methods.</p>\n<p>EDIT: I'll try to give a simple solution when I get the chance, but the reason <em>why</em> I think wavelets are suited here are because they are useful at parameterizing a wide variety of shapes regardless of <em>time or amplitude scaling</em>. In other words, if you have a signal with the same repeated temporal shape but at varying time scales and amplitudes, wavelet analysis can still recognize these shapes as being similar (roughly speaking). Also note that I am sort of lumping filter banks into this category. Similar things.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A piece of this puzzle is \"<a href=\"https://web.archive.org/web/20080622154745/http://www.music-ir.org/mirex2006/index.php/Audio_Onset_Detection\" rel=\"nofollow noreferrer\">onset detection</a>\" and a number of complex algorithms have been written to solve this problem.  Here is more information on <a href=\"http://en.wikipedia.org/wiki/Onset_%28audio%29\" rel=\"nofollow noreferrer\">onsets</a>.</p>\n<p>The next piece is a <a href=\"http://en.wikipedia.org/wiki/Hamming_distance\" rel=\"nofollow noreferrer\">Hamming Distance</a>. This algorithms allow you to make fuzzy comparisons,  the input is 2 arrays and the output is an integer \"distance\"  or difference between the 2 data sets. The smaller the number,  the more alike the 2 are.  This is very close to what you need,  but its not exact.  I went ahead and made some modifications to the Hamming Distance algorithm to calculate a new distance,  it probably has a name but i don't know what it is.  Basically it adds up the absolute distance between each element in the array and returns the total. Here is the code for it in python. </p>\n<pre><code>import math\n\ndef absolute_distance(a1, a2, length):\n       total_distance=0\n       for x in range(0,length):\n               total_distance+=math.fabs(a1[x]-a2[x])\n       return total_distance\n\nprint(absolute_distance([1,3,9,10],[1,3,8,11],4))\n</code></pre>\n<p>This script outputs 2,  which is the distance between these 2 arrays. </p>\n<p>Now for putting together these pieces.   You could use Onset detection to find the beginning of all waves in the data set.   You can then loop though these location comparing each wave with a sample P-Wave.  If you hit a QRS Complex the distance is going to be the largest.  If you hit another P-Wave the number isn't going to be zero,  but its going to be much smaller.   The distance between any P-Wave and any T-Wave is going to be pretty small,  HOWEVER this isn't a problem if you make the following assumption:</p>\n<p><code>The distance between any p-wave and any other p-wave will be smaller than the distance between any p-wave and any t-wave.</code> </p>\n<p>The series looks something like this: pQtpQtpQt...  The p-wave and t-wave is right next to each other,  but because this sequence is predictable it will be easier to read.</p>\n<p>On a side not,  there is probably a calculus based solution to this problem.  However in my mind curve fitting and integrals make this problem more of a mess.   The distance function I wrote will find the <em>area difference</em> which is very similar subtracting the integral of both curves.  </p>\n<p>It maybe possible to sacrifice the onset calculations in favor of iterating by 1 point at a time and thus performing O(n) distance calculations,  where n is the number of points in the graph.  If you had a list of all of these distance calculations and knew there where 50 pQt sequences then you would know the 50 shortest distances that <em>do not overlap</em> where all locations of p-waves.   <strong>Bingo!</strong>  how is that for simplicity? However the trade off is loss of efficiency due to an increased number of distance calculations.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"http://en.wikipedia.org/wiki/Cross-correlation\" rel=\"noreferrer\">cross-correlation</a>. Take a model sample of each pattern and correlate them with the signal. You will get peaks where the correlation is high. I would expect good results with this technique extracting qrs and t waves. After that, you can extract p waves by looking for peaks on the correlation signal that are before qrs.</p>\n<p>Cross-correlation is a pretty easy to implement algorithm. Basically:</p>\n<pre><code>x is array with your signal of length Lx\ny is an array containing a sample of the signal you want to recognize of length Ly\nr is the resulting correlation\n\nfor (i=0; i&lt;Lx - Ly; i++){\n  r[i] = 0;\n  for (j=0; j&lt;Ly ; j++){\n    r[i] += x[i+j]*y[j];\n  }\n}\n</code></pre>\n<p>And look for peaks in r (values over a threshold, for instance)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-08-17 14:20:06Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/9480605/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I am using LibSVM to classify some documents. The documents seem to be a bit difficult to classify as the final results show. However, I have noticed something while training my models. and that is: If my training set is for example 1000 around 800 of them are selected as support vectors.\nI have looked everywhere to find if this is a good thing or bad. I mean is there a relation between the number of support vectors and the classifiers performance?\nI have read this <a href=\"https://stackoverflow.com/questions/5731169/svm-all-my-training-data-is-getting-selected-as-support-vectors-why\">previous post</a> but I am performing a parameter selection and also I am sure that the attributes in the feature vectors are all ordered.\nI just need to know the relation.\nThanks.\np.s: I use a linear kernel.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Support Vector Machines are an optimization problem. They are attempting to find a hyperplane that divides the two classes with the largest margin.  The support vectors are the points which fall within this margin.  It's easiest to understand if you build it up from simple to more complex.</p>\n<p><strong>Hard Margin Linear SVM</strong></p>\n<p>In a training set where the data is linearly separable, and you are using a hard margin (no slack allowed), the support vectors are the points which lie along the supporting hyperplanes (the hyperplanes parallel to the dividing hyperplane at the edges of the margin)</p>\n<p><img alt=\"Hard-Margin SVM\" src=\"https://i.sstatic.net/qt3CZ.png\"/></p>\n<p>All of the support vectors lie exactly on the margin.  Regardless of the number of dimensions or size of data set, the number of support vectors could be as little as 2.</p>\n<p><strong>Soft-Margin Linear SVM</strong></p>\n<p>But what if our dataset isn't linearly separable? We introduce soft margin SVM. We no longer require that our datapoints lie outside the margin, we allow some amount of them to stray over the line into the margin. We use the slack parameter C to control this. (nu in nu-SVM) This gives us a wider margin and greater error on the training dataset, but improves generalization and/or allows us to find a linear separation of data that is not linearly separable.</p>\n<p><img alt=\"Soft-margin Linear SVM\" src=\"https://i.sstatic.net/npEOk.png\"/></p>\n<p>Now, the number of support vectors depends on how much slack we allow and the distribution of the data. If we allow a large amount of slack, we will have a large number of support vectors.  If we allow very little slack, we will have very few support vectors. The accuracy depends on finding the right level of slack for the data being analyzed. Some data it will not be possible to get a high level of accuracy, we must simply find the best fit we can.</p>\n<p><strong>Non-Linear SVM</strong></p>\n<p>This brings us to non-linear SVM. We are still trying to linearly divide the data, but we are now trying to do it in a higher dimensional space.  This is done via a kernel function, which of course has its own set of parameters. When we translate this back to the original feature space, the result is non-linear:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/1gvce.png\"/></p>\n<p>Now, the number of support vectors still depends on how much slack we allow, but it also depends on the complexity of our model. Each twist and turn in the final model in our input space requires one or more support vectors to define.  Ultimately, the output of an SVM is the support vectors and an alpha, which in essence is defining how much influence that specific support vector has on the final decision.</p>\n<p>Here, accuracy depends on the trade-off between a high-complexity model which may over-fit the data and a large-margin which will incorrectly classify some of the training data in the interest of better generalization.  The number of support vectors can range from very few to every single data point if you completely over-fit your data.  This tradeoff is controlled via C and through the choice of kernel and kernel parameters.</p>\n<p>I assume when you said performance you were referring to accuracy, but I thought I would also speak to performance in terms of computational complexity.  In order to test a data point using an SVM model, you need to compute the dot product of each support vector with the test point. Therefore the computational complexity of the model is linear in the number of support vectors.  Fewer support vectors means faster classification of test points.</p>\n<p>A good resource:\n<a href=\"http://research.microsoft.com/pubs/67119/svmtutorial.pdf\" rel=\"noreferrer\" title=\"A Tutorial on Support Vector Machines for Pattern Recognition\">A Tutorial on Support Vector Machines for Pattern Recognition</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>800 out of 1000 basically tells you that the SVM needs to use almost every single training sample to encode the training set.  That basically tells you that there isn't much regularity in your data.  </p>\n<p>Sounds like you have major issues with not enough training data.  Also, maybe think about some specific features that separate this data better.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Both <strong>number of samples</strong> and <strong>number of attributes</strong> <em>may</em> influence the number of support vectors, making model more complex. I believe you use words or even ngrams as attributes, so there are quite many of them, and natural language models are very complex themselves. So, 800 support vectors of 1000 samples seem to be ok. (Also pay attention to @karenu's comments about C/nu parameters that also have large effect on SVs number).</p>\n<p>To get intuition about this recall SVM main idea. SVM works in a <strong>multidimensional feature space</strong> and tries to find <strong>hyperplane</strong> that separates all given samples. If you have a lot of samples and only 2 features (2 dimensions), the data and hyperplane may look like this: </p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/MMCes.png\"/></p>\n<p>Here there are only 3 support vectors, all the others are behind them and thus don't play any role. Note, that these support vectors are defined by only 2 coordinates.</p>\n<p>Now imagine that you have 3 dimensional space and thus support vectors are defined by 3 coordinates. </p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zeRTm.png\"/></p>\n<p>This means that there's one more parameter (coordinate) to be adjusted, and this adjustment may need more samples to find optimal hyperplane. In other words, in worst case SVM finds only 1 hyperplane coordinate per sample. </p>\n<p>When the data is well-structured (i.e. holds patterns quite well) only several support vectors may be needed - all the others will stay behind those. But text is very, very bad structured data. SVM does its best, trying to fit sample as well as possible, and thus takes as support vectors even more samples than drops. With increasing number of samples this \"anomaly\" is reduced (more insignificant samples appear), but absolute number of support vectors stays very high. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to separate my data into train and test set, should I apply normalization over data before or after the split? Does it make any difference while building predictive model?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You first need to split the data into training and test set (validation set could be useful too).</p>\n<p>Don't forget that testing data points represent real-world data.\nFeature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance).</p>\n<p>Therefore, you should perform feature normalisation over the training data.  Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.</p>\n<p><strong>For a more comprehensive read, you can read my article <a href=\"https://medium.com/@giorgosmyrianthous/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b\" rel=\"noreferrer\">Feature Scaling and Normalisation in a nutshell</a></strong></p>\n<hr/>\n<p>As an example, assuming we have the following data:</p>\n<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; \n&gt;&gt;&gt; X, y = np.arange(10).reshape((5, 2)), range(5)\n</code></pre>\n<p>where <code>X</code> represents our features:</p>\n<pre><code>&gt;&gt;&gt; X\n[[0 1]\n [2 3]\n [4 5]\n [6 7]\n [8 9]]\n</code></pre>\n<p>and <code>Y</code> contains the corresponding label</p>\n<pre><code>&gt;&gt;&gt; list(y)\n&gt;&gt;&gt; [0, 1, 2, 3, 4]\n</code></pre>\n<hr/>\n<p><strong>Step 1: Create training/testing sets</strong></p>\n<pre><code>&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n&gt;&gt;&gt; X_train\n[[4 5]\n [0 1]\n [6 7]]\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_test\n[[2 3]\n [8 9]]\n&gt;&gt;&gt;\n&gt;&gt;&gt; y_train\n[2, 0, 3]\n&gt;&gt;&gt;\n&gt;&gt;&gt; y_test\n[1, 4]\n</code></pre>\n<p><strong>Step 2: Normalise training data</strong></p>\n<pre><code>&gt;&gt;&gt; from sklearn import preprocessing\n&gt;&gt;&gt; \n&gt;&gt;&gt; normalizer = preprocessing.Normalizer()\n&gt;&gt;&gt; normalized_train_X = normalizer.fit_transform(X_train)\n&gt;&gt;&gt; normalized_train_X\narray([[0.62469505, 0.78086881],\n       [0.        , 1.        ],\n       [0.65079137, 0.7592566 ]])\n</code></pre>\n<p><strong>Step 3: Normalize testing data</strong></p>\n<pre><code>&gt;&gt;&gt; normalized_test_X = normalizer.transform(X_test)\n&gt;&gt;&gt; normalized_test_X\narray([[0.5547002 , 0.83205029],\n       [0.66436384, 0.74740932]])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the specific setting of a train/test split, we need to distinguish between two transformations:</p>\n<ol>\n<li>transformations that change the value of an observation (row) according to information about a feature (column) and</li>\n<li>transformations that change the value of an observation according to information <em>about that observation <strong>alone</strong></em>.</li>\n</ol>\n<p>Two common examples of (1) are mean-centering (subtracting the mean of the feature) or scaling to unit variance (dividing by the <em>standard deviation</em>). Subtracting the mean and dividing by the <em>standard deviation</em> is a common transformation. In <code>sklearn</code>, it is implemented in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" rel=\"noreferrer\"><code>sklearn.preprocessing.StandardScaler</code></a>. <strong>Importantly, this is not the same as <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\" rel=\"noreferrer\"><code>Normalizer</code></a>.</strong> See below for exhaustive detail.</p>\n<p>An example of (2) is transforming a feature by taking the logarithm, or raising each value to a power (e.g. squaring).</p>\n<p>Transformations of the first type are best applied to the training data, with the centering and scaling values retained and applied to the test data afterwards. This is because using information about the test set to train the model may bias model comparison metrics to be overly optimistic. This can result in over-fitting &amp; selection of a bogus model.</p>\n<p>Transformations of the second type can be applied without regard to train/test splits, because the modified value of each observation depends only on the data about the observation itself, and not on any other data or observation(s).</p>\n<hr/>\n<p>This question has garnered some misleading answers. The rest of this answer is dedicated to showing how and why they are misleading.</p>\n<p>The term \"normalization\" is ambiguous, and different authors and disciplines will use the term \"normalization\" in different ways. In the absence of a specific articulation of what \"normalization\" means, I think it's best to approach the question in the most general sense possible.</p>\n<p>In this view, the question is not about <code>sklearn.preprocessing.Normalizer</code> specifically. Indeed, the <code>Normalizer</code> class is not mentioned in the question. For that matter, no software, programming language or library is mentioned, either. Moreover, even if the intent is to ask about <code>Normalizer</code>, the answers are still misleading because they mischaracterize what <code>Normalizer</code> does.</p>\n<p>Even within the same library, the terminology can be inconsistent. For example, PyTorch implements normalize <code>torchvision.transforms.Normalize</code> and <code>torch.nn.functional.normalize</code>. One of these can be used to create output tensors with mean 0 and standard deviation 1, while the other creates outputs that have a norm of 1.</p>\n<hr/>\n<h3>What the <code>Normalizer</code> Class Does</h3>\n<p>The <code>Normalizer</code> class is an example of (2) because it rescales each observation (row) <em>individually</em> so that the sum-of-squares is 1 for every row. (In the corner-case that a row has sum-of-squares equal to 0, no rescaling is done.) The first sentence of <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\" rel=\"noreferrer\">the documentation for the <code>Normalizer</code></a> says</p>\n<blockquote>\n<p>Normalize samples individually to unit norm.</p>\n</blockquote>\n<p>This simple test code validates this understanding:</p>\n<pre><code>X = np.arange(10).reshape((5, 2))\nnormalizer = preprocessing.Normalizer()\nnormalized_all_X = normalizer.transform(X)\nsum_of_squares = np.square(normalized_all_X).sum(1)\nprint(np.allclose(sum_of_squares,np.ones_like(sum_of_squares)))\n</code></pre>\n<p>This prints <code>True</code> because the result is an array of 1s, as described in the documentation.</p>\n<p>The normalizer implements <code>fit</code>, <code>transform</code> and <code>fit_transform</code> methods even though some of these are just \"pass-through\" methods. This is so that there is a consistent interface across preprocessing methods, <strong>not</strong> because the methods' behaviors needs to distinguish between different data partitions.</p>\n<hr/>\n<h3>Misleading Presentation 1</h3>\n<p><strong>The <code>Normalizer</code> class does not subtract the column means</strong></p>\n<p><a href=\"https://stackoverflow.com/a/49444783/2482661\">Another answer</a> writes:</p>\n<blockquote>\n<p>Don't forget that testing data points represent real-world data. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance.</p>\n</blockquote>\n<p>Ok, so let's try this out. Using the code snippet from the answer, we have</p>\n<pre><code>X = np.arange(10).reshape((5, 2))\n\nX_train = X[:3]\nX_test = X[3:]\n\nnormalizer = preprocessing.Normalizer()\nnormalized_train_X = normalizer.fit_transform(X_train)\ncolumn_means_train_X = normalized_train_X.mean(0)\n</code></pre>\n<p>This is the value of <code>column_means_train_X</code>. It is not zero!</p>\n<pre><code>[0.42516214 0.84670847]\n</code></pre>\n<p>If the column means had been subtracted from the columns, then the centered column means would be 0.0. (This is simple to prove. The sum of <code>n</code> numbers <code>x=[x1,x2,x3,...,xn]</code> is <code>S</code>. The mean of those numbers is <code>S / n</code>. Then we have <code>sum(x - S/n) = S - n * (S / n) = 0</code>.)</p>\n<p>We can write similar code to show that the columns have not been divided by the <em>variance</em>. (Neither have the columns been divided by the <em>standard deviation</em>, which would be the more usual choice).</p>\n<h3>Misleading Presentation 2</h3>\n<p><strong>Applying the <code>Normalizer</code> class to the whole data set does not change the result.</strong></p>\n<blockquote>\n<p>If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance).</p>\n</blockquote>\n<p>This claim is true as far as it goes, but it has absolutely no bearing on the <code>Normalizer</code> class. Indeed, <strong>Giorgos Myrianthous's chosen example is actually immune to the effect that they are describing.</strong></p>\n<p>If the <code>Normalizer</code> class did involve the means of the features, then we would expect that the normalize results will change depending on which of our data are included in the training set.</p>\n<p>For example, the sample mean is a weighted sum of every observation in the sample. If we were computing column means and subtracting them, the results of applying this to all of the data would differ from applying it to only the training data subset. But we've already demonstrated that <code>Normalizer</code> doesn't subtract column means.</p>\n<p>Furthermore, these tests show that applying <code>Normalizer</code> to all of the data or just some of the data makes no difference for the results.</p>\n<p>If we apply this method separately, we have</p>\n<pre><code>[[0.         1.        ]\n [0.5547002  0.83205029]\n [0.62469505 0.78086881]]\n\n[[0.65079137 0.7592566 ]\n [0.66436384 0.74740932]]\n</code></pre>\n<p>And if we apply it together, we have</p>\n<pre><code>[[0.         1.        ]\n [0.5547002  0.83205029]\n [0.62469505 0.78086881]\n [0.65079137 0.7592566 ]\n [0.66436384 0.74740932]]\n</code></pre>\n<p>where the only difference is that we have 2 arrays in the first case, due to partitioning. Let's just double-check that the combined arrays are the same:</p>\n<pre><code>normalized_train_X = normalizer.fit_transform(X_train)\nnormalized_test_X = normalizer.transform(X_test)\nnormalized_all_X = normalizer.transform(X)\nassert np.allclose(np.vstack((normalized_train_X, normalized_test_X)),normalized_all_X )\n</code></pre>\n<p>No exception is raised; they're numerically identical.</p>\n<p>But sklearn's transformers are sometimes stateful, so let's make a new object just to make sure this isn't some state-related behavior.</p>\n<pre><code>new_normalizer = preprocessing.Normalizer()\nnew_normalized_all_X = new_normalizer.fit_transform(X)\nassert np.allclose(np.vstack((normalized_train_X, normalized_test_X)),new_normalized_all_X )\n</code></pre>\n<p>In the second case, we still have no exception raised.</p>\n<p>We can conclude that for the <code>Normalizer</code> class, it makes no difference if the data are partitioned or not.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can use fit then transform\nlearn</p>\n<pre><code>normalizer = preprocessing.Normalizer().fit(xtrain)\n</code></pre>\n<p>transform</p>\n<pre><code>xtrainnorm = normalizer.transform(xtrain) \nxtestnorm = normalizer.transform(Xtest) \n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is a multi-headed model in deep learning?</p>\n<p>The only explanation I found so far is this: <em>Every model might be thought of as a backbone plus a head, and if you pre-train backbone and put a random head, you can fine tune it and it is a good idea</em><br/>\nCan someone please provide a more detailed explanation.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The explanation you found is accurate. Depending on what you want to predict on your data you require an adequate <strong>backbone network</strong> and a certain amount of <strong>prediction heads</strong>.</p>\n<p>For a basic classification network for example you can view ResNet, AlexNet, VGGNet, Inception,... as the backbone and the fully connected layer as the sole prediction head.</p>\n<p>A good example for a problem where you need multiple-heads is localization, where you not only want to classify what is in the image but also want to localize the object (find the coordinates of the bounding box around it).</p>\n<p>The image below shows the general architecture\n<a href=\"https://i.sstatic.net/FGrD1.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/FGrD1.png\"/></a></p>\n<p>The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome.</p>\n<p>The loss that you optimize for during training is usually a weighted sum of the individual losses for each prediction head.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>Head is the top of a network. For instance, on the bottom (where data comes in) you take convolution layers of some model, say resnet. If you call ConvLearner.pretrained, CovnetBuilder will build a network with appropriate head to your data in Fast.ai (if you are working on a classification problem, it will create a head with a cross entropy loss, if you are working on a regression problem, it will create a head suited to that).</p>\n<p>But you could build a model that has multiple heads. The model could take inputs from the base network (resnet conv layers) and feed the activations to some model, say head1 and then same data to head2. Or you could have some number of shared layers built on top of resnet and only those layers feeding to head1 and head2.</p>\n<p>You could even have different layers feed to different heads! There are some nuances to this (for instance, with regards to the fastai lib, ConvnetBuilder will add an AdaptivePooling layer on top of the base network if you don’t specify the custom_head argument and if you do it won’t) but this is the general picture.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://forums.fast.ai/t/terminology-question-head-of-neural-network/14819/2\" rel=\"nofollow noreferrer\">https://forums.fast.ai/t/terminology-question-head-of-neural-network/14819/2</a></li>\n<li><a href=\"https://youtu.be/h5Tz7gZT9Fo?t=3613\" rel=\"nofollow noreferrer\">https://youtu.be/h5Tz7gZT9Fo?t=3613</a> (1:13:00)</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have performed a PCA analysis over my original dataset and from the compressed dataset transformed by the PCA I have also selected the number of PC I want to keep (they explain almost the 94% of the variance). Now I am struggling with the identification of the original features that are important in the reduced dataset. \nHow do I find out which feature is important and which is not among the remaining Principal Components after the dimension reduction?\nHere is my code:</p>\n<pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=8)\npca.fit(scaledDataset)\nprojection = pca.transform(scaledDataset)\n</code></pre>\n<p>Furthermore, I tried also to perform a clustering algorithm on the reduced dataset but surprisingly for me, the score is lower than on the original dataset. How is it possible? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<hr/>\n<p>First of all, I assume that you call <strong><code>features</code> the variables and <code>not the samples/observations</code></strong>. In this case, you could do something like the following by creating a <code>biplot</code> function that shows everything in one plot. In this example, I am using the iris data.</p>\n<p>Before the example, please note that the <strong>basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings)</strong>. See my last paragraph after the plot for more details.</p>\n<hr/>\n<p><strong>Overview:</strong></p>\n<p><strong>PART1</strong>: I explain how to check the importance of the features and how to plot a biplot.</p>\n<p><strong>PART2</strong>: I explain how to check the importance of the features and how to save them into a pandas dataframe using the feature names.</p>\n<hr/>\n<h2>PART 1:</h2>\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n#In general a good idea is to scale the data\nscaler = StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)    \n\npca = PCA()\nx_new = pca.fit_transform(X)\n\ndef myplot(score,coeff,labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley, c = y)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\nplt.xlim(-1,1)\nplt.ylim(-1,1)\nplt.xlabel(\"PC{}\".format(1))\nplt.ylabel(\"PC{}\".format(2))\nplt.grid()\n\n#Call the function. Use only the 2 PCs.\nmyplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]))\nplt.show()\n</code></pre>\n<hr/>\n<p><strong>Visualize what's going on using the biplot</strong></p>\n<p><a href=\"https://i.sstatic.net/c45Fy.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/c45Fy.png\"/></a></p>\n<hr/>\n<p><strong>Now, the importance of each feature is reflected by the magnitude of the corresponding values in the eigenvectors (higher magnitude - higher importance)</strong></p>\n<p>Let's see first what amount of variance does each PC explain.</p>\n<pre><code>pca.explained_variance_ratio_\n[0.72770452, 0.23030523, 0.03683832, 0.00515193]\n</code></pre>\n<p><code>PC1 explains 72%</code> and <code>PC2 23%</code>. Together, if we keep PC1 and PC2 only, they explain <code>95%</code>.</p>\n<p>Now, let's find the most important features.</p>\n<pre><code>print(abs( pca.components_ ))\n\n[[0.52237162 0.26335492 0.58125401 0.56561105]\n [0.37231836 0.92555649 0.02109478 0.06541577]\n [0.72101681 0.24203288 0.14089226 0.6338014 ]\n [0.26199559 0.12413481 0.80115427 0.52354627]]\n</code></pre>\n<p><strong>Here, <code>pca.components_</code> has shape <code>[n_components, n_features]</code>. Thus, by looking at the <code>PC1</code> (First Principal Component) which is the first row: <code>[0.52237162 0.26335492 0.58125401 0.56561105]]</code> we can conclude that <code>feature 1, 3 and 4</code> (or Var 1, 3 and 4 in the biplot) are the most important.</strong> This is also clearly visible from the biplot (that's why we often use this plot to summarize the information in a visual way).</p>\n<p>To sum up, look at the absolute values of the Eigenvectors' components corresponding to the k largest Eigenvalues. In <code>sklearn</code> the components are sorted by <code>explained_variance_</code>. The larger they are these absolute values, the more a specific feature contributes to that principal component.</p>\n<hr/>\n<h2>PART 2:</h2>\n<p><strong>The important features are the ones that influence more the components and thus, have a large absolute value/score on the component.</strong></p>\n<p>To  <strong>get the most important features on the PCs</strong> with names and save them into a <strong>pandas dataframe</strong> use this:</p>\n<pre><code>from sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\n# 10 samples with 5 features\ntrain_features = np.random.rand(10,5)\n\nmodel = PCA(n_components=2).fit(train_features)\nX_pc = model.transform(train_features)\n\n# number of components\nn_pcs= model.components_.shape[0]\n\n# get the index of the most important feature on EACH component\n# LIST COMPREHENSION HERE\nmost_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n\ninitial_feature_names = ['a','b','c','d','e']\n# get the names\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n\n# LIST COMPREHENSION HERE AGAIN\ndic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n\n# build the dataframe\ndf = pd.DataFrame(dic.items())\n</code></pre>\n<p><strong>This prints:</strong></p>\n<pre><code>     0  1\n 0  PC0  e\n 1  PC1  d\n</code></pre>\n<p><strong>So on the PC1 the feature named <code>e</code> is the most important and on PC2 the <code>d</code>.</strong></p>\n<hr/>\n<hr/>\n<p>Nice article as well here: <a href=\"https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&amp;sk=65bf5440e444c24aff192fedf9f8b64f\" rel=\"noreferrer\">https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e?source=friends_link&amp;sk=65bf5440e444c24aff192fedf9f8b64f</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>the pca library contains this functionality.</p>\n<pre><code>pip install pca\n</code></pre>\n<p>A demonstration to extract the feature importance is as following:</p>\n<pre><code># Import libraries\nimport numpy as np\nimport pandas as pd\nfrom pca import pca\n\n# Lets create a dataset with features that have decreasing variance. \n# We want to extract feature f1 as most important, followed by f2 etc\nf1=np.random.randint(0,100,250)\nf2=np.random.randint(0,50,250)\nf3=np.random.randint(0,25,250)\nf4=np.random.randint(0,10,250)\nf5=np.random.randint(0,5,250)\nf6=np.random.randint(0,4,250)\nf7=np.random.randint(0,3,250)\nf8=np.random.randint(0,2,250)\nf9=np.random.randint(0,1,250)\n\n# Combine into dataframe\nX = np.c_[f1,f2,f3,f4,f5,f6,f7,f8,f9]\nX = pd.DataFrame(data=X, columns=['f1','f2','f3','f4','f5','f6','f7','f8','f9'])\n\n# Initialize\nmodel = pca()\n# Fit transform\nout = model.fit_transform(X)\n\n# Print the top features. The results show that f1 is best, followed by f2 etc\nprint(out['topfeat'])\n\n#     PC      feature\n# 0  PC1      f1\n# 1  PC2      f2\n# 2  PC3      f3\n# 3  PC4      f4\n# 4  PC5      f5\n# 5  PC6      f6\n# 6  PC7      f7\n# 7  PC8      f8\n# 8  PC9      f9\n</code></pre>\n<p>Plot the explained variance</p>\n<pre><code>model.plot()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/Wb1rN.png\" rel=\"noreferrer\"><img alt=\"Explained variance\" src=\"https://i.sstatic.net/Wb1rN.png\"/></a></p>\n<p>Make the biplot. It can be nicely seen that the first feature with most variance (f1), is almost horizontal in the plot, whereas the second most variance (f2) is almost vertical. This is expected because most of the variance is in f1, followed by f2 etc.</p>\n<pre><code>ax = model.biplot(n_feat=10, legend=False)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/V6BYZ.png\" rel=\"noreferrer\"><img alt=\"biplot\" src=\"https://i.sstatic.net/V6BYZ.png\"/></a></p>\n<p>Biplot in 3d. Here we see the nice addition of the expected f3 in the plot in the z-direction.</p>\n<pre><code>ax = model.biplot3d(n_feat=10, legend=False)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/831NF.png\" rel=\"noreferrer\"><img alt=\"biplot3d\" src=\"https://i.sstatic.net/831NF.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code># original_num_df the original numeric dataframe\n# pca is the model\ndef create_importance_dataframe(pca, original_num_df):\n\n    # Change pcs components ndarray to a dataframe\n    importance_df  = pd.DataFrame(pca.components_)\n\n    # Assign columns\n    importance_df.columns  = original_num_df.columns\n\n    # Change to absolute values\n    importance_df =importance_df.apply(np.abs)\n\n    # Transpose\n    importance_df=importance_df.transpose()\n\n    # Change column names again\n\n    ## First get number of pcs\n    num_pcs = importance_df.shape[1]\n\n    ## Generate the new column names\n    new_columns = [f'PC{i}' for i in range(1, num_pcs + 1)]\n\n    ## Now rename\n    importance_df.columns  =new_columns\n\n    # Return importance df\n    return importance_df\n\n# Call function to create importance df\nimportance_df  =create_importance_dataframe(pca, original_num_df)\n\n# Show first few rows\ndisplay(importance_df.head())\n\n# Sort depending on PC of interest\n\n## PC1 top 10 important features\npc1_top_10_features = importance_df['PC1'].sort_values(ascending = False)[:10]\nprint(), print(f'PC1 top 10 feautres are \\n')\ndisplay(pc1_top_10_features )\n\n## PC2 top 10 important features\npc2_top_10_features = importance_df['PC2'].sort_values(ascending = False)[:10]\nprint(), print(f'PC2 top 10 feautres are \\n')\ndisplay(pc2_top_10_features )\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a simple NN model for detecting hand-written digits from a 28x28px image written in python using Keras (Theano backend):</p>\n<pre><code>model0 = Sequential()\n\n#number of epochs to train for\nnb_epoch = 12\n#amount of data each iteration in an epoch sees\nbatch_size = 128\n\nmodel0.add(Flatten(input_shape=(1, img_rows, img_cols)))\nmodel0.add(Dense(nb_classes))\nmodel0.add(Activation('softmax'))\nmodel0.compile(loss='categorical_crossentropy', \n         optimizer='sgd',\n         metrics=['accuracy'])\n\nmodel0.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n      verbose=1, validation_data=(X_test, Y_test))\n\nscore = model0.evaluate(X_test, Y_test, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n</code></pre>\n<p>This runs well and I get ~90% accuracy. I then perform the following command to get a summary of my network's structure by doing <code>print(model0.summary())</code>. This outputs the following:</p>\n<pre><code>Layer (type)         Output Shape   Param #     Connected to                     \n=====================================================================\nflatten_1 (Flatten)   (None, 784)     0           flatten_input_1[0][0]            \ndense_1 (Dense)     (None, 10)       7850        flatten_1[0][0]                  \nactivation_1        (None, 10)          0           dense_1[0][0]                    \n======================================================================\nTotal params: 7850\n</code></pre>\n<p>I don't understand how they get to 7850 total params and what that actually means?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The number of parameters is 7850 because with every hidden unit you have 784 input weights and one weight of connection with bias. This means that every hidden unit gives you 785 parameters. You have 10 units so it sums up to 7850. </p>\n<p>The role of this additional bias term is really important. It significantly increases the capacity of your model. You can read details e.g. here <a href=\"https://stackoverflow.com/q/2480650/3924118\">Role of Bias in Neural Networks</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I feed a 514 dimensional real-valued input to a <code>Sequential</code> model in Keras.\nMy model is constructed in following way :</p>\n<pre><code>    predictivemodel = Sequential()\n    predictivemodel.add(Dense(514, input_dim=514, W_regularizer=WeightRegularizer(l1=0.000001,l2=0.000001), init='normal'))\n    predictivemodel.add(Dense(257, W_regularizer=WeightRegularizer(l1=0.000001,l2=0.000001), init='normal'))\n    predictivemodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n</code></pre>\n<p>When I print <code>model.summary()</code> I get following result:</p>\n<pre><code>Layer (type)    Output Shape  Param #     Connected to                   \n================================================================\ndense_1 (Dense) (None, 514)   264710      dense_input_1[0][0]              \n________________________________________________________________\nactivation_1    (None, 514)   0           dense_1[0][0]                    \n________________________________________________________________\ndense_2 (Dense) (None, 257)   132355      activation_1[0][0]               \n================================================================\nTotal params: 397065\n________________________________________________________________ \n</code></pre>\n<p>For the dense_1 layer , number of params is 264710.\nThis is obtained as : 514 (input values) * 514 (neurons in the first layer) + 514 (bias values)</p>\n<p>For dense_2 layer, number of params is 132355.\nThis is obtained as : 514 (input values) * 257 (neurons in the second layer) + 257 (bias values for neurons in the second layer)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For Dense Layers: </p>\n<pre><code>output_size * (input_size + 1) == number_parameters \n</code></pre>\n<p>For Conv Layers:</p>\n<pre><code>output_channels * (input_channels * window_size + 1) == number_parameters\n</code></pre>\n<p>Consider following example,</p>\n<pre><code>model = Sequential([\nConv2D(32, (3, 3), activation='relu', input_shape=input_shape),\nConv2D(64, (3, 3), activation='relu'),\nConv2D(128, (3, 3), activation='relu'),\nDense(num_classes, activation='softmax')\n])\n\nmodel.summary()\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 220, 220, 64)      18496     \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 218, 218, 128)     73856     \n_________________________________________________________________\ndense_9 (Dense)              (None, 218, 218, 10)      1290      \n=================================================================\n</code></pre>\n<p>Calculating params,</p>\n<pre><code>assert 32 * (3 * (3*3) + 1) == 896\nassert 64 * (32 * (3*3) + 1) == 18496\nassert 128 * (64 * (3*3) + 1) == 73856\nassert num_classes * (128 + 1) == 1290\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm following some lectures from lynda.com about deep learning using Keras-TensorFlow in a PyCharmCE enviroment and they didn't have this problem.\nI get this error:</p>\n<blockquote>\n<p>raise ImportError('Could not import PIL.Image. '\nImportError: Could not import PIL.Image. The use of <code>array_to_img</code> requires PIL.</p>\n</blockquote>\n<p>I have checked if others get the same error, but for me installing pillow using pip with the command <code>pip install Pillow</code> doesn't solve anything.</p>\n<blockquote>\n<p>MacBook-Pro-de-Rogelio:~ Rogelio$ pip install Pillow\nRequirement already satisfied: Pillow in ./anaconda3/lib/python3.6/site-packages\nMacBook-Pro-de-Rogelio:~ Rogelio$</p>\n</blockquote>\n<p>Any solution?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>All you need to do is install pillow:</p>\n<pre><code>pip install pillow\n</code></pre>\n<p>Then you should be all set. Found this after hours of searching.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had the exact same error and I fixed it the following way: </p>\n<p>1) Run this command in your Jupyter Notebook: </p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import sys\r\nfrom PIL import Image\r\nsys.modules['Image'] = Image </code></pre>\n</div>\n</div>\n</p>\n<p>2) Run the following two lines in your notebook to be sure that they are correctly pointing to the same directory (if not it's because your PIL old library is messing up with the Pillow library)</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from PIL import Image\r\nprint(Image.__file__)</code></pre>\n</div>\n</div>\n</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import Image\r\nprint(Image.__file__)</code></pre>\n</div>\n</div>\n</p>\n<p>3) If that's working correctly and both import prints pointing to the same python3 directory then move on.\nIf not: \n  3.a) Go to your OS console and to your conda environment (be sure you are working within your desire conda environment) :</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>conda uninstall PIL\r\nconda uninstall Pillow\r\nconda install Pillow</code></pre>\n</div>\n</div>\n</p>\n<p>You should now have successfully installed all the libraries for Pillow and let behind any problems with PIL. \n  3.b) Now try to execute the code of your jupyer notebook again, now the paths to both the imports should look exactly the same</p>\n<p>4) Now, in the OS console/terminal, having your desired conda environment active, run the following commands: </p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>conda install keras\r\nconda install tensorflow</code></pre>\n</div>\n</div>\n</p>\n<p>5) Run your jupyter notebook script again, It should be fixed and working now! </p>\n<p>If it's still not working, it must be because you have opened a jupyter notebook kernel that's not point to the right environment. Fix that and you will be fine! </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If this problem is being seen on an Anaconda env,\nuse</p>\n<pre><code>conda install pillow \n</code></pre>\n<p>and reopen</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>After I instantiate a scikit model (e.g. <code>LinearRegression</code>), if I call its <code>fit()</code> method multiple times (with different <code>X</code> and <code>y</code> data), what happens? Does it fit the model on the data like if I just re-instantiated the model (i.e. from scratch), or does it keep into accounts data already fitted from the previous call to <code>fit()</code>?</p>\n<p>Trying with <code>LinearRegression</code> (also looking at its source code) it seems to me that every time I call <code>fit()</code>, it fits from scratch, ignoring the result of any previous call to the same method. I wonder if this true in general, and I can rely on this behavior for all models/pipelines of scikit learn.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you will execute <code>model.fit(X_train, y_train)</code> for a second time - it'll overwrite all previously fitted coefficients, weights, intercept (bias), etc.</p>\n<p>If you want to fit just a portion of your data set and then to improve your model by fitting a new data, then you can use <a href=\"https://scikit-learn.org/0.15/modules/scaling_strategies.html\" rel=\"noreferrer\">estimators, supporting \"Incremental learning\" (those, that implement <code>partial_fit()</code> method)</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use term <strong>fit()</strong> and <strong>train()</strong> word interchangeably in machine learning. Based on classification model you have instantiated, may be a <code>clf = GBNaiveBayes()</code> or <code>clf = SVC()</code>,  your model uses specified machine learning technique.<br/>\nAnd as soon as you call <code>clf.fit(features_train, label_train)</code> your model starts training using the features and labels that you have passed.</p>\n<p>you can use <code>clf.predict(features_test)</code> to predict.<br/>\nIf you will again call <code>clf.fit(features_train2, label_train2)</code> it will start training again using passed data and will remove the previous results. Your model will <strong>reset</strong> the following inside model:</p>\n<ul>\n<li>Weights</li>\n<li>Fitted Coefficients</li>\n<li>Bias</li>\n<li>And other training related stuff...</li>\n</ul>\n<p>You can use <strong>partial_fit()</strong> method as well if you want your previous calculated stuff to stay and additionally train using next data</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Beware that the model is passed kind of \"by reference\". Here, model1 will be overwritten:</p>\n<pre><code>df1 = pd.DataFrame(np.random.rand(100).reshape(10,10))\ndf2 = df1.copy()\ndf2.iloc[0,0] = df2.iloc[0,0] -2 # change one value\n\npca = PCA()\nmodel1 = pca.fit(df)\nmodel2 = pca.fit(df2)\n\nnp.unique(model1.explained_variance_ == model2.explained_variance_)\n</code></pre>\n<p>Returns</p>\n<pre><code>array([ True])\n</code></pre>\n<p>To avoid this use</p>\n<pre><code>from copy import deepcopy\nmodel1 = deepcopy(pca.fit(df))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a binary prediction model trained by logistic regression algorithm. I want know which features (predictors) are more important for the decision of positive or negative class. I know there is <code>coef_</code> parameter which comes from the scikit-learn package, but I don't know whether it is enough for the importance. Another thing is how I can evaluate the <code>coef_</code> values in terms of the importance for negative and positive classes. I also read about standardized regression coefficients and I don't know what it is.</p>\n<p>Lets say there are features like size of tumor, weight of tumor, and etc to make a decision for a test case like malignant or not malignant. I want to know which of the features are more important for malignant and not malignant prediction.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One of the simplest options to get a feeling for the \"influence\" of a given parameter in a linear classification model (logistic being one of those), is to consider the magnitude of its coefficient times the standard deviation of the corresponding parameter in the data.</p>\n<p>Consider this example:</p>\n<pre><code>import numpy as np    \nfrom sklearn.linear_model import LogisticRegression\n\nx1 = np.random.randn(100)\nx2 = 4*np.random.randn(100)\nx3 = 0.5*np.random.randn(100)\ny = (3 + x1 + x2 + x3 + 0.2*np.random.randn()) &gt; 0\nX = np.column_stack([x1, x2, x3])\n\nm = LogisticRegression()\nm.fit(X, y)\n\n# The estimated coefficients will all be around 1:\nprint(m.coef_)\n\n# Those values, however, will show that the second parameter\n# is more influential\nprint(np.std(X, 0)*m.coef_)\n</code></pre>\n<p>An alternative way to get a similar result is to examine the coefficients of the model fit on standardized parameters:</p>\n<pre><code>m.fit(X / np.std(X, 0), y)\nprint(m.coef_)\n</code></pre>\n<p>Note that this is the most basic approach and a number of other techniques for finding feature importance or parameter influence exist (using p-values, bootstrap scores, various \"discriminative indices\", etc). </p>\n<p>I am pretty sure you would get more interesting answers at <a href=\"https://stats.stackexchange.com/\">https://stats.stackexchange.com/</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since scikit-learn 0.22, <code>sklearn</code> defines a <code>sklearn.inspection</code> module which implements <code>permutation_importance</code>, which can be used to find the most important features - higher value indicates higher \"importance\" or the the corresponding feature contributes a larger fraction of whatever metrics was used to evaluate the model (the default for <code>LogisticRegression</code> is accuracy).</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import permutation_importance\n\n# initialize sample (using the same setup as in KT.'s)\nX = np.random.standard_normal((100,3)) * [1, 4, 0.5]\ny = (3 + X.sum(axis=1) + 0.2*np.random.standard_normal()) &gt; 0\n\n# fit a model\nmodel = LogisticRegression().fit(X, y)\n# compute importances\nmodel_fi = permutation_importance(model, X, y)\nmodel_fi['importances_mean']                    # array([0.07 , 0.352, 0.02 ])\n</code></pre>\n<p>So in the example above, the most important feature is the second feature, followed by the first and the third. This is the same ordinal ranking as the one suggested in <a href=\"https://stackoverflow.com/a/34052747/19123103\">KT.'s post</a>.</p>\n<p>One nice thing about <code>permutation_importance</code> is that both training and test datasets may be passed to it to identify which features might cause the model to overfit.</p>\n<hr/>\n<p>You can read more about it in the <a href=\"https://scikit-learn.org/stable/modules/permutation_importance.html\" rel=\"noreferrer\">documentation</a>, you can even find the outline of the algorithm.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As from the title I am wondering what is the difference between</p>\n<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\" rel=\"nofollow noreferrer\">StratifiedKFold</a> with the parameter <code>shuffle=True</code></p>\n<pre><code>StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n</code></pre>\n<p>and</p>\n<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\" rel=\"nofollow noreferrer\">StratifiedShuffleSplit</a></p>\n<pre><code>StratifiedShuffleSplit(n_splits=10, test_size=’default’, train_size=None, random_state=0)\n</code></pre>\n<p>and what is the advantage of using StratifiedShuffleSplit</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In <code>stratKFolds</code>, each test set should not overlap, even when shuffle is included. With <code>stratKFolds</code> and <code>shuffle=True</code>, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest.</p>\n<p>In <code>ShuffleSplit</code>, the data is shuffled every time, and then split. This means the test sets may overlap between the splits.</p>\n<p>See this block for an example of the difference. Note the overlap of the elements in the test sets for <code>ShuffleSplit</code>.</p>\n<pre><code>splits = 5\n\ntx = range(10)\nty = [0] * 5 + [1] * 5\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn import datasets\n\nstratKfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\nshufflesplit = StratifiedShuffleSplit(n_splits=splits, random_state=42, test_size=2)\n\nprint(\"stratKFold\")\nfor train_index, test_index in stratKfold.split(tx, ty):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"Shuffle Split\")\nfor train_index, test_index in shufflesplit.split(tx, ty):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n</code></pre>\n<p>Output:</p>\n<pre><code>stratKFold\nTRAIN: [0 2 3 4 5 6 7 9] TEST: [1 8]\nTRAIN: [0 1 2 3 5 7 8 9] TEST: [4 6]\nTRAIN: [0 1 3 4 5 6 8 9] TEST: [2 7]\nTRAIN: [1 2 3 4 6 7 8 9] TEST: [0 5]\nTRAIN: [0 1 2 4 5 6 7 8] TEST: [3 9]\nShuffle Split\nTRAIN: [8 4 1 0 6 5 7 2] TEST: [3 9]\nTRAIN: [7 0 3 9 4 5 1 6] TEST: [8 2]\nTRAIN: [1 2 5 6 4 8 9 0] TEST: [3 7]\nTRAIN: [4 6 7 8 3 5 1 2] TEST: [9 0]\nTRAIN: [7 2 6 5 4 3 0 9] TEST: [1 8]\n</code></pre>\n<p>As for when to use them, I tend to use <code>stratKFolds</code> for any cross validation, and I use <code>ShuffleSplit</code> with a split of 2 for my train/test set splits. But I'm sure there are other use cases for both.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>@Ken Syme already has a very good answer. I just want to add something.</p>\n<ul>\n<li><strong><code>StratifiedKFold</code></strong> is a variation of <code>KFold</code>. First, <code>StratifiedKFold</code> shuffles your data, after that splits the data into <code>n_splits</code> parts and Done. \nNow, it will use each part as a test set. Note that <strong>it only and always shuffles data one time</strong> before splitting.</li>\n</ul>\n<p>With  <code>shuffle = True</code>, the data is shuffled by your <code>random_state</code>. Otherwise, \nthe data is shuffled by <code>np.random</code> (as default).\nFor example, with <code>n_splits = 4</code>, and your data has 3 classes (label) for <code>y</code> (dependent variable). 4 test sets cover all the data without any overlap.</p>\n<p><a href=\"https://i.sstatic.net/XJZve.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/XJZve.png\"/></a></p>\n<ul>\n<li>On the other hand, <strong><code>StratifiedShuffleSplit</code></strong> is a variation of <code>ShuffleSplit</code>.\nFirst, <code>StratifiedShuffleSplit</code> shuffles your data, and then it also splits the data into <code>n_splits</code> parts. However, it's not done yet. After this step, <code>StratifiedShuffleSplit</code> picks one part to use as a test set.\nThen it repeats the same process <code>n_splits - 1</code> other times, to get <code>n_splits - 1</code> other test sets. Look at the picture below, with the same data, but this time, the 4 test sets do not cover all the data, i.e there are overlaps among test sets.</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/AGv9B.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/AGv9B.png\"/></a></p>\n<p>So, the difference here is that <code>StratifiedKFold</code> <strong>just shuffles and splits once, therefore the test sets do not overlap</strong>, while <code>StratifiedShuffleSplit</code> <strong>shuffles each time before splitting, and it splits <code>n_splits</code> times, the test sets can overlap</strong>. </p>\n<ul>\n<li><strong>Note</strong>: the two methods uses \"stratified fold\" (that why \"stratified\" appears in both names). It means each part preserves the same percentage of samples of each class (label) as the original data. You can read more at <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\" rel=\"noreferrer\">cross_validation documents</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Output examples of KFold, StratifiedKFold, StratifiedShuffleSplit:\n<a href=\"https://i.sstatic.net/PUBCh.jpg\" rel=\"noreferrer\"><img alt=\"Output examples of KFold, StratifiedKFold, StratifiedShuffleSplit\" src=\"https://i.sstatic.net/PUBCh.jpg\"/></a></p>\n<p>The above pictorial output is an extension of <code>@Ken Syme</code>'s code:</p>\n<pre><code>from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\nSEED = 43\nSPLIT = 3\n\nX_train = [0,1,2,3,4,5,6,7,8]\ny_train = [0,0,0,0,0,0,1,1,1]   # note 6,7,8 are labelled class '1'\n\nprint(\"KFold, shuffle=False (default)\")\nkf = KFold(n_splits=SPLIT, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"KFold, shuffle=True\")\nkf = KFold(n_splits=SPLIT, shuffle=True, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"\\nStratifiedKFold, shuffle=False (default)\")\nskf = StratifiedKFold(n_splits=SPLIT, random_state=SEED)\nfor train_index, test_index in skf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \nprint(\"StratifiedKFold, shuffle=True\")\nskf = StratifiedKFold(n_splits=SPLIT, shuffle=True, random_state=SEED)\nfor train_index, test_index in skf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \nprint(\"\\nStratifiedShuffleSplit\")\nsss = StratifiedShuffleSplit(n_splits=SPLIT, random_state=SEED, test_size=3)\nfor train_index, test_index in sss.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"\\nStratifiedShuffleSplit (can customise test_size)\")\nsss = StratifiedShuffleSplit(n_splits=SPLIT, random_state=SEED, test_size=2)\nfor train_index, test_index in sss.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have come across few (Machine learning-classification problem) journal papers mentioned about evaluate accuracy with Top-N approach. Data was show that Top 1 accuracy = 42.5%, and Top-5 accuracy = 72.5% in the same training, testing condition.\nI wonder how to calculate this percentage of top-1 and top-5?</p>\n<p>Can some one show me example and steps to calculate this?   </p>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Top-1 accuracy is the conventional accuracy: the model answer (the one with highest probability) must be exactly the expected answer.</p>\n<p>Top-5 accuracy means that <em>any</em> of your model 5 highest probability answers must match the expected answer.</p>\n<p>For instance, let's say you're applying machine learning to object recognition using a neural network. A picture of a cat is shown, and these are the outputs of your neural network:</p>\n<ul>\n<li>Tiger: 0.4</li>\n<li>Dog: 0.3</li>\n<li>Cat: 0.1</li>\n<li>Lynx: 0.09</li>\n<li>Lion: 0.08</li>\n<li>Bird: 0.02</li>\n<li>Bear: 0.01</li>\n</ul>\n<p>Using top-1 accuracy, you count this output as <strong>wrong</strong>, because it predicted a tiger.</p>\n<p>Using top-5 accuracy, you count this output as <strong>correct</strong>, because cat is among the top-5 guesses.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The Complement of the accuracy is the error, The top-1 error is the percentage of time that the classifier did not give the correct class highest probability score.\nThe top-5 error:- The percentage of time that the classifier did not include the correct class among the top 5 probabilities or guesses.   </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/10554052/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2016-09-03 20:47:53Z\">8 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/10554052/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm Working on document classification tasks in java.</p>\n<p>Both algorithms came highly recommended, what are the benefits and disadvantages of each and which is more commonly used in the literature for Natural Language Processing tasks? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>At the very basics of it, the major difference between the porter and lancaster stemming algorithms is that the lancaster stemmer is significantly more aggressive than the porter stemmer. The three major stemming algorithms in use today are Porter, Snowball(Porter2), and Lancaster (Paice-Husk), with the aggressiveness continuum basically following along those same lines. Porter is the least aggressive algorithm, with the specifics of each algorithm actually being fairly lengthy and technical. Here is a break down for you though:</p>\n<p>Porter: Most commonly used stemmer without a doubt, also one of the most gentle stemmers. One of the few stemmers that actually has Java support which is a plus, though it is also the most computationally intensive of the algorithms(Granted not by a very significant margin). It is also the oldest stemming algorithm by a large margin.</p>\n<p>Porter2: Nearly universally regarded as an improvement over porter, and for good reason. Porter himself in fact admits that it is better than his original algorithm. Slightly faster computation time than porter, with a fairly large community around it.</p>\n<p>Lancaster: Very aggressive stemming algorithm, sometimes to a fault. With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, not so with Lancaster, as many shorter words will become totally obfuscated. The fastest algorithm here, and will reduce your working set of words hugely, but if you want more distinction, not the tool you would want.</p>\n<p>Honestly, I feel that Snowball is usually the way to go. There are certain circumstances in which Lancaster will hugely trim down your working set, which can be very useful, however the marginal speed increase over snowball in my opinion is not worth the lack of precision. Porter has the most implementations though and so is usually the default go-to algorithm, but if you can, use snowball.</p>\n<h2>Snowball - Additional info</h2>\n<blockquote>\n<p><a href=\"https://snowballstem.org/\" rel=\"noreferrer\">Snowball</a> is a small string processing language designed for creating\nstemming algorithms for use in Information Retrieval.</p>\n<p>The Snowball compiler translates a Snowball script into another\nlanguage - currently ISO C, C#, Go, Java, Javascript, Object Pascal,\nPython and Rust are supported.</p>\n</blockquote>\n<h3>History of the name</h3>\n<blockquote>\n<p>Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed\nwith the idea of calling it ‘strippergram’, but good sense has\nprevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the\nexcellent string handling language of Messrs Farber, Griswold, Poage\nand Polonsky from the 1960s.<br/>\n---Martin Porter</p>\n</blockquote>\n<p>Stemmers implemented in the Snowball language are sometimes simply referred to as Snowball stemmers. For example, see the Natural Language Toolkit: <a href=\"https://www.nltk.org/_modules/nltk/stem/snowball.html\" rel=\"noreferrer\">nltk.stem.snowball</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using scikit-learn in Python to develop a classification algorithm to predict the gender of certain customers. Amongst others, I want to use the Naive Bayes classifier but my problem is that I have a mix of categorical data (ex: \"Registered online\", \"Accepts email notifications\" etc) and continuous data (ex: \"Age\", \"Length of membership\" etc). I haven't used scikit much before but I suppose that that Gaussian Naive Bayes is suitable for continuous data and that Bernoulli Naive Bayes can be used for categorical data. However, since I want to have <strong>both</strong> categorical and continuous data in my model, I don't really know how to handle this. Any ideas would be much appreciated!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You have at least two options:</p>\n<ul>\n<li><p>Transform all your data into a categorical representation by computing percentiles for each continuous variables and then binning the continuous variables using the percentiles as bin boundaries. For instance for the height of a person create the following bins: \"very small\", \"small\", \"regular\", \"big\", \"very big\" ensuring that each bin contains approximately 20% of the population of your training set. We don't have any utility to perform this automatically in scikit-learn but it should not be too complicated to do it yourself. Then fit a unique multinomial NB on those categorical representation of your data.</p></li>\n<li><p>Independently fit a gaussian NB model on the continuous part of the data and a multinomial NB model on the categorical part. Then transform all the dataset by taking the class assignment probabilities (with <code>predict_proba</code> method) as new features: <code>np.hstack((multinomial_probas, gaussian_probas))</code> and then refit a new model (e.g. a new gaussian NB) on the new features.</p></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Hope I'm not too late. I recently wrote a library called Mixed Naive Bayes, written in NumPy. It can assume a mix of Gaussian and categorical (multinoulli) distributions on the training data features.</p>\n<p><a href=\"https://github.com/remykarem/mixed-naive-bayes\" rel=\"noreferrer\">https://github.com/remykarem/mixed-naive-bayes</a></p>\n<p>The  library is written such that the APIs are similar to <a href=\"http://scikit-learn.org\" rel=\"noreferrer\">scikit-learn</a>'s.</p>\n<p>In the example below, let's assume that the first 2 features are from a categorical distribution and the last 2 are Gaussian. In the <code>fit()</code> method, just specify <code>categorical_features=[0,1]</code>, indicating that Columns 0 and 1 are to follow categorical distribution.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from mixed_naive_bayes import MixedNB\nX = [[0, 0, 180.9, 75.0],\n     [1, 1, 165.2, 61.5],\n     [2, 1, 166.3, 60.3],\n     [1, 1, 173.0, 68.2],\n     [0, 2, 178.4, 71.0]]\ny = [0, 0, 1, 1, 0]\nclf = MixedNB(categorical_features=[0,1])\nclf.fit(X,y)\nclf.predict(X)\n</code></pre>\n<p>Pip installable via <code>pip install mixed-naive-bayes</code>. More information on the usage in the README.md file. Pull requests are greatly appreciated :)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The simple answer: multiply result!! it's the same.</p>\n<p>Naive Bayes based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features - meaning you calculate the Bayes probability dependent on a specific feature without holding the others - which means that the algorithm multiply each probability from one feature with the probability from the second feature (and we totally ignore the denominator - since it is just a normalizer).</p>\n<p>so the right answer is:</p>\n<ol>\n<li>calculate the probability from the categorical variables.</li>\n<li>calculate the probability from the continuous variables.</li>\n<li>multiply 1. and 2. </li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Regression algorithms seem to be working on features represented as numbers. \nFor example:</p>\n<p><a href=\"https://i.sstatic.net/deFE1.jpg\" rel=\"noreferrer\"><img alt=\"simple data without categorical features\" src=\"https://i.sstatic.net/deFE1.jpg\"/></a></p>\n<p>This data set doesn't contain categorical features/variables. It's quite clear how to do regression on this data and predict price.</p>\n<hr/>\n<p>But now I want to do a regression analysis on data that contain categorical features:</p>\n<p><a href=\"https://i.sstatic.net/gLRrh.jpg\" rel=\"noreferrer\"><img alt=\"data-set with categorical features\" src=\"https://i.sstatic.net/gLRrh.jpg\"/></a></p>\n<p>There are <strong>5</strong> features: <code>District</code>, <code>Condition</code>, <code>Material</code>, <code>Security</code>, <code>Type</code></p>\n<hr/>\n<p>How can I do a regression on this data? Do I have to transform all the string/categorical data to numbers manually? I mean if I have to create some encoding rules and according to that rules transform all data to numeric values. </p>\n<p>Is there any simple way to transform string data to numbers without having to create my own encoding rules manually? Maybe there are some libraries in <strong>Python</strong> that can be used for that? Are there some risks that the regression model will be somehow incorrect due to \"bad encoding\"?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, you will have to convert everything to numbers. That requires thinking about what these attributes represent.</p>\n<p>Usually there are three possibilities:</p>\n<ol>\n<li>One-Hot encoding for categorical data</li>\n<li>Arbitrary numbers for ordinal data</li>\n<li>Use something like group means for categorical data (e. g. mean prices for city districts).</li>\n</ol>\n<p>You have to be carefull to not infuse information you do not have in the application case.</p>\n<h1>One hot encoding</h1>\n<p>If you have categorical data, you can create dummy variables with 0/1 values for each possible value.</p>\n<p>E. g.</p>\n<pre><code>idx color\n0   blue\n1   green\n2   green\n3   red\n</code></pre>\n<p>to</p>\n<pre><code>idx blue green red\n0   1    0     0\n1   0    1     0\n2   0    1     0\n3   0    0     1\n</code></pre>\n<p>This can easily be done with pandas:</p>\n<pre><code>import pandas as pd\n\ndata = pd.DataFrame({'color': ['blue', 'green', 'green', 'red']})\nprint(pd.get_dummies(data))\n</code></pre>\n<p>will result in:</p>\n<pre><code>   color_blue  color_green  color_red\n0           1            0          0\n1           0            1          0\n2           0            1          0\n3           0            0          1\n</code></pre>\n<h1>Numbers for ordinal data</h1>\n<p>Create a mapping of your sortable categories,  e. g.\nold &lt; renovated &lt; new → 0, 1, 2</p>\n<p>This is also possible with pandas:</p>\n<pre><code>data = pd.DataFrame({'q': ['old', 'new', 'new', 'ren']})\ndata['q'] = data['q'].astype('category')\ndata['q'] = data['q'].cat.reorder_categories(['old', 'ren', 'new'], ordered=True)\ndata['q'] = data['q'].cat.codes\nprint(data['q'])\n</code></pre>\n<p>Result:</p>\n<pre><code>0    0\n1    2\n2    2\n3    1\nName: q, dtype: int8\n</code></pre>\n<h1>Using categorical data for groupby operations</h1>\n<p>You could use the mean for each category over past (known events).</p>\n<p>Say you have a DataFrame with the last known mean prices for cities:</p>\n<pre><code>prices = pd.DataFrame({\n    'city': ['A', 'A', 'A', 'B', 'B', 'C'],\n    'price': [1, 1, 1, 2, 2, 3],\n})\nmean_price = prices.groupby('city').mean()\ndata = pd.DataFrame({'city': ['A', 'B', 'C', 'A', 'B', 'A']})\n\nprint(data.merge(mean_price, on='city', how='left'))\n</code></pre>\n<p>Result:</p>\n<pre><code>  city  price\n0    A      1\n1    B      2\n2    C      3\n3    A      1\n4    B      2\n5    A      1\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In linear regression with categorical variables you should be careful of the Dummy Variable Trap. The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others. This can produce singularity of a model, meaning your model just won't work. <a href=\"http://www.algosome.com/articles/dummy-variable-trap-regression.html\" rel=\"noreferrer\">Read about it here</a></p>\n<p>Idea is to use dummy variable encoding with <code>drop_first=True</code>, this will omit one column from each category after converting categorical variable into dummy/indicator variables. You <strong>WILL NOT</strong> lose any relevant information by doing that simply because your all point in dataset can fully be explained by rest of the features. </p>\n<p><strong>Here is complete code on how you can do it for your housing dataset</strong></p>\n<p>So you have categorical features: </p>\n<pre><code>District, Condition, Material, Security, Type\n</code></pre>\n<p>And one numerical features that you are trying to predict:</p>\n<pre><code>Price\n</code></pre>\n<p>First you need to split your initial dataset on input variables and prediction, assuming its pandas dataframe it would look like this:</p>\n<p>Input variables:</p>\n<pre><code>X = housing[['District','Condition','Material','Security','Type']]\n</code></pre>\n<p>Prediction:</p>\n<pre><code>Y = housing['Price']\n</code></pre>\n<p>Convert categorical variable into dummy/indicator variables and drop one in each category:</p>\n<pre><code>X = pd.get_dummies(data=X, drop_first=True)\n</code></pre>\n<p>So now if you check shape of X with <code>drop_first=True</code> you will see that it has 4 columns less - one for each of your categorical variables. </p>\n<p>You can now continue to use them in your linear model. For scikit-learn implementation it could look like this:</p>\n<pre><code>from sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 40)\n\nregr = linear_model.LinearRegression() # Do not use fit_intercept = False if you have removed 1 column after dummy encoding\nregr.fit(X_train, Y_train)\npredicted = regr.predict(X_test)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use \"Dummy Coding\" in this case.\nThere are Python libraries to do dummy coding, you have a few options:</p>\n<ul>\n<li>You may use <code>scikit-learn</code> library. Take a look at <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\" rel=\"noreferrer\">here</a>. </li>\n<li>Or, if you are working with <code>pandas</code>, it has a built-in function to <a href=\"http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.get_dummies.html\" rel=\"noreferrer\">create dummy variables</a>.</li>\n</ul>\n<p>An example with pandas is below:</p>\n<pre><code>import pandas as pd\n\nsample_data = [[1,2,'a'],[3,4,'b'],[5,6,'c'],[7,8,'b']]\ndf = pd.DataFrame(sample_data, columns=['numeric1','numeric2','categorical'])\ndummies = pd.get_dummies(df.categorical)\ndf.join(dummies)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-07-20 17:10:20Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/3345079/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I am looking for a method on how to calculate the number of layers and the number of neurons per layer. As input I only have the size of the input vector, the size of the output vector and the size of the training set.</p>\n<p>Usually the best net is determined by trying different net topologies and selecting the one with the least error. Unfortunately I cannot do that.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a really hard problem.</p>\n<p>The more internal structure a network has, the better that network will be at representing complex solutions.  On the other hand, too much internal structure is slower, may cause training to diverge, or lead to overfitting -- which would prevent your network from generalizing well to new data.</p>\n<p>People have traditionally approached this problem in several different ways:</p>\n<ol>\n<li><p><strong>Try different configurations, see what works best.</strong>  You can divide your training set into two pieces -- one for training, one for evaluation -- and then train and evaluate different approaches.  Unfortunately it sounds like in your case this experimental approach isn't available.</p>\n</li>\n<li><p><strong>Use a rule of thumb.</strong>  A lot of people have come up with a lot of guesses as to what works best.  Concerning the number of neurons in the hidden layer, people have speculated that (for example) it should (a) be between the input and output layer size, (b) set to something near (inputs+outputs) * 2/3, or (c) never larger than twice the size of the input layer.\n<br/><br/>The problem with rules of thumb is that they <em>don't always take into account vital pieces of information</em>, <a href=\"http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html\" rel=\"nofollow noreferrer\">like how \"difficult\" the problem is, what the size of the training and testing sets</a> are, etc.  Consequently, these rules are often used as rough starting points for the \"let's-try-a-bunch-of-things-and-see-what-works-best\" approach.</p>\n</li>\n<li><p><strong>Use an algorithm that dynamically adjusts the network configuration.</strong>  Algorithms like <a href=\"https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92\" rel=\"nofollow noreferrer\">Cascade Correlation</a> start with a minimal network, then add hidden nodes during training.  This can make your experimental setup a bit simpler, and (in theory) can result in better performance (because you won't accidentally use an inappropriate number of hidden nodes).</p>\n</li>\n</ol>\n<p>There's a lot of research on this subject -- so if you're really interested, there is a lot to read.  Check out the citations <a href=\"http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html\" rel=\"nofollow noreferrer\">on this summary</a>, in particular:</p>\n<ul>\n<li><p>Lawrence, S., Giles, C.L., and Tsoi, A.C. (1996), <a href=\"http://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf\" rel=\"nofollow noreferrer\">\"What size neural network gives optimal generalization? Convergence properties of backpropagation\"</a>.  <em>Technical Report UMIACS-TR-96-22 and CS-TR-3617, Institute for Advanced Computer Studies, University of Maryland, College Park.</em></p>\n</li>\n<li><p>Elisseeff, A., and Paugam-Moisy, H. (1997), <a href=\"ftp://www.neurocolt.com/pub/neurocolt/tech_reports/1997/nc-tr-97-002.ps.gz\" rel=\"nofollow noreferrer\">\"Size of multilayer networks for exact learning: analytic approach\"</a>.  <em>Advances in Neural Information Processing Systems 9, Cambridge, MA: The MIT Press, pp.162-168.</em></p>\n</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In practice, this is not difficult (based on having coded and trained dozens of MLPs).</p>\n<p>In a textbook sense, getting the architecture \"right\" is hard--i.e., to tune your network architecture such that performance (resolution) cannot be improved by further optimization of the architecture is hard, i agree. But only in rare cases is that degree of optimization required. </p>\n<p>In practice, to meet or exceed the prediction accuracy from a neural network required by your spec, you almost never need to spend a lot of time with the network architecture--three reasons why this is true: </p>\n<ul>\n<li><p><em>most of the parameters</em> required to specify the network architecture\n<em>are fixe</em>d once you have decided on your data model (number of\nfeatures in the input vector, whether the desired response variable\nis numerical or categorical, and if the latter, how many unique class\nlabels you've chosen);</p></li>\n<li><p>the few remaining architecture parameters that are in fact tunable,\nare nearly always (100% of the time in my experience) <em>highly constrained</em> by those fixed architecture\nparameters--i.e., the values of those parameters are tightly bounded by a max and min value; and</p></li>\n<li><p>the optimal architecture does not have to be determined before\ntraining begins, indeed, it is very common for neural network code to\ninclude a small module to programmatically tune the network\narchitecture during training (by removing nodes whose weight values\nare approaching zero--usually called \"<em>pruning</em>.\")\n<br/></p></li>\n</ul>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/xvb0C.png\"/></p>\n<p>According to the Table above, the architecture of a neural network is completely specified by <strong><em>six</em></strong> parameters (the six cells in the interior grid). Two of those (number of layer type for the input and output layers) are always one and one--neural networks have a single input layer and a single output layer. Your NN must have at least one input layer and one output layer--no more, no less. Second, the number of nodes comprising each of those two layers is fixed--the input layer, by the size of the input vector--i.e., the number of nodes in the input layer is equal to the length of the input vector (actually one more neuron is nearly always added to the input layer as a <em>bias node</em>).</p>\n<p>Similarly, the output layer size is fixed by the response variable (single node for numerical response variable, and (assuming softmax is used, if response variable is a class label, the the number of nodes in the output layer simply equals the number of unique class labels).</p>\n<p>That leaves <em>just two</em> parameters for which there is any discretion at all--the number of hidden layers and the number of nodes comprising each of those layers. \n<br/><br/></p>\n<h2>The Number of Hidden Layers</h2>\n<p>if your data is linearly separable (which you often know by the time you begin coding a NN) then you don't need any hidden layers at all. (If that's in fact the case, i would not use a NN for this problem--choose a simpler linear classifier). \nThe first of these--the number of hidden layers--is nearly always one. There is a lot of empirical weight behind this presumption--in practice very few problems that cannot be solved with a single hidden layer become soluble by adding another hidden layer. Likewise, there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very small. One hidden layer is sufficient for the large majority of problems.</p>\n<p>In your question, you mentioned that for whatever reason, you cannot find the optimum network architecture by trial-and-error. Another way to tune your NN configuration (without using trial-and-error) is '<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.2976&amp;rep=rep1&amp;type=pdf\" rel=\"noreferrer\">pruning</a>'. The gist of this technique is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look for weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on a network architecture, err on the side of more neurons, if you add a pruning step. </p>\n<p>Put another way, by applying a pruning algorithm to your network during training, you can much closer to an optimized network configuration than any a priori theory is ever likely to give you.\n<br/><br/></p>\n<h2>The Number of Nodes Comprising the Hidden Layer</h2>\n<p>but what about the number of nodes comprising the hidden layer? Granted this value is more or less unconstrained--i.e., it can be smaller or larger than the size of the input layer. Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the famous <a href=\"http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html\" rel=\"noreferrer\">NN FAQ</a> for an excellent summary of that commentary). There are many empirically-derived rules-of-thumb, but of these, the most commonly relied on is <strong><em>the size of the hidden layer is between the input and output layers</em></strong>. Jeff Heaton, author of \"<a href=\"http://www.heatonresearch.com/node/707\" rel=\"noreferrer\">Introduction to Neural Networks in Java</a>\" offers a few more, which are recited on the page i just linked to. Likewise, a scan of the application-oriented neural network literature, will almost certainly reveal that the hidden layer size is usually <em>between</em> the input and output layer sizes. But <em>between</em> doesn't mean in the middle; in fact, it is usually better to set the hidden layer size closer to the size of the input vector. The reason is that if the hidden layer is too small, the network might have difficultly converging. For the initial configuration, err on the larger size--a larger hidden layer gives the network more capacity which helps it converge, compared with a smaller hidden layer. Indeed, this justification is often used to recommend a hidden layer size <em>larger than</em> (more nodes) the input layer--ie, begin with an initial architecture that will encourage quick convergence, after which you can prune the 'excess' nodes (identify the nodes in the hidden layer with very low weight values and eliminate them from your re-factored network).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When I want to evaluate the performance of my model on the validation set, is it preferred to use <code>with torch.no_grad:</code> or <code>model.eval()</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>TL;DR:</h2>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/19160#issuecomment-482188706\" rel=\"noreferrer\">Use <em>both</em></a>. They do different things, and have different scopes.</p>\n<ul>\n<li><code>with torch.no_grad</code> - disables tracking of gradients in <code>autograd</code>.</li>\n<li><code>model.eval()</code> changes the <code>forward()</code> behaviour of the module it is called upon</li>\n<li>eg, it disables dropout and has batch norm use the entire population statistics</li>\n</ul>\n<hr/>\n<h3><code>with torch.no_grad</code></h3>\n<p>The <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad\" rel=\"noreferrer\"><code>torch.autograd.no_grad</code> documentation</a>  says:</p>\n<blockquote>\n<p>Context-manager that disabled [sic] gradient calculation.</p>\n</blockquote>\n<blockquote>\n<p>Disabling gradient calculation is useful for inference, when you are sure that you will not call <code>Tensor.backward()</code>. It will reduce memory consumption for computations that would otherwise have <code>requires_grad=True</code>. In this mode, the result of every computation will have <code>requires_grad=False</code>, even when the inputs have <code>requires_grad=True</code>.</p>\n</blockquote>\n<h3><code>model.eval()</code></h3>\n<p>The <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval\" rel=\"noreferrer\"><code>nn.Module.eval</code> documentation</a> says:</p>\n<blockquote>\n<p>Sets the module in evaluation mode.</p>\n</blockquote>\n<blockquote>\n<p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p>\n</blockquote>\n<hr/>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/19160#issuecomment-482188706\" rel=\"noreferrer\">The creator of pytorch said the documentation should be updated to suggest the usage of both</a>, and I raised the <a href=\"https://github.com/pytorch/pytorch/pull/46173\" rel=\"noreferrer\">pull request</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>with torch.no_grad:</code> disables computation of gradients for the <strong>backward</strong> pass. Since these calculations are unnecessary during inference, and add non-trivial computational overhead, it is essessential to use this context if evaluating the model's speed. It will not however affect results.</p>\n<p><code>model.eval()</code> ensures <a href=\"https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train\">certain modules</a> which behave differently in training vs inference (e.g. Dropout and BatchNorm) are defined appropriately during the <strong>forward</strong> pass in inference. As such, if your model contains such modules it is essential to enable this.</p>\n<p>For the reasons above it is good practice to use both during inference.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're reading this post because you've been encountering <code>RuntimeError: CUDA out of memory</code>, then <code>with torch.no grad():</code> will likely to help save the memory. Using only <code>model.eval()</code> is unlikely to help with the OOM error.</p>\n<p>The reason for this is that <code>torch.no grad()</code> disables autograd completely (you can no longer backpropagate), reducing memory consumption and speeding up computations.</p>\n<p>However, you will still be able to call the gardients when using <code>model.eval()</code>. Personally, I find this design decision intriguing. So, what is the purpose of <code>.eval()</code>? It seems its main functionality is to deactivate the Dropout during the evaluation time.</p>\n<p>To summarize, if you use <code>torch.no grad()</code>, no intermediate tensors are saved, and you can possibly increase the batch size in your inference.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I understand what Gradient Descent does. Basically it tries to move towards the local optimal solution by slowly moving down the curve. I am trying to understand what is the actual difference between the plain gradient descent and the Newton's method?</p>\n<p>From Wikipedia, I read this short line \"Newton's method uses curvature information to take a more direct route.\" What does this intuitively mean?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>At a local minimum (or maximum) <code>x</code>, the derivative of the target function <code>f</code> vanishes: <code>f'(x) = 0</code> (assuming sufficient smoothness of <code>f</code>).</p>\n<p>Gradient descent tries to find such a minimum <code>x</code> by using information from the first derivative of <code>f</code>: It simply follows the steepest descent from the current point. This is like rolling a ball down the graph of <code>f</code> until it comes to rest (while neglecting inertia).</p>\n<p>Newton's method tries to find a point <code>x</code> satisfying <code>f'(x) = 0</code> by approximating <code>f'</code> with a linear function <code>g</code> and then solving for the root of that function explicitely (this is called Newton's root-finding method). The root of <code>g</code> is not necessarily the root of <code>f'</code>, but it is under many circumstances a good guess (the <a href=\"http://en.wikipedia.org/wiki/Newton%27s_method\" rel=\"noreferrer\">Wikipedia article on Newton's method for root finding</a> has more information on convergence criteria). While approximating <code>f'</code>, Newton's method makes use of <code>f''</code> (the curvature of <code>f</code>). This means it has higher requirements on the smoothness of <code>f</code>, but it also means that (by using more information) it often converges faster.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Put simply, gradient descent you just take a small step towards where you think the zero is and then recalculate; Newton's method, you go all the way there.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Edit 2017</strong>: The original link is dead -\n but the way back machine still got it :) <a href=\"https://web.archive.org/web/20151122203025/http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf\" rel=\"nofollow noreferrer\">https://web.archive.org/web/20151122203025/http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf</a></p>\n<p>this power point the main ideas are explained simply <a href=\"http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf\" rel=\"nofollow noreferrer\">http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf</a></p>\n<p>I hope this help :)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using the <code>MinMaxScaler</code> model in sklearn to normalize the features of a model.</p>\n<pre><code>training_set = np.random.rand(4,4)*10\ntraining_set\n\n       [[ 6.01144787,  0.59753007,  2.0014852 ,  3.45433657],\n       [ 6.03041646,  5.15589559,  6.64992437,  2.63440202],\n       [ 2.27733136,  9.29927394,  0.03718093,  7.7679183 ],\n       [ 9.86934288,  7.59003904,  6.02363739,  2.78294206]]\n\n\nscaler = MinMaxScaler()\nscaler.fit(training_set)    \nscaler.transform(training_set)\n\n\n   [[ 0.49184811,  0.        ,  0.29704831,  0.15972182],\n   [ 0.4943466 ,  0.52384506,  1.        ,  0.        ],\n   [ 0.        ,  1.        ,  0.        ,  1.        ],\n   [ 1.        ,  0.80357559,  0.9052909 ,  0.02893534]]\n</code></pre>\n<p>Now I want to use the same scaler to normalize the test set:</p>\n<pre><code>   [[ 8.31263467,  7.99782295,  0.02031658,  9.43249727],\n   [ 1.03761228,  9.53173021,  5.99539478,  4.81456067],\n   [ 0.19715961,  5.97702519,  0.53347403,  5.58747666],\n   [ 9.67505429,  2.76225253,  7.39944931,  8.46746594]]\n</code></pre>\n<p>But I don't want so use the <code>scaler.fit()</code> with the training data all the time. Is there a way to save the scaler and load it later from a different file?    </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Update:</strong> <code>sklearn.externals.joblib</code> is deprecated. Install and use the pure <code>joblib</code> instead. Please see <a href=\"https://stackoverflow.com/a/56978681/3466910\">Engineero's answer below</a>, which is otherwise identical to mine.</p>\n<h3>Original answer</h3>\n<p>Even better than <code>pickle</code> (which creates much larger files than this method), you can use <code>sklearn</code>'s built-in tool:</p>\n<pre><code>from sklearn.externals import joblib\nscaler_filename = \"scaler.save\"\njoblib.dump(scaler, scaler_filename) \n\n# And now to load...\n\nscaler = joblib.load(scaler_filename) \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just a note that <code>sklearn.externals.joblib</code> has been deprecated and is superseded by plain old <a href=\"https://joblib.readthedocs.io/en/latest/index.html\" rel=\"noreferrer\"><code>joblib</code></a>, which can be installed with <code>pip install joblib</code>:</p>\n<pre><code>import joblib\njoblib.dump(my_scaler, 'scaler.gz')\nmy_scaler = joblib.load('scaler.gz')\n</code></pre>\n<p>Note that file extensions can be anything, but if it is one of <code>['.z', '.gz', '.bz2', '.xz', '.lzma']</code> then the corresponding compression protocol will be used. Docs for <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"noreferrer\"><code>joblib.dump()</code></a> and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"noreferrer\"><code>joblib.load()</code></a> methods.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So I'm actually not an expert with this but from a bit of research and a few helpful <a href=\"https://stackoverflow.com/questions/10592605/save-classifier-to-disk-in-scikit-learn\">links</a>, I think <code>pickle</code> and <code>sklearn.externals.joblib</code> are going to be your friends here.</p>\n<p>The package <code>pickle</code> lets you save models or \"dump\" models to a file. </p>\n<p>I think this <a href=\"http://scikit-learn.org/stable/modules/model_persistence.html\" rel=\"noreferrer\">link</a> is also helpful. It talks about creating a persistence model. Something that you're going to want to try is:</p>\n<pre><code># could use: import pickle... however let's do something else\nfrom sklearn.externals import joblib \n\n# this is more efficient than pickle for things like large numpy arrays\n# ... which sklearn models often have.   \n\n# then just 'dump' your file\njoblib.dump(clf, 'my_dope_model.pkl') \n</code></pre>\n<p><a href=\"https://pythonhosted.org/joblib/generated/joblib.dump.html\" rel=\"noreferrer\">Here</a> is where you can learn more about the sklearn  externals.</p>\n<p>Let me know if that doesn't help or I'm not understanding something about your model.</p>\n<p>Note: <code>sklearn.externals.joblib</code> is deprecated. Install and use the pure <code>joblib</code> instead</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have some data I'm trying to organize into a <code>DataFrame</code> in <code>Pandas</code>.  I was trying to make each row a <code>Series</code> and append it to the <code>DataFrame</code>.  I found a way to do it by appending the <code>Series</code> to an empty <code>list</code> and then converting the <code>list</code> of <code>Series</code> to a <code>DataFrame</code> </p>\n<p>e.g. <code>DF = DataFrame([series1,series2],columns=series1.index)</code></p>\n<p>This <code>list</code> to <code>DataFrame</code> step seems to be excessive.  I've checked out a few examples on here but none of the <code>Series</code> preserved the <code>Index</code> labels from the <code>Series</code> to use them as column labels.</p>\n<p>My long way where columns are id_names and rows are type_names:\n<a href=\"https://i.sstatic.net/L83C8.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/L83C8.png\"/></a> </p>\n<p><strong>Is it possible to append Series to rows of DataFrame without making a list first?</strong></p>\n<pre><code>#!/usr/bin/python\n\nDF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value)\n    DF.append(SR_row)\nDF.head()\n\nTypeError: Can only append a Series if ignore_index=True or if the Series has a name\n</code></pre>\n<p>Then I tried</p>\n<pre><code>DF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value,name=sample)\n    DF.append(SR_row)\nDF.head()\n</code></pre>\n<p>Empty DataFrame</p>\n<p>Tried <a href=\"https://stackoverflow.com/questions/24284342/insert-a-row-to-pandas-dataframe\">Insert a row to pandas dataframe</a>\nStill getting an empty dataframe :/ </p>\n<p><strong>I am trying to get the Series to be the rows, where the index of the Series becomes the column labels of the DataFrame</strong></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Maybe an easier way would be to add the <code>pandas.Series</code> into the <code>pandas.DataFrame</code> with <code>ignore_index=True</code> argument to <code>DataFrame.append()</code>. Example -</p>\n<pre><code>DF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value)\n    DF = DF.append(SR_row,ignore_index=True)\n</code></pre>\n<p>Demo -</p>\n<pre><code>In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([[1,2],[3,4]],columns=['A','B'])\n\nIn [3]: df\nOut[3]:\n   A  B\n0  1  2\n1  3  4\n\nIn [5]: s = pd.Series([5,6],index=['A','B'])\n\nIn [6]: s\nOut[6]:\nA    5\nB    6\ndtype: int64\n\nIn [36]: df.append(s,ignore_index=True)\nOut[36]:\n   A  B\n0  1  2\n1  3  4\n2  5  6\n</code></pre>\n<hr/>\n<p>Another issue in your code is that <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html\"><code>DataFrame.append()</code></a> is not in-place, it returns the appended dataframe, you would need to assign it back to your original dataframe for it to work. Example -</p>\n<pre><code>DF = DF.append(SR_row,ignore_index=True)\n</code></pre>\n<hr/>\n<p>To preserve the labels, you can use your solution to include name for the series along with assigning the appended DataFrame back to <code>DF</code>. Example -</p>\n<pre><code>DF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value,name=sample)\n    DF = DF.append(SR_row)\nDF.head()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html\" rel=\"noreferrer\"><code>DataFrame.append</code></a> does not modify the DataFrame in place.  You need to do <code>df = df.append(...)</code> if you want to reassign it back to the original variable.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>append is deprecating so, the best choice would be to_frame().T</p>\n<pre><code>df1 = pd.DataFrame({'name':['john','mark'],'job':['manager','salesman'],'age':[43,23]})\nser1 = df1.iloc[-1]\npd.concat([df1,ser1.to_frame().T],ignore_index=True)\n\n   name       job age\n0  john   manager  43\n1  mark  salesman  23\n2  mark  salesman  23\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>what is the benefit of using Gradient Descent in the linear regression space? looks like the we can solve the problem (finding theta0-n that minimum the cost func) with analytical method so why we still want to use gradient descent to do the same thing? thanks </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you use the <strong>normal equations</strong> for solving the cost function analytically you have to compute:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0l2z0.png\"/></p>\n<p>Where X is your matrix of input observations and y your output vector. The problem with this operation is the time complexity of calculating the inverse of a nxn matrix which is O(n^3) and as n increases it can take a very long time to finish.</p>\n<p>When n is low (n &lt; 1000 or n &lt; 10000) you can think of normal equations as the better option for calculation theta, however for greater values <strong>Gradient Descent</strong> is much more faster, so the only reason is the time :)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should provide more details about yout problem - what exactly are you asking about - are we talking about linear regression in one or many dimensions? Simple or generalized ones?</p>\n<p>In general, why do people use the GD?</p>\n<ul>\n<li>it is easy to implement</li>\n<li>it is very generic optimization technique - even if you change your model to the more general one, you can stil use it</li>\n</ul>\n<p>So what about analytical solutions? Well, we <strong>do</strong> use them, your claim is simply false here (if we are talking in general), for example the <a href=\"http://en.wikipedia.org/wiki/Ordinary_least_squares\">OLS</a> method is a closed form, analytical solution, which is widely used. If you can use the analytical solution, it is affordable computationaly (as sometimes GD is simply cheapier or faster) then you can, and even should - use it.</p>\n<p>Neverlethles this is always a matter of some pros and cons - analytical solutions are strongly connected to the model, so implementing them can be inefficient if you plan to generalize/change your models in the future. They are sometimes less efficient then their numerical approximations, and sometimes there are simply harder to implement. If none of above is true - you <strong>should</strong> use the analytical solution, and people do it, really.</p>\n<p>To sum up, you rather use GD over analytical solution if:</p>\n<ul>\n<li>you are considering changes in the model, generalizations, adding some more complex terms/regularization/modifications</li>\n<li>you need a generic method because you do not know much about the future of the code and the model (you are only one of the developers)</li>\n<li>analytical solution is more expensive computationaly, and you need efficiency</li>\n<li>analytical solution requires more memory, which you do not have</li>\n<li>analytical solution is hard to implement and you need easy, simple code </li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I saw a very good answer from <a href=\"https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent\">https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent</a></p>\n<p>Basically, the reasons are:</p>\n<p>1.For most nonlinear regression problems there is no closed form solution.</p>\n<p>2.Even in linear regression (one of the few cases where a closed form solution is available), it may be impractical to use the formula. The following example shows one way in which this can happen.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-07-20 16:02:38Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/11752727/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>By processing a time series graph, I Would like to detect patterns that look similar to this:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/irfjs.png\"/></p>\n<p>Using a sample time series as an example, I would like to be able to detect the patterns as marked here:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/tPu3y.png\"/></p>\n<p>What kind of AI algorithm (I am assuming marchine learning techniques) do I need to use to achieve this? Is there any library (in C/C++) out there that I can use?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a sample result from a small project I did to partition ecg data.</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/picJY.png\"/></p>\n<p>My approach was a \"switching autoregressive HMM\" (google this if you haven't heard of it) where each datapoint is predicted from the previous datapoint using a Bayesian regression model. I created 81 hidden states: a junk state to capture data between each beat, and 80 separate hidden states corresponding to different positions within the heartbeat pattern. The pattern 80 states were constructed directly from a subsampled single beat pattern and had two transitions - a self transition and a transition to the next state in the pattern. The final state in the pattern transitioned to either itself or the junk state.</p>\n<p>I trained the model with <a href=\"https://stats.stackexchange.com/questions/581/differences-between-baum-welch-and-viterbi-training\">Viterbi training</a>, updating only the regression parameters.</p>\n<p>Results were adequate in most cases. A similarly structure Conditional Random Field would probably perform better, but training a CRF would require manually labeling patterns in the dataset if you don't already have labelled data.</p>\n<p><strong>Edit:</strong></p>\n<p>Here's some example python code - it is not perfect, but it gives the general approach. It implements EM rather than Viterbi training, which may be slightly more stable.\nThe ecg dataset is from <a href=\"http://www.cs.ucr.edu/%7Eeamonn/discords/ECG_data.zip\" rel=\"noreferrer\">http://www.cs.ucr.edu/~eamonn/discords/ECG_data.zip</a></p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport numpy.random as rnd\nimport matplotlib.pyplot as plt \nimport scipy.linalg as lin\nimport re\n    \ndata=np.array(map(lambda l: map(float, filter(lambda x:len(x)&gt;0,            \n    re.split('\\\\s+',l))), open('chfdb_chf01_275.txt'))).T\ndK=230\npattern=data[1,:dK]\ndata=data[1,dK:]\n    \ndef create_mats(dat):\n    '''\n    create \n        A - an initial transition matrix \n        pA - pseudocounts for A\n        w - emission distribution regression weights\n        K - number of hidden states\n    '''\n    step=5  #adjust this to change the granularity of the pattern\n    eps=.1\n    dat=dat[::step]\n    K=len(dat)+1\n    A=np.zeros( (K,K) )\n    A[0,1]=1.\n    pA=np.zeros( (K,K) )\n    pA[0,1]=1.\n    for i in xrange(1,K-1):\n        A[i,i]=(step-1.+eps)/(step+2*eps)\n        A[i,i+1]=(1.+eps)/(step+2*eps)\n        pA[i,i]=1.\n        pA[i,i+1]=1.\n    A[-1,-1]=(step-1.+eps)/(step+2*eps)\n    A[-1,1]=(1.+eps)/(step+2*eps)\n    pA[-1,-1]=1.\n    pA[-1,1]=1.\n        \n    w=np.ones( (K,2) , dtype=np.float)\n    w[0,1]=dat[0]\n    w[1:-1,1]=(dat[:-1]-dat[1:])/step\n    w[-1,1]=(dat[0]-dat[-1])/step\n        \n    return A,pA,w,K\n    \n# Initialize stuff\nA,pA,w,K=create_mats(pattern)\n        \neta=10. # precision parameter for the autoregressive portion of the model \nlam=.1  # precision parameter for the weights prior \n    \nN=1 #number of sequences\nM=2 #number of dimensions - the second variable is for the bias term\nT=len(data) #length of sequences\n    \nx=np.ones( (T+1,M) ) # sequence data (just one sequence)\nx[0,1]=1\nx[1:,0]=data\n    \n# Emissions\ne=np.zeros( (T,K) )\n\n# Residuals\nv=np.zeros( (T,K) )\n    \n# Store the forward and backward recurrences\nf=np.zeros( (T+1,K) )\nfls=np.zeros( (T+1) )\nf[0,0]=1\nb=np.zeros( (T+1,K) )\nbls=np.zeros( (T+1) )\nb[-1,1:]=1./(K-1)\n    \n# Hidden states\nz=np.zeros( (T+1),dtype=np.int )\n    \n# Expected hidden states\nex_k=np.zeros( (T,K) )\n    \n# Expected pairs of hidden states\nex_kk=np.zeros( (K,K) )\nnkk=np.zeros( (K,K) )\n    \ndef fwd(xn):\n    global f,e\n    for t in xrange(T):\n        f[t+1,:]=np.dot(f[t,:],A)*e[t,:]\n        sm=np.sum(f[t+1,:])\n        fls[t+1]=fls[t]+np.log(sm)\n        f[t+1,:]/=sm\n        assert f[t+1,0]==0\n    \ndef bck(xn):\n    global b,e\n    for t in xrange(T-1,-1,-1):\n        b[t,:]=np.dot(A,b[t+1,:]*e[t,:])\n        sm=np.sum(b[t,:])\n        bls[t]=bls[t+1]+np.log(sm)\n        b[t,:]/=sm\n    \ndef em_step(xn):\n    global A,w,eta\n    global f,b,e,v\n    global ex_k,ex_kk,nkk\n        \n    x=xn[:-1] #current data vectors\n    y=xn[1:,:1] #next data vectors predicted from current\n    \n    # Compute residuals\n    v=np.dot(x,w.T) # (N,K) &lt;- (N,1) (N,K)\n    v-=y\n    e=np.exp(-eta/2*v**2,e)\n        \n    fwd(xn)\n    bck(xn)\n        \n    # Compute expected hidden states\n    for t in xrange(len(e)):\n        ex_k[t,:]=f[t+1,:]*b[t+1,:]\n        ex_k[t,:]/=np.sum(ex_k[t,:])\n        \n    # Compute expected pairs of hidden states    \n    for t in xrange(len(f)-1):\n        ex_kk=A*f[t,:][:,np.newaxis]*e[t,:]*b[t+1,:]\n        ex_kk/=np.sum(ex_kk)\n        nkk+=ex_kk\n        \n    # max w/ respect to transition probabilities\n    A=pA+nkk\n    A/=np.sum(A,1)[:,np.newaxis]\n        \n    # Solve the weighted regression problem for emissions weights\n    # x and y are from above \n    for k in xrange(K):\n        ex=ex_k[:,k][:,np.newaxis]\n        dx=np.dot(x.T,ex*x)\n        dy=np.dot(x.T,ex*y)\n        dy.shape=(2)\n        w[k,:]=lin.solve(dx+lam*np.eye(x.shape[1]), dy)\n            \n    # Return the probability of the sequence (computed by the forward algorithm)\n    return fls[-1]\n    \nif __name__=='__main__':\n    # Run the em algorithm\n    for i in xrange(20):\n        print em_step(x)\n    \n    # Get rough boundaries by taking the maximum expected hidden state for each position\n    r=np.arange(len(ex_k))[np.argmax(ex_k,1)&lt;3]\n        \n    # Plot\n    plt.plot(range(T),x[1:,0])\n        \n    yr=[np.min(x[:,0]),np.max(x[:,0])]\n    for i in r:\n        plt.plot([i,i],yr,'-r')\n    \n    plt.show()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Why not using a simple matched filter? Or its general statistical counterpart called cross correlation. Given a known pattern x(t) and a noisy compound time series containing your pattern shifted in <strong>a,b,...,z</strong> like <code>y(t) = x(t-a) + x(t-b) +...+ x(t-z) + n(t).</code> The cross correlation function between x and y should give peaks in a,b, ...,z</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://www.cs.waikato.ac.nz/ml/weka/\" rel=\"nofollow noreferrer\">Weka</a> is a powerful collection of machine-learning software, and supports some time-series analysis tools, but I do not know enough about the field to recommend a best method. However, it is Java-based; and you can <a href=\"https://stackoverflow.com/questions/819536/how-to-call-java-function-from-c\">call Java code from C/C++</a> without great fuss.</p>\n<p>Packages for time-series manipulation are mostly directed at the stock-market. I suggested <a href=\"http://www.stat.cmu.edu/~abrock/oldcronos/\" rel=\"nofollow noreferrer\">Cronos</a> in the comments; I have no idea how to do pattern recognition with it, beyond the obvious: any good model of a length of your series should be able to predict that, after small bumps at a certain distance to the last small bump, big bumps follow. That is, your series exhibits self-similarity, and the models used in Cronos are designed to model it.</p>\n<p>If you don't mind C#, you should request a version of <a href=\"http://www.cs.umd.edu/hcil/timesearcher/#ts2\" rel=\"nofollow noreferrer\">TimeSearcher2</a> from the folks at HCIL - pattern recognition is, for this system, drawing what a pattern looks like, and then checking whether your model is general enough to capture most instances with a low false-positive rate. Probably the most user-friendly approach you will find; all others require quite a background in statistics or pattern recognition strategies.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to write a *.txt file with the neural network hyperparameters and the model architecture. Is it possible to write the object model.summary() to my output file?</p>\n<pre><code>(...)\nsummary = str(model.summary())\n(...)\nout = open(filename + 'report.txt','w')\nout.write(summary)\nout.close\n</code></pre>\n<p>It happens that I'm getting \"None\" as you can see below.</p>\n<pre><code>Hyperparameters\n=========================\n\nlearning_rate: 0.01\nmomentum: 0.8\ndecay: 0.0\nbatch size: 128\nno. epochs: 3\ndropout: 0.5\n-------------------------\n\nNone\nval_acc: 0.232323229313\nval_loss: 3.88496732712\ntrain_acc: 0.0965207634216\ntrain_loss: 4.07161939425\ntrain/val loss ratio: 1.04804469418\n</code></pre>\n<p>Any idea how to deal with that?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>With my version of Keras (<code>2.0.6</code>) and Python (<code>3.5.0</code>), this works for me:</p>\n<pre><code># Create an empty model\nfrom keras.models import Sequential\nmodel = Sequential()\n\n# Open the file\nwith open(filename + 'report.txt','w') as fh:\n    # Pass the file handle in as a lambda function to make it callable\n    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n</code></pre>\n<p>This outputs the following lines to the file:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For me, this worked to just get the model summary as a string:</p>\n<pre><code>stringlist = []\nmodel.summary(print_fn=lambda x: stringlist.append(x))\nshort_model_summary = \"\\n\".join(stringlist)\nprint(short_model_summary)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>And if you want to write to a log:</p>\n<pre><code>import logging\nlogger = logging.getLogger(__name__)\n\nmodel.summary(print_fn=logger.info)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The classifiers in machine learning packages like liblinear and nltk offer a method <code>show_most_informative_features()</code>, which is really helpful for debugging features:</p>\n<pre><code>viagra = None          ok : spam     =      4.5 : 1.0\nhello = True           ok : spam     =      4.5 : 1.0\nhello = None           spam : ok     =      3.3 : 1.0\nviagra = True          spam : ok     =      3.3 : 1.0\ncasino = True          spam : ok     =      2.0 : 1.0\ncasino = None          ok : spam     =      1.5 : 1.0\n</code></pre>\n<p>My question is if something similar is implemented for the classifiers in scikit-learn. I searched the documentation, but couldn't find anything the like.</p>\n<p>If there is no such function yet, does somebody know a workaround how to get to those values?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The classifiers themselves do not record feature names, they just see numeric arrays. However, if you extracted your features using a <code>Vectorizer</code>/<code>CountVectorizer</code>/<code>TfidfVectorizer</code>/<code>DictVectorizer</code>, <em>and</em> you are using a linear model (e.g. <code>LinearSVC</code> or Naive Bayes) then you can apply the same trick that the <a href=\"http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\" rel=\"noreferrer\">document classification example</a> uses. Example (<em>untested</em>, may contain a bug or two):</p>\n<pre><code>def print_top10(vectorizer, clf, class_labels):\n    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(\"%s: %s\" % (class_label,\n              \" \".join(feature_names[j] for j in top10)))\n</code></pre>\n<p>This is for multiclass classification; for the binary case, I think you should use <code>clf.coef_[0]</code> only. You may have to sort the <code>class_labels</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>With the help of larsmans code I came up with this code for the binary case:</p>\n<pre><code>def show_most_informative_features(vectorizer, clf, n=20):\n    feature_names = vectorizer.get_feature_names()\n    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n    for (coef_1, fn_1), (coef_2, fn_2) in top:\n        print \"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To add an update, <code>RandomForestClassifier</code> now supports the <code>.feature_importances_</code> attribute. This <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">attribute</a> tells you how much of the observed variance is explained by that feature. Obviously, the sum of all these values must be &lt;= 1. </p>\n<p>I find this attribute very useful when performing feature engineering. </p>\n<p>Thanks to the scikit-learn team and contributors for implementing this!</p>\n<p>edit: This works for both RandomForest and GradientBoosting. So <code>RandomForestClassifier</code>, <code>RandomForestRegressor</code>, <code>GradientBoostingClassifier</code> and <code>GradientBoostingRegressor</code> all support this. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2022-07-18 14:32:10Z\">2 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2022-07-18 14:36:30Z\">2 years ago</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/19170603/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>In <a href=\"http://www.youtube.com/watch?v=qkcFRr7LqAw\">this</a> video from Sebastian Thrum he says that supervised learning works with \"labeled\" data and unsupervised learning works with \"unlabeled\" data. What does he mean by this? Googling \"labeled vs unlabeled data\" returns a bunch of scholarly papers on this topic. I just want to know the basic difference. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Typically, <strong>unlabeled</strong> data consists of samples of natural or human-created artifacts that you can obtain relatively easily from the world. Some examples of unlabeled data might include photos, audio recordings, videos, news articles, tweets, x-rays (if you were working on a medical application), etc. There is no \"explanation\" for each piece of unlabeled data -- it just contains the data, and nothing else.</p>\n<p><strong>Labeled</strong> data typically takes a set of unlabeled data and augments each piece of that unlabeled data with some sort of meaningful \"tag,\" \"label,\" or \"class\" that is somehow informative or desirable to know. For example, labels for the above types of unlabeled data might be whether this photo contains a horse or a cow, which words were uttered in this audio recording, what type of action is being performed in this video, what the topic of this news article is, what the overall sentiment of this tweet is, whether the dot in this x-ray is a tumor, etc.</p>\n<p>Labels for data are often obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., \"Does this photo contain a horse or a cow?\") and are significantly more expensive to obtain than the raw unlabeled data.</p>\n<p>After obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.</p>\n<p>There are many active areas of research in machine learning that are aimed at integrating unlabeled and labeled data to build better and more accurate models of the world. Semi-supervised learning attempts to combine unlabeled and labeled data (or, more generally, sets of unlabeled data where only some data points have labels) into integrated models. Deep neural networks and feature learning are areas of research that attempt to build models of the unlabeled data alone, and then apply information from the labels to the interesting parts of the models.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Labeled data</strong>, used by <strong>Supervised learning</strong> add meaningful <em>tags</em> or <em>labels</em> or <em>class</em> to the observations (or rows). These tags can come from observations or asking people or specialists about the data. </p>\n<p><strong>Classification</strong> and <strong>Regression</strong> could be applied to labelled datasets for Supervised learning.</p>\n<p>Machine learning models can be applied to the labeled data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted.\n<a href=\"https://i.sstatic.net/4WE6N.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/4WE6N.png\"/></a></p>\n<p><strong>Unlabeled data</strong>, used by <strong>Unsupervised learning</strong>  however do not have any meaningful tags or labels associated with it. \n<a href=\"https://i.sstatic.net/xqnJr.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/xqnJr.png\"/></a>\nUnsupervised learning has more difficult algorithms than supervised learning since we know little to no information about the data, or the outcomes that are to be expected.</p>\n<p><strong>Clustering</strong> is considered to be one of the most popular unsupervised machine learning techniques used for grouping data points, or objects that are somehow similar.</p>\n<p>Unsupervised learning has fewer models, and fewer evaluation methods that can be used to ensure that the outcome of the model is accurate. As such, unsupervised learning creates a less controllable environment as the machine is creating outcomes for us.</p>\n<p>Picture courtesy of <a href=\"https://www.coursera.org/learn/machine-learning-with-python/lecture/jgzpX/supervised-vs-unsupervised\" rel=\"noreferrer\">Coursera: Machine Learning with Python</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are many different problems in Machine Learning so I'll pick <em>classification</em> as a case in point. In classification, labelled data typically consists of a bag of multidimensional feature vectors (normally called X) and for each vector a label, Y which is often just an integer corresponding to a category eg. (face=1, non-face=-1). \nUnlabelled data misses the Y component.\nThere are many scenarios where unlabelled data is plentiful and easily obtained but labelled data often requires a human/expert to annotate.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/18541923/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-08-31 00:58:44Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/18541923/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is out of bag error in Random Forests?\nIs it the optimal parameter for finding the right number of trees in a Random Forest?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I will take an attempt to explain: </p>\n<p>Suppose our training data set is represented by T and suppose data set has M features (or attributes or variables).</p>\n<p><code>T = {(X1,y1), (X2,y2), ... (Xn, yn)}</code></p>\n<p>and </p>\n<pre><code>Xi is input vector {xi1, xi2, ... xiM}\n\nyi is the label (or output or class). \n</code></pre>\n<p>summary of RF: </p>\n<p>Random Forests algorithm is a classifier based on primarily two methods -</p>\n<ul>\n<li>Bagging</li>\n<li>Random subspace method. </li>\n</ul>\n<p>Suppose we decide to have <code>S</code> number of trees in our forest then we first create <code>S</code> datasets of <code>\"same size as original\"</code> created from random resampling of data in T with-replacement (n times for each dataset). This will result in <code>{T1, T2, ... TS}</code> datasets. Each of these is called a bootstrap dataset. Due to \"with-replacement\" every dataset <code>Ti</code> can have duplicate data records and Ti can be missing several data records from original datasets. This is called <code>Bootstrapping</code>. (en.wikipedia.org/wiki/Bootstrapping_(statistics)) </p>\n<p>Bagging is the process of taking bootstraps &amp; then aggregating the models learned on each bootstrap.</p>\n<p>Now, RF creates <code>S</code> trees and uses <code>m (=sqrt(M) or =floor(lnM+1))</code> random subfeatures out of <code>M</code> possible features to create any tree. This is called random subspace method. </p>\n<p>So for each <code>Ti</code> bootstrap dataset you create a tree <code>Ki</code>. If you want to classify some input data <code>D = {x1, x2, ..., xM}</code> you let it pass through each tree and produce <code>S</code> outputs (one for each tree) which can be denoted by <code>Y = {y1, y2, ..., ys}</code>. Final prediction is a majority vote on this set. </p>\n<p>Out-of-bag error:</p>\n<p>After creating the classifiers (<code>S</code> trees), for each <code>(Xi,yi)</code> in the original training set i.e. <code>T</code>, select all <code>Tk</code> which does not include <code>(Xi,yi)</code>. This subset, pay attention, is a set of boostrap datasets which does not contain a particular record from the original dataset. This set is called out-of-bag examples. There are <code>n</code> such subsets (one for each data record in original dataset T). OOB classifier is the aggregation of votes ONLY over <code>Tk</code> such that it does not contain <code>(xi,yi)</code>. </p>\n<p>Out-of-bag estimate for the generalization error is the error rate of the out-of-bag classifier on the training set (compare it with known <code>yi</code>'s).</p>\n<p>Why is it important?</p>\n<blockquote>\n<p>The study of error estimates for bagged classifiers in <a href=\"https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf\" rel=\"noreferrer\">Breiman\n  [1996b]</a>, gives empirical evidence to show that the out-of-bag estimate\n  is as accurate as using a test set of the same size as the training\n  set. Therefore, using the out-of-bag error estimate removes the need\n  for a set aside test set.<sup><a href=\"https://doi.org/10.1023/A:1010933404324.\" rel=\"noreferrer\">1</a></sup></p>\n</blockquote>\n<p>(Thanks @Rudolf for corrections. His comments below.)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In Breiman's original implementation of the random forest algorithm, each tree is trained on about 2/3 of the total training data. As the forest is built, each tree can thus be tested (similar to leave one out cross validation) on the samples not used in building that tree. This is the out of bag error estimate - an internal error estimate of a random forest as it is being constructed. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Does tensorflow have something similar to scikit learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">one hot encoder</a> for processing categorical data?  Would using a placeholder of tf.string behave as categorical data?</p>\n<p>I realize I can manually pre-process the data before sending it to tensorflow, but having it built in is very convenient.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As of TensorFlow 0.8, there is now a <a href=\"https://www.tensorflow.org/api_docs/python/tf/one_hot\" rel=\"noreferrer\">native one-hot op, <code>tf.one_hot</code></a> that can convert a set of sparse labels to a dense one-hot representation.  This is in addition to <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\" rel=\"noreferrer\"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>, which can in some cases let you compute the cross entropy directly on the sparse labels instead of converting them to one-hot.</p>\n<p><strong>Previous answer, in case you want to do it the old way:</strong>\n@Salvador's answer is correct - there (used to be) no native op to do it.  Instead of doing it in numpy, though, you can do it natively in tensorflow using the sparse-to-dense operators:</p>\n<pre><code>num_labels = 10\n\n# label_batch is a tensor of numeric labels to process\n# 0 &lt;= label &lt; num_labels\n\nsparse_labels = tf.reshape(label_batch, [-1, 1])\nderived_size = tf.shape(label_batch)[0]\nindices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\nconcated = tf.concat(1, [indices, sparse_labels])\noutshape = tf.pack([derived_size, num_labels])\nlabels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n</code></pre>\n<p>The output, labels, is a one-hot matrix of batch_size x num_labels.</p>\n<p>Note also that as of 2016-02-12 (which I assume will eventually be part of a 0.7 release), TensorFlow also has the <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> op, which in some cases can let you do training without needing to convert to a one-hot encoding.</p>\n<p>Edited to add:  At the end, you may need to explicitly set the shape of labels.  The shape inference doesn't recognize the size of the num_labels component.  If you don't need a dynamic batch size with derived_size, this can be simplified.</p>\n<p>Edited 2016-02-12 to change the assignment of outshape per comment below.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/one_hot\" rel=\"noreferrer\"><code>tf.one_hot()</code></a> is available in TF and easy to use. </p>\n<p>Lets assume you have 4 possible categories (cat, dog, bird, human) and 2 instances (cat, human). So your <code>depth=4</code> and your <code>indices=[0, 3]</code></p>\n<pre><code>import tensorflow as tf\nres = tf.one_hot(indices=[0, 3], depth=4)\nwith tf.Session() as sess:\n    print sess.run(res)\n</code></pre>\n<p>Keep in mind that if you provide index=-1 you will get all zeros in your one-hot vector.</p>\n<p><strong>Old answer, when this function was not available.</strong></p>\n<p>After looking though the <a href=\"http://tensorflow.org/api_docs/python/index.md\" rel=\"noreferrer\">python documentation</a>, I have not found anything similar. One thing that strengthen my belief that it does not exist is that in <a href=\"https://github.com/tensorflow/tensorflow/blob/1d76583411038767f673a0c96174c80eaf9ff42f/tensorflow/g3doc/tutorials/mnist/input_data.py\" rel=\"noreferrer\">their own example</a> they write <code>one_hot</code> manually.</p>\n<pre><code>def dense_to_one_hot(labels_dense, num_classes=10):\n  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n</code></pre>\n<p>You can also do this in <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\" rel=\"noreferrer\">scikitlearn</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>numpy</code> does it!</p>\n<pre><code>import numpy as np\nnp.eye(n_labels)[target_vector]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using a Naive Bayes Classifier to categorize several thousand documents into 30 different categories. I have implemented a Naive Bayes Classifier, and with some feature selection (mostly filtering useless words), I've gotten about a 30% test accuracy, with 45% training accuracy. This is significantly better than random, but I want it to be better.</p>\n<p>I've tried implementing AdaBoost with NB, but it does not appear to give appreciably better results (the literature seems split on this, some papers say AdaBoost with NB doesn't give better results, others do). Do you know of any other extensions to NB that may possibly give better accuracy?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In my experience, properly trained Naive Bayes classifiers are usually astonishingly accurate (and very fast to train--noticeably faster than any classifier-builder i have everused).</p>\n<p>so when you want to improve classifier prediction, you can look in several places:</p>\n<ul>\n<li><p><em>tune your classifier</em> (adjusting the classifier's tunable paramaters);</p></li>\n<li><p>apply some sort of <em>classifier combination technique</em> (eg,\nensembling, boosting, bagging); or you can</p></li>\n<li><p>look at <em>the data</em> fed to the classifier--either add more data,\nimprove your basic parsing, or refine the features you select from\nthe data.</p></li>\n</ul>\n<p>w/r/t naive Bayesian classifiers, parameter tuning is limited; i recommend to focus on your data--ie, the quality of your pre-processing and the feature selection.\n<br/></p>\n<p><strong>I. Data Parsing (pre-processing)</strong></p>\n<p>i assume your raw data is something like a string of raw text for each data point, which by a series of processing steps you transform each string into a structured vector (1D array) for each data point such that each offset corresponds to one feature (usually a word) and the value in that offset corresponds to frequency. </p>\n<ul>\n<li><p><em>stemming</em>: either manually or by using a stemming library? the popular open-source ones are Porter, Lancaster, and Snowball. So for\ninstance, if you have the terms <em>programmer, program, progamming,\nprogrammed</em> in a given data point, a stemmer will reduce them to a\nsingle stem (probably <em>program</em>) so your term vector for that data\npoint will have a value of 4 for the feature program, which is\nprobably what you want.</p></li>\n<li><p><em>synonym finding</em>: same idea as stemming--fold related words into a single word; so a synonym finder can identify developer, programmer,\ncoder, and software engineer and roll them into a single term</p></li>\n<li><p><em>neutral words</em>: words with similar frequencies across classes make poor features</p></li>\n</ul>\n<p><br/></p>\n<p><strong>II. Feature Selection</strong></p>\n<p>consider a prototypical use case for NBCs: filtering spam; you can quickly see how it fails and just as quickly you can see how to improve it. For instance, above-average spam filters have nuanced features like: frequency of words in all caps, frequency of words in title, and the occurrence of exclamation point in the title. In addition, <em>the best features are often not single words but e.g., pairs of words, or larger word groups</em>.<br/><br/></p>\n<p><strong>III. Specific Classifier Optimizations</strong></p>\n<p>Instead of 30 classes use a <strong>'one-against-many' scheme</strong>--in other words, you begin with a two-class classifier (Class A and 'all else') then the results in the 'all else' class are returned to the algorithm for classification into Class B and 'all else', etc.</p>\n<p><strong>The Fisher Method</strong> (probably the most common way to optimize a Naive Bayes classifier.) To me,\ni think of Fisher as <em>normalizing</em> (more correctly, <em>standardizing</em>) the input probabilities An NBC uses the feature probabilities to construct a 'whole-document' probability. The Fisher Method calculates the probability of a category for <em>each</em> feature of the document then combines these feature probabilities and compares that combined probability with the probability of a random set of features.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would suggest using a <strong>SGDClassifier</strong> as in <a href=\"http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\" rel=\"noreferrer\">this</a> and tune it in terms of regularization strength.</p>\n<p>Also try to tune the formula in TFIDF you're using by tuning the parameters of <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"noreferrer\">TFIFVectorizer</a>.</p>\n<ul>\n<li><p>I usually see that for text classification problems <strong>SVM or Logistic Regressioin</strong> when trained one-versus-all outperforms NB. As you can see in <a href=\"http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf\" rel=\"noreferrer\">this nice article by Stanford people</a> for longer documents SVM outperforms NB. The code for the paper which uses a combination of SVM and NB (<strong>NBSVM</strong>) is <a href=\"https://github.com/sidaw/nbsvm\" rel=\"noreferrer\">here</a>.</p></li>\n<li><p>Second, tune your TFIDF formula (e.g. sublinear tf, smooth_idf).</p></li>\n<li><p><strong>Normalize</strong> your samples with l2 or l1 normalization (default in Tfidfvectorization) because it compensates for different document lengths.</p></li>\n<li><p><strong>Multilayer Perceptron</strong>, usually gets better results than NB or SVM because of the non-linearity introduced which is inherent to many text classification problems. I have implemented a highly parallel one using Theano/Lasagne which is easy to use and downloadable <a href=\"https://github.com/afshinrahimi/sparsemultilayerperceptron\" rel=\"noreferrer\">here</a>.</p></li>\n<li><p>Try to <strong>tune your l1/l2/elasticnet regularization</strong>. It makes a huge difference in SGDClassifier/SVM/Logistic Regression.</p></li>\n<li><p>Try to use <strong>n-grams</strong> which is configurable in tfidfvectorizer.</p></li>\n<li><p>If your documents have structure (e.g. have <strong>titles</strong>) consider using different features for different parts. For example add title_word1 to your document if word1 happens in the title of the document.</p></li>\n<li><p>Consider using the <strong>length of the document</strong> as a feature (e.g. number of words or characters).</p></li>\n<li><p>Consider using <strong>meta information</strong> about the document (e.g. time of creation, author name, url of the document, etc.).</p></li>\n<li><p>Recently <strong>Facebook</strong> published their <a href=\"https://github.com/facebookresearch/fastText\" rel=\"noreferrer\">FastText classification code</a> which performs very well across many tasks, be sure to try it.</p></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is my perceptron implementation in ANSI C:</p>\n<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n\nfloat randomFloat()\n{\n    srand(time(NULL));\n    float r = (float)rand() / (float)RAND_MAX;\n    return r;\n}\n\nint calculateOutput(float weights[], float x, float y)\n{\n    float sum = x * weights[0] + y * weights[1];\n    return (sum &gt;= 0) ? 1 : -1;\n}\n\nint main(int argc, char *argv[])\n{\n    // X, Y coordinates of the training set.\n    float x[208], y[208];\n\n    // Training set outputs.\n    int outputs[208];\n\n    int i = 0; // iterator\n\n    FILE *fp;\n\n    if ((fp = fopen(\"test1.txt\", \"r\")) == NULL)\n    {\n        printf(\"Cannot open file.\\n\");\n    }\n    else\n    {\n        while (fscanf(fp, \"%f %f %d\", &amp;x[i], &amp;y[i], &amp;outputs[i]) != EOF)\n        {\n            if (outputs[i] == 0)\n            {\n                outputs[i] = -1;\n            }\n            printf(\"%f   %f   %d\\n\", x[i], y[i], outputs[i]);\n            i++;\n        }\n    }\n\n    system(\"PAUSE\");\n\n    int patternCount = sizeof(x) / sizeof(int);\n\n    float weights[2];\n    weights[0] = randomFloat();\n    weights[1] = randomFloat();\n\n    float learningRate = 0.1;\n\n    int iteration = 0;\n    float globalError;\n\n    do {\n        globalError = 0;\n        int p = 0; // iterator\n        for (p = 0; p &lt; patternCount; p++)\n        {\n            // Calculate output.\n            int output = calculateOutput(weights, x[p], y[p]);\n\n            // Calculate error.\n            float localError = outputs[p] - output;\n\n            if (localError != 0)\n            {\n                // Update weights.\n                for (i = 0; i &lt; 2; i++)\n                {\n                    float add = learningRate * localError;\n                    if (i == 0)\n                    {\n                        add *= x[p];\n                    }\n                    else if (i == 1)\n                    {\n                        add *= y[p];\n                    }\n                    weights[i] +=  add;\n                }\n            }\n\n            // Convert error to absolute value.\n            globalError += fabs(localError);\n\n            printf(\"Iteration %d Error %.2f %.2f\\n\", iteration, globalError, localError);\n\n            iteration++;\n        }\n\n        system(\"PAUSE\");\n\n    } while (globalError != 0);\n\n    system(\"PAUSE\");\n    return 0;\n}\n</code></pre>\n<p>The training set I'm using: <a href=\"http://neuron-ai.tuke.sk/~vascak/predmety/UI/test1.txt\" rel=\"noreferrer\">Data Set</a></p>\n<p>I have removed all irrelevant code. Basically what it does now it reads <code>test1.txt</code> file and loads values from it to three arrays: <code>x</code>, <code>y</code>, <code>outputs</code>.</p>\n<p>Then there is a <a href=\"http://en.wikipedia.org/wiki/Perceptron#Learning_algorithm\" rel=\"noreferrer\">perceptron learning algorithm</a> which, for some reason, is not converging to 0 (<code>globalError</code> should converge to 0) and therefore I get an infinite do while loop.</p>\n<p>When I use a smaller training set (like 5 points), it works pretty well. Any ideas where could be the problem?</p>\n<p>I wrote this algorithm very similar to this <a href=\"http://dynamicnotions.blogspot.com/2008/09/single-layer-perceptron.html\" rel=\"noreferrer\">C# Perceptron algorithm</a>:</p>\n<hr/>\n<p><strong>EDIT:</strong></p>\n<p>Here is an example with a smaller training set:</p>\n<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n\nfloat randomFloat()\n{\n    float r = (float)rand() / (float)RAND_MAX;\n    return r;\n}\n\nint calculateOutput(float weights[], float x, float y)\n{\n    float sum = x * weights[0] + y * weights[1];\n    return (sum &gt;= 0) ? 1 : -1;\n}\n\nint main(int argc, char *argv[])\n{\n    srand(time(NULL));\n\n    // X coordinates of the training set.\n    float x[] = { -3.2, 1.1, 2.7, -1 };\n\n    // Y coordinates of the training set.\n    float y[] = { 1.5, 3.3, 5.12, 2.1 };\n\n    // The training set outputs.\n    int outputs[] = { 1, -1, -1, 1 };\n\n    int i = 0; // iterator\n\n    FILE *fp;\n\n    system(\"PAUSE\");\n\n    int patternCount = sizeof(x) / sizeof(int);\n\n    float weights[2];\n    weights[0] = randomFloat();\n    weights[1] = randomFloat();\n\n    float learningRate = 0.1;\n\n    int iteration = 0;\n    float globalError;\n\n    do {\n        globalError = 0;\n        int p = 0; // iterator\n        for (p = 0; p &lt; patternCount; p++)\n        {\n            // Calculate output.\n            int output = calculateOutput(weights, x[p], y[p]);\n\n            // Calculate error.\n            float localError = outputs[p] - output;\n\n            if (localError != 0)\n            {\n                // Update weights.\n                for (i = 0; i &lt; 2; i++)\n                {\n                    float add = learningRate * localError;\n                    if (i == 0)\n                    {\n                        add *= x[p];\n                    }\n                    else if (i == 1)\n                    {\n                        add *= y[p];\n                    }\n                    weights[i] +=  add;\n                }\n            }\n\n            // Convert error to absolute value.\n            globalError += fabs(localError);\n\n            printf(\"Iteration %d Error %.2f\\n\", iteration, globalError);          \n        }\n\n        iteration++;\n\n    } while (globalError != 0);\n\n    // Display network generalisation.\n    printf(\"X       Y     Output\\n\");\n    float j, k;\n    for (j = -1; j &lt;= 1; j += .5)\n    {\n        for (j = -1; j &lt;= 1; j += .5)\n        {\n            // Calculate output.\n            int output = calculateOutput(weights, j, k);\n            printf(\"%.2f  %.2f  %s\\n\", j, k, (output == 1) ? \"Blue\" : \"Red\");\n        }\n    }\n\n    // Display modified weights.\n    printf(\"Modified weights: %.2f %.2f\\n\", weights[0], weights[1]);\n\n    system(\"PAUSE\");\n    return 0;\n}\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In your current code, the <a href=\"https://en.wikipedia.org/wiki/Perceptron\" rel=\"noreferrer\">perceptron</a> successfully learns the direction of the decision boundary BUT is unable to <em>translate</em> it.</p>\n<pre>\n    y                              y\n    ^                              ^\n    |  - + \\\\  +                   |  - \\\\ +   +\n    | -    +\\\\ +   +               | -   \\\\  + +   +\n    | - -    \\\\ +                  | - -  \\\\    +\n    | -  -  + \\\\  +                | -  -  \\\\ +   +\n    ---------------------&gt; x       --------------------&gt; x\n        stuck like this            need to get like this\n</pre>\n<p><sup><em>(as someone pointed out, here is a <a href=\"https://stackoverflow.com/a/26607180/97160\">more accurate version</a>)</em></sup></p>\n<p>The problem lies in the fact that your perceptron has no <strong>bias term</strong>, i.e. a third weight component connected to an input of value 1.</p>\n<pre>\n       w0   -----\n    x ----&gt;|     |\n           |  f  |----&gt; output (+1/-1)\n    y ----&gt;|     |\n       w1   -----\n               ^ w2\n    1(bias) ---|\n</pre>\n<p>The following is how I corrected the problem:</p>\n<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;time.h&gt;\n\n#define LEARNING_RATE    0.1\n#define MAX_ITERATION    100\n\nfloat randomFloat()\n{\n    return (float)rand() / (float)RAND_MAX;\n}\n\nint calculateOutput(float weights[], float x, float y)\n{\n    float sum = x * weights[0] + y * weights[1] + weights[2];\n    return (sum &gt;= 0) ? 1 : -1;\n}\n\nint main(int argc, char *argv[])\n{\n    srand(time(NULL));\n\n    float x[208], y[208], weights[3], localError, globalError;\n    int outputs[208], patternCount, i, p, iteration, output;\n\n    FILE *fp;\n    if ((fp = fopen(\"test1.txt\", \"r\")) == NULL) {\n        printf(\"Cannot open file.\\n\");\n        exit(1);\n    }\n\n    i = 0;\n    while (fscanf(fp, \"%f %f %d\", &amp;x[i], &amp;y[i], &amp;outputs[i]) != EOF) {\n        if (outputs[i] == 0) {\n            outputs[i] = -1;\n        }\n        i++;\n    }\n    patternCount = i;\n\n    weights[0] = randomFloat();\n    weights[1] = randomFloat();\n    weights[2] = randomFloat();\n\n    iteration = 0;\n    do {\n        iteration++;\n        globalError = 0;\n        for (p = 0; p &lt; patternCount; p++) {\n            output = calculateOutput(weights, x[p], y[p]);\n\n            localError = outputs[p] - output;\n            weights[0] += LEARNING_RATE * localError * x[p];\n            weights[1] += LEARNING_RATE * localError * y[p];\n            weights[2] += LEARNING_RATE * localError;\n\n            globalError += (localError*localError);\n        }\n\n        /* Root Mean Squared Error */\n        printf(\"Iteration %d : RMSE = %.4f\\n\",\n            iteration, sqrt(globalError/patternCount));\n    } while (globalError &gt; 0 &amp;&amp; iteration &lt;= MAX_ITERATION);\n\n    printf(\"\\nDecision boundary (line) equation: %.2f*x + %.2f*y + %.2f = 0\\n\",\n        weights[0], weights[1], weights[2]);\n\n    return 0;\n}\n</code></pre>\n<p>... with the following output:</p>\n<pre><code>Iteration 1 : RMSE = 0.7206\nIteration 2 : RMSE = 0.5189\nIteration 3 : RMSE = 0.4804\nIteration 4 : RMSE = 0.4804\nIteration 5 : RMSE = 0.3101\nIteration 6 : RMSE = 0.4160\nIteration 7 : RMSE = 0.4599\nIteration 8 : RMSE = 0.3922\nIteration 9 : RMSE = 0.0000\n\nDecision boundary (line) equation: -2.37*x + -2.51*y + -7.55 = 0\n</code></pre>\n<p>And here's a short animation of the code above using MATLAB, showing the <a href=\"https://en.wikipedia.org/wiki/Decision_boundary\" rel=\"noreferrer\">decision boundary</a> at each iteration:</p>\n<p><a href=\"https://i.sstatic.net/eP250.png\" rel=\"noreferrer\"><img alt=\"screenshot\" src=\"https://i.sstatic.net/eP250.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It might help if you put the seeding of the random generator at the start of your main instead of reseeding on every call to <code>randomFloat</code>, i.e.</p>\n<pre><code>float randomFloat()\n{\n    float r = (float)rand() / (float)RAND_MAX;\n    return r;\n}\n\n// ...\n\nint main(int argc, char *argv[])\n{\n    srand(time(NULL));\n\n    // X, Y coordinates of the training set.\n    float x[208], y[208];\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Some small errors I spotted in your source code:</p>\n<pre><code>int patternCount = sizeof(x) / sizeof(int);\n</code></pre>\n<p>Better change this to </p>\n<pre><code>int patternCount = i;\n</code></pre>\n<p>so you doesn't have to rely on your x array to have the right size.</p>\n<p>You increase iterations inside the p loop, whereas the original C# code does this outside the p loop. Better move the printf and the iteration++ outside the p loop before the PAUSE statement - also I'd remove the PAUSE statement or change it to</p>\n<pre><code>if ((iteration % 25) == 0) system(\"PAUSE\");\n</code></pre>\n<p>Even doing all those changes, your program still doesn't terminate using your data set, but the output is more consistent, giving an error oscillating somewhere between 56 and 60.</p>\n<p>The last thing you could try is to test the original C# program on this dataset, if it also doesn't terminate, there's something wrong with the algorithm (because your dataset looks correct, see my visualization comment).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I encounter a <code>RunTimeError</code> while I am trying to run the code in my machine's CPU instead of GPU. The code is originally from this GitHub project - <a href=\"https://github.com/CSAILVision/IBD\" rel=\"noreferrer\">IBD: Interpretable Basis Decomposition for Visual Explanation</a>. This is for a research project. I tried putting the CUDA as <code>false</code> and looked at other solutions on this website. </p>\n<pre><code>GPU = False               # running on GPU is highly suggested\nCLEAN = False             # set to \"True\" if you want to clean the temporary large files after generating result\nAPP = \"classification\"    # Do not change! mode choide: \"classification\", \"imagecap\", \"vqa\". Currently \"imagecap\" and \"vqa\" are not supported.\nCATAGORIES = [\"object\", \"part\"]   # Do not change! concept categories that are chosen to detect: \"object\", \"part\", \"scene\", \"material\", \"texture\", \"color\"\n\nCAM_THRESHOLD = 0.5                 # the threshold used for CAM visualization\nFONT_PATH = \"components/font.ttc\"   # font file path\nFONT_SIZE = 26                      # font size\nSEG_RESOLUTION = 7                  # the resolution of cam map\nBASIS_NUM = 7                       # In decomposition, this is to decide how many concepts are used to interpret the weight vector of a class.\n</code></pre>\n<p>Here is the error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 22, in &lt;module&gt;\n    model = loadmodel()\n  File \"/home/joshuayun/Desktop/IBD/loader/model_loader.py\", line 48, in loadmodel\n    checkpoint = torch.load(settings.MODEL_FILE)\n  File \"/home/joshuayun/.local/lib/python3.6/site-packages/torch/serialization.py\", line 387, in load\n    return _load(f, map_location, pickle_module, **pickle_load_args)\n  File \"/home/joshuayun/.local/lib/python3.6/site-packages/torch/serialization.py\", line 574, in _load\n    result = unpickler.load()\n  File \"/home/joshuayun/.local/lib/python3.6/site-packages/torch/serialization.py\", line 537, in persistent_load\n    deserialized_objects[root_key] = restore_location(obj, location)\n  File \"/home/joshuayun/.local/lib/python3.6/site-packages/torch/serialization.py\", line 119, in default_restore_location\n    result = fn(storage, location)\n  File \"/home/joshuayun/.local/lib/python3.6/site-packages/torch/serialization.py\", line 95, in _cuda_deserialize\n    device = validate_cuda_device(location)\n  File \"/home/joshuayun/.local/lib/python3.6/site-packages/torch/serialization.py\", line 79, in validate_cuda_device\n    raise RuntimeError('Attempting to deserialize object on a CUDA '\nRuntimeError: Attempting to deserialize object on a CUDA device but \n  torch.cuda.is_available() is False. If you are running on a CPU-only machine, \n  please use torch.load with map_location='cpu' to map your storages to the CPU.\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you don't have gpu then use <strong>map_location=torch.device('cpu')</strong> with load model.load()    </p>\n<pre><code>my_model = net.load_state_dict(torch.load('classifier.pt', map_location=torch.device('cpu')))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just giving a smaller answer. To solve this, you could change the parameters of the function named <code>load()</code> in the <code>serialization.py</code> file. This is stored in: <code>./site-package/torch/serialization.py</code></p>\n<p>Write:</p>\n<pre><code>def load(f, map_location='cpu', pickle_module=pickle, **pickle_load_args):\n</code></pre>\n<p>instead of:</p>\n<pre><code>def load(f, map_location=None, pickle_module=pickle, **pickle_load_args):\n</code></pre>\n<p>Hope it helps.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>\"If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\"</p>\n<pre><code>model = torch.load('model/pytorch_resnet50.pth',map_location ='cpu')\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between <code>categorical_accuracy</code> and <code>sparse_categorical_accuracy</code> in Keras? There is no hint in the <a href=\"https://keras.io/metrics/\" rel=\"noreferrer\">documentation for these metrics</a>, and by asking Dr. Google, I did not find answers for that either.</p>\n<p>The source code can be found <a href=\"https://github.com/fchollet/keras/blob/master/keras/metrics.py\" rel=\"noreferrer\">here</a>:</p>\n<pre><code>def categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n                          K.argmax(y_pred, axis=-1)),\n                  K.floatx())\n\n\ndef sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                  K.floatx())\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So in <code>categorical_accuracy</code> you need to specify your target (<code>y</code>) as one-hot encoded vector (e.g. in case of 3 classes, when a true class is second class, <code>y</code> should be <code>(0, 1, 0)</code>. In <code>sparse_categorical_accuracy</code> you need should only provide an integer of the true class (in the case from previous example - it would be <code>1</code> as classes indexing is <code>0</code>-based).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Looking at the <a href=\"https://github.com/fchollet/keras/blob/0bc8fac4463c68faa3b3c415c26eab02aa361fd5/keras/metrics.py#L24\" rel=\"noreferrer\">source</a> </p>\n<pre><code>def categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n                          K.argmax(y_pred, axis=-1)),\n                  K.floatx())\n\n\ndef sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\nK.floatx())\n</code></pre>\n<p><code>categorical_accuracy</code> checks to see if the <em>index</em> of the maximal true value is equal to the <em>index</em> of the maximal predicted value.</p>\n<p><code>sparse_categorical_accuracy</code> checks to see if the maximal true value is equal to the <em>index</em> of the maximal predicted value.</p>\n<p>From Marcin's answer above the <code>categorical_accuracy</code> corresponds to a <code>one-hot</code> encoded vector for <code>y_true</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>sparse_categorical_accuracy</code> expects <strong>sparse targets</strong>:</p>\n<pre><code>[[0], [1], [2]]\n</code></pre>\n<p>For instance:</p>\n<pre><code>import tensorflow as tf\n\nsparse = [[0], [1], [2]]\nlogits = [[.8, .1, .1], [.5, .3, .2], [.2, .2, .6]]\n\nsparse_cat_acc = tf.metrics.SparseCategoricalAccuracy()\nsparse_cat_acc(sparse, logits)\n</code></pre>\n<pre><code>&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.6666666666666666&gt;\n</code></pre>\n<hr/>\n<p><code>categorical_accuracy</code> expects <strong>one hot encoded targets</strong>:</p>\n<pre><code>[[1., 0., 0.],  [0., 1., 0.], [0., 0., 1.]]\n</code></pre>\n<p>For instance:</p>\n<pre><code>onehot = [[1., 0., 0.],  [0., 1., 0.], [0., 0., 1.]]\nlogits = [[.8, .1, .1], [.5, .3, .2], [.2, .2, .6]]\n\ncat_acc = tf.metrics.CategoricalAccuracy()\ncat_acc(sparse, logits)\n</code></pre>\n<pre><code>&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.6666666666666666&gt;\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Given a linearly separable dataset, is it necessarily better to use a a hard margin SVM over a soft-margin SVM?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would expect soft-margin SVM to be better even when training dataset is linearly separable. The reason is that in a hard-margin SVM, a single outlier can determine the boundary, which makes the classifier overly sensitive to noise in the data.</p>\n<p>In the diagram below, a single red outlier essentially determines the boundary, which is the hallmark of overfitting</p>\n<p><a href=\"https://i.sstatic.net/p8mA3.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/p8mA3.png\"/></a></p>\n<p>To get a sense of what soft-margin SVM is doing, it's better to look at it in the dual formulation, where you can see that it has the same margin-maximizing objective (margin could be negative) as the hard-margin SVM, but with an additional constraint that each lagrange multiplier associated with support vector is bounded by C. Essentially this bounds the influence of any single point on the decision boundary, for derivation, see Proposition 6.12 in Cristianini/Shaw-Taylor's \"An Introduction to Support Vector Machines and Other Kernel-based Learning Methods\".</p>\n<p>The result is that soft-margin SVM could choose decision boundary that has non-zero training error even if dataset is linearly separable, and is less likely to overfit.</p>\n<p>Here's an example using libSVM on a synthetic problem. Circled points show support vectors. You can see that decreasing C causes classifier to sacrifice linear separability in order to gain stability, in a sense that influence of any single datapoint is now bounded by C.</p>\n<p><a href=\"https://i.sstatic.net/0aYO8.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0aYO8.png\"/></a></p>\n<p>Meaning of support vectors:</p>\n<p>For hard margin SVM, support vectors are the points which are \"on the margin\". In the picture above, C=1000 is pretty close to hard-margin SVM, and you can see the circled points are the ones that will touch the margin (margin is almost 0 in that picture, so it's essentially the same as the separating hyperplane)</p>\n<p>For soft-margin SVM, it's easer to explain them in terms of dual variables. Your support vector predictor in terms of dual variables is the following function.</p>\n<p><a href=\"https://i.sstatic.net/wzgIb.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/wzgIb.png\"/></a></p>\n<p>Here, alphas and b are parameters that are found during training procedure, xi's, yi's are your training set and x is the new datapoint. Support vectors are datapoints from training set which are are included in the predictor, ie, the ones with non-zero alpha parameter.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In my opinion, Hard Margin SVM overfits to a particular dataset and thus can not generalize. Even in a linearly separable dataset (as shown in the above diagram), outliers well within the boundaries can influence the margin. Soft Margin SVM has more versatility because we have control over choosing the support vectors by tweaking the C.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/36515202/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-08-31 11:51:45Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/36515202/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Although both of the above methods provide a better score for the better closeness of prediction, still cross-entropy is preferred. Is it in every case or there are some peculiar scenarios where we prefer cross-entropy over MSE? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross-entropy is prefered for <strong>classification</strong>, while mean squared error is one of the best choices for <strong>regression</strong>. This comes directly from the statement of the problems itself - in classification you work with very particular set of possible output values thus MSE is badly defined (as it does not have this kind of knowledge thus penalizes errors in incompatible way).  To better understand the phenomena it is good to follow and understand the relations between</p>\n<ol>\n<li>cross entropy </li>\n<li>logistic regression (binary cross entropy)</li>\n<li>linear regression (MSE)</li>\n</ol>\n<p>You will notice that both can be seen as a maximum likelihood estimators, simply with different assumptions about the dependent variable.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you derive the cost function from the aspect of probability and distribution, you can observe that MSE happens when you assume the error follows Normal Distribution and cross entropy when you assume binomial distribution. It means that implicitly when you use MSE, you are doing regression (estimation) and when you use CE, you are doing classification. Hope it helps a little bit. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you do logistic regression for example, you will use the sigmoid function to estimate the probability, the cross entropy as the loss function and gradient descent to minimize it. Doing this but using MSE as the loss function might lead to a non-convex problem where you might find local minima. Using cross entropy will lead to a convex problem where you might find the optimum solution.</p>\n<p><a href=\"https://www.youtube.com/watch?v=rtD0RvfBJqQ&amp;list=PL0Smm0jPm9WcCsYvbhPCdizqNKps69W4Z&amp;index=35\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=rtD0RvfBJqQ&amp;list=PL0Smm0jPm9WcCsYvbhPCdizqNKps69W4Z&amp;index=35</a></p>\n<p>There is also an interesting analysis here:\n<a href=\"https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/\" rel=\"nofollow noreferrer\">https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis.</p>\n<p>I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>import boto3\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nbucket='my-bucket'\ndata_key = 'train.csv'\ndata_location = 's3://{}/{}'.format(bucket, data_key)\n\npd.read_csv(data_location)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the simplest case you don't need <code>boto3</code>, because you just <strong>read</strong> resources.<br/>\nThen it's even simpler:</p>\n<pre><code>import pandas as pd\n\nbucket='my-bucket'\ndata_key = 'train.csv'\ndata_location = 's3://{}/{}'.format(bucket, data_key)\n\npd.read_csv(data_location)\n</code></pre>\n<p>But as Prateek stated make sure to configure your SageMaker notebook instance to have access to s3. This is done at configuration step in Permissions &gt; IAM role</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you have a look <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf\" rel=\"noreferrer\">here</a> it seems you can specify this in the <em>InputDataConfig</em>. Search for \"S3DataSource\" (<a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html\" rel=\"noreferrer\">ref</a>) in the document. The first hit is even in Python, on page 25/26.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a set of dataframes where one of the columns contains a categorical variable. I'd like to convert it to several dummy variables, in which case I'd normally use <code>get_dummies</code>.</p>\n<p>What happens is that <code>get_dummies</code> looks at the data available in each dataframe to find out how many categories there are, and thus create the appropriate number of dummy variables. However, in the problem I'm working right now, I actually know in advance what the possible categories are. But when looking at each dataframe individually, not all categories necessarily appear.</p>\n<p>My question is: is there a way to pass to <code>get_dummies</code> (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s?</p>\n<p>Something that would make this:</p>\n<pre><code>categories = ['a', 'b', 'c']\n\n   cat\n1   a\n2   b\n3   a\n</code></pre>\n<p>Become this:</p>\n<pre><code>  cat_a  cat_b  cat_c\n1   1      0      0\n2   0      1      0\n3   1      0      0\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>TL;DR</strong>:</p>\n<pre><code>pd.get_dummies(cat.astype(pd.CategoricalDtype(categories=categories)))\n</code></pre>\n<ul>\n<li>Older pandas: <code>pd.get_dummies(cat.astype('category', categories=categories))</code></li>\n</ul>\n<blockquote>\n<p>is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s?</p>\n</blockquote>\n<p>Yes, there is! Pandas has a special type of Series just for <a href=\"http://pandas.pydata.org/pandas-docs/stable/categorical.html\" rel=\"nofollow noreferrer\">categorical data</a>. One of the attributes of this series is the possible categories, which <code>get_dummies</code> takes into account. Here's an example:</p>\n<pre><code>In [1]: import pandas as pd\n\nIn [2]: possible_categories = list('abc')\n\nIn [3]: dtype = pd.CategoricalDtype(categories=possible_categories)\n\nIn [4]: cat = pd.Series(list('aba'), dtype=dtype)\nIn [5]: cat\nOut[5]: \n0    a\n1    b\n2    a\ndtype: category\nCategories (3, object): [a, b, c]\n</code></pre>\n<p>Then, <code>get_dummies</code> will do exactly what you want!</p>\n<pre><code>In [6]: pd.get_dummies(cat)\nOut[6]: \n   a  b  c\n0  1  0  0\n1  0  1  0\n2  1  0  0\n</code></pre>\n<p>There are a bunch of other ways to create a categorical <code>Series</code> or <code>DataFrame</code>, this is just the one I find most convenient. You can read about all of them in <a href=\"http://pandas.pydata.org/pandas-docs/stable/categorical.html\" rel=\"nofollow noreferrer\">the pandas documentation</a>.</p>\n<p><strong>EDIT:</strong></p>\n<p>I haven't followed the exact versioning, but there was a <a href=\"https://github.com/pydata/pandas/issues/10627\" rel=\"nofollow noreferrer\">bug</a> in how pandas treats sparse matrices, at least until version 0.17.0. It was corrected by version 0.18.1 (released May 2016).</p>\n<p>For version 0.17.0, if you try to do this with the <code>sparse=True</code> option with a <code>DataFrame</code>, the column of zeros for the missing dummy variable will be a column of <code>NaN</code>, and it will be converted to dense.</p>\n<p>It looks like pandas 0.21.0 added a <code>CategoricalDType</code>, and creating categoricals which explicitly include the categories as in the original answer was deprecated, I'm not quite sure when.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using transpose and reindex</p>\n<pre><code>import pandas as pd\n\ncats = ['a', 'b', 'c']\ndf = pd.DataFrame({'cat': ['a', 'b', 'a']})\n\ndummies = pd.get_dummies(df, prefix='', prefix_sep='')\ndummies = dummies.T.reindex(cats).T.fillna(0)\n\nprint dummies\n\n    a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  1.0  0.0  0.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I did ask this on the pandas github. Turns out it is really easy to get around it when you define the column as a <code>Categorical</code> where you define all the possible categories.</p>\n<pre><code>df['col'] = pd.Categorical(df['col'], categories=['a', 'b', 'c', 'd'])\n</code></pre>\n<p><code>get_dummies()</code> will do the rest then as expected.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to train a network with an unbalanced data. I have A (198 samples), B (436 samples), C (710 samples), D (272 samples) and I have read about the \"weighted_cross_entropy_with_logits\" but all the examples I found are for binary classification so I'm not very confident in how to set those weights.</p>\n<p>Total samples: 1616</p>\n<p>A_weight: 198/1616 = 0.12?</p>\n<p>The idea behind, if I understood, is to penalize the errors of the majority class and value more positively the hits in the minority one, right?</p>\n<p>My piece of code:</p>\n<pre><code>weights = tf.constant([0.12, 0.26, 0.43, 0.17])\ncost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=pred, targets=y, pos_weight=weights))\n</code></pre>\n<p>I have read <a href=\"https://stackoverflow.com/questions/40698709/tensorflow-interpretation-of-weight-in-weighted-cross-entropy\">this one</a> and others examples with binary classification but still not very clear.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Note that <code>weighted_cross_entropy_with_logits</code> is the weighted variant of <code>sigmoid_cross_entropy_with_logits</code>. Sigmoid cross entropy is typically used for <em>binary</em> classification. Yes, it can handle multiple labels, but sigmoid cross entropy basically makes a (binary) decision on each of them -- for example, for a face recognition net, those (not mutually exclusive) labels could be \"<em>Does the subject wear glasses?</em>\", \"<em>Is the subject female?</em>\", etc.</p>\n<p>In binary classification(s), each output channel corresponds to a binary (soft) decision. Therefore, the weighting needs to happen within the computation of the loss. This is what <code>weighted_cross_entropy_with_logits</code> does, by weighting one term of the cross-entropy over the other.</p>\n<p>In mutually exclusive multilabel classification, we use <code>softmax_cross_entropy_with_logits</code>, which behaves differently: each output channel corresponds to the score of a class candidate. The decision comes <em>after</em>, by comparing the respective outputs of each channel.</p>\n<p>Weighting in before the final decision is therefore a simple matter of modifying the scores before comparing them, typically by multiplication with weights. For example, for a ternary classification task,</p>\n<pre><code># your class weights\nclass_weights = tf.constant([[1.0, 2.0, 3.0]])\n# deduce weights for batch samples based on their true label\nweights = tf.reduce_sum(class_weights * onehot_labels, axis=1)\n# compute your (unweighted) softmax cross entropy loss\nunweighted_losses = tf.nn.softmax_cross_entropy_with_logits(onehot_labels, logits)\n# apply the weights, relying on broadcasting of the multiplication\nweighted_losses = unweighted_losses * weights\n# reduce the result to get your final loss\nloss = tf.reduce_mean(weighted_losses)\n</code></pre>\n<p>You could also rely on <code>tf.losses.softmax_cross_entropy</code> to handle the last three steps.</p>\n<p>In your case, where you need to tackle data imbalance, the class weights could indeed be inversely proportional to their frequency in your train data. Normalizing them so that they sum up to one or to the number of classes also makes sense.</p>\n<p>Note that in the above, we penalized the loss based on the true label of the samples. We could also have penalized the loss based on the <em>estimated</em> labels by simply defining</p>\n<pre><code>weights = class_weights\n</code></pre>\n<p>and the rest of the code need not change thanks to broadcasting magic.</p>\n<p>In the general case, you would want weights that depend on the kind of error you make. In other words, for each pair of labels <code>X</code> and <code>Y</code>, you could choose how to penalize choosing label <code>X</code> when the true label is <code>Y</code>. You end up with a whole prior weight matrix, which results in <code>weights</code> above being a full <code>(num_samples, num_classes)</code> tensor. This goes a bit beyond what you want, but it might be useful to know nonetheless that only your definition of the weight tensor need to change in the code above.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>See <a href=\"https://stackoverflow.com/a/46984951/2379009\">this answer</a> for an alternate solution which works with sparse_softmax_cross_entropy:</p>\n<pre><code>import  tensorflow as tf\nimport numpy as np\n\nnp.random.seed(123)\nsess = tf.InteractiveSession()\n\n# let's say we have the logits and labels of a batch of size 6 with 5 classes\nlogits = tf.constant(np.random.randint(0, 10, 30).reshape(6, 5), dtype=tf.float32)\nlabels = tf.constant(np.random.randint(0, 5, 6), dtype=tf.int32)\n\n# specify some class weightings\nclass_weights = tf.constant([0.3, 0.1, 0.2, 0.3, 0.1])\n\n# specify the weights for each sample in the batch (without having to compute the onehot label matrix)\nweights = tf.gather(class_weights, labels)\n\n# compute the loss\ntf.losses.sparse_softmax_cross_entropy(labels, logits, weights).eval()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Tensorflow 2.0 Compatible Answer</strong>: Migrating the Code specified in P-Gn's Answer to 2.0, for the benefit of the community.</p>\n<pre><code># your class weights\nclass_weights = tf.compat.v2.constant([[1.0, 2.0, 3.0]])\n# deduce weights for batch samples based on their true label\nweights = tf.compat.v2.reduce_sum(class_weights * onehot_labels, axis=1)\n# compute your (unweighted) softmax cross entropy loss\nunweighted_losses = tf.compat.v2.nn.softmax_cross_entropy_with_logits(onehot_labels, logits)\n# apply the weights, relying on broadcasting of the multiplication\nweighted_losses = unweighted_losses * weights\n# reduce the result to get your final loss\nloss = tf.reduce_mean(weighted_losses)\n</code></pre>\n<p>For more information about migration of code from Tensorflow Version 1.x to 2.x, please refer this <a href=\"https://www.tensorflow.org/guide/migrate\" rel=\"nofollow noreferrer\">Migration Guide</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm hoping to use either Haskell or OCaml on a new project because R is too slow.  I need to be able to use support vectory machines, ideally separating out each execution to run in parallel.  I want to use a functional language and I have the feeling that these two are the best so far as performance and elegance are concerned (I like Clojure, but it wasn't as fast in a short test).  I am leaning towards OCaml because there appears to be more support for integration with other languages so it could be a better fit in the long run (e.g. <a href=\"http://home.gna.org/ocaml-r/\" rel=\"noreferrer\">OCaml-R</a>).</p>\n<p>Does anyone know of a good tutorial for this kind of analysis, or a code example, in either Haskell or OCaml? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://hal3.name/software.html\" rel=\"noreferrer\">Hal Daume</a> has written several major machine learning algorithms during his Ph.D. (now he is an assistant professor and rising star in machine learning community)</p>\n<p>On his web page, there are a SVM, a simple decision tree and a logistic regression all in OCaml. By reading these code, you can have a feeling how machine learning models are implemented in OCaml. </p>\n<p>Another good example of writing basic machine learning models is <a href=\"https://github.com/owlbarn/owl\" rel=\"noreferrer\">Owl library</a> for scientific and numeric computations in OCaml.</p>\n<p>I'd also like to mention F#, a new .Net language similar to OCaml. Here's <a href=\"http://blogs.technet.com/apg/archive/2008/04/05/trueskill-through-time.aspx\" rel=\"noreferrer\">a factor graph model</a> written in F# analyzing Chess play data. This research also has a NIPS publication. </p>\n<p>While FP is suitable for implementing machine learning and data mining models. But what you can get here most is NOT performance. It is right that FP supports parallel computing better than imperative languages, like C# or Java. But implementing a parallel SVM, or decision tree, has very little relation to do with the language! Parallel is parallel. The numerical optimizations behind machine learning and data mining are usually imperative, writing them pure-functionally is usually hard and less efficient. Making these sophisticated algorithms parallel is very hard task in the algorithm level, not in the language level. If you want to run 100 SVM in parallel, FP helps here. But I don't see the difficulty running 100 libsvm parallel in C++, not to consider that the single thread libsvm is more efficient than a not-well-tested haskell svm package. </p>\n<p>Then what do FP languages, like F#, OCaml, Haskell, give? </p>\n<ol>\n<li><p>Easy to test your code. FP languages usually have a top-level interpreter, you can test your functions on the fly. </p></li>\n<li><p>Few mutable states. This means that passing the same parameter to a function, this function always gives the same result, thus debugging is easy in FPs.</p></li>\n<li><p>Code is succinct. Type inference, pattern matching, closures, etc. You focus more on the domain logic, and less on the language part. So when you write the code, your mind is mainly thinking about the programming logic itself. </p></li>\n<li><p>Writing code in FPs is fun.</p></li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The only problem I can see is that OCaml doesn't really support multicore parallelism, while GHC has excellent support and performance. If you're looking to use multiple threads of execution, on multiple calls, GHC Haskell will be a lot easier.</p>\n<p>Secondly, the Haskell FFI is more powerful (that is, it does more with less code) than OCaml's, and more libraries are avaliable (via Hackage: <a href=\"http://hackage.haskell.org\" rel=\"noreferrer\">http://hackage.haskell.org</a> ) so I don't think foreign interfaces will be a deciding factor. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As far as multi-language integration goes, combining C and Haskell is remarkably easy, and I say this as someone who is (unlike <strong>dons</strong>) not really much of an expert on either. Any other language that integrates well with C shouldn't be much trickier; you can always fall back to a thin interface layer in C if nothing else. For better or worse, C is still the <em>lingua franca</em> of programming, so Haskell is more than acceptable for most cases.</p>\n<p>...but. You say you're motivated by performance issues, and want to use \"a functional language\". From this I infer you're not previously familiar with the languages you ask about. Among Haskell's defining features are that it, by default, uses <em>non-strict evaluation</em> and <em>immutable data structures</em>--which are both incredibly useful in many ways, but it also means that optimizing Haskell for performance is often dramatically different from other languages, and well-honed instincts may lead you astray in baffling ways. You may want to browse <a href=\"http://www.haskell.org/haskellwiki/Performance\" rel=\"noreferrer\">performance-related topics on the Haskell wiki</a> to get a feel for the issues.</p>\n<p>Which isn't to say that you can't do what you want in Haskell--you certainly can. Both laziness and immutability can in fact be exploited for performance benefits (<a href=\"http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf\" rel=\"noreferrer\">Chris Okasaki's thesis</a> provides some nice examples). But be aware that there'll be a bit of a learning curve when it comes to dealing with performance. </p>\n<p>Both Haskell and OCaml provide the lovely benefits of using an ML-family language, but for most programmers, OCaml is likely to offer a gentler learning curve and better immediate results.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using TfidfVectorizer from scikit-learn to do some feature extraction from text data. I have a CSV file with a Score (can be +1 or -1) and a Review (text). I pulled this data into a DataFrame so I can run the Vectorizer.</p>\n<p>This is my code: </p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndf = pd.read_csv(\"train_new.csv\",\n             names = ['Score', 'Review'], sep=',')\n\n# x = df['Review'] == np.nan\n#\n# print x.to_csv(path='FindNaN.csv', sep=',', na_rep = 'string', index=True)\n#\n# print df.isnull().values.any()\n\nv = TfidfVectorizer(decode_error='replace', encoding='utf-8')\nx = v.fit_transform(df['Review'])\n</code></pre>\n<p>This is the traceback for the error I get: </p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/PycharmProjects/Review/src/feature_extraction.py\", line 16, in &lt;module&gt;\nx = v.fit_transform(df['Review'])\n File \"/home/b/hw1/local/lib/python2.7/site-   packages/sklearn/feature_extraction/text.py\", line 1305, in fit_transform\n   X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n File \"/home/b/work/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py\", line 817, in fit_transform\nself.fixed_vocabulary_)\n File \"/home/b/work/local/lib/python2.7/site- packages/sklearn/feature_extraction/text.py\", line 752, in _count_vocab\n   for feature in analyze(doc):\n File \"/home/b/work/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py\", line 238, in &lt;lambda&gt;\ntokenize(preprocess(self.decode(doc))), stop_words)\n File \"/home/b/work/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py\", line 118, in decode\n raise ValueError(\"np.nan is an invalid document, expected byte or \"\n ValueError: np.nan is an invalid document, expected byte or unicode string.\n</code></pre>\n<p>I checked the CSV file and DataFrame for anything that's being read as NaN but I can't find anything. There are 18000 rows, none of which return <code>isnan</code> as True. </p>\n<p>This is what <code>df['Review'].head()</code> looks like: </p>\n<pre><code>  0    This book is such a life saver.  It has been s...\n  1    I bought this a few times for my older son and...\n  2    This is great for basics, but I wish the space...\n  3    This book is perfect!  I'm a first time new mo...\n  4    During your postpartum stay at the hospital th...\n  Name: Review, dtype: object\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to convert the dtype <code>object</code> to <code>unicode</code> string as is clearly mentioned in the traceback.</p>\n<pre><code>x = v.fit_transform(df['Review'].values.astype('U'))  ## Even astype(str) would work\n</code></pre>\n<p>From the Doc page of TFIDF Vectorizer:</p>\n<blockquote>\n<p>fit_transform(raw_documents, y=None) <br/></p>\n<p>Parameters:     raw_documents : iterable <br/>\nan iterable which yields either <em>str</em>, <em>unicode</em> or <em>file objects</em></p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I find a more efficient way to solve this problem.</p>\n<pre><code>x = v.fit_transform(df['Review'].apply(lambda x: np.str_(x)))\n</code></pre>\n<p>Of course you can use <code>df['Review'].values.astype('U')</code> to convert the entire Series. But I found using this function will consume much more memory if the Series you want to convert is really big. (I test this with a Series with 800k rows of data, and doing this <code>astype('U')</code> will consume about 96GB of memory)</p>\n<p>Instead, if you use the lambda expression to only convert the data in the Series from <code>str</code> to <code>numpy.str_</code>, which the result will also be accepted by the <code>fit_transform</code> function, this will be faster and will not increase the memory usage.</p>\n<p>I'm not sure why this will work because in the Doc page of TFIDF Vectorizer:</p>\n<blockquote>\n<p>fit_transform(raw_documents, y=None)</p>\n<p>Parameters: raw_documents : iterable</p>\n<p>an iterable which yields either str, unicode or file objects</p>\n</blockquote>\n<p>But actually this iterable must yields <code>np.str_</code> instead of <code>str</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was getting MemoryError even after using <code>.values.astype('U')</code> for the reviews in my dataset. </p>\n<p>So i tried <code>.astype('U').values</code> and it worked. </p>\n<p>This is a answer from: <a href=\"https://stackoverflow.com/questions/49957069/python-how-to-avoid-memoryerror-when-transform-text-data-into-unicode-using-ast\">Python: how to avoid MemoryError when transform text data into Unicode using astype('U')</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/45704226/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2017-08-25 16:03:42Z\">7 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/45704226/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Could you please explain what the \"fit\" method in scikit-learn does? Why is it useful?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>In a nutshell</strong>: <em>fitting</em> is equal to <em>training</em>. Then, after it is trained, the model can be used to make predictions, usually with a <code>.predict()</code> method call.</p>\n<p><strong>To elaborate</strong>: Fitting your model to (i.e. using the <code>.fit()</code> method on) the training data is essentially the training part of the modeling process. It finds the coefficients for the equation specified via the algorithm being used (take for example <a href=\"https://stackoverflow.com/users/826970/umutto\">umutto's</a> linear regression example, above).</p>\n<p>Then, for a classifier, you can classify incoming data points (from a test set, or otherwise) using the <code>predict</code> method. Or, in the case of regression, your model will interpolate/extrapolate when <code>predict</code> is used on incoming data points.</p>\n<p>It also should be noted that sometimes the \"fit\" nomenclature is used for non-machine-learning methods, such as scalers and other preprocessing steps. In this case, you are merely \"applying\" the specified function to your data, as in the case with a min-max scaler, TF-IDF, or other transformation.</p>\n<p>Note: here are a couple of references...</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/34727919/fit-method-in-python-sklearn\">fit method in python sklearn</a></li>\n<li><a href=\"http://scikit-learn.org/stable/tutorial/basic/tutorial.html\" rel=\"noreferrer\">http://scikit-learn.org/stable/tutorial/basic/tutorial.html</a></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently trying to understand the architecture behind the <em>word2vec</em> neural net learning algorithm, for representing words as vectors based on their context.</p>\n<p>After reading <a href=\"http://arxiv.org/pdf/1301.3781v3.pdf\" rel=\"noreferrer\">Tomas Mikolov paper</a> I came across what he defines as a <strong>projection layer</strong>. Even though this term is widely used when referred to <em>word2vec</em>, I couldn't find a precise definition of what it actually is in the neural net context.</p>\n<p><a href=\"https://i.sstatic.net/W46yb.png\" rel=\"noreferrer\"><img alt=\"Word2Vec Neural Net architecture\" src=\"https://i.sstatic.net/W46yb.png\"/></a></p>\n<p>My question is, in the neural net context, what is a projection layer? Is it the name given to a hidden layer whose links to previous nodes share the same weights? Do its units actually have an activation function of some kind?</p>\n<p>￼Another resource that also refers more broadly to the problem can be found in <a href=\"http://www.coling-2014.org/COLING%202014%20Tutorial-fix%20-%20Tomas%20Mikolov.pdf\" rel=\"noreferrer\">this tutorial</a>, which also refers to a <em>projection layer</em> around page 67.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I find the previous answers here a bit overcomplicated - a projection layer is just a simple matrix multiplication, or in the context of NN, a regular/dense/linear layer, without the non-linear activation in the end (sigmoid/tanh/relu/etc.) The idea is to <strong>project</strong> the (e.g.) 100K-dimensions discrete vector into a 600-dimensions continuous vector (I chose the numbers here randomly, \"your mileage may vary\"). The exact matrix parameters are learned through the training process.</p>\n<p>What happens before/after already depends on the model and context, and is not what OP asks.</p>\n<p>(In <a href=\"https://youtu.be/VkjSaOZSZVs\" rel=\"noreferrer\">practice</a> you wouldn't even bother with the matrix multiplication (as you are multiplying a 1-hot vector which has 1 for the word index and 0's everywhere else), and would treat the trained matrix as a lookout table (i.e. the 6257th word in the corpus = the 6257th row/column (depends how you define it) in the projection matrix).)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>The projection layer maps the discrete word indices of an n-gram context to a continuous vector space.</p>\n</blockquote>\n<p>As explained in this <a href=\"http://mi.eng.cam.ac.uk/%7Ewjb31/ppubs/gwbthesis2010.pdf\" rel=\"noreferrer\">thesis</a></p>\n<blockquote>\n<p>The projection layer is shared such that for contexts containing the same word multiple times, the same set of weights is applied to form each part of the projection vector.\nThis organization effectively increases the amount of data available for training the projection layer weights since each word of each context training pattern individually contributes changes to the weight values.</p>\n</blockquote>\n<p><a href=\"https://i.sstatic.net/qEuy5.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/qEuy5.png\"/></a></p>\n<p>this figure shows the trivial topology how the output of the projection layer can be efficiently assembled by copying columns from the projection layer weights matrix.</p>\n<p>Now, the Hidden layer:</p>\n<blockquote>\n<p>The hidden layer processes the output of the projection layer and is also created with a\nnumber of neurons specified in the topology configuration file.</p>\n</blockquote>\n<p><em>Edit</em>: An explanation of what is happening in the diagram</p>\n<blockquote>\n<p>Each neuron in the projection layer is represented by a number of weights equal to the size of the vocabulary. The projection layer differs from the hidden and output layers by not using a non-linear activation function. Its purpose is simply to provide an efficient means of projecting the given n- gram context onto a reduced continuous vector space for subsequent processing by hidden and output layers trained to classify such vectors. Given the one-or-zero nature of the input vector elements, the output for a particular word with index i is simply the ith column of the trained matrix of projection layer weights (where each row of the matrix represents the weights of a single neuron).</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>continuous bag of words</code> is used to predict a single word given its prior and future entries: thus it is a contextual result. </p>\n<p>The inputs are the computed weights from the prior and future entries: and all are given new weights identically: thus the complexity / features count of this model is much smaller than many other NN architectures.</p>\n<p>RE: <code>what is the projection layer</code>: from the paper you cited</p>\n<blockquote>\n<p>the non-linear hidden layer is removed and the projection layer is\n  shared for all words (not just the projection matrix); thus, all words\n  get projected into the same position (their vectors are averaged).</p>\n</blockquote>\n<p>So the projection layer is a single set of <code>shared weights</code> and no activation function is indicated.</p>\n<blockquote>\n<p>Note that the weight matrix between the input and the projection layer\n  is shared for all word positions in the same way as in the NNLM</p>\n</blockquote>\n<p>So the <code>hidden layer</code> is in fact represented by this single set of shared weights - as you correctly implied that is identical across all of the input nodes.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I know the basics of feedforward neural networks, and how to train them using the backpropagation algorithm, but I'm looking for an algorithm than I can use for training an ANN online with reinforcement learning.</p>\n<p>For example, the <a href=\"http://www.google.com/search?q=cart%20pole%20swing%20up\" rel=\"noreferrer\">cart pole swing up</a> problem is one I'd like to solve with an ANN. In that case, I don't know what should be done to control the pendulum, I only know how close I am to the ideal position. I need to have the ANN learn based on reward and punishment. Thus, supervised learning isn't an option.</p>\n<p>Another situation is something like the <a href=\"http://en.wikipedia.org/wiki/Snake_%28video_game%29\" rel=\"noreferrer\">snake game</a>, where feedback is delayed, and limited to goals and anti-goals, rather than reward.</p>\n<p>I can think of some algorithms for the first situation, like hill-climbing or genetic algorithms, but I'm guessing they would both be slow. They might also be applicable in the second scenario, but incredibly slow, and not conducive to online learning.</p>\n<p>My question is simple: <strong>Is there a simple algorithm for training an artificial neural network with reinforcement learning?</strong> I'm mainly interested in real-time reward situations, but if an algorithm for goal-based situations is available, even better.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are some research papers on the topic:</p>\n<ul>\n<li><a href=\"http://nn.cs.utexas.edu/downloads/papers/stanley.gecco02_1.pdf\" rel=\"noreferrer\">Efficient Reinforcement Learning Through Evolving Neural Network Topologies (2002)</a> </li>\n<li><a href=\"http://www.remi-coulom.fr/Thesis/\" rel=\"noreferrer\">Reinforcement Learning Using Neural Networks, with Applications to Motor Control</a></li>\n<li><a href=\"http://www.ice.ci.ritsumei.ac.jp/~ruck/CLASSES/INTELISYS/NN-Q.pdf\" rel=\"noreferrer\">Reinforcement Learning Neural Network To The Problem Of Autonomous Mobile Robot Obstacle Avoidance</a></li>\n</ul>\n<p>And some code:</p>\n<ul>\n<li><a href=\"http://www.cs.colostate.edu/~anderson/code/\" rel=\"noreferrer\">Code examples</a> for neural network reinforcement learning.</li>\n</ul>\n<p>Those are just some of the top google search results on the topic. The first couple of papers look like they're pretty good, although I haven't read them personally. I think you'll find even more information on neural networks with reinforcement learning if you do a quick search on Google Scholar.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If the output that lead to a reward <code>r</code> is backpropagated into the network <code>r</code> times, you will reinforce the network proportionally to the reward. This is not directly applicable to negative rewards, but I can think of two solutions that will produce different effects: </p>\n<p>1) If you have a set of rewards in a range rmin-rmax, rescale them to <code>0-(rmax-rmin)</code> so that they are all non-negative. The bigger the reward, the stronger the reinforcement that is created.</p>\n<p>2) For a negative reward <code>-r</code>, backpropagate a random output <code>r</code> times, as long as it's different from the one that lead to the negative reward. This will not only reinforce desirable outputs, but also diffuses or avoids bad outputs.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am playing with a ANN which is part of Udacity DeepLearning course.</p>\n<p>I have an assignment which involves introducing generalization to the network with one hidden ReLU layer using L2 loss. I wonder how to properly introduce it so that ALL weights are penalized, not only weights of the output layer.</p>\n<p>Code for network <em>without</em> generalization is at the bottom of the post (code to actually run the training is out of the scope of the question).</p>\n<p>Obvious way of introducing the L2 is to replace the loss calculation with something like this (if beta is 0.01):</p>\n<pre><code>loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(out_layer, tf_train_labels) + 0.01*tf.nn.l2_loss(out_weights))\n</code></pre>\n<p>But in such case it will take into account values of output layer's weights. I am not sure, how do we properly penalize the weights which come INTO the hidden ReLU layer. Is it needed at all or introducing penalization of output layer will somehow keep the hidden weights in check also?</p>\n<pre><code>#some importing\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import cPickle as pickle\nfrom six.moves import range\n\n#loading data\npickle_file = '/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCode/tensorflow/examples/udacity/notMNIST.pickle'\n\nwith open(pickle_file, 'rb') as f:\n  save = pickle.load(f)\n  train_dataset = save['train_dataset']\n  train_labels = save['train_labels']\n  valid_dataset = save['valid_dataset']\n  valid_labels = save['valid_labels']\n  test_dataset = save['test_dataset']\n  test_labels = save['test_labels']\n  del save  # hint to help gc free up memory\n  print('Training set', train_dataset.shape, train_labels.shape)\n  print('Validation set', valid_dataset.shape, valid_labels.shape)\n  print('Test set', test_dataset.shape, test_labels.shape)\n\n\n#prepare data to have right format for tensorflow\n#i.e. data is flat matrix, labels are onehot\n\nimage_size = 28\nnum_labels = 10\n\ndef reformat(dataset, labels):\n  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n  return dataset, labels\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint('Training set', train_dataset.shape, train_labels.shape)\nprint('Validation set', valid_dataset.shape, valid_labels.shape)\nprint('Test set', test_dataset.shape, test_labels.shape)\n\n\n#now is the interesting part - we are building a network with\n#one hidden ReLU layer and out usual output linear layer\n\n#we are going to use SGD so here is our size of batch\nbatch_size = 128\n\n#building tensorflow graph\ngraph = tf.Graph()\nwith graph.as_default():\n      # Input data. For the training data, we use a placeholder that will be fed\n  # at run time with a training minibatch.\n  tf_train_dataset = tf.placeholder(tf.float32,\n                                    shape=(batch_size, image_size * image_size))\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n\n  #now let's build our new hidden layer\n  #that's how many hidden neurons we want\n  num_hidden_neurons = 1024\n  #its weights\n  hidden_weights = tf.Variable(\n    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))\n  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))\n\n  #now the layer itself. It multiplies data by weights, adds biases\n  #and takes ReLU over result\n  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n\n  #time to go for output linear layer\n  #out weights connect hidden neurons to output labels\n  #biases are added to output labels  \n  out_weights = tf.Variable(\n    tf.truncated_normal([num_hidden_neurons, num_labels]))  \n\n  out_biases = tf.Variable(tf.zeros([num_labels]))  \n\n  #compute output  \n  out_layer = tf.matmul(hidden_layer,out_weights) + out_biases\n  #our real output is a softmax of prior result\n  #and we also compute its cross-entropy to get our loss\n  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(out_layer, tf_train_labels))\n\n  #now we just minimize this loss to actually train the network\n  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n  #nice, now let's calculate the predictions on each dataset for evaluating the\n  #performance so far\n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(out_layer)\n  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) \n\n  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)\n  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A shorter and scalable way of doing this would be ;</p>\n<pre><code>vars   = tf.trainable_variables() \nlossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\n</code></pre>\n<p>This basically sums the l2_loss of all your trainable variables. You could also make a dictionary where you specify only the variables you want to add to your cost and use the second line above. Then you can add lossL2 with your softmax cross entropy value in order to calculate your total loss. </p>\n<p><strong>Edit</strong> : As mentioned by Piotr Dabkowski, <em>the code above will also regularise biases</em>. This can be avoided by adding an if statement in the second line ; </p>\n<pre><code>lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars\n                    if 'bias' not in v.name ]) * 0.001\n</code></pre>\n<p>This can be used to exclude other variables. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>hidden_weights</code>, <code>hidden_biases</code>, <code>out_weights</code>, and <code>out_biases</code> are all the model parameters that you are creating. You can add L2 regularization to ALL these parameters as follows :</p>\n<pre><code>loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=out_layer, labels=tf_train_labels)) +\n    0.01*tf.nn.l2_loss(hidden_weights) +\n    0.01*tf.nn.l2_loss(hidden_biases) +\n    0.01*tf.nn.l2_loss(out_weights) +\n    0.01*tf.nn.l2_loss(out_biases))\n</code></pre>\n<p>With the note of @Keight Johnson, to not regularize the bias:</p>\n<pre><code>loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=out_layer, labels=tf_train_labels)) +\n    0.01*tf.nn.l2_loss(hidden_weights) +\n    0.01*tf.nn.l2_loss(out_weights) +\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In fact, we usually do not regularize bias terms (intercepts). \nSo, I go for:</p>\n<pre><code>loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=out_layer, labels=tf_train_labels)) +\n    0.01*tf.nn.l2_loss(hidden_weights) +\n    0.01*tf.nn.l2_loss(out_weights))\n</code></pre>\n<p>By penalizing the intercept term, as the intercept is added to  y values, it will result in changing the y values, adding a constant c to the intercepts.  Having it or not will not change the results but takes some computations</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have the below F1 and AUC scores for 2 different cases</p>\n<blockquote>\n<p>Model 1: Precision: 85.11 Recall: 99.04 F1: 91.55 AUC: 69.94</p>\n<p>Model 2: Precision: 85.1 Recall: 98.73 F1: 91.41 AUC: 71.69</p>\n</blockquote>\n<p>The main motive of my problem to predict the positive cases correctly,ie, reduce the False Negative cases (FN). Should I use F1 score and choose Model 1 or use AUC and choose Model 2. Thanks </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Introduction</h2>\n<p>As a rule of thumb, every time you want to compare <strong>ROC AUC</strong> vs <strong>F1 Score</strong>, think about it as if you are comparing your model performance based on:</p>\n<pre><code>[Sensitivity vs (1-Specificity)] VS [Precision vs Recall]\n</code></pre>\n<p><strong>Note that Sensitivity is the Recall (they are the same exact metric).</strong></p>\n<p>Now we need to understand what are: Specificity, Precision and Recall (Sensitivity) <em>intuitively</em>!</p>\n<hr/>\n<h2>Background</h2>\n<p><strong>Specificity:</strong> is given by the following formula:</p>\n<p><a href=\"https://i.sstatic.net/TE01E.png\" rel=\"noreferrer\"><img alt=\"specificity formula\" src=\"https://i.sstatic.net/TE01E.png\"/></a></p>\n<p>Intuitively speaking, if we have 100% specific model, that means it did <strong>NOT</strong> miss any True Negative, in other words, there were <strong>NO</strong> False Positives (<em>i.e. negative result that is falsely labeled as positive</em>). Yet, there is a risk of having a lot of False Negatives!</p>\n<p><strong>Precision:</strong> is given by the following formula:\n<a href=\"https://i.sstatic.net/bSmbY.png\" rel=\"noreferrer\"><img alt=\"Precision Formula\" src=\"https://i.sstatic.net/bSmbY.png\"/></a></p>\n<p>Intuitively speaking, if we have a 100% precise model, that means it <strong>could catch all</strong> True positive but there were <strong>NO</strong> False Positive.</p>\n<p><strong>Recall:</strong> is given by the following formula:</p>\n<p><a href=\"https://i.sstatic.net/J6EUS.png\" rel=\"noreferrer\"><img alt=\"Recall Formula\" src=\"https://i.sstatic.net/J6EUS.png\"/></a></p>\n<p>Intuitively speaking, if we have a 100% recall model, that means it did <strong>NOT</strong> miss any True Positive, in other words, there were <strong>NO</strong> False Negatives (<em>i.e. a positive result that is falsely labeled as negative</em>). Yet, there is a risk of having a lot of False Positives!</p>\n<p>As you can see, the three concepts are very close to each other!</p>\n<p><a href=\"https://i.sstatic.net/qP0LO.png\" rel=\"noreferrer\"><img alt=\"f1 score\" src=\"https://i.sstatic.net/qP0LO.png\"/></a></p>\n<hr/>\n<p><strong>As a rule of thumb,</strong> if the cost of having False negative is high, we want to increase the model sensitivity and recall (<em>which are the exact same in regard to their formula</em>)!.</p>\n<p>For instance, in fraud detection or sick patient detection, we don't want to label/predict a fraudulent transaction (True Positive) as non-fraudulent (False  Negative). Also, we don't want to label/predict a contagious sick patient (True Positive) as not sick (False Negative).</p>\n<p>This is because the consequences will be worse than a False Positive (incorrectly labeling a a harmless transaction as fraudulent or a non-contagious patient as contagious).</p>\n<p>On the other hand, if the cost of having False Positive is high, then we want to increase the model specificity and precision!.</p>\n<p>For instance, in email spam detection, we don't want to label/predict a non-spam email (True Negative) as spam (False Positive). On the other hand, failing to label a spam email as spam (False Negative) is less costly.</p>\n<hr/>\n<h2>F1 Score</h2>\n<p>It's given by the following formula:</p>\n<p><a href=\"https://i.sstatic.net/8uiwI.png\" rel=\"noreferrer\"><img alt=\"F1 Score Formula\" src=\"https://i.sstatic.net/8uiwI.png\"/></a></p>\n<p>F1 Score keeps a <strong>balance</strong> between Precision and Recall. We use it if there is uneven class distribution, as precision and recall may give misleading results!</p>\n<p>So we use F1 Score as a comparison indicator between Precision and Recall Numbers!</p>\n<hr/>\n<h2>Area Under the Receiver Operating Characteristic curve (AUROC)</h2>\n<p>It compares the Sensitivity vs (1-Specificity), in other words, compare the True Positive Rate vs False Positive Rate.</p>\n<p><a href=\"https://i.sstatic.net/BeUi6.png\" rel=\"noreferrer\"><img alt=\"area under the curve\" src=\"https://i.sstatic.net/BeUi6.png\"/></a></p>\n<p>So, the bigger the AUROC, the greater the distinction between True Positives and True Negatives!</p>\n<hr/>\n<h2>AUROC vs F1 Score (Conclusion)</h2>\n<p>In general, the ROC is for many different levels of thresholds and thus it has many F score values. F1 score is applicable for any particular point on the ROC curve.</p>\n<p>You may think of it as a measure of precision and recall at a particular threshold value whereas AUC is the area under the ROC curve. For F score to be high, both precision and recall should be high.</p>\n<p><strong>Consequently</strong>,  when you have a data <strong>imbalance</strong> between positive and negative samples, you should always use F1-score because ROC <strong>averages</strong> over all possible thresholds!</p>\n<hr/>\n<p>Further read:</p>\n<p><a href=\"https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve\" rel=\"noreferrer\">Credit Card Fraud: Handling highly imbalance classes and why Receiver Operating Characteristics Curve (ROC Curve) should not be used, and Precision/Recall curve should be preferred in highly imbalanced situations</a></p>\n<hr/>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you look at the definitions, you can that both AUC and F1-score optimize \"something\" together with the fraction of the sample labeled \"positive\" that is actually true positive.</p>\n<p>This \"something\" is:</p>\n<ul>\n<li>For the AUC, the specificity, which is the fraction of the negatively labeled sample that is correctly labeled. You're not looking at the fraction of your positively labeled samples that is correctly labeled. </li>\n<li>Using the F1 score, it's precision: the fraction of the positively labeled sample that is correctly labeled. And using the F1-score you don't consider the purity of the sample labeled as negative (the specificity).</li>\n</ul>\n<p>The difference becomes important when you have highly unbalanced or skewed classes: For example there are many more true negatives than true positives.</p>\n<p>Suppose you are looking at data from the general population to find people with a rare disease. There are far more people \"negative\" than \"positive\", and trying to optimize how well you are doing on the positive and the negative samples simultaneously, using AUC, is not optimal. You want the positive sample to include all positives if possible and you don't want it to be huge, due to a high false positive rate. So in this case you use the F1 score.</p>\n<p>Conversely if both classes make up 50% of your dataset, or both make up a sizable fraction, and you care about your performance in identifying each class equally, then you should use the AUC, which optimizes for both classes, positive and negative.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>just adding my 2 cents here:</p>\n<p>AUC does an implicit weighting of the samples, which F1 does not.</p>\n<p>In my last use case comparing the effectiveness of drugs on patients, it's easy to learn which drugs are generally strong, and which are weak. The big question is whether you can hit the outliers (the few positives for a weak drug or the few negatives for a strong drug). To answer that, you have to specifically weigh the outliers up using F1, which you don't need to do with AUC.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am applying transfer-learning on a pre-trained network using the GPU version of keras. I don't understand how to define the parameters <strong><code>max_queue_size</code></strong>, <strong><code>workers</code></strong>, and <strong><code>use_multiprocessing</code></strong>. If I change these parameters (primarily to speed-up learning), I am unsure whether all data is still seen per epoch.</p>\n<p><strong><code>max_queue_size</code></strong>:</p>\n<ul>\n<li><p>maximum size of the internal training queue which is used to \"precache\" samples from the generator </p></li>\n<li><p><em>Question:</em> Does this refer to how many batches are prepared on CPU? How is it related to <code>workers</code>? How to define it optimally?</p></li>\n</ul>\n<p><strong><code>workers</code></strong>: </p>\n<ul>\n<li><p>number of threads generating batches in parallel. Batches are computed in parallel on the CPU and passed on the fly onto the GPU for neural network computations </p></li>\n<li><p><em>Question:</em> How do I find out how many batches my CPU can/should generate in parallel?</p></li>\n</ul>\n<p><strong><code>use_multiprocessing</code></strong>: </p>\n<ul>\n<li><p>whether to use process-based threading</p></li>\n<li><p><em>Question:</em> Do I have to set this parameter to true if I change <code>workers</code>? Does it relate to CPU usage?</p></li>\n</ul>\n<p><strong>Related questions</strong> can be found here:</p>\n<ul>\n<li><a href=\"https://github.com/keras-team/keras/issues/8540\" rel=\"noreferrer\">Detailed explanation of model.fit_generator() parameters: queue size, workers and use_multiprocessing</a></li>\n<li><p><a href=\"https://stackoverflow.com/questions/51790943/what-does-worker-mean-in-fit-generator-in-keras\">What does worker mean in fit_generator in Keras?</a></p></li>\n<li><p><a href=\"https://stackoverflow.com/questions/36986815/what-is-the-parameter-max-q-size-used-for-in-model-fit-generator/36989864#36989864\">What is the parameter “max_q_size” used for in “model.fit_generator”?</a></p></li>\n<li><p><a href=\"https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\" rel=\"noreferrer\">A detailed example of how to use data generators with Keras</a>.</p></li>\n</ul>\n<p>I am using <code>fit_generator()</code> as follows:</p>\n<pre><code>    history = model.fit_generator(generator=trainGenerator,\n                                  steps_per_epoch=trainGenerator.samples//nBatches,     # total number of steps (batches of samples)\n                                  epochs=nEpochs,                   # number of epochs to train the model\n                                  verbose=2,                        # verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n                                  callbacks=callback,               # keras.callbacks.Callback instances to apply during training\n                                  validation_data=valGenerator,     # generator or tuple on which to evaluate the loss and any model metrics at the end of each epoch\n                                  validation_steps=\n                                  valGenerator.samples//nBatches,   # number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch\n                                  class_weight=classWeights,                # optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function\n                                  max_queue_size=10,                # maximum size for the generator queue\n                                  workers=1,                        # maximum number of processes to spin up when using process-based threading\n                                  use_multiprocessing=False,        # whether to use process-based threading\n                                  shuffle=True,                     # whether to shuffle the order of the batches at the beginning of each epoch\n                                  initial_epoch=0)   \n</code></pre>\n<p>The specs of my machine are:</p>\n<pre><code>CPU : 2xXeon E5-2260 2.6 GHz\nCores: 10\nGraphic card: Titan X, Maxwell, GM200\nRAM: 128 GB\nHDD: 4TB\nSSD: 512 GB\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Q_0: </p>\n<blockquote>\n<p>Question: Does this refer to how many batches are prepared on CPU? How is it related to workers? How to define it optimally?</p>\n</blockquote>\n<p>From the <a href=\"https://stackoverflow.com/questions/36986815/what-is-the-parameter-max-q-size-used-for-in-model-fit-generator/36989864#36989864\">link</a> you posted, you can learn that your CPU keeps creating batches until the queue is at the maximum queue size or reaches the stop. You want to have batches ready for your GPU to \"take\" so that the GPU doesn't have to wait for the CPU. \nAn ideal value for the queue size would be to make it large enough that your GPU is always running near the maximum and never has to wait for the CPU to prepare new batches. </p>\n<p>Q_1:</p>\n<blockquote>\n<p>Question: How do I find out how many batches my CPU can/should generate in parallel?</p>\n</blockquote>\n<p>If you see that your GPU is idling and waiting for batches, try to increase the amount of workers and perhaps also the queue size.</p>\n<p>Q_2:</p>\n<blockquote>\n<p>Do I have to set this parameter to true if I change workers? Does it relate to CPU usage?</p>\n</blockquote>\n<p><a href=\"https://keunwoochoi.wordpress.com/2017/08/24/tip-fit_generator-in-keras-how-to-parallelise-correctly/\" rel=\"noreferrer\">Here</a> is a practical analysis of what happens when you set it to <code>True</code> or <code>False</code>. <a href=\"https://stackoverflow.com/questions/54620551/confusion-about-multiprocessing-and-workers-in-keras-fit-generator-with-window\">Here</a> is a recommendation to set it to <code>False</code> to prevent freezing (in my setup <code>True</code> works fine without freezing). Perhaps someone else can increase our understanding of the topic.</p>\n<h3>In summary:</h3>\n<p>Try not to have a sequential setup, try to enable the CPU to provide enough data for the GPU.\n<img alt=\"\" src=\"https://www.embedded-vision.com/sites/default/files/technical-articles/OpenCLGPUs/Figure1.jpg\"/></p>\n<p>Also: You could (should?) create several questions the next time, so that it is easier to answer them.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross entropy formula:</p>\n<p><a href=\"https://i.sstatic.net/W3xm0.gif\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/W3xm0.gif\"/></a></p>\n<p>But why does the following give <code>loss = 0.7437</code> instead of <code>loss = 0</code> (since <code>1*log(1) = 0</code>)?</p>\n<pre class=\"lang-python prettyprint-override\"><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\noutput = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)\ntarget = Variable(torch.LongTensor([3]))\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(output, target)\nprint(loss) # 0.7437\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In your example you are treating output <code>[0, 0, 0, 1]</code> as probabilities as required by the mathematical definition of cross entropy.  But PyTorch treats them as outputs, that don’t need to sum to <code>1</code>, and need to be first converted into probabilities for which it uses the softmax function.</p>\n<p>So <code>H(p, q)</code> becomes:</p>\n<pre><code>H(p, softmax(output))\n</code></pre>\n<p>Translating the output <code>[0, 0, 0, 1]</code> into probabilities:</p>\n<pre><code>softmax([0, 0, 0, 1]) = [0.1749, 0.1749, 0.1749, 0.4754]\n</code></pre>\n<p>whence:</p>\n<pre><code>-log(0.4754) = 0.7437\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your understanding is correct but pytorch doesn't compute <a href=\"http://pytorch.org/docs/0.3.1/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\" rel=\"noreferrer\"><strong>cross entropy</strong></a> in that way. Pytorch uses the following formula.</p>\n<pre><code>loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n               = -x[class] + log(\\sum_j exp(x[j]))\n</code></pre>\n<p>Since, in your scenario, <code>x = [0, 0, 0, 1]</code> and <code>class = 3</code>, if you evaluate the above expression, you would get:</p>\n<pre><code>loss(x, class) = -1 + log(exp(0) + exp(0) + exp(0) + exp(1))\n               = 0.7437\n</code></pre>\n<p>Pytorch considers natural logarithm.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to add an important note, as this often leads to confusion.</p>\n<p><strong>Softmax is not a loss function</strong>, nor is it really an activation function. It has a very specific task: It is used for multi-class classification to normalize the scores for the given classes. By doing so we get probabilities for each class that sum up to <strong>1</strong>.</p>\n<p><strong>Softmax is combined with Cross-Entropy-Loss</strong> to calculate the loss of a model.</p>\n<p>Unfortunately, because this combination is so common, it is often abbreviated. Some are using the term <strong>Softmax-Loss</strong>, whereas PyTorch calls it only <strong>Cross-Entropy-Loss</strong>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While training a tensorflow seq2seq model I see the following messages :</p>\n<pre>\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 27282 get requests, put_count=9311 evicted_count=1000 eviction_rate=0.1074 and unsatisfied allocation rate=0.699032\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 100 to 110\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 13715 get requests, put_count=14458 evicted_count=10000 eviction_rate=0.691659 and unsatisfied allocation rate=0.675684\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 110 to 121\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6965 get requests, put_count=6813 evicted_count=5000 eviction_rate=0.733891 and unsatisfied allocation rate=0.741421\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 133 to 146\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 44 get requests, put_count=9058 evicted_count=9000 eviction_rate=0.993597 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 46 get requests, put_count=9062 evicted_count=9000 eviction_rate=0.993158 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1029 evicted_count=1000 eviction_rate=0.971817 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 2 get requests, put_count=1030 evicted_count=1000 eviction_rate=0.970874 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 44 get requests, put_count=6074 evicted_count=6000 eviction_rate=0.987817 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 12 get requests, put_count=6045 evicted_count=6000 eviction_rate=0.992556 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 2 get requests, put_count=1042 evicted_count=1000 eviction_rate=0.959693 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 44 get requests, put_count=6093 evicted_count=6000 eviction_rate=0.984737 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1069 evicted_count=1000 eviction_rate=0.935454 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 17722 get requests, put_count=9036 evicted_count=1000 eviction_rate=0.110668 and unsatisfied allocation rate=0.550615\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 792 to 871\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6 get requests, put_count=1093 evicted_count=1000 eviction_rate=0.914913 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6 get requests, put_count=1101 evicted_count=1000 eviction_rate=0.908265 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 3224 get requests, put_count=4684 evicted_count=2000 eviction_rate=0.426985 and unsatisfied allocation rate=0.200062\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 1158 to 1273\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 17794 get requests, put_count=17842 evicted_count=9000 eviction_rate=0.504428 and unsatisfied allocation rate=0.510228\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 1400 to 1540\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 31 get requests, put_count=1185 evicted_count=1000 eviction_rate=0.843882 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 40 get requests, put_count=8209 evicted_count=8000 eviction_rate=0.97454 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=2272 evicted_count=2000 eviction_rate=0.880282 and unsatisfied allocation rate=-nan\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=2362 evicted_count=2000 eviction_rate=0.84674 and unsatisfied allocation rate=-nan\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 38 get requests, put_count=5436 evicted_count=5000 eviction_rate=0.919794 and unsatisfied allocation rate=0\n\n</pre>\n<p>What does it mean , does it mean I am having some resource allocation issues? Am running on Titan X 3500+ CUDA ,12 GB GPU</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>TensorFlow has multiple memory allocators, for memory that will be used in different ways.  Their behavior has some adaptive aspects.</p>\n<p>In your particular case, since you're using a GPU, there is a PoolAllocator for CPU memory that is pre-registered with the GPU for fast DMA.  A tensor that is expected to be transferred from CPU to GPU, e.g., will be allocated from this pool.</p>\n<p>The PoolAllocators attempt to amortize the cost of calling a more expensive underlying allocator by keeping around a pool of allocated then freed chunks that are eligible for immediate reuse.  Their default behavior is to grow slowly until the eviction rate drops below some constant.  (The eviction rate is the proportion of free calls where we return an unused chunk from the pool to the underlying pool in order not to exceed the size limit.)  In the log messages above, you see \"Raising pool_size_limit_\" lines that show the pool size growing.  Assuming that your program actually has a steady state behavior with a maximum size collection of chunks it needs, the pool will grow to accommodate it, and then grow no more.  It behaves this way rather than simply retaining all chunks ever allocated so that sizes needed only rarely, or only during program startup, are less likely to be retained in the pool.</p>\n<p>These messages should only be a cause for concern if you run out of memory.  In such a case the log messages may help diagnose the problem.  Note also that peak execution speed may only be attained after the memory pools have grown to the proper size.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I cannot get a satisfying answer to this question. As I understand it, TensorFlow is a library for numerical computations, often used in deep learning applications, and Scikit-learn is a framework for general machine learning. </p>\n<p>But what is the exact difference between them, what is the purpose and function of TensorFlow? Can I use them together, and does it make any sense?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The Tensorflow is a library for constructing Neural Networks. The scikit-learn contains ready to use algorithms. The TF can work with a variety of data types: tabular, text, images, audio. The scikit-learn is intended to work with tabular data.</p>\n<p>Yes, you can use both packages. But if you need only classic Multi-Layer implementation then the <code>MLPClassifier</code> and <code>MLPRegressor</code> available in scikit-learn is a very good choice. I have run a comparison of MLP implemented in TF vs Scikit-learn and there weren't significant differences and scikit-learn MLP works about 2 times faster than TF on CPU. You can read the details of the comparison in <a href=\"https://mljar.com/blog/tensorflow-vs-scikit-learn/\" rel=\"noreferrer\">my blog post</a>.</p>\n<p>Below the scatter plots of performance comparison:</p>\n<p><a href=\"https://i.sstatic.net/54VVq.png\" rel=\"noreferrer\"><img alt=\"Tensorflow vs Scikit-learn on classification task\" src=\"https://i.sstatic.net/54VVq.png\"/></a></p>\n<p><a href=\"https://i.sstatic.net/7zJGr.png\" rel=\"noreferrer\"><img alt=\"Tensorflow vs Scikit-learn on regression task\" src=\"https://i.sstatic.net/7zJGr.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Both are 3rd party machine learning modules, and both are good at it.\nTensorflow is the more popular of the two.</p>\n<p>Tensorflow is typically used more in Deep Learning and Neural Networks.</p>\n<p>SciKit learn is more general Machine Learning.</p>\n<p>And although I don't think I've come across anyone using both simultaneously, no one is saying you can't.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Scikit learn or more generally if you use in code as sklearn is a machine learning library that comes with out of the box models. You can use these models in your projects if you know how to use them and what models you will need to fulfil your needs.</p>\n<p>This is more of a usage for Data Scientists and Machine Learning users who want to use already pre built models from the library, like Decision Trees or Random Forest Algorithms.</p>\n<p>You can just import the Built in Models and use them in code.\nSklearn is much more easier to use and is also a popular library for quick to implement ML solutions.</p>\n<p>However, Tensorflow is more of a machine learning / deep learning library, where you kind of actually make the entire model by yourself, from scratch using tensors.\nFrom scratch as in, you make the model's architecture and provide its parameters like:</p>\n<ol>\n<li>How many hidden layers will be there</li>\n<li>How many neurons will be there, from 1 to 10 to 1000 or even more in each layer.</li>\n<li>What are the input values and output values, their matrix sizes.</li>\n<li>What sort of learning rule it will use, the metrics for analysis and evaluating your model.</li>\n<li>Neural Networks evaluation, experimentation and then porting them for other usages.</li>\n</ol>\n<p>You basically design your own neural network, which is either a basic one or a deep neural network depending on how complex it is.</p>\n<p>Tensorflow gives you full control of your ML model as well, for proper visualization and seeing the architecture of your model as well (this is what I love about it).</p>\n<p>On a nutshell, sklearn is more popular for data scientists while Tensorflow (along with PyTorch) is more popular among ML engineers or deep learning engineers or ML experts.</p>\n<p>Edit. PyTorch is becoming more common due to its ability to run the training over the GPU(just any GPU with CUDA support or AMD GPU) without any need for manual configuration or installations of CuDNNs or CUDA toolkit. The PyTorch installation already comes with all these things. If you are interested in Deep Learning, PyTorch is a very good platform to go for.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>def gradient(X_norm,y,theta,alpha,m,n,num_it):\n    temp=np.array(np.zeros_like(theta,float))\n    for i in range(0,num_it):\n        h=np.dot(X_norm,theta)\n        #temp[j]=theta[j]-(alpha/m)*(  np.sum( (h-y)*X_norm[:,j][np.newaxis,:] )  )\n        temp[0]=theta[0]-(alpha/m)*(np.sum(h-y))\n        temp[1]=theta[1]-(alpha/m)*(np.sum((h-y)*X_norm[:,1]))\n        theta=temp\n    return theta\n\n\n\nX_norm,mean,std=featureScale(X)\n#length of X (number of rows)\nm=len(X)\nX_norm=np.array([np.ones(m),X_norm])\nn,m=np.shape(X_norm)\nnum_it=1500\nalpha=0.01\ntheta=np.zeros(n,float)[:,np.newaxis]\nX_norm=X_norm.transpose()\ntheta=gradient(X_norm,y,theta,alpha,m,n,num_it)\nprint theta\n</code></pre>\n<p>My theta from the above code is <code>100.2 100.2</code>, but it should be <code>100.2 61.09</code> in matlab which is correct.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think your code is a bit too complicated and it needs more structure, because otherwise you'll be lost in all equations and operations. In the end this regression boils down to four operations:</p>\n<ol>\n<li>Calculate the hypothesis h = X * theta</li>\n<li>Calculate the loss = h - y and maybe the squared cost (loss^2)/2m</li>\n<li>Calculate the gradient = X' * loss / m</li>\n<li>Update the parameters theta = theta - alpha * gradient</li>\n</ol>\n<p>In your case, I guess you have confused <code>m</code> with <code>n</code>. Here <code>m</code> denotes the number of examples in your training set, not the number of features.</p>\n<p>Let's have a look at my variation of your code:</p>\n<pre><code>import numpy as np\nimport random\n\n# m denotes the number of examples here, not the number of features\ndef gradientDescent(x, y, theta, alpha, m, numIterations):\n    xTrans = x.transpose()\n    for i in range(0, numIterations):\n        hypothesis = np.dot(x, theta)\n        loss = hypothesis - y\n        # avg cost per example (the 2 in 2*m doesn't really matter here.\n        # But to be consistent with the gradient, I include it)\n        cost = np.sum(loss ** 2) / (2 * m)\n        print(\"Iteration %d | Cost: %f\" % (i, cost))\n        # avg gradient per example\n        gradient = np.dot(xTrans, loss) / m\n        # update\n        theta = theta - alpha * gradient\n    return theta\n\n\ndef genData(numPoints, bias, variance):\n    x = np.zeros(shape=(numPoints, 2))\n    y = np.zeros(shape=numPoints)\n    # basically a straight line\n    for i in range(0, numPoints):\n        # bias feature\n        x[i][0] = 1\n        x[i][1] = i\n        # our target variable\n        y[i] = (i + bias) + random.uniform(0, 1) * variance\n    return x, y\n\n# gen 100 points with a bias of 25 and 10 variance as a bit of noise\nx, y = genData(100, 25, 10)\nm, n = np.shape(x)\nnumIterations= 100000\nalpha = 0.0005\ntheta = np.ones(n)\ntheta = gradientDescent(x, y, theta, alpha, m, numIterations)\nprint(theta)\n</code></pre>\n<p>At first I create a small random dataset which should look like this:</p>\n<p><img alt=\"Linear Regression\" src=\"https://i.sstatic.net/xCyZn.png\"/></p>\n<p>As you can see I also added the generated regression line and formula that was calculated by excel.</p>\n<p>You need to take care about the intuition of the regression using gradient descent. As you do a complete batch pass over your data X, you need to reduce the m-losses of every example to a single weight update. In this case, this is the average of the sum over the gradients, thus the division by <code>m</code>. </p>\n<p>The next thing you need to take care about is to track the convergence and adjust the learning rate. For that matter you should always track your cost every iteration, maybe even plot it.</p>\n<p>If you run my example, the theta returned will look like this:</p>\n<pre><code>Iteration 99997 | Cost: 47883.706462\nIteration 99998 | Cost: 47883.706462\nIteration 99999 | Cost: 47883.706462\n[ 29.25567368   1.01108458]\n</code></pre>\n<p>Which is actually quite close to the equation that was calculated by excel (y = x + 30). Note that as we passed the bias into the first column, the first theta value denotes the bias weight.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Below you can find my implementation of gradient descent for linear regression problem. </p>\n<p>At first, you calculate gradient like <code>X.T * (X * w - y) / N</code> and update your current theta with this gradient simultaneously. </p>\n<ul>\n<li>X: feature matrix </li>\n<li>y: target values </li>\n<li>w: weights/values </li>\n<li>N: size of training set</li>\n</ul>\n<p>Here is the python code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport random\n\ndef generateSample(N, variance=100):\n    X = np.matrix(range(N)).T + 1\n    Y = np.matrix([random.random() * variance + i * 10 + 900 for i in range(len(X))]).T\n    return X, Y\n\ndef fitModel_gradient(x, y):\n    N = len(x)\n    w = np.zeros((x.shape[1], 1))\n    eta = 0.0001\n\n    maxIteration = 100000\n    for i in range(maxIteration):\n        error = x * w - y\n        gradient = x.T * error / N\n        w = w - eta * gradient\n    return w\n\ndef plotModel(x, y, w):\n    plt.plot(x[:,1], y, \"x\")\n    plt.plot(x[:,1], x * w, \"r-\")\n    plt.show()\n\ndef test(N, variance, modelFunction):\n    X, Y = generateSample(N, variance)\n    X = np.hstack([np.matrix(np.ones(len(X))).T, X])\n    w = modelFunction(X, Y)\n    plotModel(X, Y, w)\n\n\ntest(50, 600, fitModel_gradient)\ntest(50, 1000, fitModel_gradient)\ntest(100, 200, fitModel_gradient)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/rJhFs.png\" rel=\"noreferrer\"><img alt=\"test1\" src=\"https://i.sstatic.net/rJhFs.png\"/></a>\n<a href=\"https://i.sstatic.net/iXWTq.png\" rel=\"noreferrer\"><img alt=\"test2\" src=\"https://i.sstatic.net/iXWTq.png\"/></a>\n<a href=\"https://i.sstatic.net/aRP0S.png\" rel=\"noreferrer\"><img alt=\"test2\" src=\"https://i.sstatic.net/aRP0S.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Most of these answers are missing out some explanation on linear regression, as well as having code that is a little convoluted IMO.</p>\n<p>The thing is, if you have a dataset of \"m\" samples, each sample called \"x^i\" (n-dimensional vector), and a vector of outcomes y (m-dimensional vector), you can construct the following matrices:</p>\n<p><a href=\"https://i.sstatic.net/1XPV2.png\" rel=\"nofollow noreferrer\"><img alt=\"Gradient Descent Inputs\" src=\"https://i.sstatic.net/1XPV2.png\"/></a></p>\n<p>Now, the goal is to find \"w\" (n+1 dimensional vector), which describes the line for your linear regression, \"w_0\" is the constant term, \"w_1\" and so on are your coefficients of each dimension (feature) in an input sample. So in essence, you want to find \"w\" such that \"X*w\" is as close to \"y\" as possible, i.e. your line predictions will be as close to the original outcomes as possible.</p>\n<p>Note also that we added an extra component/dimension at the start of each \"x^i\", which is just \"1\", to account for the constant term. In addition, \"X\" is just the matrix you get by \"stacking\" each outcome as a row, so it's an (m by n+1) matrix.</p>\n<p>Once you construct that, the Python &amp; Numpy code for gradient descent is actually very straight forward:</p>\n<pre><code>def descent(X, y, learning_rate = 0.001, iters = 100):\n    w = np.zeros((X.shape[1], 1))\n    for i in range(iters):\n        grad_vec = -(X.T).dot(y - X.dot(w))\n        w = w - learning_rate*grad_vec\n    return w\n</code></pre>\n<p>And voila! That returns the vector \"w\", or description of your prediction line.</p>\n<p><strong>But how does it work?</strong>\nIn the code above, I am finding the gradient vector of the cost function (squared differences, in this case), then we are going \"against the flow\", to find the minimum cost given by the best \"w\". The actual formula used is in the line</p>\n<pre><code>grad_vec = -(X.T).dot(y - X.dot(w))\n</code></pre>\n<p>For the full maths explanation, and code including the creation of the matrices, see this post on <a href=\"https://matgomes.com/gradient-descent-for-linear-regression-in-python/\" rel=\"nofollow noreferrer\">how to implement gradient descent in Python</a>.</p>\n<p>Edit: For illustration, the above code estimates a line which you can use to make predictions. The image below shows an example of the \"learned\" gradient descent line (in red), and the original data samples (in blue scatter) from the \"fish market\" dataset from Kaggle.</p>\n<p><a href=\"https://i.sstatic.net/SyNnB.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/SyNnB.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've trained a sentiment classifier model using Keras library by following the below steps(broadly).</p>\n<ol>\n<li>Convert Text corpus into sequences using Tokenizer object/class</li>\n<li>Build a model using the model.fit() method </li>\n<li>Evaluate this model</li>\n</ol>\n<p>Now for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The most common way is to use either <a href=\"https://docs.python.org/3/library/pickle.html\" rel=\"noreferrer\"><code>pickle</code></a> or <a href=\"https://pypi.python.org/pypi/joblib\" rel=\"noreferrer\"><code>joblib</code></a>. Here you have an example on how to use <code>pickle</code> in order to save <code>Tokenizer</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\n# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# loading\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Tokenizer class has a function to save date into JSON format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>tokenizer_json = tokenizer.to_json()\nwith io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n</code></pre>\n<p>The data can be loaded using <code>tokenizer_from_json</code> function from <code>keras_preprocessing.text</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>with open('tokenizer.json') as f:\n    data = json.load(f)\n    tokenizer = tokenizer_from_json(data)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The accepted answer clearly demonstrates how to save the tokenizer. The following is a comment on the problem of (generally) scoring <em>after</em> fitting or saving. Suppose that a list <code>texts</code> is comprised of two lists <code>Train_text</code> and <code>Test_text</code>, where the set of tokens in <code>Test_text</code> is a subset of the set of tokens in <code>Train_text</code> (an optimistic assumption). Then <code>fit_on_texts(Train_text)</code> gives different results for <code>texts_to_sequences(Test_text)</code> as compared with first calling <code>fit_on_texts(texts)</code> and then <code>text_to_sequences(Test_text)</code>.</p>\n<p>Concrete Example:</p>\n<pre><code>from keras.preprocessing.text import Tokenizer\n\ndocs = [\"A heart that\",\n         \"full up like\",\n         \"a landfill\",\n        \"no surprises\",\n        \"and no alarms\"\n         \"a job that slowly\"\n         \"Bruises that\",\n         \"You look so\",\n         \"tired happy\",\n         \"no alarms\",\n        \"and no surprises\"]\ndocs_train = docs[:7]\ndocs_test = docs[7:]\n# EXPERIMENT 1: FIT  TOKENIZER ONLY ON TRAIN\nT_1 = Tokenizer()\nT_1.fit_on_texts(docs_train)  # only train set\nencoded_train_1 = T_1.texts_to_sequences(docs_train)\nencoded_test_1 = T_1.texts_to_sequences(docs_test)\nprint(\"result for test 1:\\n%s\" %(encoded_test_1,))\n\n# EXPERIMENT 2: FIT TOKENIZER ON BOTH TRAIN + TEST\nT_2 = Tokenizer()\nT_2.fit_on_texts(docs)  # both train and test set\nencoded_train_2 = T_2.texts_to_sequences(docs_train)\nencoded_test_2 = T_2.texts_to_sequences(docs_test)\nprint(\"result for test 2:\\n%s\" %(encoded_test_2,))\n</code></pre>\n<p>Results:</p>\n<pre><code>result for test 1:\n[[3], [10, 3, 9]]\nresult for test 2:\n[[1, 19], [5, 1, 4]]\n</code></pre>\n<p>Of course, if the above optimistic assumption is not satisfied and the set of tokens in Test_text is disjoint from that of Train_test, then test 1 results in a list of empty brackets <code>[].</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a number of classes and corresponding feature vectors, and when I run predict_proba() I will get this:</p>\n<pre><code>classes = ['one','two','three','one','three']\n\nfeature = [[0,1,1,0],[0,1,0,1],[1,1,0,0],[0,0,0,0],[0,1,1,1]]\n\nfrom sklearn.naive_bayes import BernoulliNB\n\nclf = BernoulliNB()\nclf.fit(feature,classes)\nclf.predict_proba([0,1,1,0])\n&gt;&gt; array([[ 0.48247836,  0.40709111,  0.11043053]])\n</code></pre>\n<p>I would like to get what probability that corresponds to what class. On this page it says that they are ordered by arithmetical order, i'm not 100% sure of what that means: <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba\">http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba</a></p>\n<p>Does it mean that I have go trough my training examples assign the corresponding index to the first encounter of a class, or is there a command like </p>\n<p><code>clf.getClasses() = ['one','two','three']?</code></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just use the <code>.classes_</code> attribute of the classifier to recover the mapping. In your example that gives:</p>\n<pre><code>&gt;&gt;&gt; clf.classes_\narray(['one', 'three', 'two'], \n      dtype='|S5')\n</code></pre>\n<p>And thanks for putting a minimalistic reproduction script in your question, it makes answering really easy by just copy and pasting in a IPython shell :)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>import pandas as pd\ntest = [[0,1,1,0],[1,1,1,0]]\npd.DataFrame(clf.predict_proba(test), columns=clf.classes_)\n\nOut[2]:\n         one       three         two\n0   0.542815    0.361876    0.095309\n1   0.306431    0.612863    0.080706\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As a rule, any attribute in a learner that ends with _ is a learned one. In your case you're looking for <code>clf.classes_</code>.</p>\n<p>Generally in Python, you can use the <code>dir</code> function to find out which attributes an object has.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can not for the life of me figure out how to switch the image ordering. images are read in (x,x,3) format, theano requires it to be in (3,x,x) format. I tried changing the order with\n<code>numpy.array([img[:,:,i] for i in range(3)])</code></p>\n<p>which i guess gets the job done, but it is both ugly and i can't figure out how to reverse it to get the original image back.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I agree with @Qualia 's comment, <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.moveaxis.html\" rel=\"noreferrer\">np.moveaxis(a, source, destination)</a> is easier to understand. This does the job: </p>\n<pre><code>x = np.zeros((12, 12, 3))\nx.shape\n#yields: \n(12, 12, 3)\n\nx = np.moveaxis(x, -1, 0)\nx.shape\n#yields: \n(3, 12, 12)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h3>To reorder data</h3>\n<p>You can use <a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.rollaxis.html\" rel=\"noreferrer\">numpy.rollaxis</a> to roll the axis 3 to position 1 (considering you have the batch size as dimension 0).</p>\n<pre><code>np.rollaxis(imagesArray, 3, 1)  \n</code></pre>\n<p>But, if you're using keras, you might want to change its configuration or define it per layer. Theano doesn't require anything from you if you're using Keras.</p>\n<p>Keras can be configured with channels first or channels last, besides allowing you to define it in every individual layer, so you don't have to change your data.</p>\n<h3>To configure keras</h3>\n<p>Find the <code>keras.json</code> file and change it. The file is usually installed in <code>C:\\Users\\yourusername\\.keras</code> or <code>~/.keras</code> depending on your OS.</p>\n<p>Change <code>\"image_data_format\": \"channels_last\"</code> to <code>\"channels_first\"</code> or vice-versa, as you wish.</p>\n<p>Usually, working with \"channels_last\" is less troublesome because of a great amount of other (non convolutional) functions that work only on the last axis.</p>\n<h3>Defining channel order in layers.</h3>\n<p>The <a href=\"https://keras.io/layers/convolutional/\" rel=\"noreferrer\">Keras documentation</a> has all information about parameters for layers, including the <code>data_format</code> parameter.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're looking at the fastest option, go for <code>.transpose(...)</code>. It's even faster than <code>np.einsum</code>. </p>\n<pre class=\"lang-py prettyprint-override\"><code>img = np.random.random((1000, 1000, 3))\nimg.shape\n# (1000, 1000, 3)\n\n%timeit img.transpose(2, 0, 1)\n# 385 ns ± 1.11 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n%timeit np.rollaxis(img, -1, 0)\n# 2.7 µs ± 50.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n%timeit np.einsum('ijk-&gt;kij', img)\n# 2.75 µs ± 31.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n%timeit np.moveaxis(img, -1, 0)\n# 7.26 µs ± 57.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\nnp.allclose(img.transpose(2, 0, 1), np.einsum('ijk-&gt;kij', img))\n# True\nnp.allclose(img.transpose(2, 0, 1), np.moveaxis(img, -1, 0))\n# True\nnp.allclose(img.transpose(2, 0, 1), np.rollaxis(img,-1, 0))\n# True\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I used Keras biomedical image segmentation to segment brain neurons. I used <code>model.evaluate()</code> it gave me Dice coefficient: 0.916. However, when I used <code>model.predict()</code>, then loop through the predicted images by calculating the Dice coefficient, the Dice coefficient is 0.82. Why are these two values different?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>model.evaluate</code> function predicts the output for the given input and then computes the metrics function specified in the <code>model.compile</code> and based on <code>y_true</code> and <code>y_pred</code> and returns the computed metric value as the output.</p>\n<p>The <code>model.predict</code> just returns back the <code>y_pred</code></p>\n<p>So if you use <code>model.predict</code> and then compute the metrics yourself, the computed metric value should turn out to be the same as <code>model.evaluate</code></p>\n<p>For example, one would use <code>model.predict</code> instead of <code>model.evaluate</code> in evaluating an RNN/ LSTM based models where the output needs to be fed as input in next time step</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The problem lies in the fact that every metric in <code>Keras</code> is evaluated in a following manner:</p>\n<ol>\n<li>For each <code>batch</code> a metric value is evaluated.</li>\n<li>A current value of loss (after <code>k</code> batches is equal to a mean value of your metric across computed <code>k</code> batches).</li>\n<li>The final result is obtained as a mean of all losses computed for all batches.</li>\n</ol>\n<p>Most of the most popular metrics (like <code>mse</code>, <code>categorical_crossentropy</code>, <code>mae</code>) etc. - as a mean of loss value of each example - have a property that such evaluation ends up with a proper result. But in case of Dice Coefficient - a mean of its value across all of the batches is not equal to actual value computed on a whole dataset and as <code>model.evaluate()</code> uses such way of computations - this is the direct cause of your problem.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>keras.evaluate()</code> function will give you the loss value for every batch. The <code>keras.predict()</code> function will give you the actual predictions for all samples in a batch, for all batches. So even if you use the same data, the differences will be there because the value of a loss function will be almost always different than the predicted values. These are two different things.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have built a simple Keras network:</p>\n<pre><code>import numpy as np;\n\nfrom keras.models import Sequential;\nfrom keras.layers import Dense,Activation;\n\ndata= np.genfromtxt(\"./kerastests/mydata.csv\", delimiter=';')\nx_target=data[:,29]\nx_training=np.delete(data,6,axis=1)\nx_training=np.delete(x_training,28,axis=1)\n\nmodel=Sequential()\nmodel.add(Dense(20,activation='relu', input_dim=x_training.shape[1]))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(1));\n\nmodel.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\nmodel.fit(x_training, x_target)\n</code></pre>\n<p>From my source data, I have removed 2 columns, as you can see. One is a column that came with dates in a string format (in the dataset, besides it, I have a column for the day, another for the month, and another for the year, so I don't need that column) and the other column is the column I use as target for the model).</p>\n<p>When I train this model I get this output:</p>\n<pre><code>32/816 [&gt;.............................] - ETA: 23s - loss: 13541942.0000 - acc: 0.0000e+00\n800/816 [============================&gt;.] - ETA: 0s - loss: 11575466.0400 - acc: 0.0000e+00 \n816/816 [==============================] - 1s - loss: 11536905.2353 - acc: 0.0000e+00     \nEpoch 2/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 6794785.0000 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5381360.4314 - acc: 0.0000e+00     \nEpoch 3/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 6235184.0000 - acc: 0.0000e+00\n800/816 [============================&gt;.] - ETA: 0s - loss: 5199512.8700 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5192977.4216 - acc: 0.0000e+00     \nEpoch 4/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 4680165.5000 - acc: 0.0000e+00\n736/816 [==========================&gt;...] - ETA: 0s - loss: 5050110.3043 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5168771.5490 - acc: 0.0000e+00     \nEpoch 5/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 5932391.0000 - acc: 0.0000e+00\n768/816 [===========================&gt;..] - ETA: 0s - loss: 5198882.9167 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5159585.9020 - acc: 0.0000e+00     \nEpoch 6/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 4488318.0000 - acc: 0.0000e+00\n768/816 [===========================&gt;..] - ETA: 0s - loss: 5144843.8333 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5151492.1765 - acc: 0.0000e+00     \nEpoch 7/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 6920405.0000 - acc: 0.0000e+00\n800/816 [============================&gt;.] - ETA: 0s - loss: 5139358.5000 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5169839.2941 - acc: 0.0000e+00     \nEpoch 8/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 3973038.7500 - acc: 0.0000e+00\n672/816 [=======================&gt;......] - ETA: 0s - loss: 5183285.3690 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5141417.0000 - acc: 0.0000e+00     \nEpoch 9/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 4969548.5000 - acc: 0.0000e+00\n768/816 [===========================&gt;..] - ETA: 0s - loss: 5126550.1667 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5136524.5098 - acc: 0.0000e+00     \nEpoch 10/10\n 32/816 [&gt;.............................] - ETA: 0s - loss: 6334703.5000 - acc: 0.0000e+00\n768/816 [===========================&gt;..] - ETA: 0s - loss: 5197778.8229 - acc: 0.0000e+00\n816/816 [==============================] - 0s - loss: 5141391.2059 - acc: 0.0000e+00    \n</code></pre>\n<p>Why is this happening? My data is a time series. I know that for time series people do not usually use <code>Dense</code> neurons, but it is just a test. What really tricks me is that accuracy is always 0. And, with other tests, I did even lose: gets to a \"NAN\" value.</p>\n<p>Could anybody help here?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your model seems to correspond to a regression model for the following reasons: </p>\n<ul>\n<li><p>You are using <code>linear</code> (the default one) as an activation function in the output layer (and <code>relu</code> in the layer before).</p></li>\n<li><p>Your loss is <code>loss='mean_squared_error'</code>. </p></li>\n</ul>\n<p>However, the metric that you use- <code>metrics=['accuracy']</code> corresponds to a classification problem. If you want to do regression, remove <code>metrics=['accuracy']</code>. That is, use</p>\n<pre><code>model.compile(optimizer='adam',loss='mean_squared_error')\n</code></pre>\n<p>Here is a list of keras metrics for regression and classification (taken from <a href=\"http://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\" rel=\"noreferrer\">this blog post</a>):</p>\n<blockquote>\n<p><strong>Keras Regression Metrics</strong></p>\n<p>•Mean Squared Error: mean_squared_error, MSE or mse </p>\n<p>•Mean Absolute Error: mean_absolute_error, MAE, mae </p>\n<p>•Mean Absolute Percentage Error: mean_absolute_percentage_error, MAPE,\n  mape </p>\n<p>•Cosine Proximity: cosine_proximity, cosine</p>\n<p><strong>Keras Classification Metrics</strong></p>\n<p>•Binary Accuracy: binary_accuracy, acc</p>\n<p>•Categorical Accuracy: categorical_accuracy, acc</p>\n<p>•Sparse Categorical Accuracy: sparse_categorical_accuracy</p>\n<p>•Top k Categorical Accuracy: top_k_categorical_accuracy (requires you\n  specify a k parameter)</p>\n<p>•Sparse Top k Categorical Accuracy: sparse_top_k_categorical_accuracy\n  (requires you specify a k parameter)</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Add following to get metrics:</p>\n<pre><code>   history = model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n   # OR\n   history = model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])\n   history.history.keys()\n   history.history\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to point out something that is very important and has been unfortunately neglected: <code>mean_squared_error</code> is <strong>not</strong> an <strong>invalid</strong> loss function for <strong>classification</strong>.</p>\n<p>The mathematical properties of <strong>cross_entropy</strong> in conjunction with the assumptions of <strong>mean_squared_error</strong>(both of which I will not expand upon in this comment) make the latter inappropriate or <strong>worse</strong> than the <strong>cross_entropy</strong> when it comes to training on classification problems.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2015-04-14 15:11:06Z\">9 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/11477145/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I am looking for an open source neural network library.  So far, I have looked at FANN, WEKA, and OpenNN.  Are the others that I should look at?  The criteria, of course, is documentation, examples, and ease of use.  </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Last update: 2024/09/25</strong> (I will update this answer from time to time... Please let me know if anything is missing!)</p>\n<h1>Simple Implementations of Neural Networks</h1>\n<ul>\n<li>Since version 0.18 scikit-learn (Python) has an implementation of feed-forward neural networks (<a href=\"https://scikit-learn.org/stable/modules/neural_networks_supervised.html\" rel=\"nofollow noreferrer\">API documentation</a>).</li>\n<li><a href=\"http://leenissen.dk/fann/wp/\" rel=\"nofollow noreferrer\">FANN</a> is a very popular implementation in C/C++ and has bindings for many other languages.</li>\n</ul>\n<h1>Deep Learning</h1>\n<p>Neural networks are very popular in research and industry (\"deep learning\"). There are many research libraries available. Most of them are kind of easy to set up, integrate, and use. Although not as easy as the libraries mentioned above. They provide leading edge functionality and high performance (with GPUs etc.). Most of these libraries also have automatic differentiation. You can easily specify new architectures, loss functions etc. and don't have to specify the backpropagation manually.</p>\n<ul>\n<li><a href=\"https://github.com/keras-team/keras\" rel=\"nofollow noreferrer\">Keras</a>: has a long history as a high-level interface to other neural network libraries. Its current purpose is to serve as a high-level interface for <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"nofollow noreferrer\">TensorFlow</a>, <a href=\"http://pytorch.org/\" rel=\"nofollow noreferrer\">PyTorch</a>, and <a href=\"https://github.com/google/jax\" rel=\"nofollow noreferrer\">Jax</a>; (Previously it was part of TensorFlow and before that it could use <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"nofollow noreferrer\">Tensorflow</a>, <a href=\"http://deeplearning.net/software/theano/\" rel=\"nofollow noreferrer\">Theano</a>, and <a href=\"https://github.com/Microsoft/CNTK\" rel=\"nofollow noreferrer\">CNTK</a> as a backend.)</li>\n<li><a href=\"https://github.com/google/jax\" rel=\"nofollow noreferrer\">jax</a> (Python) has a numpy-like interface and is very low-level, but there are high-level interfaces: <a href=\"https://github.com/google/trax\" rel=\"nofollow noreferrer\">trax</a>, <a href=\"https://github.com/google/flax\" rel=\"nofollow noreferrer\">flax</a>, <a href=\"https://github.com/deepmind/dm-haiku\" rel=\"nofollow noreferrer\">Haiku</a>, or <a href=\"https://github.com/patrick-kidger/equinox\" rel=\"nofollow noreferrer\">equinox</a></li>\n<li><a href=\"http://pytorch.org/\" rel=\"nofollow noreferrer\">PyTorch</a> from Facebook, in Python, can be extended with C/C++, high-level interfaces: <a href=\"https://github.com/Lightning-AI/lightning\" rel=\"nofollow noreferrer\">Lightning</a>, <a href=\"https://github.com/fastai/fastai\" rel=\"nofollow noreferrer\">fastai</a>, <a href=\"https://github.com/pytorch/ignite\" rel=\"nofollow noreferrer\">Ignite</a>, <a href=\"https://github.com/skorch-dev/skorch\" rel=\"nofollow noreferrer\">skorch</a>, <a href=\"https://github.com/catalyst-team/catalyst\" rel=\"nofollow noreferrer\">catalyst</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow\" rel=\"nofollow noreferrer\">TensorFlow</a> from Google (C++/Python)</li>\n<li><a href=\"https://github.com/deeplearning4j/deeplearning4j\" rel=\"nofollow noreferrer\">Deeplearning4j</a> (Java)</li>\n<li><a href=\"https://github.com/baidu/Paddle\" rel=\"nofollow noreferrer\">PaddlePaddle</a> from Baidu in CUDA/C++ with Python bindings</li>\n<li><a href=\"https://github.com/sony/nnabla\" rel=\"nofollow noreferrer\">NNabla</a> from Sony in Cuda/C++11 with Python bindings</li>\n</ul>\n<p>Inactive:</p>\n<ul>\n<li><a href=\"https://github.com/dmlc/mxnet\" rel=\"nofollow noreferrer\">mxnet</a> (C++, Python, R, Scala, Julia, Matlab, Javascript)</li>\n<li><a href=\"https://github.com/Microsoft/CNTK\" rel=\"nofollow noreferrer\">CNTK</a> from Microsoft (training in Python / evaluation in C++/C#/Java/Python)</li>\n<li><a href=\"https://github.com/pfnet/chainer\" rel=\"nofollow noreferrer\">Chainer</a> (Python)</li>\n<li><a href=\"http://caffe.berkeleyvision.org/\" rel=\"nofollow noreferrer\">Caffe</a> from Berkeley Vision and Learning Center in C++ with Python bindings</li>\n<li><a href=\"https://github.com/pjreddie/darknet\" rel=\"nofollow noreferrer\">Darknet</a>: CNNs in C, known for the implementations of the YOLO object detector.</li>\n<li><a href=\"https://github.com/NervanaSystems/neon\" rel=\"nofollow noreferrer\">Neon</a> from Intel Nervana provides very efficient implementations (Python)</li>\n<li><a href=\"http://www.vlfeat.org/matconvnet/\" rel=\"nofollow noreferrer\">MatConvNet</a> (Matlab)</li>\n<li><a href=\"http://deeplearning.net/software/theano/\" rel=\"nofollow noreferrer\">Theano</a> (Python) and its high-level APIs <a href=\"http://deeplearning.net/software/pylearn2/\" rel=\"nofollow noreferrer\">Pylearn 2</a>, <a href=\"https://github.com/lmjohns3/theano-nets\" rel=\"nofollow noreferrer\">Theanets</a>, <a href=\"https://github.com/aigamedev/scikit-neuralnetwork\" rel=\"nofollow noreferrer\">scikit-neuralnetwork</a>, <a href=\"https://github.com/Lasagne/Lasagne\" rel=\"nofollow noreferrer\">Lasagne</a>, <a href=\"http://blocks.readthedocs.org/en/latest/\" rel=\"nofollow noreferrer\">Blocks</a></li>\n<li><a href=\"https://github.com/akrizhevsky/cuda-convnet2\" rel=\"nofollow noreferrer\">cuda-convnet2</a> in CUDA/C++ with Python bindings</li>\n<li><a href=\"https://github.com/hannes-brt/hebel\" rel=\"nofollow noreferrer\">Hebel</a> (Python)</li>\n<li><a href=\"https://caffe2.ai/\" rel=\"nofollow noreferrer\">Caffe2</a> from Facebook in C++ with Python bindings; has been joined with PyTorch</li>\n<li><a href=\"https://github.com/torch/nn\" rel=\"nofollow noreferrer\">Neural Networks</a> for Torch 7 (Lua, Torch 7 is a \"Matlab-like environment\", <a href=\"https://github.com/torch/torch7/wiki/Cheatsheet#machine-learning\" rel=\"nofollow noreferrer\">overview of machine learning algorithms in Torch</a>)</li>\n<li><a href=\"http://pybrain.org/\" rel=\"nofollow noreferrer\">PyBrain</a> (Python) contains different types of neural networks and training methods.</li>\n<li><a href=\"http://www.heatonresearch.com/encog\" rel=\"nofollow noreferrer\">Encog</a> (Java and C#)</li>\n<li>And I must mention my own project, which is called <a href=\"https://github.com/OpenANN/OpenANN\" rel=\"nofollow noreferrer\">OpenANN</a> (<a href=\"http://openann.github.io/OpenANN-apidoc/\" rel=\"nofollow noreferrer\">Documentation</a>). It is written in C++ and has Python bindings.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you want flexibility in defining network configurations, like sharing parameters or creating different types of convolutional architectures, then you should look at the family of Torch libraries: <a href=\"http://www.torch.ch/\" rel=\"noreferrer\">http://www.torch.ch/</a>.</p>\n<p>I haven't gone through the documentation for Torch 7 yet, but documentation for the other versions was pretty decent and the code is very readable (in Lua and C++).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use accord.net framework. <a href=\"http://accord-framework.net/\" rel=\"nofollow\">http://accord-framework.net/</a></p>\n<p>It contains Neural learning algorithms such as Levenberg-Marquardt, Parallel Resilient Backpropagation, the Nguyen-Widrow initialization algorithm, Deep Belief Networks and Restrictured Boltzmann Machines, and many other neural network related items. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Update(July 2020): Question is 9 years old but still one that I'm deeply interested in. In the time since, machine learning(RNN's, CNN's, GANS,etc), new approaches and cheap GPU's have risen that enable new approaches. I thought it would be fun to revisit this question to see if there are new approaches.</strong></p>\n<p>I am learning programming (Python and algorithms) and was trying to work on a project that I find interesting. I have created a few basic Python scripts, but I’m not sure how to approach a solution to a game I am trying to build.</p>\n<p><strong>Here’s how the game will work:</strong></p>\n<p>Users will be given items with a value. For example,</p>\n<pre><code>Apple = 1\nPears = 2\nOranges  = 3\n</code></pre>\n<p>They will then get a chance to choose any combo of them they like (i.e. 100 apples, 20 pears, and one orange). The only output the computer gets is the total value (in this example, it's currently $143). The computer will try to guess what they have. Which obviously it won’t be able to get correctly the first turn.</p>\n<pre><code>         Value    quantity(day1)    value(day1)\nApple      1        100                100\nPears      2         20                 40\nOrange     3          1                  3\nTotal               121                143\n</code></pre>\n<p>The next turn the user can modify their numbers but no more than 5% of the total quantity (or some other percent we may chose. I’ll use 5% for example.). The prices of fruit can change(at random) so the total value may change based on that also (for simplicity I am not changing fruit prices in this example). Using the above example, on day 2 of the game, the user returns a value of $152 and $164 on day 3. Here's an example:</p>\n<pre><code>Quantity (day2)   %change (day2)    Value (day2)   Quantity (day3)   %change (day3)   Value(day3)\n 104                                 104            106                                106\n  21                                  42             23                                 46\n   2                                   6              4                                 12\n 127               4.96%             152            133               4.72%            164\n</code></pre>\n<p>*(I hope the tables show up right, I had to manually space them so hopefully it's not just doing it on my screen, if it doesn't work let me know and I'll try to upload a screenshot.)</p>\n<p>I am trying to see if I can figure out what the quantities are over time (assuming the user will have the patience to keep entering numbers). I know right now my only restriction is the total value cannot be more than 5% so I cannot be within 5% accuracy right now so the user will be entering it forever.</p>\n<p><strong>What I have done so far</strong></p>\n<p>Here’s my solution so far (not much). Basically, I take all the values and figure out all the possible combinations of them (I am done this part). Then I take all the possible combos and put them in a database as a dictionary (so for example for $143, there could be a dictionary entry {apple:143, Pears:0, Oranges :0}..all the way to {apple:0, Pears:1, Oranges :47}. I do this each time I get a new number so I have a list of all possibilities.</p>\n<p>Here’s where I’m stuck. In using the rules above, how can I figure out the best possible solution? I think I’ll need a fitness function that automatically compares the two days data and removes any possibilities that have more than 5% variance of the previous days data.</p>\n<p><strong>Questions:</strong></p>\n<p>So my question with user changing the total and me having a list of all the probabilities, how should I approach this? What do I need to learn? Is there any algorithms out there or theories that I can use that are applicable? Or, to help me understand my mistake, can you suggest what rules I can add to make this goal feasible (if it's not in its current state. I was thinking adding more fruits and saying they must pick at least 3, etc..)?  Also, I only have a vague understanding of genetic algorithms, but I thought I could use them here, if is there something I can use?</p>\n<p>I'm very very eager to learn so any advice or tips would be greatly appreciated (just please don't tell me this game is impossible).</p>\n<p>UPDATE: Getting feedback that this is hard to solve. So I thought I'd add another condition to the game that won't interfere with what the player is doing (game stays the same for them) but everyday the value of the fruits change price (randomly). Would that make it easier to solve? Because within a 5% movement and certain fruit value changes, only a few combinations are probable over time.</p>\n<p>Day 1, anything is possible and getting a close enough range is almost impossible, but as the prices of fruits change and the user can only choose a 5% change, then shouldn't (over time) the range be narrow and narrow. In the above example, if prices are volatile enough I think I could brute force a solution that gave me a range to guess in, but I'm trying to figure out if there's a more elegant solution or other solutions to keep narrowing this range over time.</p>\n<p>UPDATE2: After reading and asking around, I believe this is a hidden Markov/Viterbi problem that tracks the changes in fruit prices as well as total sum (weighting the last data point the heaviest). I'm not sure how to apply the relationship though. I think this is the case and could be wrong but at the least I'm starting to suspect this is a some type of machine learning problem.</p>\n<p>Update 3: I am created a test case (with smaller numbers) and a generator to help automate the user generated data and I am trying to create a graph from it to see what's more likely.</p>\n<p>Here's the code, along with the total values and comments on what the users actually fruit quantities are.</p>\n<pre><code>#!/usr/bin/env python\nimport itertools\n\n# Fruit price data\nfruitPriceDay1 = {'Apple':1, 'Pears':2, 'Oranges':3}\nfruitPriceDay2 = {'Apple':2, 'Pears':3, 'Oranges':4}\nfruitPriceDay3 = {'Apple':2, 'Pears':4, 'Oranges':5}\n\n# Generate possibilities for testing (warning...will not scale with large numbers)\ndef possibilityGenerator(target_sum, apple, pears, oranges):\n    allDayPossible = {}\n    counter = 1\n    apple_range = range(0, target_sum + 1, apple)\n    pears_range = range(0, target_sum + 1, pears)\n    oranges_range = range(0, target_sum + 1, oranges)\n    for i, j, k in itertools.product(apple_range, pears_range, oranges_range):\n        if i + j + k == target_sum:\n            currentPossible = {}\n\n            #print counter\n            #print 'Apple', ':', i/apple, ',', 'Pears', ':', j/pears, ',', 'Oranges', ':', k/oranges\n            currentPossible['apple'] = i/apple\n            currentPossible['pears'] = j/pears\n            currentPossible['oranges'] = k/oranges\n\n            #print currentPossible\n            allDayPossible[counter] = currentPossible\n            counter = counter +1\n    return allDayPossible\n\n# Total sum being returned by user for value of fruits\ntotalSumDay1=26 # Computer does not know this but users quantities are apple: 20, pears 3, oranges 0 at the current prices of the day\ntotalSumDay2=51 # Computer does not know this but users quantities are apple: 21, pears 3, oranges 0 at the current prices of the day\ntotalSumDay3=61 # Computer does not know this but users quantities are apple: 20, pears 4, oranges 1 at the current prices of the day\ngraph = {}\ngraph['day1'] = possibilityGenerator(totalSumDay1, fruitPriceDay1['Apple'], fruitPriceDay1['Pears'], fruitPriceDay1['Oranges'] )\ngraph['day2'] = possibilityGenerator(totalSumDay2, fruitPriceDay2['Apple'], fruitPriceDay2['Pears'], fruitPriceDay2['Oranges'] )\ngraph['day3'] = possibilityGenerator(totalSumDay3, fruitPriceDay3['Apple'], fruitPriceDay3['Pears'], fruitPriceDay3['Oranges'] )\n\n# Sample of dict = 1 : {'oranges': 0, 'apple': 0, 'pears': 0}..70 : {'oranges': 8, 'apple': 26, 'pears': 13}\nprint graph\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>We'll combine graph-theory and probability:</p>\n<p>On the 1st day, build a set of all feasible solutions. Lets denote the solutions set as A1={a1(1), a1(2),...,a1(n)}.</p>\n<p>On the second day you can again build the solutions set A2.</p>\n<p>Now, for each element in A2, you'll need to check if it can be reached from each element of A1 (given x% tolerance). If so - connect A2(n) to A1(m). If it can't be reached from any node in A1(m) - you can delete this node.</p>\n<p>Basically we are building a connected directed acyclic graph.</p>\n<p>All paths in the graph are equally likely. You can find an exact solution only when there is a single edge from Am to Am+1 (from a node in Am to a node in Am+1).</p>\n<p>Sure, some nodes appear in more paths than other nodes. The probability for each node can be directly deduced based on the number of paths that contains this node.</p>\n<p>By assigning a weight to each node, which equals to the number of paths that leads to this node, there is no need to keep all history, but only the previous day.</p>\n<p>Also, have a look at <a href=\"https://stackoverflow.com/questions/1467907/algorithm-to-determine-non-negative-values-solution-existance-for-linear-diophant\">non-negative-values linear diphantine equations</a> - A question I asked a while ago. The accepted answer is a great way to enumarte all combos in each step.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>Disclaimer: I changed my answer dramatically after temporarily deleting my answer and re-reading the question carefully as I misread some critical parts of the question. While still referencing similar topics and algorithms, the answer was greatly improved after I attempted to solve some of the problem in C# myself.</em></p>\n<h2>Hollywood version</h2>\n<ul>\n<li>The problem is a <a href=\"http://en.wikipedia.org/wiki/Constraint_satisfaction_problem#Dynamic_CSPs\" rel=\"noreferrer\">Dynamic constraint satisfaction problem</a> (DCSP), a variation on <a href=\"http://en.wikipedia.org/wiki/Constraint_satisfaction_problem\" rel=\"noreferrer\">Constraint satisfaction problems</a> (CSP.)</li>\n<li>Use <a href=\"http://en.wikipedia.org/wiki/Monte_Carlo_algorithm\" rel=\"noreferrer\">Monte Carlo</a> to find potential solutions for a given day if values and quantity ranges are not tiny. Otherwise, use brute force to find every potential solutions.</li>\n<li>Use <em>Constraint Recording</em> (related to DCSP), applied in cascade to previous days to restrict the potential solution set.</li>\n<li>Cross your fingers, aim and <em>shoot</em> (Guess), based on probability.</li>\n<li><em>(Optional)</em> Bruce Willis wins.</li>\n</ul>\n<h2>Original version</h2>\n<p>First, I would like to state what I see two main problems here:</p>\n<ol>\n<li><p>The sheer number of possible solutions. Knowing only the number of items and the total value, lets say 3 and 143 for example, will yield <em>a lot</em> of possible solutions. Plus, it is not easy to have an algorithm picking valid solution without inevitably trying invalid solutions (total not equal to 143.)</p></li>\n<li><p>When possible solutions are found for a given day D<sub>i</sub>, one must find a way to eliminate potential solutions with the added information given by { D<sub>i+1</sub> .. D<sub>i+n</sub> }.</p></li>\n</ol>\n<p>Let's lay down some bases for the upcoming examples:</p>\n<ul>\n<li>Lets keep the same item values, the whole game. It can either be random or chosen by the user.</li>\n<li>The possible item values is bound to the very limited range of [1-10], where no two items can have the same value.</li>\n<li>No item can have a quantity greater than 100. That means: [0-100].</li>\n</ul>\n<p>In order to solve this more easily <strong>I took the liberty to change one constraint</strong>, which makes the algorithm converge faster:</p>\n<ul>\n<li>The \"total quantity\" rule is overridden by this rule: You can add or remove any number of items within the [1-10] range, total, in one day. However, you cannot add or remove the same number of items, total, more than twice. This also gives the game a maximum lifecycle of 20 days.</li>\n</ul>\n<p>This rule enables us to rule out solutions more easily. And, with non-tiny ranges, renders <a href=\"http://en.wikipedia.org/wiki/Backtracking\" rel=\"noreferrer\">Backtracking algorithms</a> still useless, just like your original problem and rules.</p>\n<p>In my humble opinion, this rule is not the <em>essence</em> of the game but only a facilitator, enabling the computer to solve the problem.</p>\n<h3>Problem 1: Finding potential solutions</h3>\n<p>For starters, <em>problem 1.</em> can be solved using a <a href=\"http://en.wikipedia.org/wiki/Monte_Carlo_algorithm\" rel=\"noreferrer\">Monte Carlo algorithm</a> to find a set of potential solutions. The technique is simple: Generate random numbers for item values and quantities (within their respective accepted range). Repeat the process for the required number of items. Verify whether or not the solution is acceptable. That means verifying if items have distinct values and the total is equal to our target total (say, 143.)</p>\n<p>While this technique has the advantage of being easy to implement it has some drawbacks:</p>\n<ul>\n<li>The user's solution is not guaranteed to appear in our results.</li>\n<li>There is a lot of \"misses\". For instance, it takes more or less 3,000,000 tries to find 1,000 potential solutions given our constraints.</li>\n<li>It takes a lot of time: around 4 to 5 seconds on my lazy laptop.</li>\n</ul>\n<p>How to get around these drawback? Well...</p>\n<ul>\n<li>Limit the range to smaller values and</li>\n<li>Find an adequate number of potential solutions so there is a good chance the user's solution appears in your solution set.</li>\n<li>Use heuristics to find solutions more easily (more on that later.)</li>\n</ul>\n<p>Note that the more you restrict the ranges, the less useful while be the Monte Carlo algorithm is, since there will be few enough valid solutions to iterate on them all in reasonable time. For constraints { 3, [1-10], [0-100] } there is around 741,000,000 valid solutions (not constrained to a target total value.) Monte Carlo is usable there. For { 3, [1-5], [0-10] }, there is only around 80,000. No need to use Monte Carlo; brute force <code>for</code> loops will do just fine.</p>\n<p>I believe the <em>problem 1</em> is what you would call a <a href=\"http://en.wikipedia.org/wiki/Constraint_satisfaction_problem\" rel=\"noreferrer\">Constraint satisfaction problem</a> (or CSP.)</p>\n<h3>Problem 2: Restrict the set of potential solutions</h3>\n<p>Given the fact that <em>problem 1</em> is a CSP, I would go ahead and call <em>problem 2</em>, and the problem in general, a <a href=\"http://en.wikipedia.org/wiki/Constraint_satisfaction_problem#Dynamic_CSPs\" rel=\"noreferrer\">Dynamic CSP</a> (or DCSP.)</p>\n<blockquote>\n<p>[DCSPs] are useful when the original formulation of a\n  problem is altered in some way, typically because the set of\n  constraints to consider evolves because of the environment. DCSPs\n  are viewed as a sequence of static CSPs, each one a transformation of\n  the previous one in which variables and constraints can be added\n  (restriction) or removed (relaxation).</p>\n</blockquote>\n<p>One technique used with CSPs that might be useful to this problem is called <em>Constraint Recording</em>:</p>\n<ul>\n<li>With each change in the environment (user entered values for D<sub>i+1</sub>), find information about the new constraint: What are the possibly \"used\" quantities for the add-remove constraint.</li>\n<li>Apply the constraint to every preceding day in cascade. Rippling effects might significantly reduce possible solutions.</li>\n</ul>\n<p>For this to work, you need to get a new set of possible solutions every day; Use either brute force or Monte Carlo. Then, compare solutions of D<sub>i</sub> to D<sub>i-1</sub> and keep only solutions that can succeed to previous days' solutions without violating constraints.</p>\n<p>You will probably have to keep an history of what solutions lead to what other solutions (probably in a directed graph.) Constraint recording enables you to <em>remember</em> possible add-remove quantities and rejects solutions based on that.</p>\n<p>There is a lot of other steps that could be taken to further improve your solution. Here are some ideas:</p>\n<ul>\n<li>Record constraints for item-value combinations found in previous days solutions. Reject other solutions immediately (as item values must not change.) You could even find a smaller solution sets for each existing solution using solution-specific constraints to reject invalid solutions earlier.</li>\n<li>Generate some \"mutant\", full-history, solutions each day in order to \"repair\" the case where the D<sub>1</sub> solution set doesn't contain the user's solution. You could use a genetic algorithm to find a mutant population based on an existing solution set.)</li>\n<li>Use heuristics in order find solutions easily (e.g. when a valid solution is found, try and find variations of this solution by substituting quantities around.)</li>\n<li>Use behavioral heuristics in order to predict some user actions (e.g. same quantity for every item, extreme patterns, etc.)</li>\n<li>Keep making some computations while the user is entering new quantities.</li>\n</ul>\n<p>Given all of this, try and figure out a ranking system based on occurrence of solutions and heuristics to determine a candidate solution.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This problem is impossible to solve.</p>\n<p>Let's say that you know exactly for what ratio number of items was increased, not just what is the maximum ratio for this.</p>\n<p>A user has N fruits and you have D days of guessing.</p>\n<p>In each day you get N new variables and then you have in total D*N variables.</p>\n<p>For each day you can generate only two equations. One equation is the sum of  n_item*price and other is based on a known ratio. In total you have at most 2*D equations if they are all independent.</p>\n<p>2*D &lt; N*D for all N &gt; 2</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you run a Keras neural network model you might see something like this in the console: </p>\n<pre><code>Epoch 1/3\n   6/1000 [..............................] - ETA: 7994s - loss: 5111.7661\n</code></pre>\n<p>As time goes on the loss hopefully improves. I want to log these losses to a file over time so that I can learn from them. I have tried: </p>\n<pre><code>logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)\n</code></pre>\n<p>but this doesn't work. I am not sure what level of logging I need in this situation. </p>\n<p>I have also tried using a callback like in: </p>\n<pre><code>def generate_train_batch():\n    while 1:\n        for i in xrange(0,dset_X.shape[0],3):\n            yield dset_X[i:i+3,:,:,:],dset_y[i:i+3,:,:]\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\nlogloss=LossHistory()\ncolorize.fit_generator(generate_train_batch(),samples_per_epoch=1000,nb_epoch=3,callbacks=['logloss'])\n</code></pre>\n<p>but obviously this isn't writing to a file. Whatever the method, through a callback or the logging module or anything else, I would love to hear your solutions for logging loss of a keras neural network to a file. Thanks! </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <em>CSVLogger</em> callback.</p>\n<p>as example:</p>\n<pre><code>from keras.callbacks import CSVLogger\n\ncsv_logger = CSVLogger('log.csv', append=True, separator=';')\nmodel.fit(X_train, Y_train, callbacks=[csv_logger])\n</code></pre>\n<p>Look at: <a href=\"https://keras.io/callbacks/#csvlogger\" rel=\"noreferrer\">Keras Callbacks</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is a simple solution to your problem. Every time any of the <code>fit</code> methods are used - as a result the special callback called <strong>History Callback</strong> is returned. It has a field <code>history</code> which is a dictionary of all metrics registered after every epoch. So to get list of loss function values after every epoch you can easly do:</p>\n<pre><code>history_callback = model.fit(params...)\nloss_history = history_callback.history[\"loss\"]\n</code></pre>\n<p>It's easy to save such list to a file (e.g. by converting it to <code>numpy</code> array and using <code>savetxt</code> method).</p>\n<p><strong>UPDATE:</strong></p>\n<p>Try:</p>\n<pre><code>import numpy\nnumpy_loss_history = numpy.array(loss_history)\nnumpy.savetxt(\"loss_history.txt\", numpy_loss_history, delimiter=\",\")\n</code></pre>\n<p><strong>UPDATE 2:</strong></p>\n<p>The solution to the problem of recording a loss after every batch is written in <a href=\"https://keras.io/callbacks/#create-a-callback\" rel=\"noreferrer\">Keras Callbacks Documentation</a> in a <strong>Create a Callback</strong> paragraph.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Old question, but here goes. Keras history output perfectly matches pandas DataSet input.</p>\n<p>If you want the entire history to csv in one line:\n<code>\npandas.DataFrame(model.fit(...).history).to_csv(\"history.csv\")\n</code></p>\n<p>Cheers</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When I load the whole dataset in memory and train the network in Keras using following code:</p>\n<pre><code>model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1)\n</code></pre>\n<p>This generates a progress bar per epoch with metrics like ETA, accuracy, loss, etc</p>\n<p>When I train the network in batches, I'm using the following code</p>\n<pre><code>for e in range(40):\n        for X, y in data.next_batch():\n            model.fit(X, y, nb_epoch=1, batch_size=data.batch_size, verbose=1)\n</code></pre>\n<p>This will generate a progress bar for each batch instead of each epoch. Is it possible to generate a progress bar for each epoch during batchwise training? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ol>\n<li></li>\n</ol>\n<pre><code>model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1)\n</code></pre>\n<p>In the above change to <code>verbose=2</code>, as it is mentioned in the <a href=\"https://keras.io/api/models/model_training_apis/#fit-method\" rel=\"noreferrer\">documentation</a>:</p>\n<blockquote>\n<p>verbose: 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch</p>\n</blockquote>\n<p>It'll show your output as:</p>\n<pre><code>Epoch 1/100\n0s - loss: 0.2506 - acc: 0.5750 - val_loss: 0.2501 - val_acc: 0.3750\nEpoch 2/100\n0s - loss: 0.2487 - acc: 0.6250 - val_loss: 0.2498 - val_acc: 0.6250\nEpoch 3/100\n0s - loss: 0.2495 - acc: 0.5750 - val_loss: 0.2496 - val_acc: 0.6250\n.....\n.....\n</code></pre>\n<ol start=\"2\">\n<li></li>\n</ol>\n<p>If you want to show a progress bar for completion of epochs, keep <code>verbose=0</code> (which shuts out logging to stdout) and implement in the following manner:</p>\n<pre><code>from time import sleep\nimport sys\n\nepochs = 10\n\nfor e in range(epochs):\n    sys.stdout.write('\\r')\n\n    for X, y in data.next_batch():\n        model.fit(X, y, nb_epoch=1, batch_size=data.batch_size, verbose=0)\n\n    # print loss and accuracy\n\n    # the exact output you're looking for:\n    sys.stdout.write(\"[%-60s] %d%%\" % ('='*(60*(e+1)/10), (100*(e+1)/10)))\n    sys.stdout.flush()\n    sys.stdout.write(\", epoch %d\"% (e+1))\n    sys.stdout.flush()\n</code></pre>\n<p>The output will be as follows:</p>\n<pre><code>[============================================================] 100%, epoch 10\n</code></pre>\n<ol start=\"3\">\n<li></li>\n</ol>\n<p>If you want to show loss after every n batches, you can use:</p>\n<pre><code>out_batch = NBatchLogger(display=1000)\nmodel.fit([X_train_aux,X_train_main],Y_train,batch_size=128,callbacks=[out_batch])\n</code></pre>\n<p>Though, I haven't ever tried it before. The above example was taken from this keras github issue: <a href=\"https://github.com/fchollet/keras/issues/2850\" rel=\"noreferrer\">Show Loss Every N Batches #2850</a></p>\n<p>You can also follow a demo of <code>NBatchLogger</code> here:</p>\n<pre><code>class NBatchLogger(Callback):\n    def __init__(self, display):\n        self.seen = 0\n        self.display = display\n\n    def on_batch_end(self, batch, logs={}):\n        self.seen += logs.get('size', 0)\n        if self.seen % self.display == 0:\n            metrics_log = ''\n            for k in self.params['metrics']:\n                if k in logs:\n                    val = logs[k]\n                    if abs(val) &gt; 1e-3:\n                        metrics_log += ' - %s: %.4f' % (k, val)\n                    else:\n                        metrics_log += ' - %s: %.4e' % (k, val)\n            print('{}/{} ... {}'.format(self.seen,\n                                        self.params['samples'],\n                                        metrics_log))\n</code></pre>\n<ol start=\"4\">\n<li></li>\n</ol>\n<p>You can also use <code>progbar</code> for progress, but it'll print progress batchwise</p>\n<pre><code>from keras.utils import generic_utils\n\nprogbar = generic_utils.Progbar(X_train.shape[0])\n\nfor X_batch, Y_batch in datagen.flow(X_train, Y_train):\n    loss, acc = model_test.train([X_batch]*2, Y_batch, accuracy=True)\n    progbar.add(X_batch.shape[0], values=[(\"train loss\", loss), (\"acc\", acc)])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>tqdm</code> (version &gt;= 4.41.0) has also just added built-in support for <code>keras</code> so you could do:</p>\n<pre><code>from tqdm.keras import TqdmCallback\n...\nmodel.fit(..., verbose=0, callbacks=[TqdmCallback(verbose=2)])\n</code></pre>\n<p>This turns off <code>keras</code>' progress (<code>verbose=0</code>), and uses <code>tqdm</code> instead. For the callback, <code>verbose=2</code> means separate progressbars for epochs and batches. <code>1</code> means clear batch bars when done. <code>0</code> means only show epochs (never show batch bars).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can set verbose=0 and set callbacks that will update progress at the end of each fitting, </p>\n<pre><code>clf.fit(X, y, nb_epoch=1, batch_size=data.batch_size, verbose=0, callbacks=[some_callback])\n</code></pre>\n<p><a href=\"https://keras.io/callbacks/#example-model-checkpoints\" rel=\"nofollow\">https://keras.io/callbacks/#example-model-checkpoints</a></p>\n<p>or set callback <a href=\"https://keras.io/callbacks/#remotemonitor\" rel=\"nofollow\">https://keras.io/callbacks/#remotemonitor</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently seeing the API of theano,</p>\n<pre><code>theano.tensor.nnet.conv2d(input, filters, input_shape=None, filter_shape=None, border_mode='valid', subsample=(1, 1), filter_flip=True, image_shape=None, **kwargs)\n</code></pre>\n<p>where the <code>filter_shape</code> is a tuple of <code>(num_filter, num_channel, height, width)</code>, I am confusing about this because isn't that the number of filter decided by the stride while sliding the filter window on the image? How can I specify on filter number just like this? It would be reasonable to me if it is calculated by the parameter stride (if there is any).</p>\n<p>Also, I am confused with the term feature map as well, is it the neurons at each layer? How about the batch size? How are they correlated?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The number of filters is the number of neurons, since each neuron performs a different convolution on the input to the layer (more precisely, the neurons' input weights form convolution kernels).</p>\n<p>A feature map is the result of applying a filter (thus, you have as many feature maps as filters), and its size is a result of window/kernel size of your filter and stride.</p>\n<p>The following image was the best I could find to explain the concept at high level:\n<a href=\"https://i.sstatic.net/pLlwx.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/pLlwx.png\"/></a>\nNote that 2 different convolutional filters are applied to the input image, resulting in 2 different feature maps (the output of the filters). Each pixel of each feature map is an output of the convolutional layer.</p>\n<p>For instance, if you have 28x28 input images and a convolutional layer with 20 7x7 filters and stride 1, you will get 20 22x22 feature maps at the output of this layer. Note that this is presented to the next layer as a volume with width = height = 22 and depth = num_channels = 20. You could use the same representation to train your CNN on RGB images such as the ones from the CIFAR10 dataset, which would be 32x32x3 volumes (convolution is applied only to the 2 spatial dimensions).</p>\n<p>EDIT: There seems to be some confusion going on in the comments that I'd like to clarify. First, <em>there are no neurons</em>. Neurons are just a metaphor in neural networks. That said, \"how many neurons are there in a convolutional layer\" cannot be answered objectively, but relative to your view of the computations performed by the layer. In my view, a filter is a single neuron that <em>sweeps through the image</em>, providing <em>different activations</em> for each position. An entire feature map is produced by a single neuron/filter at multiple positions in my view. The commentors seem to have another view that is as valid as mine. They see each filter as a set of weights for a convolution operation, and one neuron for each attended position in the image, all sharing the same set of weights defined by the filter. Note that both views are <em>functionally (and even fundamentally) the same</em>, as they use the same parameters, computations, and produce the same results. Therefore, this is a non-issue.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is no correct answer as to what the best number of filters is. This strongly depends on the type and complexity of your (image) data. A suitable number of features is learnd from experience after working with similar types of datasets repeatedly over time. In general, the more features you want to capture (and are potentially available) in an image the higher the number of filters required in a CNN. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>More than 0 and less than the number of parameters in each filter.  For instance, if you have a 5x5 filter, 1 color channel (so, 5x5x1), then you should have less than 25 filters in that layer.  The reason being is that if you have 25 or more filters, you have at least 1 filter per pixel.  The filter bank should provide some lossy compression of the input, and if there are as many filters as parameters per filter, then it doesn't lose any data, it just massively overfits.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have used the</p>\n<pre><code>sklearn.preprocessing.OneHotEncoder\n</code></pre>\n<p>to transform some data the output is <code>scipy.sparse.csr.csr_matrix</code>\nhow can I merge it back into my original dataframe along with the other columns?</p>\n<p>I tried to use <code>pd.concat</code> but I get </p>\n<pre><code>TypeError: cannot concatenate a non-NDFrame object\n</code></pre>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If A is <code>csr_matrix</code>, you can use <a href=\"http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray\" rel=\"noreferrer\"><code>.toarray()</code></a> (there's also <code>.todense()</code> that produces a <code>numpy</code> <code>matrix</code>, which is also works for the <code>DataFrame</code> constructor):</p>\n<pre><code>df = pd.DataFrame(A.toarray())\n</code></pre>\n<p>You can then use this with <code>pd.concat()</code>.</p>\n<pre><code>A = csr_matrix([[1, 0, 2], [0, 3, 0]])\n    \n  (0, 0)    1\n  (0, 2)    2\n  (1, 1)    3\n\n&lt;class 'scipy.sparse.csr.csr_matrix'&gt;\n\npd.DataFrame(A.todense())\n\n   0  1  2\n0  1  0  2\n1  0  3  0\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null int64\n1    2 non-null int64\n2    2 non-null int64\n</code></pre>\n<p>In version 0.20, <code>pandas</code> introduced <a href=\"https://pandas.pydata.org/pandas-docs/stable/sparse.html#sparse-data-structures\" rel=\"noreferrer\">sparse data structures</a>, including the <a href=\"https://pandas.pydata.org/pandas-docs/stable/sparse.html#sparsedataframe\" rel=\"noreferrer\"><code>SparseDataFrame</code></a>.</p>\n<p>In pandas 1.0, <code>SparseDataFrame</code> was <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#migrating\" rel=\"noreferrer\">removed</a>:</p>\n<blockquote>\n<p>In older versions of pandas, the <code>SparseSeries</code> and <code>SparseDataFrame</code> classes were the preferred way to work with sparse data. With the advent of extension arrays, these subclasses are no longer needed. Their purpose is better served by using a regular Series or DataFrame with sparse values instead.</p>\n</blockquote>\n<p>The <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#migrating\" rel=\"noreferrer\">migration</a> guide shows how to use these new data structures.</p>\n<p>For instance, to create a <code>DataFrame</code> from a sparse matrix:</p>\n<pre><code>from scipy.sparse import csr_matrix\n\nA = csr_matrix([[1, 0, 2], [0, 3, 0]])\n\ndf = pd.DataFrame.sparse.from_spmatrix(A, columns=['A', 'B', 'C'])\n\ndf\n\n   A  B  C\n0  1  0  2\n1  0  3  0\n\ndf.dtypes\nA    Sparse[float64, 0]\nB    Sparse[float64, 0]\nC    Sparse[float64, 0]\ndtype: object\n</code></pre>\n<p>Alternatively, you can pass sparse matrices to <code>sklearn</code> to avoid running out of memory when converting back to <code>pandas</code>. Just convert your other data to sparse format by passing a <code>numpy</code> <code>array</code> to the <code>scipy.sparse.csr_matrix</code> constructor and use <code>scipy.sparse.hstack</code> to combine (see <a href=\"https://docs.scipy.org/doc/scipy/reference/sparse.html\" rel=\"noreferrer\">docs</a>).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>UPDATE for Pandas 1.0+</h1>\n<p>Per the Pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html\" rel=\"noreferrer\">Sparse data structures</a> documentation, <code>SparseDataFrame</code> and <code>SparseSeries</code> have been removed.</p>\n<h1>Sparse Pandas Dataframes</h1>\n<h3>Previous Way</h3>\n<pre><code>pd.SparseDataFrame({\"A\": [0, 1]})\n</code></pre>\n<h3>New Way</h3>\n<pre><code>pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 1])})\n</code></pre>\n<h1>Working with SciPy sparse <code>csr_matrix</code></h1>\n<h3>Previous Way</h3>\n<pre><code>from scipy.sparse import csr_matrix\nmatrix = csr_matrix((3, 4), dtype=np.int8)\ndf = pd.SparseDataFrame(matrix, columns=['A', 'B', 'C'])\n</code></pre>\n<h3>New Way</h3>\n<pre><code>from scipy.sparse import csr_matrix\nimport numpy as np\nimport pandas as pd\n\nmatrix = csr_matrix((3, 4), dtype=np.int8)\ndf = pd.DataFrame.sparse.from_spmatrix(matrix, columns=['A', 'B', 'C', 'D'])\ndf.dtypes\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>A    Sparse[int8, 0]\nB    Sparse[int8, 0]\nC    Sparse[int8, 0]\nD    Sparse[int8, 0]\ndtype: object\n</code></pre>\n<hr/>\n<h3>Conversion from Sparse to Dense</h3>\n<pre><code>df.sparse.to_dense()                                                                                                                                                                            \n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>   A  B  C  D\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n</code></pre>\n<hr/>\n<h3>Sparse Properties</h3>\n<pre><code>df.sparse.density                                                                                                                                                                           \n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>0.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could also avoid getting back a sparse matrix in the first place by setting the parameter <code>sparse</code> to <code>False</code> when creating the Encoder.</p>\n<p>The documentation of the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\" rel=\"nofollow noreferrer\">OneHotEncoder</a> states:</p>\n<blockquote>\n<p>sparse : boolean, default=True</p>\n<p>Will return sparse matrix if set True else will return an array.</p>\n</blockquote>\n<p>Then you can again call the DataFrame constructor to transform the numpy array to a DataFrame.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/54527439/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2019-02-06 15:17:28Z\">5 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2023-09-14 21:31:20Z\">last year</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p><strong>Needs more focus</strong> Update the question so it focuses on one problem only by <a href=\"/posts/54527439/edit\">editing this post</a>.</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/54527439/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Are these libraries fairly interchangeable?</p>\n<p>Looking here, <a href=\"https://stackshare.io/stackups/keras-vs-pytorch-vs-scikit-learn\" rel=\"noreferrer\">https://stackshare.io/stackups/keras-vs-pytorch-vs-scikit-learn</a>, it seems the major difference is the underlying framework (at least for PyTorch).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, there is a major difference.</p>\n<p>SciKit Learn is a general machine learning library, built on top of NumPy. It features a lot of machine learning algorithms such as support vector machines, random forests, as well as a lot of utilities for general pre- and postprocessing of data. It is not a neural network framework.</p>\n<p>PyTorch is a deep learning framework, consisting of</p>\n<ol>\n<li>A vectorized math library similar to NumPy, but with GPU support and a lot of neural network related operations (such as softmax or various kinds of activations)</li>\n<li>Autograd - an algorithm which can automatically calculate gradients of your functions, defined in terms of the basic operations</li>\n<li>Gradient-based optimization routines for large scale optimization, dedicated to neural network optimization</li>\n<li>Neural-network related utility functions</li>\n</ol>\n<p>Keras is a higher-level deep learning framework, which abstracts many details away, making code simpler and more concise than in PyTorch or TensorFlow, at the cost of limited hackability. It abstracts away the computation backend, which can be TensorFlow, Theano or CNTK. It does not support a PyTorch backend, but that's not something unfathomable - you can consider it a simplified and streamlined subset of the above.</p>\n<p>In short, if you are going with \"classic\", non-neural algorithms, neither PyTorch nor Keras will be useful for you. If you're doing deep learning, scikit-learn may still be useful for its utility part; aside from it you will need the actual deep learning framework, where you can choose between Keras and PyTorch but you're unlikely to use both at the same time. This is very subjective, but in my view, if you're working on a novel algorithm, you're more likely to go with PyTorch (or TensorFlow or some other lower-level framework) for flexibility. If you're adapting a known and tested algorithm to a new problem setting, you may want to go with Keras for its greater simplicity and lower entry level.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a few thousand audio files and I want to classify them using Keras and Theano. So far, I generated a 28x28 spectrograms (bigger is probably better, but I am just trying to get the algorithm work at this point) of each audio file and read the image into a matrix. So in the end I get this big image matrix to feed into the network for image classification.</p>\n<p>In a tutorial I found this mnist classification code:</p>\n<pre><code>import numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.utils import np_utils\n\nbatch_size = 128\nnb_classes = 10\nnb_epochs = 2\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\nX_train /= 255\nX_test /= 255\n\nprint(X_train.shape[0], \"train samples\")\nprint(X_test.shape[0], \"test samples\")\n\ny_train = np_utils.to_categorical(y_train, nb_classes)\ny_test =  np_utils.to_categorical(y_test, nb_classes)\n\nmodel = Sequential()\n\nmodel.add(Dense(output_dim = 100, input_dim = 784, activation= \"relu\"))\nmodel.add(Dense(output_dim = 200, activation = \"relu\"))\nmodel.add(Dense(output_dim = 200, activation = \"relu\"))\nmodel.add(Dense(output_dim = nb_classes, activation = \"softmax\"))\n\nmodel.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\")\n\nmodel.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epochs, show_accuracy = True, verbose = 2, validation_data = (X_test, y_test))\nscore = model.evaluate(X_test, y_test, show_accuracy = True, verbose = 0)\nprint(\"Test score: \", score[0])\nprint(\"Test accuracy: \", score[1])\n</code></pre>\n<p>This code runs, and I get the result as expected:</p>\n<pre><code>(60000L, 'train samples')\n(10000L, 'test samples')\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/2\n2s - loss: 0.2988 - acc: 0.9131 - val_loss: 0.1314 - val_acc: 0.9607\nEpoch 2/2\n2s - loss: 0.1144 - acc: 0.9651 - val_loss: 0.0995 - val_acc: 0.9673\n('Test score: ', 0.099454972004890438)\n('Test accuracy: ', 0.96730000000000005)\n</code></pre>\n<p>Up to this point everything runs perfectly, however when I apply the above algorithm to my dataset, accuracy gets stuck.</p>\n<p>My code is as follows:</p>\n<pre><code>import os\n\nimport pandas as pd\n\nfrom sklearn.cross_validation import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.layers.core import Dense, Activation, Dropout, Flatten\nfrom keras.utils import np_utils\n\nimport AudioProcessing as ap\nimport ImageTools as it\n\nbatch_size = 128\nnb_classes = 2\nnb_epoch = 10  \n\n\nfor i in range(20):\n    print \"\\n\"\n# Generate spectrograms if necessary\nif(len(os.listdir(\"./AudioNormalPathalogicClassification/Image\")) &gt; 0):\n    print \"Audio files are already processed. Skipping...\"\nelse:\n    print \"Generating spectrograms for the audio files...\"\n    ap.audio_2_image(\"./AudioNormalPathalogicClassification/Audio/\",\"./AudioNormalPathalogicClassification/Image/\",\".wav\",\".png\",(28,28))\n\n# Read the result csv\ndf = pd.read_csv('./AudioNormalPathalogicClassification/Result/result.csv', header = None)\n\ndf.columns = [\"RegionName\",\"IsNormal\"]\n\nbool_mapping = {True : 1, False : 0}\n\nnb_classes = 2\n\nfor col in df:\n    if(col == \"RegionName\"):\n        a = 3      \n    else:\n        df[col] = df[col].map(bool_mapping)\n\ny = df.iloc[:,1:].values\n\ny = np_utils.to_categorical(y, nb_classes)\n\n# Load images into memory\nprint \"Loading images into memory...\"\nX = it.load_images(\"./AudioNormalPathalogicClassification/Image/\",\".png\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nX_train = X_train.reshape(X_train.shape[0], 784)\nX_test = X_test.reshape(X_test.shape[0], 784)\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\nX_train /= 255\nX_test /= 255\n\nprint(\"X_train shape: \" + str(X_train.shape))\nprint(str(X_train.shape[0]) + \" train samples\")\nprint(str(X_test.shape[0]) + \" test samples\")\n\nmodel = Sequential()\n\n\nmodel.add(Dense(output_dim = 100, input_dim = 784, activation= \"relu\"))\nmodel.add(Dense(output_dim = 200, activation = \"relu\"))\nmodel.add(Dense(output_dim = 200, activation = \"relu\"))\nmodel.add(Dense(output_dim = nb_classes, activation = \"softmax\"))\n\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")\n\nprint model.summary()\n\nmodel.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epoch, show_accuracy = True, verbose = 1, validation_data = (X_test, y_test))\nscore = model.evaluate(X_test, y_test, show_accuracy = True, verbose = 1)\nprint(\"Test score: \", score[0])\nprint(\"Test accuracy: \", score[1])\n</code></pre>\n<p>AudioProcessing.py</p>\n<pre><code>import os\nimport scipy as sp\nimport scipy.io.wavfile as wav\nimport matplotlib.pylab as pylab\nimport Image\n\ndef save_spectrogram_scipy(source_filename, destination_filename, size):\n    dt = 0.0005\n    NFFT = 1024       \n    Fs = int(1.0/dt)  \n    fs, audio = wav.read(source_filename)\n    if(len(audio.shape) &gt;= 2):\n        audio = sp.mean(audio, axis = 1)\n    fig = pylab.figure()    \n    ax = pylab.Axes(fig, [0,0,1,1])    \n    ax.set_axis_off()\n    fig.add_axes(ax) \n    pylab.specgram(audio, NFFT = NFFT, Fs = Fs, noverlap = 900, cmap=\"gray\")\n    pylab.savefig(destination_filename)\n    img = Image.open(destination_filename).convert(\"L\")\n    img = img.resize(size)\n    img.save(destination_filename)\n    pylab.clf()\n    del img\n\ndef audio_2_image(source_directory, destination_directory, audio_extension, image_extension, size):\n    nb_files = len(os.listdir(source_directory));\n    count = 0\n    for file in os.listdir(source_directory):\n        if file.endswith(audio_extension):        \n            destinationName = file[:-4]\n            save_spectrogram_scipy(source_directory + file, destination_directory + destinationName + image_extension, size)\n            count += 1\n            print (\"Generating spectrogram for files \" + str(count) + \" / \" + str(nb_files) + \".\")\n</code></pre>\n<p>ImageTools.py</p>\n<pre><code>import os\nimport numpy as np\nimport matplotlib.image as mpimg\ndef load_images(source_directory, image_extension):\n    image_matrix = []\n    nb_files = len(os.listdir(source_directory));\n    count = 0\n    for file in os.listdir(source_directory):\n        if file.endswith(image_extension):\n            with open(source_directory + file,\"r+b\") as f:\n                img = mpimg.imread(f)\n                img = img.flatten()                \n                image_matrix.append(img)\n                del img\n                count += 1\n                #print (\"File \" + str(count) + \" / \" + str(nb_files) + \" loaded.\")\n    return np.asarray(image_matrix)\n</code></pre>\n<p>So I run the above code and recieve:</p>\n<pre><code>Audio files are already processed. Skipping...\nLoading images into memory...\nX_train shape: (2394L, 784L)\n2394 train samples\n1027 test samples\n--------------------------------------------------------------------------------\nInitial input shape: (None, 784)\n--------------------------------------------------------------------------------\nLayer (name)                  Output Shape                  Param #\n--------------------------------------------------------------------------------\nDense (dense)                 (None, 100)                   78500\nDense (dense)                 (None, 200)                   20200\nDense (dense)                 (None, 200)                   40200\nDense (dense)                 (None, 2)                     402\n--------------------------------------------------------------------------------\nTotal params: 139302\n--------------------------------------------------------------------------------\nNone\nTrain on 2394 samples, validate on 1027 samples\nEpoch 1/10\n2394/2394 [==============================] - 0s - loss: 0.6898 - acc: 0.5455 - val_loss: 0.6835 - val_acc: 0.5716\nEpoch 2/10\n2394/2394 [==============================] - 0s - loss: 0.6879 - acc: 0.5522 - val_loss: 0.6901 - val_acc: 0.5716\nEpoch 3/10\n2394/2394 [==============================] - 0s - loss: 0.6880 - acc: 0.5522 - val_loss: 0.6842 - val_acc: 0.5716\nEpoch 4/10\n2394/2394 [==============================] - 0s - loss: 0.6883 - acc: 0.5522 - val_loss: 0.6829 - val_acc: 0.5716\nEpoch 5/10\n2394/2394 [==============================] - 0s - loss: 0.6885 - acc: 0.5522 - val_loss: 0.6836 - val_acc: 0.5716\nEpoch 6/10\n2394/2394 [==============================] - 0s - loss: 0.6887 - acc: 0.5522 - val_loss: 0.6832 - val_acc: 0.5716\nEpoch 7/10\n2394/2394 [==============================] - 0s - loss: 0.6882 - acc: 0.5522 - val_loss: 0.6859 - val_acc: 0.5716\nEpoch 8/10\n2394/2394 [==============================] - 0s - loss: 0.6882 - acc: 0.5522 - val_loss: 0.6849 - val_acc: 0.5716\nEpoch 9/10\n2394/2394 [==============================] - 0s - loss: 0.6885 - acc: 0.5522 - val_loss: 0.6836 - val_acc: 0.5716\nEpoch 10/10\n2394/2394 [==============================] - 0s - loss: 0.6877 - acc: 0.5522 - val_loss: 0.6849 - val_acc: 0.5716\n1027/1027 [==============================] - 0s\n('Test score: ', 0.68490593621422047)\n('Test accuracy: ', 0.57156767283349563)\n</code></pre>\n<p>I tried changing the network, adding more epochs, but I always get the same result no matter what. I don't understand why I am getting the same result.</p>\n<p>Any help would be appreciated. Thank you.</p>\n<p>Edit:\nI found a mistake where pixel values were not read correctly. I fixed the ImageTools.py below as:</p>\n<pre><code>import os\nimport numpy as np\nfrom scipy.misc import imread\n\ndef load_images(source_directory, image_extension):\n    image_matrix = []\n    nb_files = len(os.listdir(source_directory));\n    count = 0\n    for file in os.listdir(source_directory):\n        if file.endswith(image_extension):\n            with open(source_directory + file,\"r+b\") as f:\n                img = imread(f)                \n                img = img.flatten()                        \n                image_matrix.append(img)\n                del img\n                count += 1\n                #print (\"File \" + str(count) + \" / \" + str(nb_files) + \" loaded.\")\n    return np.asarray(image_matrix)\n</code></pre>\n<p>Now I actually get grayscale pixel values from 0 to 255, so now my dividing it by 255 makes sense. However, I still get the same result.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The most likely reason is that the optimizer is not suited to your dataset. Here is a list of <a href=\"http://keras.io/optimizers/\" rel=\"noreferrer\">Keras optimizers</a> from the documentation.</p>\n<p>I recommend you first try SGD with default parameter values. If it still doesn't work, divide the learning rate by 10. Do that a few times if necessary. If your learning rate reaches 1e-6 and it still doesn't work, then you have another problem.</p>\n<p>In summary, replace this line:</p>\n<pre><code>model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")\n</code></pre>\n<p>with this:</p>\n<pre><code>from keras.optimizers import SGD\nopt = SGD(lr=0.01)\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = opt)\n</code></pre>\n<p>and change the learning rate a few times if it doesn't work.</p>\n<p>If it was the problem, you should see the loss getting lower after just a few epochs.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Another solution that I do not see mentioned here, but caused a similar problem for me was the activiation function of the last neuron, especialy if it is <code>relu</code> and not something non linear like <code>sigmoid</code>.</p>\n<p>In other words, it might help you to <strong>use a non-linear activation function in the last layer</strong></p>\n<p>Last layer:</p>\n<pre><code>model.add(keras.layers.Dense(1, activation='relu'))\n</code></pre>\n<p>Output:</p>\n<pre><code>7996/7996 [==============================] - 1s 76us/sample - loss: 6.3474 - accuracy: 0.5860\nEpoch 2/30\n7996/7996 [==============================] - 0s 58us/sample - loss: 6.3473 - accuracy: 0.5860\nEpoch 3/30\n7996/7996 [==============================] - 0s 58us/sample - loss: 6.3473 - accuracy: 0.5860\nEpoch 4/30\n7996/7996 [==============================] - 0s 57us/sample - loss: 6.3473 - accuracy: 0.5860\nEpoch 5/30\n7996/7996 [==============================] - 0s 58us/sample - loss: 6.3473 - accuracy: 0.5860\nEpoch 6/30\n7996/7996 [==============================] - 0s 60us/sample - loss: 6.3473 - accuracy: 0.5860\nEpoch 7/30\n7996/7996 [==============================] - 0s 57us/sample - loss: 6.3473 - accuracy: 0.5860\nEpoch 8/30\n7996/7996 [==============================] - 0s 57us/sample - loss: 6.3473 - accuracy: 0.5860\n</code></pre>\n<p>Now I used a non linear activation function:</p>\n<pre><code>model.add(keras.layers.Dense(1, activation='sigmoid'))\n</code></pre>\n<p>Output:</p>\n<pre><code>7996/7996 [==============================] - 1s 74us/sample - loss: 0.7663 - accuracy: 0.5899\nEpoch 2/30\n7996/7996 [==============================] - 0s 59us/sample - loss: 0.6243 - accuracy: 0.5860\nEpoch 3/30\n7996/7996 [==============================] - 0s 56us/sample - loss: 0.5399 - accuracy: 0.7580\nEpoch 4/30\n7996/7996 [==============================] - 0s 56us/sample - loss: 0.4694 - accuracy: 0.7905\nEpoch 5/30\n7996/7996 [==============================] - 0s 57us/sample - loss: 0.4363 - accuracy: 0.8040\nEpoch 6/30\n7996/7996 [==============================] - 0s 60us/sample - loss: 0.4139 - accuracy: 0.8099\nEpoch 7/30\n7996/7996 [==============================] - 0s 58us/sample - loss: 0.3967 - accuracy: 0.8228\nEpoch 8/30\n7996/7996 [==============================] - 0s 61us/sample - loss: 0.3826 - accuracy: 0.8260\n</code></pre>\n<p><em>This is not directly a solution to the original answer, but as the answer is #1 on Google when searching for this problem, it might benefit someone.</em></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If the accuracy is not changing, it means the optimizer has found a local minimum for the loss. This may be an undesirable minimum. One common local minimum is to  always predict the class with the most number of data points. You should use weighting on the classes to avoid this minimum.</p>\n<pre><code>from sklearn.utils import compute_class_weight\nclassWeight = compute_class_weight('balanced', outputLabels, outputs) \nclassWeight = dict(enumerate(classWeight))\nmodel.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epochs, show_accuracy = True, verbose = 2, validation_data = (X_test, y_test), class_weight=classWeight)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The problem is that my train data could not be placed into RAM due to train data size. So I need a method which first builds one tree on whole train data set, calculate residuals build another tree and so on (like gradient boosted tree do). Obviously if I call <code>model = xgb.train(param, batch_dtrain, 2)</code> in some loop - it will not help, because in such case it just rebuilds whole model for each batch.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try saving your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model.</p>\n<p>Here's a small experiment that I ran to convince myself that it works:</p>\n<p>First, split the boston dataset into training and testing sets.\nThen split the training set into halves.\nFit a model with the first half and get a score that will serve as a benchmark.\nThen fit two models with the second half; one model will have the additional parameter <em>xgb_model</em>. If passing in the extra parameter didn't make a difference, then we would expect their scores to be similar..\nBut, fortunately, the new model seems to perform much better than the first.</p>\n<pre><code>import xgboost as xgb\nfrom sklearn.cross_validation import train_test_split as ttsplit\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error as mse\n\nX = load_boston()['data']\ny = load_boston()['target']\n\n# split data into training and testing sets\n# then split training set in half\nX_train, X_test, y_train, y_test = ttsplit(X, y, test_size=0.1, random_state=0)\nX_train_1, X_train_2, y_train_1, y_train_2 = ttsplit(X_train, \n                                                     y_train, \n                                                     test_size=0.5,\n                                                     random_state=0)\n\nxg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\nxg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\nxg_test = xgb.DMatrix(X_test, label=y_test)\n\nparams = {'objective': 'reg:linear', 'verbose': False}\nmodel_1 = xgb.train(params, xg_train_1, 30)\nmodel_1.save_model('model_1.model')\n\n# ================= train two versions of the model =====================#\nmodel_2_v1 = xgb.train(params, xg_train_2, 30)\nmodel_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model='model_1.model')\n\nprint(mse(model_1.predict(xg_test), y_test))     # benchmark\nprint(mse(model_2_v1.predict(xg_test), y_test))  # \"before\"\nprint(mse(model_2_v2.predict(xg_test), y_test))  # \"after\"\n\n# 23.0475232194\n# 39.6776876084\n# 27.2053239482\n</code></pre>\n<p>reference: <a href=\"https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py\" rel=\"noreferrer\">https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is now (version 0.6?) a process_update parameter that might help.  Here's an experiment with it:</p>\n<pre><code>import pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error as mse\n\nboston = load_boston()\nfeatures = boston.feature_names\nX = boston.data\ny = boston.target\n\nX=pd.DataFrame(X,columns=features)\ny = pd.Series(y,index=X.index)\n\n# split data into training and testing sets\nrs = ShuffleSplit(test_size=0.3, n_splits=1, random_state=0)\nfor train_idx,test_idx in rs.split(X):  # this looks silly\n    pass\n\ntrain_split = round(len(train_idx) / 2)\ntrain1_idx = train_idx[:train_split]\ntrain2_idx = train_idx[train_split:]\nX_train = X.loc[train_idx]\nX_train_1 = X.loc[train1_idx]\nX_train_2 = X.loc[train2_idx]\nX_test = X.loc[test_idx]\ny_train = y.loc[train_idx]\ny_train_1 = y.loc[train1_idx]\ny_train_2 = y.loc[train2_idx]\ny_test = y.loc[test_idx]\n\nxg_train_0 = xgb.DMatrix(X_train, label=y_train)\nxg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\nxg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\nxg_test = xgb.DMatrix(X_test, label=y_test)\n\nparams = {'objective': 'reg:linear', 'verbose': False}\nmodel_0 = xgb.train(params, xg_train_0, 30)\nmodel_1 = xgb.train(params, xg_train_1, 30)\nmodel_1.save_model('model_1.model')\nmodel_2_v1 = xgb.train(params, xg_train_2, 30)\nmodel_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model=model_1)\n\nparams.update({'process_type': 'update',\n               'updater'     : 'refresh',\n               'refresh_leaf': True})\nmodel_2_v2_update = xgb.train(params, xg_train_2, 30, xgb_model=model_1)\n\nprint('full train\\t',mse(model_0.predict(xg_test), y_test)) # benchmark\nprint('model 1 \\t',mse(model_1.predict(xg_test), y_test))  \nprint('model 2 \\t',mse(model_2_v1.predict(xg_test), y_test))  # \"before\"\nprint('model 1+2\\t',mse(model_2_v2.predict(xg_test), y_test))  # \"after\"\nprint('model 1+update2\\t',mse(model_2_v2_update.predict(xg_test), y_test))  # \"after\"\n</code></pre>\n<p>Output:</p>\n<pre><code>full train   17.8364309709\nmodel 1      24.2542132108\nmodel 2      25.6967017352\nmodel 1+2    22.8846455135\nmodel 1+update2  14.2816257268\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I created <a href=\"https://gist.github.com/53fef94cc61d6a3e9b3eb900482f41e0\" rel=\"noreferrer\">a gist of jupyter notebook</a> to demonstrate that xgboost model can be trained incrementally. I used boston dataset to train the model. I did 3 experiments - one shot learning, iterative one shot learning, iterative incremental learning. In incremental training, I passed the boston data to the model in batches of size 50.</p>\n<p>The gist of the gist is that you'll have to iterate over the data multiple times for the model to converge to the accuracy attained by one shot (all data) learning.</p>\n<p>Here is the corresponding code for doing iterative incremental learning with xgboost.</p>\n<pre><code>batch_size = 50\niterations = 25\nmodel = None\nfor i in range(iterations):\n    for start in range(0, len(x_tr), batch_size):\n        model = xgb.train({\n            'learning_rate': 0.007,\n            'update':'refresh',\n            'process_type': 'update',\n            'refresh_leaf': True,\n            #'reg_lambda': 3,  # L2\n            'reg_alpha': 3,  # L1\n            'silent': False,\n        }, dtrain=xgb.DMatrix(x_tr[start:start+batch_size], y_tr[start:start+batch_size]), xgb_model=model)\n\n        y_pr = model.predict(xgb.DMatrix(x_te))\n        #print('    MSE itr@{}: {}'.format(int(start/batch_size), sklearn.metrics.mean_squared_error(y_te, y_pr)))\n    print('MSE itr@{}: {}'.format(i, sklearn.metrics.mean_squared_error(y_te, y_pr)))\n\ny_pr = model.predict(xgb.DMatrix(x_te))\nprint('MSE at the end: {}'.format(sklearn.metrics.mean_squared_error(y_te, y_pr)))\n</code></pre>\n<p>XGBoost version: 0.6</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The introductory documentation, which I am reading (<a href=\"https://www.tensorflow.org/get_started/\" rel=\"noreferrer\">TOC here</a>) uses the term \"batch\" (<a href=\"https://www.tensorflow.org/tutorials/keras/classification\" rel=\"noreferrer\">for instance here</a>) without having defined it.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's say you want to do digit recognition (MNIST) and you have defined your architecture of the network (CNNs). Now, you can start feeding the images from the training data one by one to the network, get the prediction (till this step it's called as doing <em>inference</em>), compute the loss, compute the gradient, and then update the parameters of your network (i.e. <em>weights</em> and <em>biases</em>) and then proceed with the next image ... This way of training the model is sometimes called as <em>online learning</em>.</p>\n<p>But, you want the training to be faster, the gradients to be less noisy, and also take advantage of the power of GPUs which are efficient at doing array operations (<em>nD-arrays</em> to be specific). So, what you instead do is feed in <strong>say 100 images at a time</strong> (the choice of this size is up to you (i.e. it's a <em>hyperparameter</em>) and depends on your problem too). For instance, take a look at the below picture, (Author: Martin Gorner)</p>\n<p><a href=\"https://i.sstatic.net/8FzdQ.png\" rel=\"noreferrer\"><img alt=\"Batch size of 100\" src=\"https://i.sstatic.net/8FzdQ.png\"/></a></p>\n<p>Here, since you're feeding in 100 images(<code>28x28</code>) at a time (instead of 1 as in the online training case), the <strong>batch size is 100</strong>. Oftentimes this is called as <em>mini-batch size</em> or simply <code>mini-batch</code>.</p>\n<hr/>\n<p>Also the below picture: (Author: Martin Gorner)</p>\n<p><a href=\"https://i.sstatic.net/vncAa.png\" rel=\"noreferrer\"><img alt=\"batch size again\" src=\"https://i.sstatic.net/vncAa.png\"/></a></p>\n<p>Now, the matrix multiplication will all just work out perfectly fine and you will also be taking advantage of the highly optimized array operations and hence achieve faster <em>training</em> time.</p>\n<p>If you observe the above picture, it doesn't matter that much whether you give 100 or 256 or 2048 or 10000 (<em>batch size</em>) images as long as it fits in the memory of your (GPU) hardware. You'll simply get that many predictions.</p>\n<p>But, please keep in mind that this <em>batch size</em> influences the training time, the error that you achieve, the gradient shifts etc., There is no general rule of thumb as to which batch size works out best. Just try a few sizes and pick the one which works best for you. But try not to use large batch sizes since it will overfit the data. People commonly use mini-batch sizes of <code>32, 64, 128, 256, 512, 1024, 2048</code>.</p>\n<hr/>\n<p><strong>Bonus</strong>: To get a good grasp of how crazy you can go with this batch size, please give this paper a read: <a href=\"https://arxiv.org/pdf/1404.5997.pdf\" rel=\"noreferrer\">weird trick for parallelizing CNNs</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there a built-in way for getting accuracy scores for each class separatetly? I know in sklearn we can get overall accuracy by using <code>metric.accuracy_score</code>. Is there a way to get the breakdown of accuracy scores for individual classes? Something similar to <code>metrics.classification_report</code>.</p>\n<pre><code>from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\ny_true = [0, 1, 2, 2, 2]\ny_pred = [0, 0, 2, 2, 1]\ntarget_names = ['class 0', 'class 1', 'class 2']\n</code></pre>\n<p><code>classification_report</code> does not give accuracy scores:</p>\n<pre><code>print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n\nOut[9]:         precision    recall  f1-score   support\n\nclass 0     0.5000    1.0000    0.6667         1\nclass 1     0.0000    0.0000    0.0000         1\nclass 2     1.0000    0.6667    0.8000         3\n\navg / total     0.7000    0.6000    0.6133         5\n</code></pre>\n<p>Accuracy score gives only the overall accuracy:</p>\n<pre><code>accuracy_score(y_true, y_pred)\nOut[10]: 0.59999999999999998\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>from sklearn.metrics import confusion_matrix\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\nmatrix = confusion_matrix(y_true, y_pred)\nmatrix.diagonal()/matrix.sum(axis=1)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use sklearn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\" rel=\"noreferrer\">confusion matrix</a> to get the accuracy</p>\n<pre><code>from sklearn.metrics import confusion_matrix\nimport numpy as np\n\ny_true = [0, 1, 2, 2, 2]\ny_pred = [0, 0, 2, 2, 1]\ntarget_names = ['class 0', 'class 1', 'class 2']\n\n#Get the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n#array([[1, 0, 0],\n#   [1, 0, 0],\n#   [0, 1, 2]])\n\n#Now the normalize the diagonal entries\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n#array([[1.        , 0.        , 0.        ],\n#      [1.        , 0.        , 0.        ],\n#      [0.        , 0.33333333, 0.66666667]])\n\n#The diagonal entries are the accuracies of each class\ncm.diagonal()\n#array([1.        , 0.        , 0.66666667])\n</code></pre>\n<p><strong>References</strong></p>\n<ul>\n<li><a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\" rel=\"noreferrer\">plot Confusion matrix sklearn</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am adding my answer as I haven't found any answer to this exact question online, and because I think that the other calculation methods suggested here before me are incorrect.</p>\n<p>Remember that accuracy is defined as:</p>\n<pre><code>accuracy = (true_positives + true_negatives) / all_samples\n</code></pre>\n<p>Or to put it into words; it is the ratio between the number of correctly classified examples (either positive or negative) and the total number of examples in the test set.</p>\n<p>One thing that is important to note is that for both TN and FN, the \"negative\" is class agnostic, meaning \"not predicted as the specific class in question\". For example, consider the following:</p>\n<pre><code>y_true = ['cat', 'dog', 'bird', 'bird']\ny_pred = ['cat', 'dog', 'cat', 'dog']\n</code></pre>\n<p>Here, both the second 'cat' prediction and the second 'dog' prediction are false negatives simply because they are not 'bird'.</p>\n<p><strong>To your question:</strong></p>\n<p>As far as I know, there is currently no package that provides a method that does what you are looking for, but based on the definition of accuracy, we can use the confusion matrix method from sklearn to calculate it ourselves.</p>\n<pre><code>from sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Get the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# We will store the results in a dictionary for easy access later\nper_class_accuracies = {}\n\n# Calculate the accuracy for each one of our classes\nfor idx, cls in enumerate(classes):\n    # True negatives are all the samples that are not our current GT class (not the current row) \n    # and were not predicted as the current class (not the current column)\n    true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n    \n    # True positives are all the samples of our current GT class that were predicted as such\n    true_positives = cm[idx, idx]\n    \n    # The accuracy for the current class is the ratio between correct predictions to all predictions\n    per_class_accuracies[cls] = (true_positives + true_negatives) / np.sum(cm)\n</code></pre>\n<p>The original question was posted a while ago, but this might help anyone who comes here through Google, like me.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I calculated a confusion matrix for my classifier using <code>confusion_matrix()</code> from scikit-learn. The diagonal elements of the confusion matrix represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier.</p>\n<p>I would like to normalize my confusion matrix so that it contains only numbers between 0 and 1. I would like to read the percentage of correctly classified samples from the matrix.</p>\n<p>I found several methods how to normalize a matrix (row and column normalization) but I don't know much about maths and am not sure if this is the correct approach.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose that</p>\n<pre><code>&gt;&gt;&gt; y_true = [0, 0, 1, 1, 2, 0, 1]\n&gt;&gt;&gt; y_pred = [0, 1, 0, 1, 2, 2, 1]\n&gt;&gt;&gt; C = confusion_matrix(y_true, y_pred)\n&gt;&gt;&gt; C\narray([[1, 1, 1],\n       [1, 2, 0],\n       [0, 0, 1]])\n</code></pre>\n<p>Then, to find out how many samples per class have received their correct label, you need</p>\n<pre><code>&gt;&gt;&gt; C / C.astype(np.float).sum(axis=1)\narray([[ 0.33333333,  0.33333333,  1.        ],\n       [ 0.33333333,  0.66666667,  0.        ],\n       [ 0.        ,  0.        ,  1.        ]])\n</code></pre>\n<p>The diagonal contains the required values. Another way to compute these is to realize that what you're computing is the recall per class:</p>\n<pre><code>&gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support\n&gt;&gt;&gt; _, recall, _, _ = precision_recall_fscore_support(y_true, y_pred)\n&gt;&gt;&gt; recall\narray([ 0.33333333,  0.66666667,  1.        ])\n</code></pre>\n<p>Similarly, if you divide by the sum over <code>axis=0</code>, you get the precision (fraction of class-<code>k</code> predictions that have ground truth label <code>k</code>):</p>\n<pre><code>&gt;&gt;&gt; C / C.astype(np.float).sum(axis=0)\narray([[ 0.5       ,  0.33333333,  0.5       ],\n       [ 0.5       ,  0.66666667,  0.        ],\n       [ 0.        ,  0.        ,  0.5       ]])\n&gt;&gt;&gt; prec, _, _, _ = precision_recall_fscore_support(y_true, y_pred)\n&gt;&gt;&gt; prec\narray([ 0.5       ,  0.66666667,  0.5       ])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using Seaborn you can easily print a normalised AND pretty confusion matrix with a heathmap:</p>\n<p><a href=\"https://i.sstatic.net/4loB6.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/4loB6.png\"/></a></p>\n<pre><code>from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\n# Normalise\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show(block=False)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Nowadays, scikit-learn's confusion matrix comes with a <code>normalize</code> argument; from the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\" rel=\"noreferrer\">docs</a>:</p>\n<blockquote>\n<p><strong>normalize : <em>{'true', 'pred', 'all'}, default=None</em></strong></p>\n<p>Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix\nwill not be normalized.</p>\n</blockquote>\n<p>So, if you want the values normalized over all samples, you should use</p>\n<pre><code>confusion_matrix(y_true, y_pred, normalize='all')\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was trying to do a simple thing which was train a linear model with Stochastic Gradient Descent (SGD) using torch:</p>\n<pre><code>import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\n\nimport pdb\n\ndef get_batch2(X,Y,M,dtype):\n    X,Y = X.data.numpy(), Y.data.numpy()\n    N = len(Y)\n    valid_indices = np.array( range(N) )\n    batch_indices = np.random.choice(valid_indices,size=M,replace=False)\n    batch_xs = torch.FloatTensor(X[batch_indices,:]).type(dtype)\n    batch_ys = torch.FloatTensor(Y[batch_indices]).type(dtype)\n    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)\n\ndef poly_kernel_matrix( x,D ):\n    N = len(x)\n    Kern = np.zeros( (N,D+1) )\n    for n in range(N):\n        for d in range(D+1):\n            Kern[n,d] = x[n]**d;\n    return Kern\n\n## data params\nN=5 # data set size\nDegree=4 # number dimensions/features\nD_sgd = Degree+1\n##\nx_true = np.linspace(0,1,N) # the real data points\ny = np.sin(2*np.pi*x_true)\ny.shape = (N,1)\n## TORCH\ndtype = torch.FloatTensor\n# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\nX_mdl = poly_kernel_matrix( x_true,Degree )\nX_mdl = Variable(torch.FloatTensor(X_mdl).type(dtype), requires_grad=False)\ny = Variable(torch.FloatTensor(y).type(dtype), requires_grad=False)\n## SGD mdl\nw_init = torch.zeros(D_sgd,1).type(dtype)\nW = Variable(w_init, requires_grad=True)\nM = 5 # mini-batch size\neta = 0.1 # step size\nfor i in range(500):\n    batch_xs, batch_ys = get_batch2(X_mdl,y,M,dtype)\n    # Forward pass: compute predicted y using operations on Variables\n    y_pred = batch_xs.mm(W)\n    # Compute and print loss using operations on Variables. Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape (1,); loss.data[0] is a scalar value holding the loss.\n    loss = (1/N)*(y_pred - batch_ys).pow(2).sum()\n    # Use autograd to compute the backward pass. Now w will have gradients\n    loss.backward()\n    # Update weights using gradient descent; w1.data are Tensors,\n    # w.grad are Variables and w.grad.data are Tensors.\n    W.data -= eta * W.grad.data\n    # Manually zero the gradients after updating weights\n    W.grad.data.zero_()\n\n#\nc_sgd = W.data.numpy()\nX_mdl = X_mdl.data.numpy()\ny = y.data.numpy()\n#\nXc_pinv = np.dot(X_mdl,c_sgd)\nprint('J(c_sgd) = ', (1/N)*(np.linalg.norm(y-Xc_pinv)**2) )\nprint('loss = ',loss.data[0])\n</code></pre>\n<p>the code runs fine and all though my <code>get_batch2</code> method seems really dum/naive, its probably because I am new to pytorch but I have not found a good place where they discuss how to retrieve data batches. I went through their tutorials (<a href=\"http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\" rel=\"noreferrer\">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a>) and through the data set (<a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\" rel=\"noreferrer\">http://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a>) with no luck. The tutorials all seem to assume that one already has the batch and batch-size at the beginning and then proceeds to train with that data without changing it (specifically look at <a href=\"http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd\" rel=\"noreferrer\">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd</a>).</p>\n<p>So my question is do I really need to turn my data back into numpy so that I can fetch some random sample of it and then turn it back to pytorch with Variable to be able to train in memory? Is there no way to get mini-batches with torch?</p>\n<p>I looked at a few functions torch provides but with no luck:</p>\n<pre><code>#pdb.set_trace()\n#valid_indices = torch.arange(0,N).numpy()\n#valid_indices = np.array( range(N) )\n#batch_indices = np.random.choice(valid_indices,size=M,replace=False)\n#indices = torch.LongTensor(batch_indices)\n#batch_xs, batch_ys = torch.index_select(X_mdl, 0, indices), torch.index_select(y, 0, indices)\n#batch_xs,batch_ys = torch.index_select(X_mdl, 0, indices), torch.index_select(y, 0, indices)\n</code></pre>\n<p>even though the code I provided works fine I am worried that its not an efficient implementation AND that if I were to use GPUs that there would be a considerable further slow down (because my guess it putting things in memory and then fetching them back to put them GPU like that is silly).</p>\n<hr/>\n<p>I implemented a new one based on the answer that suggested to use <code>torch.index_select()</code>:</p>\n<pre><code>def get_batch2(X,Y,M):\n    '''\n    get batch for pytorch model\n    '''\n    # TODO fix and make it nicer, there is pytorch forum question\n    #X,Y = X.data.numpy(), Y.data.numpy()\n    X,Y = X, Y\n    N = X.size()[0]\n    batch_indices = torch.LongTensor( np.random.randint(0,N+1,size=M) )\n    pdb.set_trace()\n    batch_xs = torch.index_select(X,0,batch_indices)\n    batch_ys = torch.index_select(Y,0,batch_indices)\n    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)\n</code></pre>\n<p>however, this seems to have issues because it does not work if <code>X,Y</code> are NOT variables...which is really odd. I added this to the pytorch forum: <a href=\"https://discuss.pytorch.org/t/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way/10322\" rel=\"noreferrer\">https://discuss.pytorch.org/t/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way/10322</a></p>\n<p>Right now what I am struggling with is making this work for gpu. My most current version:</p>\n<pre><code>def get_batch2(X,Y,M,dtype):\n    '''\n    get batch for pytorch model\n    '''\n    # TODO fix and make it nicer, there is pytorch forum question\n    #X,Y = X.data.numpy(), Y.data.numpy()\n    X,Y = X, Y\n    N = X.size()[0]\n    if dtype ==  torch.cuda.FloatTensor:\n        batch_indices = torch.cuda.LongTensor( np.random.randint(0,N,size=M) )# without replacement\n    else:\n        batch_indices = torch.LongTensor( np.random.randint(0,N,size=M) ).type(dtype)  # without replacement\n    pdb.set_trace()\n    batch_xs = torch.index_select(X,0,batch_indices)\n    batch_ys = torch.index_select(Y,0,batch_indices)\n    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)\n</code></pre>\n<p>the error:</p>\n<pre><code>RuntimeError: tried to construct a tensor from a int sequence, but found an item of type numpy.int64 at index (0)\n</code></pre>\n<p>I don't get it, do I really have to do:</p>\n<pre><code>ints = [ random.randint(0,N) for i i range(M)]\n</code></pre>\n<p>to get the integers?</p>\n<p>It would also be ideal if the data could be a variable. It seems that it <code>torch.index_select</code> does not work for <code>Variable</code> type data.</p>\n<p>this list of integers thing still doesn't work:</p>\n<pre><code>TypeError: torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:\n * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If I'm understanding your code correctly, your <code>get_batch2</code> function appears to be taking random mini-batches from your dataset without tracking which indices you've used already in an epoch. The issue with this implementation is that it likely will not make use of all of your data.</p>\n<p>The way I usually do batching is creating a random permutation of all the possible vertices using <code>torch.randperm(N)</code> and loop through them in batches. For example:</p>\n<pre><code>n_epochs = 100 # or whatever\nbatch_size = 128 # or whatever\n\nfor epoch in range(n_epochs):\n\n    # X is a torch Variable\n    permutation = torch.randperm(X.size()[0])\n\n    for i in range(0,X.size()[0], batch_size):\n        optimizer.zero_grad()\n\n        indices = permutation[i:i+batch_size]\n        batch_x, batch_y = X[indices], Y[indices]\n\n        # in case you wanted a semi-full example\n        outputs = model(batch_x)\n        loss = lossfunction(outputs,batch_y)\n\n        loss.backward()\n        optimizer.step()\n</code></pre>\n<p>If you like to copy and paste, make sure you define your optimizer, model, and lossfunction somewhere before the start of the epoch loop.</p>\n<p>With regards to your error, try using <code>torch.from_numpy(np.random.randint(0,N,size=M)).long()</code> instead of <code>torch.LongTensor(np.random.randint(0,N,size=M))</code>. I'm not sure if this will solve the error you are getting, but it will solve a future error.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use data loaders. </p>\n<h1>Data Set</h1>\n<p>First you define a dataset. You can use packages datasets in <code>torchvision.datasets</code> or use <code>ImageFolder</code> dataset class which follows the structure of Imagenet. </p>\n<pre><code>trainset=torchvision.datasets.ImageFolder(root='/path/to/your/data/trn', transform=generic_transform)\ntestset=torchvision.datasets.ImageFolder(root='/path/to/your/data/val', transform=generic_transform)\n</code></pre>\n<h1>Transforms</h1>\n<p>Transforms are very useful for preprocessing loaded data on the fly. If you are using images, you have to use the <code>ToTensor()</code> transform to convert loaded images from <code>PIL</code> to <code>torch.tensor</code>. More transforms can be packed into a composit transform as follows.</p>\n<pre><code>generic_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.ToPILImage(),\n    #transforms.CenterCrop(size=128),\n    transforms.Lambda(lambda x: myimresize(x, (128, 128))),\n    transforms.ToTensor(),\n    transforms.Normalize((0., 0., 0.), (6, 6, 6))\n])\n</code></pre>\n<h1>Data Loader</h1>\n<p>Then you define a data loader which prepares the next batch while training. You can set number of threads for data loading. </p>\n<pre><code>trainloader=torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=8)\ntestloader=torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=8)\n</code></pre>\n<p>For training, you just enumerate on the data loader.</p>\n<pre><code>  for i, data in enumerate(trainloader, 0):\n    inputs, labels = data    \n    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n    # continue training...\n</code></pre>\n<h1>NumPy Stuff</h1>\n<p>Yes. You have to convert <code>torch.tensor</code> to <code>numpy</code> using <code>.numpy()</code> method to work on it. If you are using CUDA you have to download the data from GPU to CPU first using the <code>.cpu()</code> method before calling <code>.numpy()</code>. Personally, coming from MATLAB background, I prefer to do most of the work with torch tensor, then convert data to numpy only for visualisation. Also bear in mind that torch stores data in a channel-first mode while numpy and PIL work with channel-last. This means you need to use <code>np.rollaxis</code> to move the channel axis to the last. A sample code is below.</p>\n<pre><code>np.rollaxis(make_grid(mynet.ftrextractor(inputs).data, nrow=8, padding=1).cpu().numpy(), 0, 3)\n</code></pre>\n<h1>Logging</h1>\n<p>The best method I found to visualise the feature maps is using tensor board. A code is available at <a href=\"https://github.com/lanpa/tensorboard-pytorch\" rel=\"noreferrer\">yunjey/pytorch-tutorial</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <code>torch.utils.data</code></p>\n<p>assuming you have loaded the data from the directory, in train and test numpy arrays, you can inherit from <code>torch.utils.data.Dataset</code> class to create your dataset object</p>\n<pre><code>class MyDataset(Dataset):\n    def __init__(self, x, y):\n        super(MyDataset, self).__init__()\n        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n        self.x = x\n        self.y = y\n\n\n    def __len__(self):\n        return self.y.shape[0]\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n</code></pre>\n<p>Then, create your dataset object</p>\n<pre><code>traindata = MyDataset(train_x, train_y)\n</code></pre>\n<p>Finally, use <code>DataLoader</code> to create your mini-batches</p>\n<pre><code>trainloader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In my understanding, I thought PCA can be performed only for continuous features. But while trying to understand the difference between onehot encoding and label encoding came through a post in the following link:</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\">When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?</a></p>\n<p>It states that one hot encoding followed by PCA is a very good method, which basically means PCA is applied for categorical features.\nHence confused, please suggest me on the same.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I disagree with the others.</p>\n<p>While <strong>you can use PCA on binary data</strong> (e.g. one-hot encoded data) that does not mean it is a good thing, or it will work very well.</p>\n<p>PCA is designed for <em>continuous</em> variables. It tries to minimize variance (=squared deviations). The concept of squared deviations breaks down when you have binary variables.</p>\n<p>So yes, you can use PCA. And yes, you get an output. It even is a least-squared output: it's not as if PCA would segfault on such data. It works, but it is just much less <em>meaningful</em> than you'd want it to be; and supposedly less meaningful than e.g. frequent pattern mining.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>MCA is a known technique for categorical data dimension reduction. In R there is a lot of package to use MCA and even mix with PCA in mixed contexts. In python exist a a mca library too. MCA apply similar maths that PCA, indeed the French statistician used to say, \"data analysis is to find correct matrix to diagonalize\"</p>\n<p><a href=\"http://gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/\" rel=\"noreferrer\">http://gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Basically, PCA finds and eliminate less informative (duplicate) information on feature set and reduce the dimension of feature space. In other words, imagine a N-dimensional hyperspace, PCA finds such M (M &lt; N) features that the data variates most. In this way data may be represented as M-dimensional feature vectors. Mathematically, it is some-kind of a eigen-values &amp; eigen vectors calculation of a feature space.</p>\n<p>So, it is not important whether the features are continuous or not. </p>\n<p>PCA is used widely on many application. Mostly for eliminating noisy, less informative data that comes from some sensor or hardware before classification/recognition.</p>\n<p><strong>Edit:</strong></p>\n<p>Statistically speaking, categorical features can be seen as discrete random variables in interval [0,1]. Computation for expectation E{X} and variance E{(X-E{X})^2) are still valid and meaningful for discrete rvs. I still stand for the applicability of PCA in case of categorical features. </p>\n<p>Consider a case where you would like to predict whether \"It is going to rain for a given day or not\". You have categorical feature X which is \"Do I have to go to work for the given day\", 1 for yes and 0 for no. Clearly weather conditions do not depend on our work schedule, so P(R|X)=P(R). Assuming 5 days of work for every week, we have more 1s than 0s for X in our randomly collected dataset. PCA would probably lead to dropping this low-variance dimension in your feature representation. </p>\n<p>At the end of the day, PCA is for dimension reduction with minimal loss of information. Intuitively, we rely on variance of the data on a given axis to measure its usefulness for the task. I don't think there is any theoretical limitation for applying it to categorical features. Practical value depends on application and data which is also the case for continuous variables. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" rel=\"nofollow noreferrer\"><code>sklearn.linear_model.LogisticRegression</code></a> in <code>scikit learn</code> to run a Logistic Regression.</p>\n<pre class=\"lang-none prettyprint-override\"><code>C : float, optional (default=1.0) Inverse of regularization strength;\n    must be a positive float. Like in support vector machines, smaller\n    values specify stronger regularization.\n</code></pre>\n<p>What does <code>C</code> mean here in simple terms? What is regularization strength?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://en.wikipedia.org/wiki/Regularization_%28mathematics%29#Regularization_in_statistics_and_machine_learning\" rel=\"noreferrer\">Regularization</a> is applying a penalty to increasing the magnitude of parameter values in order to reduce <a href=\"http://en.wikipedia.org/wiki/Overfitting\" rel=\"noreferrer\">overfitting</a>. When you train a model such as a logistic regression model, you are choosing parameters that give you the best fit to the data. This means minimizing the error between what the model predicts for your dependent variable given your data compared to what your dependent variable actually is.</p>\n<p>The problem comes when you have a lot of parameters (a lot of independent variables) but not too much data. In this case, the model will often tailor the parameter values to idiosyncrasies in your data -- which means it fits your data almost perfectly. However because those idiosyncrasies don't appear in future data you see, your model predicts poorly.</p>\n<p>To solve this, as well as minimizing the error as already discussed, you add to what is minimized and also minimize a function that penalizes large values of the parameters. Most often the function is λΣθ<sub>j</sub><sup>2</sup>, which is some constant λ times the sum of the squared parameter values θ<sub>j</sub><sup>2</sup>. The larger λ is the less likely it is that the parameters will be increased in magnitude simply to adjust for small perturbations in the data. In your case however, rather than specifying λ, you specify C=1/λ.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there any difference between <code>torch.optim.Adam(weight_decay=0.01)</code> and <code>torch.optim.AdamW(weight_decay=0.01)</code>?</p>\n<p>Link to the docs: <a href=\"https://pytorch.org/docs/stable/optim.html\" rel=\"noreferrer\">torch.optim</a>.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, Adam and AdamW weight decay are different.</p>\n<blockquote>\n<p>Hutter pointed out in their paper (<a href=\"https://arxiv.org/abs/1711.05101\" rel=\"noreferrer\">Decoupled Weight Decay Regularization</a>) that the way weight decay is implemented in Adam in every library seems to be wrong, and proposed a simple way (which they call AdamW) to fix it.</p>\n</blockquote>\n<p>In Adam, the weight decay is usually implemented by adding <code>wd*w</code> (<code>wd</code> is weight decay here) to the gradients (Ist case), rather than actually subtracting from weights (IInd case).</p>\n<pre><code># Ist: Adam weight decay implementation (L2 regularization)\nfinal_loss = loss + wd * all_weights.pow(2).sum() / 2\n# IInd: equivalent to this in SGD\nw = w - lr * w.grad - lr * wd * w\n</code></pre>\n<blockquote>\n<p>These methods are same for vanilla SGD, but as soon as we add momentum, or use a more sophisticated optimizer like Adam, L2 regularization (first equation) and weight decay (second equation) become different.</p>\n</blockquote>\n<p>AdamW follows the second equation for weight decay.</p>\n<p>In Adam</p>\n<blockquote>\n<p>weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)</p>\n</blockquote>\n<p>In AdamW</p>\n<blockquote>\n<p>weight_decay (float, optional) – weight decay coefficient (default: 1e-2)</p>\n</blockquote>\n<p>Read more on the <a href=\"https://www.fast.ai/2018/07/02/adam-weight-decay/\" rel=\"noreferrer\">fastai blog</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In <strong>Pytorch</strong>, the implementations of Adam and AdamW are different. In the <a href=\"https://github.com/pytorch/pytorch/blob/cb2337326407a39efbb2a0d1faa7a1146ad68529/torch/optim/adam.py#L351-L352\" rel=\"noreferrer\">Adam source code</a>, weight decay is implemented as</p>\n<pre class=\"lang-py prettyprint-override\"><code>grad = grad.add(param, alpha=weight_decay)\n</code></pre>\n<p>whereas in the <a href=\"https://github.com/pytorch/pytorch/blob/cb2337326407a39efbb2a0d1faa7a1146ad68529/torch/optim/adamw.py#L401\" rel=\"noreferrer\">AdamW source code</a>, it is implemented as</p>\n<pre class=\"lang-py prettyprint-override\"><code>param.mul_(1 - lr * weight_decay)\n</code></pre>\n<p>So in each iteration, in Adam, the <em>gradient</em> is updated by the estimated parameters from the previous iteration weighted by the weight decay. On the other hand, in AdamW, the <em>parameters</em> are updated by the parameters from the previous iteration weighted by the weight decay. The pseudocode from the documentation clearly shows the difference (<em>boxed for emphasis</em>) where lambda is the weight decay.</p>\n<p><a href=\"https://i.sstatic.net/guHfM.png\" rel=\"noreferrer\"><img alt=\"pseudocode\" src=\"https://i.sstatic.net/guHfM.png\"/></a></p>\n<hr/>\n<p>However in <strong>Keras</strong>, even thought the default implementations are different because Adam has <code>weight_decay=None</code> while AdamW has <code>weight_decay=0.004</code> (in fact, it cannot be None), if <code>weight_decay</code> is not None, Adam is the same as AdamW. Both are subclassed from <code>optimizer.Optimizer</code> and in fact, their source codes are almost identical; in particular, the variables updated in each iteration are the same. The only difference is that the definition of <a href=\"https://github.com/keras-team/keras/blob/b3ffea6602dbbb481e82312baa24fe657de83e11/keras/optimizers/adam.py#L110\" rel=\"noreferrer\">Adam's</a> <code>weight_decay</code> is deferred to the parent class while <a href=\"https://github.com/keras-team/keras/blob/b3ffea6602dbbb481e82312baa24fe657de83e11/keras/optimizers/adamw.py#L117\" rel=\"noreferrer\">AdamW's</a> <code>weight_decay</code> is defined in the AdamW class itself.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While people usually tend to simply resize any image into a square while training a CNN (for example, resnet takes a 224x224 square image), that looks ugly to me, especially when the aspect ratio is not around 1.</p>\n<p>(In fact, that might change ground truth, for example, the label that an expert might give the distorted image could be different than the original one).</p>\n<p>So now I resize the image to, say, 224x160 , keeping the original ratio, and then I pad the image with 0s (by pasting it into a random location in a totally black 224x224 image).</p>\n<p>My approach doesn't seem original to me, and yet I cannot find any information whatsoever about my approach versus the \"usual\" approach.\nFunky!</p>\n<p>So, which approach is better? Why? (if the answer is data dependent, please share your thoughts regarding when one is preferable to the other.)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>According to <a href=\"http://www.fast.ai\" rel=\"noreferrer\">Jeremy Howard</a>, padding a big piece of the image (64x160 pixels) will have the following effect: The CNN will have to learn that the black part of the image is not relevant and does not help distinguishing between the classes (in a classification setting), as there is no correlation between the pixels in the black part and belonging to a given class. As you are not hard coding this, the CNN will have to learn it by gradient descent, and this might probably take some epochs. For this reason, you can do it if you have lots of images and computational power, but if you are on a budget on any of them, resizing should work better.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sorry, this is late but this answer is for anyone facing the same issue.</p>\n<p>First, if scaling with changing the aspect ratio will affect some important features, then you have to use zero-padding.</p>\n<p>Zero padding doesn't make it take longer for the network to learn because of the large black area itself but because of the different possible locations that the unpadded image could be inside the padded image since you can pad an image in many ways.</p>\n<p>For areas with zero pixels, the output of the convolution operation is zero. The same with max or average pooling. Also, you can prove that the weight is not updated after backpropagation if the input associated with that weight is zero under some activation functions (e.g. relu, sigmoid). So the large area doesn't make any updates to the weights in this sense.</p>\n<p>However, the relative position of the unpadded image inside the padded image does indeed affect training. This is not due to the convolution nor the pooling layers but the last fully connected layer(s). For example, if the unpadded image is on the left relative inside the padded image and the output of flattening the last convolution or pooling layer was [1, 0, 0] and the output for the same unpadded image but on the right relative inside the padded image was [0, 0, 1] then the fully connected layer(s) must learn that [1, 0, 0] and [0, 0, 1] are the same thing for a classification problem.</p>\n<p>Therefore, learning the equivariance of different possible positions of the image is what makes training take more time. If you have 1,000,000 images then after resizing you will have the same number of images; on the other hand, if you pad and want to consider different possible locations (10 randomly for each image) then you will have 10,000,000 images. That is, training will take 10 times longer.</p>\n<p>That said, it depends on your problem and what you want to achieve. Also, testing both methods will not hurt.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-04-20 08:06:00Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/8068040/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I have learned a Machine Learning course using Matlab as a prototyping tool. Since I got addicted to F#, I would like to continue my Machine Learning study in F#. </p>\n<p>I may want to use F# for both prototyping and production, so <strong>a Machine Learning framework</strong> would be a great start. Otherwise, I can start with a collection of libraries:</p>\n<ul>\n<li>Highly-optimized linear algebra library</li>\n<li>Statistics package</li>\n<li>Visualization library (which allows to draw and interact with charts, diagrams...)</li>\n<li>Parallel computing toolbox (similar to Matlab parallel computing toolbox)</li>\n</ul>\n<p>And the most important resources (to me) are <strong>books</strong>, blog posts and online courses regarding Machine Learning in a functional programming language (F#/OCaml/Haskell...). </p>\n<p>Can anyone suggest these kinds of resource? Thanks.</p>\n<hr/>\n<p><strong>EDIT:</strong></p>\n<p>This is a summary based on the answers below:</p>\n<p>Machine Learning frameworks:</p>\n<ul>\n<li><a href=\"http://research.microsoft.com/en-us/um/cambridge/projects/infernet/\">Infer.NET</a>: an .NET framework for Bayesian inference in graphical models with good F# support.</li>\n<li><a href=\"http://wekasharp.codeplex.com/\">WekaSharper</a>: a F# wrapper around the popular data mining framework Weka.</li>\n<li><a href=\"http://msdn.microsoft.com/en-us/library/hh304371.aspx\">Microsoft Sho</a>: a continuous environment development for data analysis (including matrix operations, optimization and visualization) on .NET platform.</li>\n</ul>\n<p>Related libraries:</p>\n<ul>\n<li><p><a href=\"http://mathnetnumerics.codeplex.com/\">Math.NET Numerics</a>: internally using Intel MKL and AMD ACML for matrix operations and supporting statistics functions too. </p></li>\n<li><p><a href=\"http://archive.msdn.microsoft.com/solverfoundation\">Microsoft Solver Foundation</a>: a good framework for linear programming and optimization tasks.</p></li>\n<li><p><a href=\"http://code.msdn.microsoft.com/windowsdesktop/FSharpChart-b59073f5\">FSharpChart</a>: a nice data visualization library in F#.</p></li>\n</ul>\n<p>Reading list:</p>\n<ul>\n<li><a href=\"http://msdn.microsoft.com/en-us/library/hh273075.aspx\">Numerical Computing</a>: It is great for starting with Machine Learning in F# and introduces various tools and tips/tricks for working with these Math libraries in F#.</li>\n<li><a href=\"http://fdatamining.blogspot.com/\">F# and Data Mining blog</a>: It is also from Yin Zhu, the author of Numerical Computing chapter, highly recommended.</li>\n<li><a href=\"http://blog.codebeside.org/blog/2011/10/27/f-as-a-octavematlab-replacement-for-machine-learning\">F# as a Octave/Matlab replacement for Machine Learning</a>: Gustavo has just started a series of blog posts using F# as the development tool. It's great to see many libraries are plugged in together.</li>\n<li><a href=\"http://www.clear-lines.com/blog/?tag=/Machine%20Learning\">\"Machine Learning in Action\" 's samples in F#</a>: Mathias has translated some samples from Python to F#. They are available in <a href=\"https://github.com/mathias-brandewinder/Machine-Learning-In-Action\">Github</a>.</li>\n<li><a href=\"http://www.umiacs.umd.edu/~hal/software.html\">Hal Daume's homepage</a>: Hal has written a number of Machine Learning libraries in OCaml. You would feel relieved if you were in doubt that functional programming was not suitable for Machine Learning.</li>\n</ul>\n<p>Any other pointers or suggestions are also welcome.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There isn't a single place to look for resources on F# and machine learning, but here are a couple of links that may be quite useful:</p>\n<ul>\n<li><p><a href=\"http://msdn.microsoft.com/en-us/library/hh273075.aspx\">Numerical Computing</a> section on MSDN is a good resource on using various numerical libraries from F#. The most advanced library that implements linear algebra and other algorithsm useful in machine learning is <a href=\"http://msdn.microsoft.com/en-us/library/hh304363.aspx\">Math.NET Numerics</a>.</p></li>\n<li><p><a href=\"http://msdn.microsoft.com/en-us/library/hh273079.aspx\">Visualizing Data</a> section on MSDN has some resources on charting in F#. The FSharpChart library is now maintained by Carl Nolan who regularly posts <a href=\"http://blogs.msdn.com/b/carlnol/\">updates to his blog</a>.</p></li>\n</ul>\n<p>There are also a few personal pages of people who are working on relevant topics:</p>\n<ul>\n<li><p>Jurgen van Gael (who did PhD in machine learning) contributed to the Math.NET library and you can read about <a href=\"http://mlg.eng.cam.ac.uk/jurgen/fsharp.html\">his experience here</a>.</p></li>\n<li><p>Yin Zhu who wrote the Numerical Computing chapter on MSDN (and is a PhD student interested in machine learning) has quite a few <a href=\"http://fdatamining.blogspot.com/\">excellent articles on his blog</a>.</p></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In addition to what Tomas mentioned, I spent some time with <a href=\"http://research.microsoft.com/en-us/um/cambridge/projects/infernet/\">Infer.NET</a> about a year ago and found it was pretty good for continuous graphical models.  I know it's improved quite a lot over the last year in both the scope of the library and F# support.  I suggest checking it out and seeing if it has what you need.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Hal Daume has implemented a lot of machine learning algorithms in OCaml and Haskell. Details see my answer in <a href=\"https://stackoverflow.com/questions/2268885/machine-learning-in-ocaml-or-haskell\">Machine learning in OCaml or Haskell?</a></p>\n<p>As side from the <a href=\"http://msdn.microsoft.com/en-us/library/hh273075.aspx\" rel=\"nofollow noreferrer\">Numerical Computing in F#</a> book chapter on MSDN, I'd also like to recommend my Wrapper for Weka, <a href=\"http://wekasharp.codeplex.com/\" rel=\"nofollow noreferrer\">WekaSharper</a>. It allows you to call machine learning algorithms in Weka using an F#-friendly interface. </p>\n<p>I wrote an article, <a href=\"http://fdatamining.blogspot.com/2010/05/why-f-is-language-for-data-mining.html\" rel=\"nofollow noreferrer\">Why F# is the language for data mining</a>, which reflects my thinking when I finished writing a alpha/prototype-like data mining package in F#. <a href=\"https://bitbucket.org/blackswift/libml/\" rel=\"nofollow noreferrer\">libml</a> is available online. But the code was written about two years ago when I started to use F#, and I didn't have time to maintain it since then. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It seems that Google Colab GPU's doesn't come with CUDA Toolkit, how can I install CUDA in Google Colab GPU's. I am getting this error in installing mxnet  in Google Colab.</p>\n<pre><code>Installing collected packages: mxnet\nSuccessfully installed mxnet-1.2.0\n</code></pre>\n<blockquote>\n<p>ERROR: Incomplete installation for leveraging GPUs for computations.\n  Please make sure you have CUDA installed and run the following line in\n  your terminal and try again:</p>\n</blockquote>\n<pre><code>pip uninstall -y mxnet &amp;&amp; pip install mxnet-cu90==1.1.0\n</code></pre>\n<blockquote>\n<p>Adjust 'cu90' depending on your CUDA version ('cu75' and 'cu80' are\n  also available).\n      You can also disable GPU usage altogether by invoking turicreate.config.set_num_gpus(0). \n      An exception has occurred, use %tb to see the full traceback.</p>\n</blockquote>\n<pre><code>SystemExit: 1\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>Cuda</code> is not showing on your notebook because you have not enabled GPU in <code>Colab</code>.</p>\n<p>The <code>Google Colab</code> comes with both options GPU or without GPU.\nYou can enable or disable <code>GPU</code> in runtime settings</p>\n<pre class=\"lang-bash prettyprint-override\"><code>Go to Menu &gt; Runtime &gt; Change runtime.\n</code></pre>\n<p>As shown in the following image, you can select one of the GPUs from the green-encircled options.</p>\n<p>Keep in mind black names are free and that grey names are paid GPUs available through <a href=\"https://colab.research.google.com/signup\" rel=\"nofollow noreferrer\">Google Colab Pro</a></p>\n<p><a href=\"https://i.sstatic.net/kpPEmcb8.jpg\" rel=\"nofollow noreferrer\"><img alt=\"GPU Settings Screenshot\" src=\"https://i.sstatic.net/kpPEmcb8.jpg\"/></a></p>\n<p>To check if the GPU is running or not, run the following command</p>\n<pre><code>!nvidia-smi\n</code></pre>\n<p>If the output is like the following image, your GPU and <code>CUDA</code> are working. You can see the <code>CUDA</code> version also.<a href=\"https://i.sstatic.net/VsA9Lith.png\" rel=\"nofollow noreferrer\"><img alt=\"cuda confirmation screenshot\" src=\"https://i.sstatic.net/VsA9Lith.png\"/></a></p>\n<p>After that to check if <code>PyTorch</code> can use GPU, run the following code.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\ntorch.cuda.is_available()\n# Output would be True if Pytorch is using GPU otherwise it would be False.\n</code></pre>\n<p>Run the following code to check if <code>TensorFlow</code> can use GPU.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\ntf.test.gpu_device_name()\n# Standard output is '/device:GPU:0'\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I pretty much believe that Google Colab has Cuda pre-installed... You can make sure by opening a new notebook and type <code>!nvcc --version</code> which would return the installed Cuda version.</p>\n<p>Here is mine:\n<a href=\"https://i.sstatic.net/3mvDu.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/3mvDu.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ol>\n<li>Go here: <a href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"noreferrer\">https://developer.nvidia.com/cuda-downloads</a></li>\n<li>Select Linux -&gt; x86_64 -&gt; Ubuntu -&gt; 16.04 -&gt; deb (local)</li>\n<li>Copy link from the download button.</li>\n<li>Now you have to compose the sequence of commands. First one will be the call to wget that will download CUDA installer from the link you saved on step 3</li>\n<li>There will be installation instruction under \"Base installer\" section. Copy them as well, but remove <code>sudo</code> from all the lines. </li>\n<li>Preface each line with commands with <code>!</code>, insert into a cell and run</li>\n<li>For me the command sequence was the following:<br/>\n<code>!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n!apt-get update\n!apt-get install cuda</code></li>\n<li>Now finally install mxnet. As cuda version I installed above is 9.2 I had to slighly change your command: <code>!pip install mxnet-cu92</code></li>\n<li><code>Successfully installed graphviz-0.8.3 mxnet-cu92-1.2.0</code></li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have web application written in Flask. As suggested by everyone, I can't use Flask in production. So I thought of <strong>Gunicorn with Flask</strong>.  </p>\n<p>In Flask application I am loading some Machine Learning models. These are of size 8GB collectively. Concurrency of my web application can go upto <strong>1000 requests</strong>. And the RAM of machine is 15GB.<br/>\nSo what is the best way to run this application?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can start your app with multiple workers or async workers with Gunicorn.</p>\n<p>Flask server.py</p>\n<pre><code>from flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello World!\"\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>\n<p>Gunicorn with gevent async worker</p>\n<pre><code>gunicorn server:app -k gevent --worker-connections 1000\n</code></pre>\n<p>Gunicorn 1 worker 12 threads:</p>\n<pre><code>gunicorn server:app -w 1 --threads 12\n</code></pre>\n<p>Gunicorn with 4 workers (multiprocessing):</p>\n<pre><code>gunicorn server:app -w 4\n</code></pre>\n<p>More information on Flask concurrency in this post: <a href=\"https://stackoverflow.com/questions/10938360/how-many-concurrent-requests-does-a-single-flask-process-receive\">How many concurrent requests does a single Flask process receive?</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The best thing to do is to use pre-fork mode (<a href=\"http://docs.gunicorn.org/en/stable/settings.html#preload-app\" rel=\"noreferrer\">preload_app=True</a>). This will initialize your code in a \"master\" process and then simply fork off worker processes to handle requests. If you are running on linux and assuming your model is read-only, the OS is smart enough to reuse the physical memory amongst all the processes.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>All the examples I have seen of neural networks are for a fixed set of inputs which works well for images and fixed length data.  How do you deal with variable length data such sentences, queries or source code?  Is there a way to encode variable length data into fixed length inputs and still get the generalization properties of neural networks?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have been there, and I faced this problem.\nThe ANN was made for fixed feature vector length, and so are many other classifiers such as KNN, SVM, Bayesian, etc. \n i.e. the input layer should be well defined and not varied, this is a design problem.\nHowever, some researchers opt for adding zeros to fill the missing gap, I personally think that this is not a good solution because those zeros (unreal values) will affect the weights that the net will converge to. in addition there might be a real signal ending with zeros. </p>\n<p>ANN is not the only classifier, there are more and even better such as the random forest. this classifier is considered the best among researchers, it uses a small number of random features, creating hundreds of decision trees using bootstrapping an bagging, this might work well, the number of the chosen  features normally the sqrt of the feature vector size. those features are random. each decision tree converges to a solution, using majority rules the most likely class will chosen then.</p>\n<p>Another solution is to use the dynamic time warping DTW, or even better to use Hidden Markov models HMM.</p>\n<p>Another solution is the interpolation, interpolate (compensate for missing values along the small signal) all the small signals to be with the same size as the max signal, interpolation methods include and not limited to averaging, B-spline, cubic.....</p>\n<p>Another solution is to use feature extraction method to use the best features (the most distinctive), this time make them fixed size, those method include PCA, LDA, etc.</p>\n<p>another solution is to use feature selection (normally after feature extraction) an easy way to select the best features that give the best accuracy.</p>\n<p>that's all for now, if non of those worked for you, please contact me.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You would usually extract features from the data and feed those to the network. It is not advisable to take just some data and feed it to net. In practice, pre-processing and choosing the right features will decide over your success and the performance of the neural net. Unfortunately, IMHO it takes experience to develop a sense for that and it's nothing one can learn from a book.</p>\n<p>Summing up: \"Garbage in, garbage out\"</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Some problems could be solved by a recurrent neural network.\nFor example, it is good for calculating parity over a sequence of inputs.</p>\n<p>The <a href=\"http://github.com/pybrain/pybrain/blob/master/examples/supervised/backprop/parityrnn.py\" rel=\"noreferrer\">recurrent neural network for calculating parity</a> would have just one input feature.\nThe bits could be fed into it over time. Its output is also fed back to the hidden layer.\nThat allows to learn the parity with just two hidden units.</p>\n<p>A normal feed-forward two-layer neural network would require 2**sequence_length hidden units to represent the parity. This <a href=\"http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf\" rel=\"noreferrer\">limitation holds</a> for any architecture with just 2 layers (e.g., SVM).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Trying to plot the decision Boundary of the k-NN Classifier but is unable to do so getting <code>TypeError: '(slice(None, None, None), 0)' is an invalid key</code></p>\n<pre><code>h = .01  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF'])\ncmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = KNeighborsClassifier(n_neighbors=6, weights=weights)\n    clf.fit(X_train, y_train)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"4-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()\n</code></pre>\n<p>Got this when running not very sure what it means dont think the clf.fit have a problem but I am not sure</p>\n<pre><code>  TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-394-bef9b05b1940&gt; in &lt;module&gt;\n     12         # Plot the decision boundary. For that, we will assign a color to each\n     13         # point in the mesh [x_min, x_max]x[y_min, y_max].\n---&gt; 14         x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n     15         y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n     16         xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n\n~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py in __getitem__(self, key)\n   2925             if self.columns.nlevels &gt; 1:\n   2926                 return self._getitem_multilevel(key)\n-&gt; 2927             indexer = self.columns.get_loc(key)\n   2928             if is_integer(indexer):\n   2929                 indexer = [indexer]\n\n~\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   2654                                  'backfill or nearest lookups')\n   2655             try:\n-&gt; 2656                 return self._engine.get_loc(key)\n   2657             except KeyError:\n   2658                 return self._engine.get_loc(self._maybe_cast_indexer(key))\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\n\nTypeError: '(slice(None, None, None), 0)' is an invalid key\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since you are trying to access directly as array, you are getting that issue. Try this:</p>\n<pre><code>from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0)\nimputer = imputer.fit(X.iloc[:, 1:3])\nX.iloc[:, 1:3] = imputer.transform(X.iloc[:, 1:3])\n</code></pre>\n<p>Using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html\" rel=\"noreferrer\"><code>iloc</code></a>/<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc\" rel=\"noreferrer\"><code>loc</code></a> will resolve the issue.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html\" rel=\"noreferrer\"><code>iloc</code></a>/<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc\" rel=\"noreferrer\"><code>loc</code></a> to acces df. Try adding iloc to X so <code>X.iloc[:, 0]</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had the same issue with the following</p>\n<pre class=\"lang-py prettyprint-override\"><code>X = dataset.iloc[:,:-1]\n</code></pre>\n<p>Then I added <code>.values</code> property, after that it worked without problem</p>\n<pre class=\"lang-py prettyprint-override\"><code>X = dataset.iloc[:,:-1].values\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to know what a learning curve in machine learning is. What is the standard way of plotting it? I mean what should be the x and y axis of my plot?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It usually refers to a plot of the <em>prediction accuracy/error</em> vs. the <em>training set size</em> (i.e: how better does the model get at predicting the target as you the increase number of instances used to train it)</p>\n<p><img alt=\"learning-curve\" src=\"https://i.sstatic.net/haGpo.png\"/></p>\n<p>Usually both the training and test/validation performance are plotted together so we can diagnose the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" rel=\"noreferrer\">bias-variance tradeoff</a> (i.e determine if we benefit from adding more training data, and assess the model complexity by controlling regularization or number of features).</p>\n<p><img alt=\"bias-variance\" src=\"https://i.sstatic.net/D8tdw.png\"/></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Notice that learning curve and ROC curve are not synonymous.</strong></p>\n<p>As indicated in the other answers to this question, a <strong>learning curve</strong> conventionally depicts improvement in performance on the vertical axis when there are changes in another parameter (on the horizontal axis), such as training set size (in machine learning) or iteration/time (in both machine and biological learning).  One salient point is that many parameters of the model are changing at different points on the plot.  Other answers here have done a great job of illustrating learning curves.</p>\n<p>(There is also another meaning of learning curve in industrial manufacturing, originating in an observation in the 1930s that the number of  labor hours needed to produce an individual unit decreases at a uniform rate as the quantity of  units manufactured doubles. It isn't really relevant but is worth noting for completeness and to avoid confusion in web searches.)</p>\n<p>In contrast, <strong>Receiver Operating Characteristic curve</strong>, or <strong>ROC curve</strong>, does not show learning; it shows performance.  An ROC curve is a graphical depiction of classifier performance that shows the trade-off between increasing true positive rates (on the vertical axis) and increasing false positive rates (on the horizontal axis) as the discrimination threshold of the classifier is varied.  Thus, only a single parameter (the decision / discrimination threshold) associated with the model is changing at different points on the plot. This ROC curve (<a href=\"http://en.wikipedia.org/wiki/File:Roccurves.png\" rel=\"nofollow noreferrer\">from Wikipedia</a>) shows performance of three different classifiers.</p>\n<p><img alt=\"ROC curve, see previous link for CC licensing\" src=\"https://i.sstatic.net/yM4xY.png\"/></p>\n<p>There is no learning being depicted here, but rather performance with respect to two different classes of success/error as the classifier's decision threshold is made more lenient/strict.  By looking at the area under the curve, we can see an overall indication of the ability of the classifier to distinguish the classes.  This area-under-the-curve metric is insensitive to the number of members in the two classes, so it may not reflect actual performance if class membership is unbalanced. The ROC curve has many subtitles and interested readers might check out:</p>\n<p><a href=\"http://home.comcast.net/%7Etom.fawcett/public_html/papers/ROC101.pdf\" rel=\"nofollow noreferrer\">Fawcett, Tom. \"ROC graphs: Notes and practical considerations for researchers.\" Machine Learning 31 (2004): 1-38.</a></p>\n<p><a href=\"http://ist-socrates.berkeley.edu/%7Emaccoun/LP_SwetsDawesMonahan2000.pdf\" rel=\"nofollow noreferrer\">Swets, John A., Robyn M. Dawes, and John Monahan. \"Better decisions through Science.\" Scientific American (2000): 83.</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Some people use \"learning curve\" to refer to the error of an iterative procedure as a function of the iteration number, i.e., it illustrates convergence of some utility function. In the example below, I plot mean-square error (MSE) of the least-mean-square (LMS) algorithm as a function of the iteration number. That illustrates how quickly LMS \"learns\", in this case, the channel impulse response.</p>\n<p><a href=\"http://up.stevetjoa.com/learningcurves.pdf\" rel=\"noreferrer\"><img alt=\"learningcurves.png\" src=\"https://i.sstatic.net/Ccvw8.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently in the process of designing a recommender system for text articles (a binary case of 'interesting' or 'not interesting'). One of my specifications is that it should continuously update to changing trends. </p>\n<p>From what I can tell, the best way to do this is to make use of machine learning algorithm that supports incremental/<a href=\"http://en.wikipedia.org/wiki/Online%5fmachine%5flearning\">online learning</a>. </p>\n<p>Algorithms like the Perceptron and Winnow support online learning but I am not completely certain about Support Vector Machines. Does the scikit-learn python library support online learning and if so, is a support vector machine one of the algorithms that can make use of it?</p>\n<p>I am obviously not completely tied down to using support vector machines, but they are usually the go to algorithm for binary classification due to their all round performance. I would be willing to change to whatever fits best in the end.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While online algorithms for SVMs do exist, it has become important to specify if you want kernel or linear SVMs, as many efficient algorithms have been developed for the special case of linear SVMs. </p>\n<p>For the linear case, if you use the <a href=\"http://scikit-learn.org/stable/modules/sgd.html#sgd\">SGD classifier in scikit-learn</a> with the hinge loss and L2 regularization you will get an SVM that can be updated online/incrementall. You can combine this with <a href=\"http://scikit-learn.org/stable/modules/kernel_approximation.html\">feature transforms that approximate a kernel</a> to get similar to an online kernel SVM. </p>\n<blockquote>\n<p>One of my specifications is that it should continuously update to changing trends.</p>\n</blockquote>\n<p>This is referred to as <em>concept drift,</em> and will not be handled well by a simple online SVM. Using the PassiveAggresive classifier will likely give you better results, as it's learning rate does not decrease over time. </p>\n<p>Assuming you get feedback while training / running, you can attempt to detect decreases in accuracy over time and begin training a new model when the accuracy starts to decrease (and switch to the new one when you believe that it has become more accurate). <a href=\"https://code.google.com/p/java-statistical-analysis-tool/\">JSAT</a> has 2 drift detection methods (see <a href=\"https://code.google.com/p/java-statistical-analysis-tool/source/browse/#svn/trunk/JSAT/src/jsat/driftdetectors\">jsat.driftdetectors</a>) that can be used to track accuracy and alert you when it has changed. </p>\n<p>It also has more online linear and kernel methods.</p>\n<p>(bias note: I'm the author of JSAT). </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Maybe it's me being naive but I think it is worth mentioning how to actually update the <a href=\"http://scikit-learn.org/stable/modules/sgd.html#sgd\" rel=\"noreferrer\">sci-kit SGD classifier</a> when you present your data incrementally:</p>\n<pre><code>clf = linear_model.SGDClassifier()\nx1 = some_new_data\ny1 = the_labels\nclf.partial_fit(x1,y1)\nx2 = some_newer_data\ny2 = the_labels\nclf.partial_fit(x2,y2)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Technical aspects</h2>\n<p>The short answer is <strong>no</strong>. Sklearn implementation (as well as most of the existing others) do not support online SVM training. It is possible to train SVM in  an incremental way, but it is not so trivial task.</p>\n<p>If you want to limit yourself to the linear case, than the answer is <strong>yes</strong>, as sklearn provides you with Stochastic Gradient Descent (SGD), which has option to minimize the SVM criterion.</p>\n<p>You can also try out pegasos library instead, which supports online SVM training.</p>\n<h2>Theoretical aspects</h2>\n<p>The problem of trend adaptation is currently very popular in ML community. As @Raff stated, it is called <em>concept drift</em>, and has numerous approaches, which are often kinds of meta models, which analyze \"how the trend is behaving\" and change the underlying ML model (by for example forcing it to retrain on the subset of the data). So you have two independent problems here:</p>\n<ul>\n<li>the online training issue, which is purely technical, and can be addressed by SGD or other libraries than sklearn</li>\n<li>concept drift, which is currently a hot topic and has no <strong>just works</strong> answers There are many possibilities, hypothesis and proofes of concepts, while there is no one, generaly accepted way of dealing with this phenomena, in fact many phd dissertations in ML are currenlly based on this issue.</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there a way to plot both the training losses and validation losses on the <em>same</em> graph?</p>\n<p>It's easy to have two separate scalar summaries for each of them individually, but this puts them on separate graphs. If both are displayed in the same graph it's much easier to see the gap between them and whether or not they have begin to diverge due to overfitting.</p>\n<p>Is there a built in way to do this? If not, a work around way? Thank you much!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The work-around I have been doing is to use two <code>SummaryWriter</code> with different log dir for training set and cross-validation set respectively. And you will see something like this:</p>\n<p><a href=\"https://i.sstatic.net/4Zqxa.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/4Zqxa.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just for anyone coming accross this via a search: The current best practice to achieve this goal is to just use the <code>SummaryWriter.add_scalars</code> method from <code>torch.utils.tensorboard</code>. From the <a href=\"https://pytorch.org/docs/stable/tensorboard.html\" rel=\"noreferrer\">docs</a>:</p>\n<pre><code>from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n  writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                'xcosx':i*np.cos(i/r),\n                                'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n</code></pre>\n<p>Expected result:</p>\n<p><a href=\"https://i.sstatic.net/ty1D5.png\" rel=\"noreferrer\"><img alt=\"expected result image\" src=\"https://i.sstatic.net/ty1D5.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Rather than displaying the two lines separately, you can instead plot the difference between validation and training losses as its own scalar summary to track the divergence.</p>\n<p>This doesn't give as much information on a single plot (compared with adding two summaries), but it helps with being able to compare multiple runs (and not adding multiple summaries per run).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was running TensorFlow and I happen to have something yielding a NaN. I'd like to know what it is but I do not know how to do this. The main issue is that in a \"normal\" procedural program I would just write a print statement just before the operation is executed. The issue with TensorFlow is that I cannot do that because I first declare (or define) the graph, so adding print statements to the graph definition does not help. Are there any rules, advice, heuristics, anything to track down what might be causing the NaN?</p>\n<hr/>\n<p>In this case I know more precisely what line to look at because I have the following:</p>\n<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance\nZ = tf.sqrt(Delta_tilde)\nZ = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)\nZ = tf.pow(Z, 2.0)\nA = tf.exp(Z) \n</code></pre>\n<p>when this line is present I have it that it returns NaN as declared by my summary writers. Why is this? Is there a way to at least explore what value Z has after its being square rooted?</p>\n<hr/>\n<p>For the specific example I posted, I tried <code>tf.Print(0,Z)</code> but with no success it printed nothing. As in:</p>\n<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance\nZ = tf.sqrt(Delta_tilde)\ntf.Print(0,[Z]) # &lt;-------- TF PRINT STATMENT\nZ = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)\nZ = tf.pow(Z, 2.0)\nA = tf.exp(Z) \n</code></pre>\n<p>I actually don't understand what <code>tf.Print</code> is suppose to do. Why does it need two arguments? If I want to print 1 tensor why would I need to pass 2? Seems bizarre to me.</p>\n<hr/>\n<p>I was looking at the function <a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/control_flow_ops.html#add_check_numerics_ops\" rel=\"noreferrer\">tf.add_check_numerics_ops()</a> but it doesn't say how to use it (plus the docs seem to not be super helpful). Does anyone know how to use this?</p>\n<hr/>\n<p>Since I've had comments addressing the data might be bad, I am using standard MNIST. However, I am computing a quantity that is positive (pair-wise eucledian distance) and then square rooting it. Thus, I wouldn't see how the data specifically would be an issue.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are a couple of reasons WHY you can get a NaN-result, often it is because of too high a learning rate but plenty other reasons are possible like for example corrupt data in your input-queue or a log of 0 calculation.</p>\n<p>Anyhow, debugging with a print as you describe cannot be done by a simple print (as this would result only in the printing of the tensor-information inside the graph and not print any actual values). </p>\n<p>However, if you use tf.print as an op in bulding the graph (<a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/control_flow_ops.html#Print\">tf.print</a>) then when the graph gets executed you will get the actual values printed (and it IS a good exercise to watch these values to debug and understand the behavior of your net).</p>\n<p>However, you are using the print-statement not entirely in the correct manner. This is an op, so you need to pass it a tensor and request a result-tensor that you need to work with later on in the executing graph. Otherwise the op is not going to be executed and no printing occurs. Try this:</p>\n<pre><code>Z = tf.sqrt(Delta_tilde)\nZ = tf.Print(Z,[Z], message=\"my Z-values:\") # &lt;-------- TF PRINT STATMENT\nZ = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)\nZ = tf.pow(Z, 2.0)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I used to find it's much tougher to pinpoint where the nans and infs may occur than to fix the bug. As a complementary to @scai's answer, I'd like to add some points here: </p>\n<p>The debug module, you can imported by: </p>\n<pre><code>from tensorflow.python import debug as tf_debug\n</code></pre>\n<p>is much better than any print or assert. </p>\n<p>You can just add the debug function by changing your wrapper you session by: </p>\n<pre><code>sess = tf_debug.LocalCLIDebugWrapperSession(sess)\nsess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n</code></pre>\n<p>And you'll prompt an command line interface, then you enter: \n<code>run -f has_inf_or_nan</code> and <code>lt -f has_inf_or_nan</code> to find where the nans or infs are. The first one is the first place where the catastrophe occurs. By the variable name you can trace the origin in your code.   </p>\n<p>Reference: <a href=\"https://developers.googleblog.com/2017/02/debug-tensorflow-models-with-tfdbg.html\" rel=\"noreferrer\">https://developers.googleblog.com/2017/02/debug-tensorflow-models-with-tfdbg.html</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As of version 0.12, TensorFlow is shipped with a builtin debugger called <code>tfdbg</code>. It optimizes the workflow of debugging this type of bad-numerical-value issues (like <code>inf</code> and <code>nan</code>). The documentation is at:\n<a href=\"https://www.tensorflow.org/programmers_guide/debugger\" rel=\"noreferrer\">https://www.tensorflow.org/programmers_guide/debugger</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm in the second week of Professor Andrew Ng's Machine Learning course through Coursera. We're working on linear regression and right now I'm dealing with coding the cost function.</p>\n<p>The code I've written solves the problem correctly but does not pass the submission process and fails the unit test because I have hard coded the values of theta and not allowed for more than two values for theta.</p>\n<p>Here's the code I've got so far</p>\n<pre><code>function J = computeCost(X, y, theta)\n\nm = length(y);\nJ = 0;\n\nfor i = 1:m,\n    h = theta(1) + theta(2) * X(i)\n    a = h - y(i);\n    b = a^2;\n    J = J + b;\n    end;\nJ = J * (1 / (2 * m));\n\nend\n</code></pre>\n<p>the unit test is </p>\n<pre><code>computeCost( [1 2 3; 1 3 4; 1 4 5; 1 5 6], [7;6;5;4], [0.1;0.2;0.3])\n</code></pre>\n<p>and should produce ans =  7.0175</p>\n<p>So I need to add another for loop to iterate over theta, therefore allowing for any number of values for theta, but I'll be damned if I can wrap my head around how/where.</p>\n<p>Can anyone suggest a way I can allow for any number of values for theta within this function?</p>\n<p>If you need more information to understand what I'm trying to ask, I will try my best to provide it.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use vectorize of operations in Octave/Matlab.\nIterate over entire vector - it is really bad idea, if your programm language let you vectorize operations.\nR, Octave, Matlab, Python (numpy) allow this operation.\nFor example, you can get scalar production, if theta = (t0, t1, t2, t3) and X = (x0, x1, x2, x3) in the next way:\ntheta * X' = (t0, t1, t2, t3) * (x0, x1, x2, x3)' = t0*x0 + t1*x1 + t2*x2 + t3*x3 \nResult will be scalar.</p>\n<p>For example, you can vectorize h in your code in the next way:</p>\n<pre><code>H = (theta'*X')';\nS = sum((H - y) .^ 2);\nJ = S / (2*m);\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Above answer is perfect but you can also do</p>\n<pre><code>H = (X*theta);\nS = sum((H - y) .^ 2);\nJ = S / (2*m);\n</code></pre>\n<p>Rather than computing</p>\n<pre><code>(theta' * X')'\n</code></pre>\n<p>and then taking the transpose you can directly calculate <br/> </p>\n<pre><code>(X * theta)\n</code></pre>\n<p>It works perfectly.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The below line return the required 32.07 cost value while we run computeCost once using θ initialized to zeros:</p>\n<pre><code>J = (1/(2*m)) * (sum(((X * theta) - y).^2));\n</code></pre>\n<p>and is similar to the original formulas that is given below.</p>\n<p><a href=\"https://i.sstatic.net/TPOVM.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/TPOVM.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The docs for an <a href=\"https://keras.io/layers/embeddings/\" rel=\"noreferrer\">Embedding Layer</a> in Keras say:</p>\n<blockquote>\n<p>Turns positive integers (indexes) into dense vectors of fixed size. eg. <code>[[4], [20]]</code> -&gt; <code>[[0.25, 0.1], [0.6, -0.2]]</code></p>\n</blockquote>\n<p>I believe this could also be achieved by encoding the inputs as one-hot vectors of length <code>vocabulary_size</code>, and feeding them into a <a href=\"https://keras.io/layers/core/#dense\" rel=\"noreferrer\">Dense Layer</a>.</p>\n<p>Is an Embedding Layer merely a convenience for this two-step process, or is something fancier going on under the hood?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>An embedding layer is faster</strong>, because it is essentially the equivalent of a dense layer that makes simplifying assumptions.</p>\n<p>Imagine a word-to-embedding layer with these weights:</p>\n<pre><code>w = [[0.1, 0.2, 0.3, 0.4],\n     [0.5, 0.6, 0.7, 0.8],\n     [0.9, 0.0, 0.1, 0.2]]\n</code></pre>\n<p>A <code>Dense</code> layer will treat these like <strong>actual weights with which to perform matrix multiplication</strong>. An embedding layer will simply treat these weights as <strong>a list of vectors, each vector representing one word</strong>; the 0th word in the vocabulary is <code>w[0]</code>, 1st is <code>w[1]</code>, etc.</p>\n<hr/>\n<p>For an example, use the weights above and this sentence:</p>\n<pre><code>[0, 2, 1, 2]\n</code></pre>\n<p>A naive <code>Dense</code>-based net needs to convert that sentence to a 1-hot encoding</p>\n<pre><code>[[1, 0, 0],\n [0, 0, 1],\n [0, 1, 0],\n [0, 0, 1]]\n</code></pre>\n<p>then do a matrix multiplication</p>\n<pre><code>[[1 * 0.1 + 0 * 0.5 + 0 * 0.9, 1 * 0.2 + 0 * 0.6 + 0 * 0.0, 1 * 0.3 + 0 * 0.7 + 0 * 0.1, 1 * 0.4 + 0 * 0.8 + 0 * 0.2],\n [0 * 0.1 + 0 * 0.5 + 1 * 0.9, 0 * 0.2 + 0 * 0.6 + 1 * 0.0, 0 * 0.3 + 0 * 0.7 + 1 * 0.1, 0 * 0.4 + 0 * 0.8 + 1 * 0.2],\n [0 * 0.1 + 1 * 0.5 + 0 * 0.9, 0 * 0.2 + 1 * 0.6 + 0 * 0.0, 0 * 0.3 + 1 * 0.7 + 0 * 0.1, 0 * 0.4 + 1 * 0.8 + 0 * 0.2],\n [0 * 0.1 + 0 * 0.5 + 1 * 0.9, 0 * 0.2 + 0 * 0.6 + 1 * 0.0, 0 * 0.3 + 0 * 0.7 + 1 * 0.1, 0 * 0.4 + 0 * 0.8 + 1 * 0.2]]\n</code></pre>\n<p>=</p>\n<pre><code>[[0.1, 0.2, 0.3, 0.4],\n [0.9, 0.0, 0.1, 0.2],\n [0.5, 0.6, 0.7, 0.8],\n [0.9, 0.0, 0.1, 0.2]]\n</code></pre>\n<hr/>\n<p>However, an <code>Embedding</code> layer simply looks at <code>[0, 2, 1, 2]</code> and takes the weights of the layer at indices zero, two, one, and two to immediately get</p>\n<pre><code>[w[0],\n w[2],\n w[1],\n w[2]]\n</code></pre>\n<p>=</p>\n<pre><code>[[0.1, 0.2, 0.3, 0.4],\n [0.9, 0.0, 0.1, 0.2],\n [0.5, 0.6, 0.7, 0.8],\n [0.9, 0.0, 0.1, 0.2]]\n</code></pre>\n<p>So it's the same result, just obtained in a hopefully faster way.</p>\n<hr/>\n<p>The <code>Embedding</code> layer does have limitations:</p>\n<ul>\n<li>The input needs to be integers in [0, vocab_length).</li>\n<li>No bias.</li>\n<li>No activation.</li>\n</ul>\n<p>However, none of those limitations should matter if you just want to convert an integer-encoded word into an embedding.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Mathematically, the difference is this:</p>\n<ul>\n<li><p>An embedding layer performs <em>select</em> operation. In keras, this layer is equivalent to:</p>\n<pre><code>K.gather(self.embeddings, inputs)      # just one matrix\n</code></pre></li>\n<li><p>A dense layer performs <em>dot-product</em> operation, plus an optional activation:</p>\n<pre><code>outputs = matmul(inputs, self.kernel)  # a kernel matrix\noutputs = bias_add(outputs, self.bias) # a bias vector\nreturn self.activation(outputs)        # an activation function\n</code></pre></li>\n</ul>\n<p>You can <em>emulate</em> an embedding layer with fully-connected layer via one-hot encoding, but the whole point of dense embedding is to <em>avoid</em> one-hot representation. In NLP, the word vocabulary size can be of the order 100k (sometimes even a million). On top of that, it's often needed to process the sequences of words in a batch. Processing the batch of sequences of word indices would be much more efficient than the batch of sequences of one-hot vectors. In addition, <code>gather</code> operation itself is faster than matrix dot-product, both in forward and backward pass.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here I want to improve the voted answer by providing more details:</p>\n<p>When we use embedding layer, it is generally to reduce one-hot input vectors (sparse) to denser representations.</p>\n<ol>\n<li><p>Embedding layer is much like a table lookup. When the table is small, it is fast.</p>\n</li>\n<li><p>When the table is large, table lookup is much slower. In practice, we would use dense layer as a dimension reducer to reduce the one-hot input instead of embedding layer in this case.</p>\n</li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a network which I want to train on some dataset (as an example, say <code>CIFAR10</code>). I can create data loader object via</p>\n<pre><code>trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n</code></pre>\n<p>My question is as follows: Suppose I want to make several different training iterations. Let's say I want at first to train the network on all images in odd positions, then on all images in even positions and so on. In order to do that, I need to be able to access to those images. Unfortunately, it seems that <code>trainset</code> does not allow such access. That is, trying to do <code>trainset[:1000]</code> or more generally <code>trainset[mask]</code> will throw an error.</p>\n<p>I could do instead </p>\n<pre><code>trainset.train_data=trainset.train_data[mask]\ntrainset.train_labels=trainset.train_labels[mask]\n</code></pre>\n<p>and then</p>\n<pre><code>trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                              shuffle=True, num_workers=2)\n</code></pre>\n<p>However, that will force me to create a new copy of the full dataset in each iteration (as I already changed <code>trainset.train_data</code> so I will need to redefine <code>trainset</code>). Is there some way to avoid it?</p>\n<p>Ideally, I would like to have something \"equivalent\" to</p>\n<pre><code>trainloader = torch.utils.data.DataLoader(trainset[mask], batch_size=4,\n                                              shuffle=True, num_workers=2)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\" rel=\"noreferrer\"><code>torch.utils.data.Subset</code></a> is easier, supports <code>shuffle</code>, and doesn't require writing your own sampler:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torchvision\nimport torch\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=None)\n\nevens = list(range(0, len(trainset), 2))\nodds = list(range(1, len(trainset), 2))\ntrainset_1 = torch.utils.data.Subset(trainset, evens)\ntrainset_2 = torch.utils.data.Subset(trainset, odds)\n\ntrainloader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=4,\n                                            shuffle=True, num_workers=2)\ntrainloader_2 = torch.utils.data.DataLoader(trainset_2, batch_size=4,\n                                            shuffle=True, num_workers=2)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can define a custom sampler for the dataset loader avoiding recreating the dataset (just creating a new loader for each different sampling).</p>\n<pre><code>class YourSampler(Sampler):\n    def __init__(self, mask):\n        self.mask = mask\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.nonzero(self.mask))\n\n    def __len__(self):\n        return len(self.mask)\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\n\nsampler1 = YourSampler(your_mask)\nsampler2 = YourSampler(your_other_mask)\ntrainloader_sampler1 = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          sampler = sampler1, shuffle=False, num_workers=2)\ntrainloader_sampler2 = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          sampler = sampler2, shuffle=False, num_workers=2)\n</code></pre>\n<p>PS: You can find more info here: <a href=\"http://pytorch.org/docs/master/_modules/torch/utils/data/sampler.html#Sampler\" rel=\"noreferrer\">http://pytorch.org/docs/master/_modules/torch/utils/data/sampler.html#Sampler</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'd like to reset (randomize) the weights of all layers in my Keras (deep learning) model. The reason is that I want to be able to train the model several times with different data splits without having to do the (slow) model recompilation every time.</p>\n<p>Inspired by <a href=\"https://github.com/fchollet/keras/pull/1908\" rel=\"noreferrer\">this discussion</a>, I'm trying the following code:</p>\n<pre><code># Reset weights\nfor layer in KModel.layers:\n    if hasattr(layer,'init'):\n        input_dim = layer.input_shape[1]\n        new_weights = layer.init((input_dim, layer.output_dim),name='{}_W'.format(layer.name))\n        layer.trainable_weights[0].set_value(new_weights.get_value())\n</code></pre>\n<p>However, it only partly works.</p>\n<p>Partly, becuase I've inspected some layer.get_weights() values, and they seem to change. But when I restart the training, the cost values are much lower than the initial cost values on the first run. It's almost like I've succeeded resetting some of the weights, but not all of them.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Save the initial weights right after compiling the model but before training it:</p>\n<pre><code>model.save_weights('model.h5')\n</code></pre>\n<p>and then after training, \"reset\" the model by reloading the initial weights:</p>\n<pre><code>model.load_weights('model.h5')\n</code></pre>\n<p>This gives you an apples to apples model to compare different data sets and should be quicker than recompiling the entire model.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Reset all layers by checking for initializers:</p>\n<pre><code>def reset_weights(model):\n    import keras.backend as K\n    session = K.get_session()\n    for layer in model.layers: \n        if hasattr(layer, 'kernel_initializer'): \n            layer.kernel.initializer.run(session=session)\n        if hasattr(layer, 'bias_initializer'):\n            layer.bias.initializer.run(session=session)     \n</code></pre>\n<p>Update: kernel_initializer is kernel.initializer now.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have found the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model\" rel=\"noreferrer\"><code>clone_model</code></a> function that creates a cloned network with the same architecture but new model weights.</p>\n<p>Example of use:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model_cloned = tensorflow.keras.models.clone_model(model_base)\n</code></pre>\n<p>Comparing the weights:</p>\n<pre class=\"lang-py prettyprint-override\"><code>original_weights = model_base.get_weights()\nprint(\"Original weights\", original_weights)\nprint(\"========================================================\")\nprint(\"========================================================\")\nprint(\"========================================================\")\nmodel_cloned = tensorflow.keras.models.clone_model(model_base)\nnew_weights = model_cloned.get_weights()\nprint(\"New weights\", new_weights)\n</code></pre>\n<p>If you execute this code several times, you will notice that the cloned model receives new weights each time.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-08-29 18:41:56Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/58764619/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm recently working on CNN and I want to know what is the function of temperature in softmax formula? and why should we use high temperatures to see a softer norm in probability distribution?<a href=\"https://i.sstatic.net/HYyQT.jpg\" rel=\"noreferrer\">Softmax Formula</a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One reason to use the temperature function is to change the output distribution computed by your neural net. It is added to the logits vector according to this equation :</p>\n<p><code>𝑞𝑖 =exp(𝑧𝑖/𝑇)/ ∑𝑗exp(𝑧𝑗/𝑇)</code></p>\n<p>where 𝑇 is the temperature parameter.</p>\n<p>You see, what this will do is change the final probabilities. You can choose T to be anything (the higher the T, the 'softer' the distribution will be - if it is 1, the output distribution will be the same as your normal softmax outputs). What I mean by 'softer' is that is that the model will basically be less confident about it's prediction. As T gets closer to 0, the 'harder' the distribution gets.</p>\n<p>a) Sample 'hard' softmax probs : <code>[0.01,0.01,0.98]</code></p>\n<p>b) Sample 'soft' softmax probs : <code>[0.2,0.2,0.6]</code></p>\n<p>'a' is a 'harder' distribution. Your model is very confident about its predictions. However, in many cases, you don't want your model to do that. For example, if you are using an RNN to generate text, you are basically sampling from your output distribution and choosing the sampled word as your output token(and next input). IF your model is extremely confident, it may produce very repetitive and uninteresting text. You want it to produce more diverse text which it will not produce because when the sampling procedure is going on, most of the probability mass will be concentrated in a few tokens and thus your model will keep selecting a select number of words over and over again. In order to give other words a chance of being sampled as well, you could plug in the temperature variable and produce more diverse text.</p>\n<p>With regards to why higher temperatures lead to softer distributions, that has to do with the exponential function. The temperature parameter penalizes bigger logits more than the smaller logits. The exponential function is an 'increasing function'. So if a term is already big, penalizing it by a small amount would make it much smaller (% wise) than if that term was small.</p>\n<p>Here's what I mean,</p>\n<pre><code>exp(6) ~ 403\nexp(3) ~ 20\n</code></pre>\n<p>Now let's 'penalize' this term with a temperature of let's say 1.5:</p>\n<pre><code>exp(6/1.5) ~ 54\nexp(3/1.5) ~ 7.4\n</code></pre>\n<p>You can see that in % terms, the bigger the term is, the more it shrinks when the temperature is used to penalize it. When the bigger logits shrink more than your smaller logits, more probability mass (to be computed by the softmax) will be assigned to the smaller logits.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When trying to get cross-entropy with sigmoid activation function, there is a difference between </p>\n<ol>\n<li><code>loss1 = -tf.reduce_sum(p*tf.log(q), 1)</code></li>\n<li><code>loss2 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q),1)</code></li>\n</ol>\n<p>But they are the same when with softmax activation function.</p>\n<p>Following is the sample code:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import tensorflow as tf\n\nsess2 = tf.InteractiveSession()\np = tf.placeholder(tf.float32, shape=[None, 5])\nlogit_q = tf.placeholder(tf.float32, shape=[None, 5])\nq = tf.nn.sigmoid(logit_q)\nsess.run(tf.global_variables_initializer())\n\nfeed_dict = {p: [[0, 0, 0, 1, 0], [1,0,0,0,0]], logit_q: [[0.2, 0.2, 0.2, 0.2, 0.2], [0.3, 0.3, 0.2, 0.1, 0.1]]}\nloss1 = -tf.reduce_sum(p*tf.log(q),1).eval(feed_dict)\nloss2 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q),1).eval(feed_dict)\n\nprint(p.eval(feed_dict), \"\\n\", q.eval(feed_dict))\nprint(\"\\n\",loss1, \"\\n\", loss2)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You're confusing the cross-entropy for <em>binary</em> and <em>multi-class</em> problems.</p>\n<h2>Multi-class cross-entropy</h2>\n<p>The formula that you use is correct and it directly corresponds to <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" rel=\"noreferrer\"><code>tf.nn.softmax_cross_entropy_with_logits</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>-tf.reduce_sum(p * tf.log(q), axis=1)\n</code></pre>\n<p><code>p</code> and <code>q</code> are expected to be probability distributions over N classes. In particular, N can be 2, as in the following example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>p = tf.placeholder(tf.float32, shape=[None, 2])\nlogit_q = tf.placeholder(tf.float32, shape=[None, 2])\nq = tf.nn.softmax(logit_q)\n\nfeed_dict = {\n  p: [[0, 1],\n      [1, 0],\n      [1, 0]],\n  logit_q: [[0.2, 0.8],\n            [0.7, 0.3],\n            [0.5, 0.5]]\n}\n\nprob1 = -tf.reduce_sum(p * tf.log(q), axis=1)\nprob2 = tf.nn.softmax_cross_entropy_with_logits(labels=p, logits=logit_q)\nprint(prob1.eval(feed_dict))  # [ 0.43748799  0.51301527  0.69314718]\nprint(prob2.eval(feed_dict))  # [ 0.43748799  0.51301527  0.69314718]\n</code></pre>\n<p>Note that <code>q</code> is computing <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax\" rel=\"noreferrer\"><code>tf.nn.softmax</code></a>, i.e. outputs a probability distribution. So it's still multi-class cross-entropy formula, only for N = 2.</p>\n<h2>Binary cross-entropy</h2>\n<p>This time the correct formula is</p>\n<pre class=\"lang-py prettyprint-override\"><code>p * -tf.log(q) + (1 - p) * -tf.log(1 - q)\n</code></pre>\n<p>Though mathematically it's a partial case of the multi-class case, the <em>meaning</em> of <code>p</code> and <code>q</code> is different. In the simplest case, each <code>p</code> and <code>q</code> is a number, corresponding to a probability of the class A. </p>\n<p><strong>Important</strong>: Don't get confused by the common <code>p * -tf.log(q)</code> part and the sum. Previous <code>p</code> was a one-hot vector, now it's a number, zero or one. Same for <code>q</code> - it was a probability distribution, now's it's a number (probability).</p>\n<p>If <code>p</code> is a vector, each individual component is considered an <em>independent binary classification</em>. See <a href=\"https://stackoverflow.com/a/47034889/712995\">this answer</a> that outlines the difference between softmax and sigmoid functions in tensorflow. So the definition <code>p = [0, 0, 0, 1, 0]</code> doesn't mean a one-hot vector, but 5 different features, 4 of which are off and 1 is on. The definition <code>q = [0.2, 0.2, 0.2, 0.2, 0.2]</code> means that each of 5 features is on with 20% probability.</p>\n<p>This explains the use of <code>sigmoid</code> function before the cross-entropy: its goal is to squash the logit to <code>[0, 1]</code> interval.</p>\n<p>The formula above still holds for multiple independent features, and that's exactly what <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\" rel=\"noreferrer\"><code>tf.nn.sigmoid_cross_entropy_with_logits</code></a> computes:</p>\n<pre class=\"lang-py prettyprint-override\"><code>p = tf.placeholder(tf.float32, shape=[None, 5])\nlogit_q = tf.placeholder(tf.float32, shape=[None, 5])\nq = tf.nn.sigmoid(logit_q)\n\nfeed_dict = {\n  p: [[0, 0, 0, 1, 0],\n      [1, 0, 0, 0, 0]],\n  logit_q: [[0.2, 0.2, 0.2, 0.2, 0.2],\n            [0.3, 0.3, 0.2, 0.1, 0.1]]\n}\n\nprob1 = -p * tf.log(q)\nprob2 = p * -tf.log(q) + (1 - p) * -tf.log(1 - q)\nprob3 = p * -tf.log(tf.sigmoid(logit_q)) + (1-p) * -tf.log(1-tf.sigmoid(logit_q))\nprob4 = tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q)\nprint(prob1.eval(feed_dict))\nprint(prob2.eval(feed_dict))\nprint(prob3.eval(feed_dict))\nprint(prob4.eval(feed_dict))\n</code></pre>\n<p>You should see that the last three tensors are equal, while the <code>prob1</code> is only a part of cross-entropy, so it contains correct value only when <code>p</code> is <code>1</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[ 0.          0.          0.          0.59813893  0.        ]\n [ 0.55435514  0.          0.          0.          0.        ]]\n[[ 0.79813886  0.79813886  0.79813886  0.59813887  0.79813886]\n [ 0.5543552   0.85435522  0.79813886  0.74439669  0.74439669]]\n[[ 0.7981388   0.7981388   0.7981388   0.59813893  0.7981388 ]\n [ 0.55435514  0.85435534  0.7981388   0.74439663  0.74439663]]\n[[ 0.7981388   0.7981388   0.7981388   0.59813893  0.7981388 ]\n [ 0.55435514  0.85435534  0.7981388   0.74439663  0.74439663]]\n</code></pre>\n<p>Now it should be clear that taking a sum of <code>-p * tf.log(q)</code> along <code>axis=1</code> doesn't make sense in this setting, though it'd be a valid formula in multi-class case.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2015-10-01 00:36:22Z\">9 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/2074956/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Does anyone know of recent academic work which has been done on logo recognition in images?\nPlease answer only if you are familiar with this specific subject (I can search Google for \"logo recognition\" myself, thank you very much).\nAnyone who is knowledgeable in computer vision and has done work on object recognition is welcome to comment as well. </p>\n<p><strong>Update</strong>:\nPlease refer to the algorithmic aspects (what approach you think is appropriate, papers in the field, whether it should work(and has been tested) for real world data, efficiency considerations) and not the technical sides (the programming language used or whether it was with OpenCV...)\nWork on image indexing and content based image retrieval can also help.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could try to use local features like SIFT here:\n<a href=\"http://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Scale-invariant_feature_transform</a></p>\n<p>It should work because logo shape is usually constant, so extracted features shall match well.</p>\n<p>The workflow will be like this:</p>\n<ol>\n<li><p>Detect corners (e.g. Harris corner detector) - for Nike logo they are two sharp ends.</p></li>\n<li><p>Compute descriptors (like SIFT - 128D integer vector)</p></li>\n<li><p>On training stage remember them; on matching stage find nearest neighbours for every feature in the database obtained during training. Finally, you have a set of matches (some of them are probably wrong).</p></li>\n<li><p>Seed out wrong matches using RANSAC. Thus you'll get the matrix that describes transform from ideal logo image to one where you find the logo. Depending on the settings, you could allow different kinds of transforms (just translation; translation and rotation; affine transform).</p></li>\n</ol>\n<p>Szeliski's book has a chapter (4.1) on local features.\n<a href=\"http://research.microsoft.com/en-us/um/people/szeliski/Book/\" rel=\"noreferrer\">http://research.microsoft.com/en-us/um/people/szeliski/Book/</a></p>\n<p>P.S. </p>\n<ol>\n<li><p>I assumed you wanna find logos in photos, for example find all Pepsi billboards, so they could be distorted. If you need to find a TV channel logo on the screen (so that it is not rotated and scaled), you could do it easier (pattern matching or something).</p></li>\n<li><p>Conventional SIFT does not consider color information. Since logos usually have constant colors (though the exact color depends on lightning and camera) you might want to consider color information somehow.</p></li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>We worked on logo detection/recognition in real-world images. We also created a dataset <a href=\"http://www.multimedia-computing.de/flickrlogos/\" rel=\"noreferrer\">FlickrLogos-32</a> and made it publicly available, including data, ground truth and evaluation scripts.</p>\n<p>In our work we treated logo recognition as retrieval problem to simplify multi-class recognition and to allow such systems to be easily scalable to many (e.g. thousands) logo classes.</p>\n<p>Recently, we developed a bundling technique called <em>Bundle min-Hashing</em> that aggregates spatial configurations of multiple local features into highly distinctive feature bundles. The bundle representation is usable for both retrieval and recognition. See the following example heatmaps for logo detections:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/af9Vx.jpg\"/>\n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/z4yyA.jpg\"/></p>\n<p>You will find more details on the internal operations, potential applications of the approach, experiments on its performance and of course also many references to related work in the papers <a href=\"http://www.multimedia-computing.de/mediawiki//images/d/da/Bundle_Min-Hashing_for_Logo_Recognition_-_ICMR2013.pdf\" rel=\"noreferrer\">[1]</a><a href=\"http://link.springer.com/article/10.1007%2Fs13735-013-0040-x\" rel=\"noreferrer\">[2]</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Worked on that: Trademark matching and retrieval in sports video databases\nget a PDF of the paper: <a href=\"http://scholar.google.it/scholar?cluster=9926471658203167449&amp;hl=en&amp;as_sdt=2000\" rel=\"nofollow noreferrer\">http://scholar.google.it/scholar?cluster=9926471658203167449&amp;hl=en&amp;as_sdt=2000</a></p>\n<p>We used SIFT as trademark and image descriptors, and a normalized threshold matching to compute the distance between models and images. In our latest work we have been able to greatly reduce computation using meta-models, created evaluating the relevance of the SIFT points that are present in different versions of the same trademark.</p>\n<p>I'd say that in general working with videos is harder than working on photos due to the very bad visual quality of the TV standards currently used.</p>\n<p>Marco</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm looking through the Apple's <a href=\"https://developer.apple.com/documentation/vision\" rel=\"noreferrer\">Vision API documentation</a> and I see a couple of classes that relate to text detection in <code>UIImages</code>:</p>\n<p>1) <a href=\"https://developer.apple.com/documentation/vision/vndetecttextrectanglesrequest\" rel=\"noreferrer\"><code>class VNDetectTextRectanglesRequest</code></a></p>\n<p>2) <a href=\"https://developer.apple.com/documentation/vision/vntextobservation\" rel=\"noreferrer\"><code>class VNTextObservation</code></a></p>\n<p>It looks like they can detect characters, but I don't see a means to do anything with the characters. Once you've got characters detected, how would you go about turning them into something that can be interpreted by <a href=\"https://developer.apple.com/documentation/foundation/nslinguistictagger\" rel=\"noreferrer\"><code>NSLinguisticTagger</code></a>?</p>\n<p>Here's a post that is a brief overview of <a href=\"https://medium.com/compileswift/swift-world-whats-new-in-ios-11-vision-456ba4156bad\" rel=\"noreferrer\"><code>Vision</code></a>.</p>\n<p>Thank you for reading. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is how to do it ... </p>\n<pre><code>    //\n//  ViewController.swift\n//\n\n\nimport UIKit\nimport Vision\nimport CoreML\n\nclass ViewController: UIViewController {\n\n    //HOLDS OUR INPUT\n    var  inputImage:CIImage?\n\n    //RESULT FROM OVERALL RECOGNITION\n    var  recognizedWords:[String] = [String]()\n\n    //RESULT FROM RECOGNITION\n    var recognizedRegion:String = String()\n\n\n    //OCR-REQUEST\n    lazy var ocrRequest: VNCoreMLRequest = {\n        do {\n            //THIS MODEL IS TRAINED BY ME FOR FONT \"Inconsolata\" (Numbers 0...9 and UpperCase Characters A..Z)\n            let model = try VNCoreMLModel(for:OCR().model)\n            return VNCoreMLRequest(model: model, completionHandler: self.handleClassification)\n        } catch {\n            fatalError(\"cannot load model\")\n        }\n    }()\n\n    //OCR-HANDLER\n    func handleClassification(request: VNRequest, error: Error?)\n    {\n        guard let observations = request.results as? [VNClassificationObservation]\n            else {fatalError(\"unexpected result\") }\n        guard let best = observations.first\n            else { fatalError(\"cant get best result\")}\n\n        self.recognizedRegion = self.recognizedRegion.appending(best.identifier)\n    }\n\n    //TEXT-DETECTION-REQUEST\n    lazy var textDetectionRequest: VNDetectTextRectanglesRequest = {\n        return VNDetectTextRectanglesRequest(completionHandler: self.handleDetection)\n    }()\n\n    //TEXT-DETECTION-HANDLER\n    func handleDetection(request:VNRequest, error: Error?)\n    {\n        guard let observations = request.results as? [VNTextObservation]\n            else {fatalError(\"unexpected result\") }\n\n       // EMPTY THE RESULTS\n        self.recognizedWords = [String]()\n\n        //NEEDED BECAUSE OF DIFFERENT SCALES\n        let  transform = CGAffineTransform.identity.scaledBy(x: (self.inputImage?.extent.size.width)!, y:  (self.inputImage?.extent.size.height)!)\n\n        //A REGION IS LIKE A \"WORD\"\n        for region:VNTextObservation in observations\n        {\n            guard let boxesIn = region.characterBoxes else {\n                continue\n            }\n\n            //EMPTY THE RESULT FOR REGION\n            self.recognizedRegion = \"\"\n\n            //A \"BOX\" IS THE POSITION IN THE ORIGINAL IMAGE (SCALED FROM 0... 1.0)\n            for box in boxesIn\n            {\n                //SCALE THE BOUNDING BOX TO PIXELS\n                let realBoundingBox = box.boundingBox.applying(transform)\n\n                //TO BE SURE\n                guard (inputImage?.extent.contains(realBoundingBox))!\n                    else { print(\"invalid detected rectangle\"); return}\n\n                //SCALE THE POINTS TO PIXELS\n                let topleft = box.topLeft.applying(transform)\n                let topright = box.topRight.applying(transform)\n                let bottomleft = box.bottomLeft.applying(transform)\n                let bottomright = box.bottomRight.applying(transform)\n\n                //LET'S CROP AND RECTIFY\n                let charImage = inputImage?\n                    .cropped(to: realBoundingBox)\n                    .applyingFilter(\"CIPerspectiveCorrection\", parameters: [\n                        \"inputTopLeft\" : CIVector(cgPoint: topleft),\n                        \"inputTopRight\" : CIVector(cgPoint: topright),\n                        \"inputBottomLeft\" : CIVector(cgPoint: bottomleft),\n                        \"inputBottomRight\" : CIVector(cgPoint: bottomright)\n                        ])\n\n                //PREPARE THE HANDLER\n                let handler = VNImageRequestHandler(ciImage: charImage!, options: [:])\n\n                //SOME OPTIONS (TO PLAY WITH..)\n                self.ocrRequest.imageCropAndScaleOption = VNImageCropAndScaleOption.scaleFill\n\n                //FEED THE CHAR-IMAGE TO OUR OCR-REQUEST - NO NEED TO SCALE IT - VISION WILL DO IT FOR US !!\n                do {\n                    try handler.perform([self.ocrRequest])\n                }  catch { print(\"Error\")}\n\n            }\n\n            //APPEND RECOGNIZED CHARS FOR THAT REGION\n            self.recognizedWords.append(recognizedRegion)\n        }\n\n        //THATS WHAT WE WANT - PRINT WORDS TO CONSOLE\n        DispatchQueue.main.async {\n            self.PrintWords(words: self.recognizedWords)\n        }\n    }\n\n    func PrintWords(words:[String])\n    {\n        // VOILA'\n        print(recognizedWords)\n\n    }\n\n    func doOCR(ciImage:CIImage)\n    {\n        //PREPARE THE HANDLER\n        let handler = VNImageRequestHandler(ciImage: ciImage, options:[:])\n\n        //WE NEED A BOX FOR EACH DETECTED CHARACTER\n        self.textDetectionRequest.reportCharacterBoxes = true\n        self.textDetectionRequest.preferBackgroundProcessing = false\n\n        //FEED IT TO THE QUEUE FOR TEXT-DETECTION\n        DispatchQueue.global(qos: .userInteractive).async {\n            do {\n                try  handler.perform([self.textDetectionRequest])\n            } catch {\n                print (\"Error\")\n            }\n        }\n\n    }\n\n    override func viewDidLoad() {\n        super.viewDidLoad()\n        // Do any additional setup after loading the view, typically from a nib.\n\n        //LETS LOAD AN IMAGE FROM RESOURCE\n        let loadedImage:UIImage = UIImage(named: \"Sample1.png\")! //TRY Sample2, Sample3 too\n\n        //WE NEED A CIIMAGE - NOT NEEDED TO SCALE\n        inputImage = CIImage(image:loadedImage)!\n\n        //LET'S DO IT\n        self.doOCR(ciImage: inputImage!)\n\n\n    }\n\n    override func didReceiveMemoryWarning() {\n        super.didReceiveMemoryWarning()\n        // Dispose of any resources that can be recreated.\n    }\n}\n</code></pre>\n<p>You'll find the complete project <a href=\"https://github.com/DrNeuroSurg/OCRwithVisionAndCoreML-Part2\" rel=\"noreferrer\">here</a> included is the trained model !</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Apple finally updated Vision to do OCR. Open a playground and dump a couple of test images in the Resources folder. In my case, I called them \"demoDocument.jpg\" and \"demoLicensePlate.jpg\".</p>\n<p>The new class is called <code>VNRecognizeTextRequest</code>. Dump this in a playground and give it a whirl:</p>\n<pre><code>import Vision\n\nenum DemoImage: String {\n    case document = \"demoDocument\"\n    case licensePlate = \"demoLicensePlate\"\n}\n\nclass OCRReader {\n    func performOCR(on url: URL?, recognitionLevel: VNRequestTextRecognitionLevel)  {\n        guard let url = url else { return }\n        let requestHandler = VNImageRequestHandler(url: url, options: [:])\n\n        let request = VNRecognizeTextRequest  { (request, error) in\n            if let error = error {\n                print(error)\n                return\n            }\n\n            guard let observations = request.results as? [VNRecognizedTextObservation] else { return }\n\n            for currentObservation in observations {\n                let topCandidate = currentObservation.topCandidates(1)\n                if let recognizedText = topCandidate.first {\n                    print(recognizedText.string)\n                }\n            }\n        }\n        request.recognitionLevel = recognitionLevel\n\n        try? requestHandler.perform([request])\n    }\n}\n\nfunc url(for image: DemoImage) -&gt; URL? {\n    return Bundle.main.url(forResource: image.rawValue, withExtension: \"jpg\")\n}\n\nlet ocrReader = OCRReader()\nocrReader.performOCR(on: url(for: .document), recognitionLevel: .fast)\n</code></pre>\n<p>There's an <a href=\"https://developer.apple.com/videos/play/wwdc2019/234/\" rel=\"noreferrer\">in-depth discussion</a> of this from WWDC19</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>SwiftOCR</strong></p>\n<p>I just got SwiftOCR to work with small sets of text.</p>\n<p><a href=\"https://github.com/garnele007/SwiftOCR\" rel=\"noreferrer\">https://github.com/garnele007/SwiftOCR</a></p>\n<p>uses</p>\n<p><a href=\"https://github.com/Swift-AI/Swift-AI\" rel=\"noreferrer\">https://github.com/Swift-AI/Swift-AI</a></p>\n<p>which uses NeuralNet-MNIST model for text recognition.</p>\n<p><strong>TODO : VNTextObservation &gt; SwiftOCR</strong></p>\n<p>Will post example of it using VNTextObservation once I have it one connected to the other.</p>\n<p><strong>OpenCV + Tesseract OCR</strong></p>\n<p>I tried to use OpenCV + Tesseract but got compile errors then found SwiftOCR.</p>\n<p><strong>SEE ALSO : Google Vision iOS</strong></p>\n<p>Note Google Vision Text Recognition - Android sdk has text detection but also has iOS cocoapod. So keep an eye on it as should add text recognition to the iOS eventually.</p>\n<p><a href=\"https://developers.google.com/vision/text-overview\" rel=\"noreferrer\">https://developers.google.com/vision/text-overview</a></p>\n<p>//Correction: just tried it but only Android version of the sdk supports text detection.</p>\n<p><a href=\"https://developers.google.com/vision/text-overview\" rel=\"noreferrer\">https://developers.google.com/vision/text-overview</a></p>\n<p>If you subscribe to releases: \n<a href=\"https://libraries.io/cocoapods/GoogleMobileVision\" rel=\"noreferrer\">https://libraries.io/cocoapods/GoogleMobileVision</a></p>\n<p>Click SUBSCRIBE TO RELEASES\nyou can see when TextDetection is added to the iOS part of the Cocoapod</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I know the basics of Reinforcement Learning, but what terms it's necessary to understand to be able read <a href=\"https://arxiv.org/abs/1707.06347\" rel=\"noreferrer\">arxiv PPO paper</a> ?</p>\n<p>What is the roadmap to learn and use <a href=\"https://blog.openai.com/openai-baselines-ppo/\" rel=\"noreferrer\">PPO</a> ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To better understand PPO, it is helpful to look at the main contributions of the paper, which are: <strong>(1)</strong> the Clipped Surrogate Objective and <strong>(2)</strong> the use of \"multiple epochs of stochastic gradient ascent to perform each policy update\".</p>\n<br/>\n<p>From the original <a href=\"https://arxiv.org/abs/1707.06347\" rel=\"noreferrer\">PPO paper</a>:</p>\n<blockquote>\n<p>We have introduced [PPO], a family of policy optimization methods that use <strong>multiple epochs of stochastic gradient ascent to perform each policy update</strong>. These methods have the stability and reliability of trust-region [<a href=\"https://arxiv.org/abs/1502.05477\" rel=\"noreferrer\">TRPO</a>] methods but are much simpler to implement, requiring <strong>only a few lines of code change to a vanilla policy gradient implementation</strong>, applicable in more general settings (for example, when using a joint architecture for the policy and value function), and have better overall performance.</p>\n</blockquote>\n<hr/>\n<h1 id=\"the-clipped-surrogate-objective-98d7\">1. The Clipped Surrogate Objective</h1>\n<p>The Clipped Surrogate Objective is a drop-in replacement for the policy gradient objective that is designed to improve training stability by limiting the change you make to your policy at each step.</p>\n<p>For vanilla policy gradients (e.g., REINFORCE) --- which you should be familiar with, or <a href=\"http://karpathy.github.io/2016/05/31/rl/\" rel=\"noreferrer\">familiarize yourself with</a> before you read this --- the objective used to optimize the neural network looks like:</p>\n<p><a href=\"https://i.sstatic.net/5VZRT.png\" rel=\"noreferrer\"><img alt=\"PG objective\" src=\"https://i.sstatic.net/5VZRT.png\"/></a></p>\n<p>This is the standard formula that you would see in the <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" rel=\"noreferrer\">Sutton book</a>, and <a href=\"http://karpathy.github.io/2016/05/31/rl/\" rel=\"noreferrer\">other</a> <a href=\"http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html\" rel=\"noreferrer\">resources</a>, where the A-hat could be the discounted return (as in REINFORCE) or the advantage function (as in <a href=\"https://arxiv.org/abs/1506.02438\" rel=\"noreferrer\">GAE</a>) for example.  By taking a gradient ascent step on this loss with respect to the network parameters, you will incentivize the actions that led to higher reward.</p>\n<p>The vanilla policy gradient method uses the log probability of your action (log π(a | s)) to trace the impact of the actions, but you could imagine using another function to do this.  Another such function, introduced in <a href=\"https://people.eecs.berkeley.edu/%7Epabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf\" rel=\"noreferrer\">this paper</a>, uses the probability of the action under the <em>current policy</em> (π(a|s)), divided by the probability of the action under your <em>previous policy</em> (π_old(a|s)).  This looks a bit similar to importance sampling if you are familiar with that:</p>\n<p><a href=\"https://i.sstatic.net/bCAEy.png\" rel=\"noreferrer\"><img alt=\"r eq\" src=\"https://i.sstatic.net/bCAEy.png\"/></a></p>\n<p>This r(θ) will be greater than 1 when the action is <em>more</em> probable for your <em>current</em> policy than it is for your <em>old</em> policy; it will be between 0 and 1 when the action is less probable for your current policy than for your old.</p>\n<p>Now to build an objective function with this r(θ), we can simply swap it in for the log π(a|s) term.  This is what is done in TRPO:</p>\n<p><a href=\"https://i.sstatic.net/EMMPa.png\" rel=\"noreferrer\"><img alt=\"TRPO objective\" src=\"https://i.sstatic.net/EMMPa.png\"/></a></p>\n<p><strong>But what would happen here if your action is much more probable (like 100x more) for your current policy?</strong>  r(θ) will tend to be really big and lead to taking big gradient steps that might wreck your policy.  To deal with this and other issues, TRPO adds several extra bells and whistles (e.g., KL Divergence constraints) to limit the amount the policy can change and help guarantee that it is monotonically improving.</p>\n<p>Instead of adding all these extra bells and whistles, what if we could build these stabilizing properties into the objective function?  As you might guess, this is what PPO does.  It gains the same performance benefits  as TRPO and avoids the complications by optimizing this simple (but kind of funny looking) Clipped Surrogate Objective:</p>\n<p><a href=\"https://i.sstatic.net/zt9mz.png\" rel=\"noreferrer\"><img alt=\"annotated clipped surrogate\" src=\"https://i.sstatic.net/zt9mz.png\"/></a></p>\n<p>The first term (blue) inside the minimization is the same (r(θ)A) term we saw in the TRPO objective.  The second term (red) is a version where the (r(θ)) is clipped between (1 - e, 1 + e). (in the paper they state a good value for e is about 0.2, so r can vary between ~(0.8, 1.2)).  Then, finally, the minimization of both of these terms is taken (green).</p>\n<p>Take your time and look at the equation carefully and make sure you know what all the symbols mean, and mathematically what is happening.  Looking at the code may also help; here is the relevant section in both the OpenAI <a href=\"https://github.com/openai/baselines/blob/9fa8e1baf1d1f975b87b369a8082122eac812eb1/baselines/ppo1/pposgd_simple.py#L111-L117\" rel=\"noreferrer\">baselines</a> and <a href=\"https://github.com/unixpickle/anyrl-py/blob/953ad68d6507b83583e342b3210ed98e03a86a4f/anyrl/algos/ppo.py#L149-L155\" rel=\"noreferrer\">anyrl-py</a> implementations.</p>\n<p>Great.</p>\n<p>Next, let's see what effect the L clip function creates. Here is a diagram from the paper that plots the value of the clip objective for when the Advantage is positive and negative:</p>\n<p><a href=\"https://i.sstatic.net/F6SxR.png\" rel=\"noreferrer\"><img alt=\"Clip intro\" src=\"https://i.sstatic.net/F6SxR.png\"/></a></p>\n<p>On the left half of the diagram, where  (A &gt; 0), this is where the action had an estimated positive effect on the outcome.  On the right half of the diagram, where (A &lt; 0), this is where the action had an estimated negative effect on the outcome.</p>\n<p>Notice how on the left half, the r-value gets clipped if it gets too high.  This will happen if the action became a lot more probable under the current policy than it was for the old policy.  When this happens, we do not want to get greedy and step too far (because this is just a local approximation and sample of our policy, so it will not be accurate if we step too far), and so we clip the objective to prevent it from growing.  (This will have the effect in the backward pass of blocking the gradient --- the flat line causing the gradient to be 0).</p>\n<p>On the right side of the diagram, where the action had an estimated <em>negative</em> effect on the outcome, we see that the clip activates near 0, where the action under the current policy is unlikely.  This clipping region will similarly prevent us from updating too much to make the action much less probable after we already just took a big step to make it less probable.</p>\n<p>So we see that both of these clipping regions prevent us from getting too greedy and trying to update too much at once and leaving the region where this sample offers a good estimate.</p>\n<p><strong>But why are we letting the r(θ) grow indefinitely on the far right side of the diagram?  This seems odd as first, but what would cause r(θ) to grow really large in this case?</strong>  r(θ) growth in this region will be caused by a gradient step that made our action  <em>a lot more probable</em>, and it turning out to make our policy <em>worse</em>.  If that was the case, we would want to be able to undo that gradient step.  And it just so happens that the L clip function allows this.  The function is negative here, so the gradient will tell us to walk the other direction and make the action less probable by an amount proportional to how much we screwed it up.  (Note that there is a similar region on the far left side of the diagram, where the action is good and we accidentally made it less probable.)</p>\n<p>These \"undo\" regions explain why we must include the weird minimization term in the objective function.  They correspond to the unclipped r(θ)A having a lower value than the clipped version and getting returned by the minimization.  This is because they were steps in the wrong direction (e.g., the action was good but we accidentally made it less probable).  If we had not included the min in the objective function, these regions would be flat (gradient = 0) and we would be prevented from fixing mistakes.</p>\n<p>Here is a diagram summarizing this:</p>\n<p><a href=\"https://i.sstatic.net/gasbI.png\" rel=\"noreferrer\"><img alt=\"L Clip Diagram\" src=\"https://i.sstatic.net/gasbI.png\"/></a></p>\n<p>And that is the gist of it.  The Clipped Surrogate Objective is just a drop-in replacement you could use in the vanilla policy gradient.  The clipping limits the effective change you can make at each step in order to improve stability, and the minimization allows us to fix our mistakes in case we screwed it up.  One thing I didn't discuss is what is meant by PPO objective forming a \"lower bound\" as discussed in the paper.  For more on that, I would suggest <a href=\"https://youtu.be/gqX8J38tESw?t=14m1s\" rel=\"noreferrer\">this part</a> of a lecture the author gave.</p>\n<h1 id=\"multiple-epochs-for-policy-updating-a67s\">2. Multiple epochs for policy updating</h1>\n<p>Unlike vanilla policy gradient methods, and <em>because of the Clipped Surrogate Objective function</em>, PPO allows you to run multiple epochs of gradient ascent on your samples without causing destructively large policy updates. This allows you to squeeze more out of your data and reduce sample inefficiency.</p>\n<p>PPO runs the policy using <em>N</em> parallel actors each collecting data, and then it samples mini-batches of this data to train for <em>K</em> epochs using the Clipped Surrogate Objective function. See full algorithm below (the approximate param values are: <em>K</em> = 3-15, <em>M</em> = 64-4096, <em>T</em> (horizon) = 128-2048):</p>\n<p><a href=\"https://i.sstatic.net/a6z3u.png\" rel=\"noreferrer\"><img alt=\"PPO Algo\" src=\"https://i.sstatic.net/a6z3u.png\"/></a></p>\n<p>The parallel actors part was popularized by the <a href=\"https://arxiv.org/abs/1602.01783\" rel=\"noreferrer\">A3C paper</a> and has become a fairly standard way for collecting data.</p>\n<p>The newish part is that they are able to run <em>K</em> epochs of gradient ascent on the trajectory samples.  As they state in the paper, it would be nice to run the vanilla policy gradient optimization for multiple passes over the data so that you could learn more from each sample. However, this generally fails in practice for vanilla methods because they take too big of steps on the local samples and this wrecks the policy.  PPO, on the other hand, has the built-in mechanism to prevent too much of an update.</p>\n<p>For each iteration, after sampling the environment with π_old (line 3) and when we start running the optimization (line 6), our policy π will be exactly equal to π_old.  So at first, none of our updates will be clipped and we are guaranteed to learn something from these examples.  However, as we update π using multiple epochs, the objective will start hitting the clipping limits, the gradient will go to 0 for those samples, and the training will gradually stop...until we move on to the next iteration and collect new samples.</p>\n<p>....</p>\n<p>And that's all for now.  If you are interested in gaining a better understanding, I would recommend digging more into the <a href=\"https://arxiv.org/abs/1707.06347\" rel=\"noreferrer\">original paper</a>, trying to implement it yourself, or diving into the <a href=\"https://github.com/openai/baselines/blob/b29c8020d7ac72256dca48fae85e96b4b3c75ccb/baselines/ppo2/ppo2.py\" rel=\"noreferrer\">baselines implementation</a> and playing with the code.</p>\n<p>[edit: 2019/01/27]: For a better background and for how PPO relates to other RL algorithms, I would also strongly recommend checking out OpenAI's <a href=\"https://spinningup.openai.com/en/latest/index.html\" rel=\"noreferrer\">Spinning Up resources and implementations</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>PPO, and including TRPO tries to update the policy conservatively, without affecting its performance adversely between each policy update.</p>\n<p>To do this, you need a way to measure how much the policy has changed after each update. This measurement is done by looking at the KL divergence between the updated policy and the old policy.</p>\n<p>This becomes a constrained optimization problem, we want to change the policy in the direction of maximum performance, following the constraints that the KL divergence between my new policy and old do not exceed some pre defined (or adaptive) threshold.</p>\n<p>With TRPO, we compute the KL constraint during update and finds the learning rate for this problem (via Fisher Matrix and conjugate gradient). This is somewhat messy to implement.</p>\n<p>With PPO, we simplify the problem by turning the KL divergence from a constraint to a penalty term, similar to for example to L1, L2 weight penalty (to prevent a weights from growing large values). PPO makes additional modifications by removing the need to compute KL divergence all together, by hard clipping the policy ratio (ratio of updated policy with old) to be within a small range around 1.0, where 1.0 means the new policy is same as old.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>PPO is a simple algorithm, which falls into policy optimization algorithms class (as opposed to value-based methods such as DQN). If you \"know\" RL basics (I mean if you have at least read thoughtfully some first chapters of <a href=\"http://www.incompleteideas.net/book/the-book-2nd.html\" rel=\"noreferrer\">Sutton's book</a> for example), then a first logical step is to get familiar with policy gradient algorithms. You can read <a href=\"https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" rel=\"noreferrer\">this paper</a> or chapter 13 of <a href=\"http://www.incompleteideas.net/book/the-book-2nd.html\" rel=\"noreferrer\">Sutton's book</a> new edition. Additionally, you may also read <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" rel=\"noreferrer\">this paper</a> on TRPO, which is a previous work from PPO's first author (this paper has numerous notational mistakes; just note). Hope that helps.  --Mehdi</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In simple words, what is the difference between cross-validation and grid search? How does grid search work? Should I do first a cross-validation and then a grid search?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross-validation is when you reserve part of your data to use in evaluating your model.  There are different cross-validation methods.  The simplest conceptually is to just take 70% (just making up a number here, it doesn't have to be 70%) of your data and use that for training, and then use the remaining 30% of the data to evaluate the model's performance.  The reason you need different data for training and evaluating the model is to protect against overfitting.  There are other (slightly more involved) cross-validation techniques, of course, like k-fold cross-validation, which often used in practice.</p>\n<p>Grid search is a method to perform hyper-parameter optimisation, that is, it is a method to find the best combination of hyper-parameters (an example of an hyper-parameter is the learning rate of the optimiser), for a given model (e.g. a CNN) and test dataset. In this scenario, you have several models, each with a different combination of hyper-parameters. Each of these combinations of parameters, which correspond to a single model, can be said to lie on a point of a \"grid\". The goal is then to train each of these models and evaluate them e.g. using cross-validation. You then select the one that performed best.</p>\n<p>To give a concrete example, if you're using a support vector machine, you could use different values for <code>gamma</code> and <code>C</code>.  So, for example, you could have a grid with the following values for <code>(gamma, C)</code>: <code>(1, 1), (0.1, 1), (1, 10), (0.1, 10)</code>.  It's a grid because it's like a product of <code>[1, 0.1]</code> for <code>gamma</code> and <code>[1, 10]</code> for <code>C</code>.  Grid-search would basically train a SVM for each of these four pair of <code>(gamma, C)</code> values, then evaluate it using cross-validation, and select the one that did best.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross-validation is a method for robustly estimating test-set performance (generalization) of a model.\nGrid-search is a way to select the best of a family of models, parametrized by a grid of parameters.</p>\n<p>Here, by \"model\", I don't mean a trained instance, more the algorithms together with the parameters, such as <code>SVC(C=1, kernel='poly')</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross-validation, simply separating test and training data and validate training results with test data. There are two cross validation techniques that I know. </p>\n<p>First, Test/Train cross validation. Splitting data as test and train. </p>\n<p>Second, k-fold cross-validation split your data into k bins, use each bin as testing data and use rest of the data as training data and validate against testing data. Repeat the process k times. And Get the average performance. k-fold cross validation especially useful for small dataset since it maximizes both the test and training data. </p>\n<p>Grid Search; systematically working through multiple combinations of parameter tunes, cross validate each and determine which one gives the best performance.You can work through many combination only changing parameters a bit.  </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can anyone tell me why we set random state to zero in splitting train and test set.</p>\n<pre><code>X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.30, random_state=0)\n</code></pre>\n<p>I have seen situations like this where random state is set to 1!</p>\n<pre><code>X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.30, random_state=1)\n</code></pre>\n<p>What is the consequence of this random state in cross validation as well?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It doesn't matter if the random_state is 0 or 1 or any other integer. What matters is that it should be set the same value, if you want to validate your processing over multiple runs of the code. By the way I have seen <code>random_state=42</code> used in many official examples of scikit as well as elsewhere also.</p>\n<p><code>random_state</code> as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case. In the <a href=\"http://scikit-learn.org/stable/developers/utilities.html\" rel=\"noreferrer\">documentation</a>, it is stated that:</p>\n<blockquote>\n<p>If random_state is None or np.random, then a randomly-initialized RandomState object is returned.</p>\n<p>If random_state is an integer, then it is used to seed a new RandomState object.</p>\n<p>If random_state is a RandomState object, then it is passed through.</p>\n</blockquote>\n<p>This is to check and validate the data when running the code multiple times. Setting <code>random_state</code> a fixed value will guarantee that same sequence of random numbers are generated each time you run the code. And unless there is some other randomness present in the process, the results produced will be same as always. This helps in verifying the output.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>when random_state set to an <strong>integer</strong>, train_test_split will return <strong>same</strong> results for each execution.</p>\n<p>when random_state set to an <strong>None</strong>, train_test_split will return <strong>different</strong> results for each execution.</p>\n<p>see below example:</p>\n<pre><code>from sklearn.model_selection import train_test_split\n\nX_data = range(10)\ny_data = range(10)\n\nfor i in range(5):\n    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3,random_state = 0) # zero or any other integer\n    print(y_test)\n\nprint(\"*\"*30)\n\nfor i in range(5): \n    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3,random_state = None)\n    print(y_test)\n</code></pre>\n<p><strong>Output</strong>:</p>\n<p>[2, 8, 4]</p>\n<p>[2, 8, 4]</p>\n<p>[2, 8, 4]</p>\n<p>[2, 8, 4]</p>\n<p>[2, 8, 4]</p>\n<hr/>\n<p>[4, 7, 6]</p>\n<p>[4, 3, 7]</p>\n<p>[8, 1, 4]</p>\n<p>[9, 5, 8]</p>\n<p>[6, 4, 5]</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you don't mention the random_state in the code, then whenever you execute your code a new random value is generated and the train and test datasets would have different values each time.</p>\n<p>However, if you use a particular value for random_state(random_state = 1 or any other value) everytime the result will be same,i.e, same values in train and test datasets.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2016-02-29 10:53:41Z\">8 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/7551262/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Where can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.</p>\n<p>I find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://www.cs.cornell.edu/home/llee/data/\" rel=\"noreferrer\">http://www.cs.cornell.edu/home/llee/data/</a></p>\n<p><a href=\"http://mpqa.cs.pitt.edu/corpora/mpqa_corpus\" rel=\"noreferrer\">http://mpqa.cs.pitt.edu/corpora/mpqa_corpus</a></p>\n<p>You can use twitter, with its smileys, like this: <a href=\"http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf\" rel=\"noreferrer\">http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf</a></p>\n<p>Hope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.</p>\n<p>To get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a list I wrote a few weeks ago, from <a href=\"https://www.keenformatics.com/sentiment-analysis-lexicons-and-datasets/\" rel=\"nofollow noreferrer\">my blog</a>. Some of these datasets have been recently included in the NLTK Python platform.</p>\n<h1>Lexicons</h1>\n<ul>\n<li><p><em><strong>Opinion Lexicon by Bing Liu</strong></em></p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/FBS/sentiment-analysis.html#lexicon\" rel=\"nofollow noreferrer\">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon</a></li>\n<li><strong>PAPERS</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/publications/kdd04-revSummary.pdf\" rel=\"nofollow noreferrer\">Mining and summarizing customer reviews</a></li>\n<li><strong>NOTES</strong>: Included in the NLTK Python platform</li>\n</ul>\n</li>\n<li><p><em><strong>MPQA Subjectivity Lexicon</strong></em></p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://mpqa.cs.pitt.edu/#subj_lexicon\" rel=\"nofollow noreferrer\">http://mpqa.cs.pitt.edu/#subj_lexicon</a></li>\n<li><strong>PAPERS</strong>: <a href=\"http://people.cs.pitt.edu/%7Ewiebe/pubs/papers/emnlp05polarity.pdf\" rel=\"nofollow noreferrer\">Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis (Theresa Wilson, Janyce Wiebe, and Paul Hoffmann, 2005)</a>.</li>\n</ul>\n</li>\n<li><p><em><strong>SentiWordNet</strong></em></p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://sentiwordnet.isti.cnr.it\" rel=\"nofollow noreferrer\">http://sentiwordnet.isti.cnr.it</a></li>\n<li><strong>NOTES</strong>: Included in the NLTK Python platform</li>\n</ul>\n</li>\n<li><p><em><strong>Harvard General Inquirer</strong></em></p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.wjh.harvard.edu/%7Einquirer\" rel=\"nofollow noreferrer\">http://www.wjh.harvard.edu/~inquirer</a></li>\n<li><strong>PAPERS</strong>: <a href=\"http://psycnet.apa.org/psycinfo/1967-04539-000\" rel=\"nofollow noreferrer\">The General Inquirer: A Computer Approach to Content Analysis (Stone, Philip J; Dexter C. Dunphry; Marshall S. Smith; and Daniel M. Ogilvie. 1966)</a></li>\n</ul>\n</li>\n<li><p><em><strong>Linguistic Inquiry and Word Counts (LIWC)</strong></em></p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.liwc.net\" rel=\"nofollow noreferrer\">http://www.liwc.net</a></li>\n</ul>\n</li>\n<li><p><em><strong>Vader Lexicon</strong></em></p>\n<ul>\n<li><strong>URLs</strong>: <a href=\"https://github.com/cjhutto/vaderSentiment\" rel=\"nofollow noreferrer\">https://github.com/cjhutto/vaderSentiment</a>, <a href=\"http://comp.social.gatech.edu/papers\" rel=\"nofollow noreferrer\">http://comp.social.gatech.edu/papers</a></li>\n<li><strong>PAPERS</strong>: <a href=\"http://psycnet.apa.org/psycinfo/1967-04539-000\" rel=\"nofollow noreferrer\">Vader: A parsimonious rule-based model for sentiment analysis of social media text (Hutto, Gilbert.  2014)</a></li>\n</ul>\n</li>\n</ul>\n<hr/>\n<h1>Datasets</h1>\n<ul>\n<li><p><em><strong>MPQA Datasets</strong></em></p>\n<ul>\n<li><p><strong>URL</strong>: <a href=\"http://mpqa.cs.pitt.edu\" rel=\"nofollow noreferrer\">http://mpqa.cs.pitt.edu</a></p>\n</li>\n<li><p><strong>NOTES</strong>: GNU Public License.</p>\n<ul>\n<li>Political Debate data</li>\n<li>Product Debate data</li>\n<li>Subjectivity Sense Annotations</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><em><strong>Sentiment140</strong></em> (Tweets)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://help.sentiment140.com/for-students\" rel=\"nofollow noreferrer\">http://help.sentiment140.com/for-students</a></li>\n<li><strong>PAPERS</strong>: <a href=\"http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\" rel=\"nofollow noreferrer\">Twitter Sent classification using Distant Supervision (Go, Alec, Richa Bhayani, and Lei Huang)</a></li>\n<li><strong>URLs</strong>: <a href=\"http://help.sentiment140.com\" rel=\"nofollow noreferrer\">http://help.sentiment140.com</a>, <a href=\"https://groups.google.com/forum/#!forum/sentiment140\" rel=\"nofollow noreferrer\">https://groups.google.com/forum/#!forum/sentiment140</a></li>\n</ul>\n</li>\n<li><p><em><strong>STS-Gold</strong></em> (Tweets)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.tweenator.com/index.php?page_id=13\" rel=\"nofollow noreferrer\">http://www.tweenator.com/index.php?page_id=13</a></li>\n<li><strong>PAPERS</strong>: <a href=\"http://ceur-ws.org/Vol-1096/paper1.pdf\" rel=\"nofollow noreferrer\">Evaluation datasets for twitter sentiment analysis (Saif, Fernandez, He, Alani)</a></li>\n<li><strong>NOTES</strong>: As Sentiment140, but the dataset is smaller and with human annotators. It comes with 3 files: tweets, entities (with their sentiment) and an aggregate set.</li>\n</ul>\n</li>\n<li><p><em><strong>Customer Review Dataset</strong></em> (Product reviews)</p>\n<ul>\n<li><p><strong>URL</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/FBS/sentiment-analysis.html#datasets\" rel=\"nofollow noreferrer\">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets</a></p>\n</li>\n<li><p><strong>PAPERS</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/publications/kdd04-revSummary.pdf\" rel=\"nofollow noreferrer\">Mining and summarizing customer reviews</a></p>\n</li>\n<li><p><strong>NOTES</strong>: Title of review, product feature, positive/negative label with opinion strength, other info (comparisons, pronoun resolution, etc.)</p>\n<p>Included in the NLTK Python platform</p>\n</li>\n</ul>\n</li>\n<li><p><em><strong>Pros and Cons Dataset</strong></em> (Pros and cons sentences)</p>\n<ul>\n<li><p><strong>URL</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/FBS/sentiment-analysis.html#datasets\" rel=\"nofollow noreferrer\">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets</a></p>\n</li>\n<li><p><strong>PAPERS</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/FBS/Coling-2008-camera-ready.pdf\" rel=\"nofollow noreferrer\">Mining Opinions in Comparative Sentences (Ganapathibhotla, Liu 2008)</a></p>\n</li>\n<li><p><strong>NOTES</strong>: A list of sentences tagged <code>&lt;pros&gt;</code> or <code>&lt;cons&gt;</code></p>\n<p>Included in the NLTK Python platform</p>\n</li>\n</ul>\n</li>\n<li><p><em><strong>Comparative Sentences</strong></em> (Reviews)</p>\n<ul>\n<li><p><strong>URL</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/FBS/sentiment-analysis.html#datasets\" rel=\"nofollow noreferrer\">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets</a></p>\n</li>\n<li><p><strong>PAPERS</strong>: <a href=\"http://www.cs.uic.edu/%7Eliub/publications/sigir06-comp.pdf\" rel=\"nofollow noreferrer\">Identifying Comparative Sentences in Text Documents (Nitin Jindal and Bing Liu)</a>, <a href=\"http://www.cs.uic.edu/%7Eliub/publications/aaai04-featureExtract.pdf\" rel=\"nofollow noreferrer\">Mining Opinion Features in Customer Reviews (Minqing Hu and Bing Liu)</a></p>\n</li>\n<li><p><strong>NOTES</strong>: Sentence, POS-tagged sentence, entities, comparison type (non-equal, equative, superlative, non-gradable)</p>\n<p>Included in the NLTK Python platform</p>\n</li>\n</ul>\n</li>\n<li><p><em><strong>Sanders Analytics Twitter Sentiment Corpus</strong></em> (Tweets)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.sananalytics.com/lab/twitter-sentiment\" rel=\"nofollow noreferrer\">http://www.sananalytics.com/lab/twitter-sentiment</a></li>\n</ul>\n<blockquote>\n<p>5513 hand-classified tweets wrt 4 different topics. Because of Twitter’s ToS, a small Python script is included to download all of the tweets. The sentiment classifications themselves are provided free of charge and without restrictions. They may be used for commercial products. They may be redistributed. They may be modified.</p>\n</blockquote>\n</li>\n<li><p><em><strong>Spanish tweets</strong></em> (Tweets)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.daedalus.es/TASS2013/corpus.php\" rel=\"nofollow noreferrer\">http://www.daedalus.es/TASS2013/corpus.php</a></li>\n</ul>\n</li>\n<li><p><em><strong>SemEval 2014</strong></em> (Tweets)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://alt.qcri.org/semeval2014/task9\" rel=\"nofollow noreferrer\">http://alt.qcri.org/semeval2014/task9</a></li>\n</ul>\n<blockquote>\n<p>You MUST NOT re-distribute the tweets, the annotations or the corpus obtained (from the readme file)</p>\n</blockquote>\n</li>\n<li><p><em><strong>Various Datasets</strong></em> (Reviews)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"https://personalwebs.coloradocollege.edu/%7Emwhitehead/html/opinion_mining.html\" rel=\"nofollow noreferrer\">https://personalwebs.coloradocollege.edu/~mwhitehead/html/opinion_mining.html</a></li>\n<li><strong>PAPERS</strong>: <a href=\"https://personalwebs.coloradocollege.edu/%7Emwhitehead/files/cross_domain_training.pdf\" rel=\"nofollow noreferrer\">Building a General Purpose Cross-Domain Sentiment Mining Model (Whitehead and Yaeger)</a>, <a href=\"https://personalwebs.coloradocollege.edu/%7Emwhitehead/files/whitehead_yaeger_scss08b.pdf\" rel=\"nofollow noreferrer\">Sentiment Mining Using Ensemble Classification Models (Whitehead and Yaeger)</a></li>\n</ul>\n</li>\n<li><p><em><strong>Various Datasets #2</strong></em> (Reviews)</p>\n<ul>\n<li><strong>URL</strong>: <a href=\"http://www.text-analytics101.com/2011/07/user-review-datasets_20.html\" rel=\"nofollow noreferrer\">http://www.text-analytics101.com/2011/07/user-review-datasets_20.html</a></li>\n</ul>\n</li>\n</ul>\n<hr/>\n<p><strong>References:</strong></p>\n<ul>\n<li><a href=\"https://www.keenformatics.com/sentiment-analysis-lexicons-and-datasets/\" rel=\"nofollow noreferrer\">Keenformatics - Sentiment Analysis lexicons and datasets</a> (my blog)</li>\n<li>Personal experience</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here are a few more;</p>\n<p><a href=\"http://inclass.kaggle.com/c/si650winter11\" rel=\"noreferrer\">http://inclass.kaggle.com/c/si650winter11</a></p>\n<p><a href=\"http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\" rel=\"noreferrer\">http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm going through the ML Class on Coursera on Logistic Regression and also the Manning Book Machine Learning in Action. I'm trying to learn by implementing everything in Python. </p>\n<p>I'm not able to understand the difference between the cost function and the gradient. There are examples on the net where people compute the cost function and then there are places where they don't and just go with the gradient descent function <code>w :=w - (alpha) * (delta)w * f(w)</code>.</p>\n<p>What is the difference between the two if any?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Whenever you train a model with your data, you are actually producing some new values (predicted) for a specific feature. However, that specific feature already has some values which are real values in the dataset. We know the closer the predicted values to their corresponding real values, the better the model.</p>\n<p>Now, we are using cost function to measure how close the predicted values are to their corresponding real values. </p>\n<p>We also should consider that the weights of the trained model are responsible for accurately predicting  the new values. Imagine that our model is y = 0.9*X + 0.1, the predicted value is nothing but (0.9*X+0.1) for different Xs. \n[0.9 and 0.1 in the equation are just random values to understand.]</p>\n<p>So, by considering Y as real value corresponding to this x, the cost formula is coming to measure how close (0.9*X+0.1) is to Y. </p>\n<p>We are responsible for finding the better weight (0.9 and 0.1) for our model to come up with a lowest cost (or closer predicted values to real ones).</p>\n<p>Gradient descent is an optimization algorithm (we have some other optimization algorithms) and its responsibility is to find the minimum cost value in the process of trying the model with different weights or indeed, updating the weights.</p>\n<p>We first run our model with some initial weights and gradient descent updates our weights and find the cost of our model with those weights in thousands of iterations to find the minimum cost.</p>\n<p>One point is that gradient descent is not minimizing the weights, it is just updating them. This algorithm is looking for minimum cost.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A cost function is something you want to minimize. For example, your cost function might be the sum of squared errors over your training set.  Gradient descent is a method for finding the minimum of a function of multiple variables.  So you can use gradient descent to minimize your cost function.  If your cost is a function of K variables, then the gradient is the length-K vector that defines the direction in which the cost is increasing most rapidly.  So in gradient descent, you follow the negative of the gradient to the point where the cost is a minimum.  If someone is talking about gradient descent in a machine learning context, the cost function is probably implied (it is the function to which you are applying the gradient descent algorithm).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It's strange to think about it, but there is more than one measure for how \"accurately\" a line fits to data points. </p>\n<p>To access how accurately a line fits the data, we have a \"cost\" function which which can compare predicted vs. actual values and provide a \"penalty\" for how wrong it is.</p>\n<p>penalty = cost_funciton(predicted, actual)</p>\n<p>A naive cost function might just take the difference between the predicted and actual.</p>\n<p>More sophisticated functions will square the value, since we'd rather have many small errors than one large error.</p>\n<p>Additionally, each point has a different \"sensitivity\" to moving the line. Some points react very strongly to movement. Others react less strongly.</p>\n<p>Often, you can make a tradeoff, and move TOWARD a point that is sensitive, and AWAY from a point that is NOT sensitive. In that scenario , you get more than you give up.</p>\n<p>The \"gradient\" is a way of measuring how sensitive each point is to moving the line.</p>\n<p>This article does a good job of describing WHY there is more than one measure, and WHY some points are more sensitive than others:</p>\n<p><a href=\"https://towardsdatascience.com/wrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5?source=friends_link&amp;sk=7117e5de8c66bd4a4c2bb2a87a928773\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/wrapping-your-head-around-gradient-descent-with-pictures-3fbd810235f5?source=friends_link&amp;sk=7117e5de8c66bd4a4c2bb2a87a928773</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Today i'm trying to learn something about K-means. I Have understand the algorithm and i know how it works. Now i'm looking for the right k... I found the elbow criterion as a method to detect the right k but i do not understand how to use it with scikit learn?! In scikit learn i'm clustering things in this way</p>\n<pre><code>kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=10) \nkmeans.fit(data)\n</code></pre>\n<p>So should i do this several times for n_clusters = 1...n and watch at the Error rate to get the right k ? think this would be stupid and would take a lot of time?!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If the true label is not known in advance(as in your case), then <code>K-Means clustering</code> can be evaluated using either Elbow Criterion or Silhouette Coefficient.</p>\n<p><strong>Elbow Criterion Method:</strong></p>\n<p>The idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k (<code>num_clusters</code>, e.g k=1 to 10), and for each value of k, calculate sum of squared errors (SSE).</p>\n<p>After that, plot a line graph of the SSE for each value of k. If the line graph looks like an arm - a red circle in below line graph (like angle), the \"elbow\" on the arm is the value of optimal k (number of cluster).\nHere, we want to minimize SSE. SSE tends to decrease toward 0 as we increase k (and SSE is 0 when k is equal to the number of data points in the dataset, because then each data point is its own cluster, and there is no error between it and the center of its cluster).</p>\n<p>So the goal is to choose a <code>small value of k</code> that still has a low SSE, and the elbow usually represents where we start to have diminishing returns by increasing k.</p>\n<p>Let's consider iris datasets,</p>\n<pre><code>import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris['feature_names'])\n#print(X)\ndata = X[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)']]\n\nsse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(data)\n    data[\"clusters\"] = kmeans.labels_\n    #print(data[\"clusters\"])\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()\n</code></pre>\n<p>Plot for above code:\n<a href=\"https://i.sstatic.net/BHYK4.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/BHYK4.png\"/></a></p>\n<p>We can see in plot, 3 is the optimal number of clusters (encircled red) for iris dataset, which is indeed correct.</p>\n<br/>\n<br/>\n<p><strong>Silhouette Coefficient Method:</strong></p>\n<p>From <a href=\"http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\" rel=\"noreferrer\">sklearn documentation</a>,</p>\n<p>A higher Silhouette Coefficient score relates to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n`</p>\n<blockquote>\n<p>a: The mean distance between a sample and all other points in the same class.</p>\n</blockquote>\n<blockquote>\n<p>b: The mean distance between a sample and all other points in the next\nnearest cluster.</p>\n</blockquote>\n<p>The Silhouette Coefficient is for a single sample is then given as:</p>\n<p><img src=\"https://latex.codecogs.com/png.latex?s=%5Cfrac%7Bb-a%7D%7Bmax(a,b)%7D\" title=\"s=\\frac{b-a}{max(a,b)}\"/></p>\n<p>Now, to find the optimal value of <code>k</code> for <code>KMeans</code>, loop through 1..n for n_clusters in <code>KMeans</code> and calculate Silhouette Coefficient for each sample.</p>\n<p>A higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.</p>\n<pre><code>from sklearn.metrics import silhouette_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\n\nX = load_iris().data\ny = load_iris().target\n   \nfor n_cluster in range(2, 11):\n    kmeans = KMeans(n_clusters=n_cluster).fit(X)\n    label = kmeans.labels_\n    sil_coeff = silhouette_score(X, label, metric='euclidean')\n    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))\n</code></pre>\n<p><strong>Output -</strong></p>\n<p>For n_clusters=2, The Silhouette Coefficient is 0.680813620271<br/>\n<strong>For n_clusters=3, The Silhouette Coefficient is 0.552591944521</strong><br/>\nFor n_clusters=4, The Silhouette Coefficient is 0.496992849949 <br/>\nFor n_clusters=5, The Silhouette Coefficient is 0.488517550854 <br/>\nFor n_clusters=6, The Silhouette Coefficient is 0.370380309351<br/>\nFor n_clusters=7, The Silhouette Coefficient is 0.356303270516<br/>\nFor n_clusters=8, The Silhouette Coefficient is 0.365164535737<br/>\nFor n_clusters=9, The Silhouette Coefficient is 0.346583642095<br/>\nFor n_clusters=10, The Silhouette Coefficient is 0.328266088778<br/></p>\n<p>As we can see, <em>n_clusters=2</em> has highest Silhouette Coefficient. This means that 2 should be the optimal number of cluster, Right?</p>\n<p>But here's the catch.</p>\n<p>Iris dataset has 3 species of flower, which contradicts the 2 as an optimal number of cluster. So despite <em>n_clusters=2</em> having highest Silhouette Coefficient, We would consider <em>n_clusters=3</em> as optimal number of cluster due to -</p>\n<ol>\n<li>Iris dataset has 3 species. <strong>(Most Important)</strong></li>\n<li><em>n_clusters=3</em> has the 2nd highest value of Silhouette Coefficient.</li>\n</ol>\n<p>So choosing <em>n_clusters=3</em> is the optimal no. of cluster for iris dataset.</p>\n<p>Choosing optimal no. of the cluster will depend on the type of datasets and the problem we are trying to solve. But most of the cases, taking highest Silhouette Coefficient will yield an optimal number of cluster.</p>\n<p>Hope it helps!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The elbow criterion is a visual method. I have not yet seen a robust mathematical definition of it.\nBut k-means is a pretty crude heuristic, too.</p>\n<p>So yes, you will need to run k-means with <code>k=1...kmax</code>, then <em>plot</em> the resulting SSQ and decide upon an \"optimal\" k.</p>\n<p>There exist advanced versions of k-means such as X-means that will start with <code>k=2</code> and then increase it until a secondary criterion (AIC/BIC) no longer improves. Bisecting k-means is an approach that also starts with k=2 and then repeatedly splits clusters until k=kmax. You could probably extract the interim SSQs from it.</p>\n<p>Either way, I have the impression that in any <em>actual use case</em> where k-mean is really good, you do actually know the k you need beforehand. In these cases, k-means is actually not so much a \"clustering\" algorithm, but a <a href=\"https://en.wikipedia.org/wiki/Vector_quantization\">vector quantization</a> algorithm. E.g. reducing the number of colors of an image to k. (where often you would choose k to be e.g. 32, because that is then 5 bits color depth and can be stored in a bit compressed way). Or e.g. in bag-of-visual-words approaches, where you would choose the vocabulary size manually. A popular value seems to be k=1000. You then don't really care much about the quality of the \"clusters\", but the main point is to be able to reduce an image to a 1000 dimensional sparse vector.\nThe performance of a 900 dimensional or a 1100 dimensional representation will not be substantially different.</p>\n<p>For actual clustering tasks, i.e. when you want to <em>analyze the resulting clusters manually</em>, people usually use more advanced methods than k-means. K-means is more of a data simplification technique.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This answer is inspired by what OmPrakash has written. This contains code to plot both the SSE and Silhouette Score. What I've given is a general code snippet you can follow through in all cases of unsupervised learning where you don't have the labels and want to know what's the optimal number of cluster. There are 2 criterion. 1) Sum of Square errors (SSE) and Silhouette Score. You can follow OmPrakash's answer for the explanation. He's done a good job at that.</p>\n<p>Assume your dataset is a data frame df1. Here I have used a different dataset just to show how we can use both the criterion to help decide optimal number of cluster. Here I think 6 is the correct number of cluster.\nThen </p>\n<pre><code>range_n_clusters = [2, 3, 4, 5, 6,7,8]\nelbow = []\nss = []\nfor n_clusters in range_n_clusters:\n   #iterating through cluster sizes\n   clusterer = KMeans(n_clusters = n_clusters, random_state=42)\n   cluster_labels = clusterer.fit_predict(df1)\n   #Finding the average silhouette score\n   silhouette_avg = silhouette_score(df1, cluster_labels)\n   ss.append(silhouette_avg)\n   print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)`\n   #Finding the average SSE\"\n   elbow.append(clusterer.inertia_) # Inertia: Sum of distances of samples to their closest cluster center\nfig = plt.figure(figsize=(14,7))\nfig.add_subplot(121)\nplt.plot(range_n_clusters, elbow,'b-',label='Sum of squared error')\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.legend()\nfig.add_subplot(122)\nplt.plot(range_n_clusters, ss,'b-',label='Silhouette Score')\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"Silhouette Score\")\nplt.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/BltNN.png\" rel=\"noreferrer\"><img alt=\"Graphs used to compare both the criterion in order to help us find optimal cluster values\" src=\"https://i.sstatic.net/BltNN.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While I'm reading in how to build ANN in <a href=\"http://pybrain.org/docs/tutorial/fnn.html\" rel=\"nofollow noreferrer\">pybrain</a>, they say:</p>\n<blockquote>\n<p>Train the network for some epochs. Usually you would set something\nlike 5 here,</p>\n<pre><code>trainer.trainEpochs( 1 )\n</code></pre>\n</blockquote>\n<p>I looked for what is that mean , then I conclude that we use an epoch of data to update weights, If I choose to train the data with 5 epochs as pybrain advice, the dataset will be divided into 5 subsets, and the wights will update 5 times as maximum.</p>\n<p>I'm familiar with online training where the wights are updated after each sample data or feature vector, My question is how to be sure that 5 epochs will be enough to build a model and setting the weights probably?  what is the advantage of this way on online training? Also the term \"epoch\" is used on online training, does it mean one feature vector?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One epoch consists of <em>one</em> full training cycle on the training set. Once every sample in the set is seen, you start again - marking the beginning of the 2nd epoch. </p>\n<p>This has nothing to do with batch or online training per se. Batch means that you update <em>once</em> at the end of the epoch (after <strong>every</strong> sample is seen, i.e. #epoch updates) and online that you update after <strong>each</strong> <em>sample</em> (#samples * #epoch updates).</p>\n<p>You can't be sure if 5 epochs or 500 is enough for convergence since it will vary from data to data. You can stop training when the error converges or gets lower than a certain threshold. This also goes into the territory of preventing overfitting. You can read up on <a href=\"https://en.wikipedia.org/wiki/Early_stopping#Early_stopping_based_on_cross-validation\">early stopping</a> and <a href=\"http://artint.info/html/ArtInt_189.html\">cross-validation</a> regarding that.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>sorry for reactivating this thread.\nim new to neural nets and im investigating the impact of 'mini-batch' training.</p>\n<p>so far, as i understand it, an epoch (<a href=\"https://stackoverflow.com/a/31157729/3798217\">as runDOSrun is saying</a>) is a through use of all in the TrainingSet (not DataSet. because DataSet = TrainingSet + ValidationSet). in mini batch training, you can sub divide the TrainingSet into small Sets and update weights inside an epoch. 'hopefully' this would make the network 'converge' faster.</p>\n<p>some definitions of neural networks are outdated and, i guess, must be redefined.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a Java app which needs to perform partial least squares regression. It would appear there are no Java implementations of PLSR out there. Weka might have had something like it at some point, but it is no longer in the API. On the other hand, I have found a good R implementation, which has an added bonus to it. It was used by the people whose result I want to replicate, which means there is less chance that things will go wrong because of differences in the way PLSR is implemented.</p>\n<p>The question is: is there a good enough (and simple to use) package that enable Java to call R, pass in some parameters to a function and read back the results? My other option is to have Java spawn R in a Process and then monitor it. Data would be read and written to disk. Which of the two would you recommend? Am I missing the obvious third option?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have successfully used two alternatives in the past.</p>\n<p><strong><a href=\"http://www.rforge.net/JRI/\" rel=\"noreferrer\">JRI</a></strong></p>\n<ul>\n<li><strong>Pros</strong>: probably better performance.</li>\n<li><strong>Cons</strong>: you have to configure some environment variables and libraries, different in Win/UNIX.</li>\n</ul>\n<p><strong><a href=\"http://www.rforge.net/Rserve/\" rel=\"noreferrer\">RServe</a></strong></p>\n<ul>\n<li><strong>Pros</strong>: easy to setup, you don't need to initialize R or link against\nany R library, can run in a different machine.</li>\n<li><strong>Cons</strong>: based on TCP/IP (a server is running), no callbacks from R.</li>\n</ul>\n<p><strong>Other alternatives</strong> I have never used : <a href=\"http://code.google.com/p/rcaller/\" rel=\"noreferrer\">RCaller</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There has been work by Duncan Temple Lang: <a href=\"http://rss.acs.unt.edu/Rdoc/library/SJava/Docs/RFromJava.pdf\">http://rss.acs.unt.edu/Rdoc/library/SJava/Docs/RFromJava.pdf</a> . </p>\n<p>My guess as to the most robust solution would be <a href=\"http://www.rforge.net/JGR/\">JGR</a>. The developers of JGR have a mailing list, <a href=\"http://mailman.rz.uni-augsburg.de/mailman/listinfo/stats-rosuda-devel\">Stats-Rosuda</a> and the <a href=\"http://mailman.rz.uni-augsburg.de/pipermail/stats-rosuda-devel/\">mailing list Archive</a> indicates the list remains active as of 2013.</p>\n<p><a href=\"http://code.google.com/p/rcaller/downloads/list\">There is also code that has been put up at Googlecode</a>, with an example here:\n<a href=\"http://stdioe.blogspot.com/2011/07/rcaller-20-calling-r-from-java.html\">http://stdioe.blogspot.com/2011/07/rcaller-20-calling-r-from-java.html</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is an old question.. but for anyone browsing through here that is still interested: I wrote a blog article that provides a detailed example of how to use JRI/rjava (a JNI based bridge) to do this type of thing (the how-to is focused on Linux dev environments).  I also compare and contrast alternative approaches for doing 'mathy' stuff by calling out to R and similar frameworks.</p>\n<p>URL &gt; <a href=\"http://buildlackey.com/integrating-r-and-java-with-jrirjava-a-jni-based-bridge/\">http://buildlackey.com/integrating-r-and-java-with-jrirjava-a-jni-based-bridge/</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to learn <code>scikit-learn</code> and Machine Learning by using the Boston Housing Data Set.</p>\n<pre><code># I splitted the initial dataset ('housing_X' and 'housing_y')\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(housing_X, housing_y, test_size=0.25, random_state=33)\n\n# I scaled those two datasets\nfrom sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\n\n# I created the model\nfrom sklearn import linear_model\nclf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None, random_state=42) \ntrain_and_evaluate(clf_sgd,X_train,y_train)\n</code></pre>\n<p>Based on this new model <code>clf_sgd</code>, I am trying to predict the <code>y</code> based on the first instance of <code>X_train</code>.</p>\n<pre><code>X_new_scaled = X_train[0]\nprint (X_new_scaled)\ny_new = clf_sgd.predict(X_new_scaled)\nprint (y_new)\n</code></pre>\n<p>However, the result is quite odd for me (<code>1.34032174</code>, instead of <code>20-30</code>, the range of the price of the houses)</p>\n<pre><code>[-0.32076092  0.35553428 -1.00966618 -0.28784917  0.87716097  1.28834383\n  0.4759489  -0.83034371 -0.47659648 -0.81061061 -2.49222645  0.35062335\n -0.39859013]\n[ 1.34032174]\n</code></pre>\n<p>I guess that this <code>1.34032174</code> value should be scaled back, but I am trying to figure out how to do it with no success. Any tip is welcome. Thank you very much.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <code>inverse_transform</code> using your <code>scalery</code> object:</p>\n<pre><code>y_new_inverse = scalery.inverse_transform(y_new)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Bit late to the game: \nJust don't scale your y. With scaling y you actually loose your units. The regression or loss optimization is actually determined by the relative differences between the features. BTW for house prices (or any other monetary value) it is common practice to take the logarithm. Then you obviously need to do an numpy.exp() to get back to the actual dollars/euros/yens...</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am working on a simple cnn classifier using keras with tensorflow background.</p>\n<pre><code>def cnnKeras(training_data, training_labels, test_data, test_labels, n_dim):\n  print(\"Initiating CNN\")\n  seed = 8\n  numpy.random.seed(seed)\n  model = Sequential()\n  model.add(Convolution2D(64, 1, 1, init='glorot_uniform', \n   border_mode='valid',input_shape=(16, 1, 1), activation='relu'))\n  model.add(MaxPooling2D(pool_size=(1, 1)))\n  model.add(Convolution2D(32, 1, 1, init='glorot_uniform', \n   activation='relu'))\n  model.add(MaxPooling2D(pool_size=(1, 1)))\n  model.add(Dropout(0.25))\n  model.add(Flatten())\n  model.add(Dense(128, activation='relu'))\n  model.add(Dropout(0.5))\n  model.add(Dense(64, activation='relu'))\n  model.add(Dense(1, activation='softmax'))\n  # Compile model\n  model.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n  model.fit(training_data, training_labels, validation_data=(\n    test_data, test_labels), nb_epoch=30, batch_size=8, verbose=2)\n\n  scores = model.evaluate(test_data, test_labels, verbose=1)\n  print(\"Baseline Error: %.2f%%\" % (100 - scores[1] * 100))\n  # model.save('trained_CNN.h5')\n  return None\n</code></pre>\n<p>It is a binary classification problem, but I keep getting the message <code>Received a label value of 1 which is outside the valid range of [0, 1)</code> which does not make any sense to me. Any suggesstions?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>Range [0, 1)</code> means every number between 0 and 1, <strong>excluding</strong> 1. So 1 is not a value in the range [0, 1).</p>\n<p>I am not 100% sure, but the issue could be due to your choice of loss function. For a binary classification, <code>binary_crossentropy</code> should be a better choice.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the last Dense layer you used <code>model.add(Dense(1, activation='softmax'))</code>. Here 1 restricts its value from <code>[0, 1)</code> change its shape to the maximum output label. For eg your output is from label <code>[0,7)</code> then use <code>model.add(Dense(7, activation='softmax'))</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Peculiarities of sparse categorical crossentropy</h2>\n<p>The loss function sparse_categorical_crossentropy interprets the final layer in the context of classifiers as a set of probabilities for each possible class, and the output value as the <strong>number of the class</strong>. (The <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\" rel=\"noreferrer\">Tensorflow/Keras documentation</a> goes into a bit more detail.) So x neurons in output layer are compared against output values in the range from 0 to x-1; and having just one neuron in the output layer is an 'unary' classifier that doesn't make sense.</p>\n<p>If it's a classification task where you want to have output data in the form from 0 to x-1, then you can keep sparse categorical crossentropy, but you need to set the number of neurons in the output layer to the number of classes you have. Alternatively, you might encode the output in a one-hot vector and use categorical crossentropy loss function instead of sparse categorical crossentropy.</p>\n<p>If it's <em>not</em> a classification task and you want to predict arbitrary real-valued  numbers as in a regression, then categorical crossentropy is not a suitable loss function at all.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I often use <code>fminunc</code> for a logistic regression problem.  I have read on web that <a href=\"http://en.wikipedia.org/wiki/Andrew_Ng\">Andrew Ng</a> uses <code>fmincg</code> instead of <code>fminunc</code>, with same arguments.  The results are different, and often <code>fmincg</code> is more exact, but not too much. (I am comparing the results of fmincg function fminunc against the same data)</p>\n<p>So, my question is : what is the difference between these two functions?  What algorithm does each function have implemented?  (Now, I just use these functions without knowing exactly how they work).</p>\n<p>Thanks :)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You will have to look inside the code of <code>fmincg</code> because it is not part of Octave. After some search I found that it's a function file provided by the Machine Learning class of Coursera as part of the homework. Read the comments and answers on <a href=\"https://stackoverflow.com/questions/10770934/matlabs-fminsearch-different-from-octaves-fmincg\">this question</a> for a discussion about the algorithms.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In contrast to other answers here suggesting the primary difference between fmincg and fminunc is accuracy or speed, perhaps the most important difference for some applications is memory efficiency. In programming exercise 4 (i.e., Neural Network Training) of Andrew Ng's Machine Learning class at Coursera, the comment in ex4.m about fmincg is</p>\n<blockquote>\n<p>%% =================== Part 8: Training NN ===================<br/>\n  %  You have now implemented all the code necessary to train a neural<br/>\n  %  network. To train your neural network, we will now use \"fmincg\", which<br/>\n  %  is a function which works similarly to \"fminunc\". Recall that these<br/>\n  %  advanced optimizers are able to train our cost functions efficiently as<br/>\n  %  long as we provide them with the gradient computations.  </p>\n</blockquote>\n<p>Like the original poster, I was also curious about how the results of ex4.m might differ using fminunc instead of fmincg. So I tried to replace the fmincg call </p>\n<pre><code>options = optimset('MaxIter', 50);\n[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n</code></pre>\n<p>with the following call to fminunc</p>\n<pre><code>options = optimset('GradObj', 'on', 'MaxIter', 50);\n[nn_params, cost, exit_flag] = fminunc(costFunction, initial_nn_params, options);\n</code></pre>\n<p>but got the following error message from a 32-bit build of Octave running on Windows:</p>\n<blockquote>\n<p>error: memory exhausted or requested size too large for range of Octave's index\n  type -- trying to return to prompt</p>\n</blockquote>\n<p>A 32-bit build of MATLAB running on Windows provides a more detailed error message:</p>\n<blockquote>\n<p>Error using find<br/>\n  Out of memory. Type HELP MEMORY for your options.<br/>\n  Error in spones (line 14)<br/>\n  [i,j] = find(S);<br/>\n  Error in color (line 26)<br/>\n  J = spones(J);<br/>\n  Error in sfminbx (line 155)<br/>\n      group = color(Hstr,p);<br/>\n  Error in fminunc (line 408)<br/>\n     [x,FVAL,~,EXITFLAG,OUTPUT,GRAD,HESSIAN] = sfminbx(funfcn,x,l,u, ...<br/>\n  Error in ex4 (line 205)<br/>\n  [nn_params, cost, exit_flag] = fminunc(costFunction, initial_nn_params, options);</p>\n</blockquote>\n<p>The MATLAB memory command on my laptop computer reports:</p>\n<blockquote>\n<p>Maximum possible array:            2046 MB (2.146e+09 bytes) *<br/>\n  Memory available for all arrays:   3402 MB (3.568e+09 bytes) **<br/>\n  Memory used by MATLAB:              373 MB (3.910e+08 bytes)<br/>\n  Physical Memory (RAM):             3561 MB (3.734e+09 bytes)<br/>\n  *  Limited by contiguous virtual address space available.<br/>\n  ** Limited by virtual address space available.  </p>\n</blockquote>\n<p>I previously was thinking that Professor Ng chose to use fmincg to train the ex4.m neural network (that has 400 input features, 401 including the bias input) to increase the training speed. However, now I believe his reason for using fmincg was to increase the memory efficiency enough to allow the training to be performed on 32-bit builds of Octave/MATLAB. A short discussion about the necessary work to get a 64-bit build of Octave that runs on Windows OS is <a href=\"http://octave.1599824.n4.nabble.com/error-memory-exhausted-or-requested-size-too-large-for-range-of-Octave-s-index-type-trying-to-returnt-td4646032.html\">here.</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>According to Andrew Ng himself, <code>fmincg</code> is used not to get a more accurate result (remember, your cost function will be same in either case, and your hypothesis no simpler or more complex) but because it is more efficient at doing gradient descent for especially complex hypotheses.  He himself seems to use <code>fminunc</code> where the hypothesis have few features, but <code>fmincg</code> where it has hundreds.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I came across <a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\" rel=\"noreferrer\">this</a> PyTorch tutorial (in neural_networks_tutorial.py) where they construct a simple neural network and run an inference. I would like to print the contents of the entire input tensor for debugging purposes. What I get when I try to print the tensor is something like this and not the entire tensor:</p>\n<p><a href=\"https://i.sstatic.net/zsLP1.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zsLP1.png\"/></a></p>\n<p>I saw a similar <a href=\"https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array\">link</a> for numpy but was not sure about what would work for PyTorch. I can convert it to numpy and may be view it, but would like to avoid the extra overhead. Is there a way for me to print the entire tensor?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To avoid truncation and to control how much of the tensor data is printed use the same API as numpy's <code>numpy.set_printoptions(threshold=10_000)</code>.</p>\n<p>Example:</p>\n<pre><code>x = torch.rand(1000, 2, 2)\nprint(x) # prints the truncated tensor\ntorch.set_printoptions(threshold=10_000)\nprint(x) # prints the whole tensor\n</code></pre>\n<p>If your tensor is very large, adjust the <code>threshold</code> value to a higher number.</p>\n<p>Another option is:</p>\n<pre><code>torch.set_printoptions(profile=\"full\")\nprint(x) # prints the whole tensor\ntorch.set_printoptions(profile=\"default\") # reset\nprint(x) # prints the truncated tensor\n</code></pre>\n<p>All the available <code>set_printoptions</code> arguments are documented <a href=\"https://pytorch.org/docs/stable/generated/torch.set_printoptions.html#torch.set_printoptions\" rel=\"nofollow noreferrer\">here</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Though I don't suggest to do that, if you want, then</p>\n<pre><code>In [18]: torch.set_printoptions(edgeitems=1)\n\nIn [19]: a\nOut[19]:\ntensor([[-0.7698,  ..., -0.1949],\n        ...,\n        [-0.7321,  ...,  0.8537]])\n\nIn [20]: torch.set_printoptions(edgeitems=3)\n\nIn [21]: a\nOut[21]:\ntensor([[-0.7698,  1.3383,  0.5649,  ...,  1.3567,  0.6896, -0.1949],\n        [-0.5761, -0.9789, -0.2058,  ..., -0.5843,  2.6311, -0.0008],\n        [ 1.3152,  1.8851, -0.9761,  ...,  0.8639, -0.6237,  0.5646],\n        ...,\n        [ 0.2851,  0.5504, -0.9471,  ...,  0.0688, -0.7777,  0.1661],\n        [ 2.9616, -0.8685, -1.5467,  ..., -1.4646,  1.1098, -1.0873],\n        [-0.7321,  0.7610,  0.3182,  ...,  2.5859, -0.9709,  0.8537]])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I came here actually looking for answers how to print the entire row of a tensor in one line of the console so I thought I'd add this.</p>\n<pre><code>tensor([[1.1573e+04, 6.0693e+02, 1.2436e+03, 2.7277e+04, 1.6673e+08, 2.0462e+00, 9.8891e-01],\n    [2.0237e+04, 5.9074e+02, 1.7208e+03, 2.7449e+04, 2.1301e+08, 2.0678e+00, 1.0011e+00],\n    [2.7456e+04, 6.1106e+02, 1.4897e+03, 2.7332e+04, 1.7310e+08, 2.0448e+00, 9.6041e-01],\n    [1.7732e+04, 6.0232e+02, 1.2608e+03, 2.7371e+04, 1.8106e+08, 1.9594e+00, 1.0040e+00],\n    ...,\n    [1.1167e+04, 5.9867e+02, 1.3440e+03, 2.7263e+04, 2.3160e+08, 2.0190e+00, 1.0075e+00],\n    [1.6003e+04, 5.9590e+02, 1.2319e+03, 2.7368e+04, 1.7155e+08, 2.0171e+00, 1.0202e+00],\n    [1.5499e+04, 6.1471e+02, 9.4877e+02, 2.7395e+04, 1.8146e+08, 1.9016e+00, 9.5884e-01],\n    [3.3886e+04, 6.0689e+02, 1.0777e+03, 2.7259e+04, 2.1599e+08, 2.0179e+00, 1.0201e+00]], dtype=torch.float64)\n</code></pre>\n<p>I did this using</p>\n<pre><code>torch.set_printoptions(linewidth=200)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In every book and example always they show only binary classification (two classes) and new vector can belong to any one class.</p>\n<p>Here the problem is I have 4 classes(c1, c2, c3, c4). I've training data for 4 classes.</p>\n<p>For new vector the output should be like</p>\n<blockquote>\n<p><strong>C1 80%</strong> (the winner)</p>\n<p>c2 10%</p>\n<p>c3 6%</p>\n<p>c4 4%</p>\n</blockquote>\n<p>How to do this? I'm planning to use libsvm (because it most popular). I don't know much about it. If any of you guys used it previously please tell me specific commands I'm supposed to use.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>LibSVM uses the <em>one-against-one</em> approach for multi-class learning problems. From the <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f419\" rel=\"noreferrer\">FAQ</a>:</p>\n<blockquote>\n<p>Q: What method does libsvm use for multi-class SVM ? Why don't you use the \"1-against-the rest\" method ? </p>\n<p>It is one-against-one. We chose it after doing the following comparison: C.-W. Hsu and C.-J. Lin. <a href=\"http://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf\" rel=\"noreferrer\">A comparison of methods for multi-class support vector machines</a>, IEEE Transactions on Neural Networks, 13(2002), 415-425.</p>\n<p>\"1-against-the rest\" is a good method whose performance is comparable to \"1-against-1.\" We do the latter simply because its training time is shorter.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Commonly used methods are One vs. Rest and One vs. One.\nIn the first method you get n classifiers and the resulting class will have the highest score.\nIn the second method the resulting class is obtained by majority votes of all classifiers.</p>\n<p>AFAIR, libsvm supports both strategies of multiclass classification.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can always reduce a multi-class classification problem to a binary problem by choosing random partititions of the set of classes, recursively. This is not necessarily any less effective or efficient than learning all at once, since the sub-learning problems require less examples since the partitioning problem is smaller. (It may require at most a constant order time more, e.g. twice as long). It may also lead to more accurate learning.</p>\n<p>I'm not necessarily recommending this, but it is one answer to your question, and is a general technique that can be applied to any binary learning algorithm.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To save a model in Keras, what are the differences between the output files of:</p>\n<ol>\n<li><code>model.save()</code> </li>\n<li><code>model.save_weights()</code></li>\n<li><code>ModelCheckpoint()</code> in the callback</li>\n</ol>\n<p>The saved file from <code>model.save()</code> is larger than the model from <code>model.save_weights()</code>, but significantly larger than a JSON or Yaml model architecture file.  Why is this?  </p>\n<p>Restating this: Why is size(model.save()) + size(something) = size(model.save_weights()) + size(model.to_json()), what is that \"something\"?</p>\n<p>Would it be more efficient to just <code>model.save_weights()</code> and <code>model.to_json()</code>, and load from these than to just do <code>model.save()</code> and <code>load_model()</code>?  </p>\n<p>What are the differences?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>save()</code> saves the weights and the model structure to a single <code>HDF5</code> file. I believe it also includes things like the optimizer state. Then you can use that HDF5 file with <code>load()</code> to reconstruct the whole model, including weights.</p>\n<p><code>save_weights()</code> only saves the weights to HDF5 and nothing else. You need extra code to reconstruct the model from a <code>JSON</code> file.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ul>\n<li><code>model.save_weights()</code>: Will only save the weights so if you need, you are able to apply them on a different architecture</li>\n<li><code>mode.save()</code>: Will save the architecture of the model + the the weights + the training configuration + the state of the optimizer</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just to add what ModelCheckPoint's output is, if it's relevant for anyone else: used as a callback during model training, it can either save the whole model or just the weights depending on what state the <code>save_weights_only</code> argument is set to. TRUE and weights only are saved, akin to calling <code>model.save_weights()</code>. FALSE (default) and the whole model is saved, as in calling <code>model.save()</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use linear SVM from scikit learn (LinearSVC) for binary classification problem. I understand that LinearSVC can give me the predicted labels, and the decision scores but I wanted probability estimates (confidence in the label). I want to continue using LinearSVC because of speed (as compared to sklearn.svm.SVC with linear kernel) Is it reasonable to use a logistic function to convert the decision scores to probabilities? </p>\n<pre><code>import sklearn.svm as suppmach\n# Fit model:\nsvmmodel=suppmach.LinearSVC(penalty='l1',C=1)\npredicted_test= svmmodel.predict(x_test)\npredicted_test_scores= svmmodel.decision_function(x_test) \n</code></pre>\n<p>I want to check if it makes sense to obtain Probability estimates simply as [1 / (1 + exp(-x)) ]  where x is the decision score. </p>\n<p>Alternately, are there other options wrt classifiers that I can use to do this efficiently? </p>\n<p>Thanks. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>scikit-learn provides <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV\" rel=\"nofollow noreferrer\">CalibratedClassifierCV</a> which can be used to solve this problem: it allows to add probability output to LinearSVC or any other classifier which implements decision_function method:</p>\n<pre><code>svm = LinearSVC()\nclf = CalibratedClassifierCV(svm) \nclf.fit(X_train, y_train)\ny_proba = clf.predict_proba(X_test)\n</code></pre>\n<p>User guide has a nice <a href=\"http://scikit-learn.org/stable/modules/calibration.html\" rel=\"nofollow noreferrer\">section</a> on that. By default CalibratedClassifierCV+LinearSVC will get you Platt scaling, but it also provides other options (isotonic regression method), and it is not limited to SVM classifiers.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I took a look at the apis in sklearn.svm.* family. All below models, e.g.,</p>\n<ul>\n<li>sklearn.svm.SVC</li>\n<li>sklearn.svm.NuSVC</li>\n<li>sklearn.svm.SVR</li>\n<li>sklearn.svm.NuSVR</li>\n</ul>\n<p>have a common <a href=\"http://scikit-learn.org/0.11/modules/generated/sklearn.svm.NuSVR.html\" rel=\"noreferrer\">interface</a> that supplies a </p>\n<pre><code>probability: boolean, optional (default=False) \n</code></pre>\n<p>parameter to the model. If this parameter is set to True, libsvm will train a probability transformation model on top of the SVM's outputs based on idea of <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639\" rel=\"noreferrer\">Platt Scaling</a>. The form of transformation is similar to a logistic function as you pointed out, however two specific constants <code>A</code> and <code>B</code> are learned in a post-processing step. Also see this <a href=\"https://stackoverflow.com/questions/15111408/how-does-sklearn-svm-svcs-function-predict-proba-work-internally\">stackoverflow</a> post for more details.</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/LcIQF.png\"/></p>\n<p>I actually don't know why this post-processing is not available for LinearSVC. Otherwise, you would just call <code>predict_proba(X)</code> to get the probability estimate. </p>\n<p>Of course, if you just apply a naive logistic transform, it will not perform as well as a calibrated approach like <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639\" rel=\"noreferrer\">Platt Scaling</a>. If you can understand the underline algorithm of platt scaling, probably you can write your own or contribute to the scikit-learn svm family. :) Also feel free to use the above four SVM variations that support <code>predict_proba</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you want speed, then just <em>replace</em> the SVM with <code>sklearn.linear_model.LogisticRegression</code>. That uses the exact same training algorithm as <code>LinearSVC</code>, but with log-loss instead of hinge loss.</p>\n<p>Using [1 / (1 + exp(-x))] will produce probabilities, in a formal sense (numbers between zero and one), but they won't adhere to any justifiable probability model.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can you please tell me the difference between Stochastic Gradient Descent(SGD) and Backpropagation?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Backpropagation is an efficient method of <strong>computing gradients</strong> in directed graphs of computations, such as neural networks. This is <strong>not</strong> a learning method, but rather a nice computational trick which is <strong>often used in learning methods</strong>. This is actually a simple implementation of <strong>chain rule</strong> of derivatives, which simply gives you the ability to compute all required partial derivatives in linear time in terms of the graph size (while naive gradient computations would scale exponentially with depth).</p>\n<p>SGD is one of many optimization methods, namely <strong>first order optimizer</strong>, meaning, that it is based on analysis of the <strong>gradient</strong> of the objective. Consequently, in terms of neural networks it is often applied together with backprop to make efficient updates. You could also apply SGD to gradients obtained in a different way (from sampling, numerical approximators etc.). Symmetrically you can use other optimization techniques with backprop as well, everything that can use gradient/jacobian.</p>\n<p>This common misconception comes from the fact, that for simplicity people sometimes say \"trained with backprop\", what actually means (if they do not specify optimizer) \"trained with SGD using backprop as a gradient computing technique\". Also, in old textbooks you can find things like \"delta rule\" and other a bit confusing terms, which describe exactly the same thing (as neural network community was for a long time a bit independent from general optimization community). </p>\n<p>Thus you have two layers of abstraction:</p>\n<ul>\n<li>gradient computation - where backprop comes to play</li>\n<li>optimization level - where techniques like SGD, Adam, Rprop, BFGS etc. come into play, which (if they are first order or higher) use gradient computed above</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Stochastic gradient descent</strong> (SGD) is an optimization method used e.g. to minimize a loss function. </p>\n<p>In the SGD, you use <em>1 example</em>, at each iteration, to update the weights of your model, depending on the error due to this example, instead of using the average of the errors of <em>all</em> examples (as in \"simple\" <em>gradient descent</em>), at each iteration. To do so, SGD needs to compute the \"gradient of your model\".</p>\n<p><strong>Backpropagation</strong> is an efficient technique to compute this \"gradient\" that SGD uses.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Back-propagation is just a method for calculating multi-variable derivatives of your model, whereas SGD is the method of locating the minimum of your loss/cost function.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using RandomForestClassifier implemented in python sklearn package to build a binary classification model. The below is the results of cross validations:</p>\n<pre><code>Fold 1 : Train: 164  Test: 40\nTrain Accuracy: 0.914634146341\nTest Accuracy: 0.55\n\nFold 2 : Train: 163  Test: 41\nTrain Accuracy: 0.871165644172\nTest Accuracy: 0.707317073171\n\nFold 3 : Train: 163  Test: 41\nTrain Accuracy: 0.889570552147\nTest Accuracy: 0.585365853659\n\nFold 4 : Train: 163  Test: 41\nTrain Accuracy: 0.871165644172\nTest Accuracy: 0.756097560976\n\nFold 5 : Train: 163  Test: 41\nTrain Accuracy: 0.883435582822\nTest Accuracy: 0.512195121951\n</code></pre>\n<p>I am using \"Price\" feature to predict \"quality\" which is a ordinal value. In each cross validation, there are 163 training examples and 41 test examples. </p>\n<p>Apparently, overfitting occurs here. So is there any parameters provided by sklearn can be used to overcome this problem? I found some parameters <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">here</a>, e.g. min_samples_split and min_sample_leaf, but I do not quite understand how to tune them.</p>\n<p>Thanks in advance!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would agree with @Falcon w.r.t. the dataset size. It's likely that the main problem is the small size of the dataset. If possible, the best thing you can do is get more data, the more data (generally) the less likely it is to overfit, as random patterns that appear predictive start to get drowned out as the dataset size increases.</p>\n<p>That said, I would look at the following params:</p>\n<ol>\n<li>n_estimators: @Falcon is wrong, in general the more trees the <strong>less likely</strong> the algorithm is to overfit. So try <strong>increasing this</strong>. The lower this number, the closer the model is to a decision tree, with a restricted feature set.</li>\n<li>max_features: try reducing this number (try 30-50% of the number of features). This determines how many features each tree is randomly assigned. The smaller, the less likely to overfit, but too small will start to introduce under fitting.</li>\n<li>max_depth: Experiment with this. This will reduce the complexity of the learned models, lowering over fitting risk. Try starting small, say 5-10, and increasing you get the best result.</li>\n<li>min_samples_leaf: Try setting this to values greater than one. This has a similar effect to the max_depth parameter, it means the branch will stop splitting once the leaves have that number of samples each.</li>\n</ol>\n<p>Note when doing this work to be scientific. Use 3 datasets, a training set, a separate 'development' dataset to tweak your parameters, and a test set that tests the final model, with the optimal parameters. Only change one parameter at a time and evaluate the result. Or experiment with the sklearn gridsearch algorithm to search across these parameters all at once.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Adding this late comment in case it helps others.</p>\n<p>In addition to the parameters mentioned above (<code>n_estimators</code>, <code>max_features</code>, <code>max_depth</code>, and <code>min_samples_leaf</code>) consider setting 'min_impurity_decrease'.</p>\n<p>Doing this manually is cumbersome. So use <code>sklearn.model_selection.GridSearchCV</code> to test a range of parameters (parameter grid) and find the optimal parameters.</p>\n<p>You can use 'gini' or 'entropy' for the Criterion, however, I recommend sticking with 'gini', the default. In the majority of cases, they produce the same result but 'entropy' is more computational expensive to compute.</p>\n<p>Max depth works well and is an intuitive way to stop a tree from growing, however, just because a node is less than the max depth doesn't always mean it should split. If the information gained from splitting only addresses a single/few misclassification(s) then splitting that node may be supporting overfitting. You may or may not find this parameter useful, depending on the size of your dataset and/or your feature space size and complexity, but it is worth considering while tuning your parameters.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How can I calculate in python the <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\">Cumulative Distribution Function (CDF)</a>?</p>\n<p>I want to calculate it from an array of points I have (discrete distribution), not with the continuous distributions that, for example, scipy has.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>(It is possible that my interpretation of the question is wrong. If the question is how to get from a discrete PDF into a discrete CDF, then <code>np.cumsum</code> divided by a suitable constant will do if the samples are equispaced. If the array is not equispaced, then <code>np.cumsum</code> of the array multiplied by the distances between the points will do.)</p>\n<p>If you have a discrete array of samples, and you would like to know the CDF of the sample, then you can just sort the array. If you look at the sorted result, you'll realize that the smallest value represents 0% , and largest value represents 100 %. If you want to know the value at 50 % of the distribution, just look at the array element which is in the middle of the sorted array.</p>\n<p>Let us have a closer look at this with a simple example:</p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# create some randomly ddistributed data:\ndata = np.random.randn(10000)\n\n# sort the data:\ndata_sorted = np.sort(data)\n\n# calculate the proportional values of samples\np = 1. * np.arange(len(data)) / (len(data) - 1)\n\n# plot the sorted data:\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.plot(p, data_sorted)\nax1.set_xlabel('$p$')\nax1.set_ylabel('$x$')\n\nax2 = fig.add_subplot(122)\nax2.plot(data_sorted, p)\nax2.set_xlabel('$x$')\nax2.set_ylabel('$p$')\n</code></pre>\n<p>This gives the following plot where the right-hand-side plot is the traditional cumulative distribution function. It should reflect the CDF of the process behind the points, but naturally, it is not as long as the number of points is finite.</p>\n<p><img alt=\"cumulative distribution function\" src=\"https://i.sstatic.net/sSjIz.png\"/></p>\n<p>This function is easy to invert, and it depends on your application which form you need.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Assuming you know how your data is distributed (i.e. you know the pdf of your data), then <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\" rel=\"noreferrer\">scipy</a> does support discrete data when calculating cdf's</p>\n<pre><code>import numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.random.randn(10000) # generate samples from normal distribution (discrete data)\nnorm_cdf = scipy.stats.norm.cdf(x) # calculate the cdf - also discrete\n\n# plot the cdf\nsns.lineplot(x=x, y=norm_cdf)\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/khD9U.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/khD9U.png\"/></a></p>\n<p>We can even print the first few values of the cdf to show they are discrete</p>\n<pre><code>print(norm_cdf[:10])\n&gt;&gt;&gt; array([0.39216484, 0.09554546, 0.71268696, 0.5007396 , 0.76484329,\n       0.37920836, 0.86010018, 0.9191937 , 0.46374527, 0.4576634 ])\n</code></pre>\n<p>The same method to calculate the cdf also works for multiple dimensions: we use 2d data below to illustrate</p>\n<pre><code>mu = np.zeros(2) # mean vector\ncov = np.array([[1,0.6],[0.6,1]]) # covariance matrix\n# generate 2d normally distributed samples using 0 mean and the covariance matrix above\nx = np.random.multivariate_normal(mean=mu, cov=cov, size=1000) # 1000 samples\nnorm_cdf = scipy.stats.norm.cdf(x)\nprint(norm_cdf.shape)\n&gt;&gt;&gt; (1000, 2)\n</code></pre>\n<p>In the above examples, I had prior knowledge that my data was normally distributed, which is why I used <code>scipy.stats.norm()</code> - there are multiple distributions scipy supports. But again, you need to know how your data is distributed beforehand to use such functions. If you don't know how your data is distributed and you just use any distribution to calculate the cdf, you most likely will get incorrect results.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <strong>empirical cumulative distribution function</strong> is a CDF that jumps exactly at the values in your data set. It is the CDF for a <strong>discrete distribution</strong> that places a mass at each of your values, where the mass is proportional to the frequency of the value. Since the sum of the masses must be 1, these constraints determine the location and height of each jump in the empirical CDF.</p>\n<p>Given an array <code>a</code> of values, you compute the empirical CDF by first obtaining the frequencies of the values. The numpy function <code>unique()</code> is helpful here because it returns not only the frequencies, but also the values in sorted order. To calculate the cumulative distribution, use the <code>cumsum()</code> function, and divide by the total sum. The following function returns the values in sorted order and the corresponding cumulative distribution:</p>\n<pre><code>import numpy as np\n\ndef ecdf(a):\n    x, counts = np.unique(a, return_counts=True)\n    cusum = np.cumsum(counts)\n    return x, cusum / cusum[-1]\n</code></pre>\n<p>To plot the empirical CDF you can use <code>matplotlib</code>'s <code>plot()</code> function. The option <code>drawstyle='steps-post'</code> ensures that jumps occur at the right place. However, you need to force a jump at the smallest data value, so it's necessary to insert an additional element in front of <code>x</code> and <code>y</code>.</p>\n<pre><code>import matplotlib.pyplot as plt\n\ndef plot_ecdf(a):\n    x, y = ecdf(a)\n    x = np.insert(x, 0, x[0])\n    y = np.insert(y, 0, 0.)\n    plt.plot(x, y, drawstyle='steps-post')\n    plt.grid(True)\n    plt.savefig('ecdf.png')\n</code></pre>\n<p>Example usages:</p>\n<pre><code>xvec = np.array([7,1,2,2,7,4,4,4,5.5,7])\nplot_ecdf(xvec)\n\ndf = pd.DataFrame({'x':[7,1,2,2,7,4,4,4,5.5,7]})\nplot_ecdf(df['x'])\n</code></pre>\n<p>with output:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/7EFdI.png\"/></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have just built my first model using Keras and this is the output. It looks like the standard output you get after building any Keras artificial neural network. Even after looking in the documentation, I do not fully understand what the epoch is and what the loss is which is printed in the output.</p>\n<p><strong>What is epoch and loss in Keras?</strong> </p>\n<p>(I know it's probably an extremely basic question, but I couldn't seem to locate the answer online, and if the answer is really that hard to glean from the documentation I thought others would have the same question and thus decided to post it here.)</p>\n<pre><code>Epoch 1/20\n1213/1213 [==============================] - 0s - loss: 0.1760     \nEpoch 2/20\n1213/1213 [==============================] - 0s - loss: 0.1840     \nEpoch 3/20\n1213/1213 [==============================] - 0s - loss: 0.1816     \nEpoch 4/20\n1213/1213 [==============================] - 0s - loss: 0.1915     \nEpoch 5/20\n1213/1213 [==============================] - 0s - loss: 0.1928     \nEpoch 6/20\n1213/1213 [==============================] - 0s - loss: 0.1964     \nEpoch 7/20\n1213/1213 [==============================] - 0s - loss: 0.1948     \nEpoch 8/20\n1213/1213 [==============================] - 0s - loss: 0.1971     \nEpoch 9/20\n1213/1213 [==============================] - 0s - loss: 0.1899     \nEpoch 10/20\n1213/1213 [==============================] - 0s - loss: 0.1957     \nEpoch 11/20\n1213/1213 [==============================] - 0s - loss: 0.1923     \nEpoch 12/20\n1213/1213 [==============================] - 0s - loss: 0.1910     \nEpoch 13/20\n1213/1213 [==============================] - 0s - loss: 0.2104     \nEpoch 14/20\n1213/1213 [==============================] - 0s - loss: 0.1976     \nEpoch 15/20\n1213/1213 [==============================] - 0s - loss: 0.1979     \nEpoch 16/20\n1213/1213 [==============================] - 0s - loss: 0.2036     \nEpoch 17/20\n1213/1213 [==============================] - 0s - loss: 0.2019     \nEpoch 18/20\n1213/1213 [==============================] - 0s - loss: 0.1978     \nEpoch 19/20\n1213/1213 [==============================] - 0s - loss: 0.1954     \nEpoch 20/20\n1213/1213 [==============================] - 0s - loss: 0.1949\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just to answer the questions more specifically, here's a definition of epoch and loss:</p>\n<p><strong>Epoch</strong>: A full pass over all of your <em>training</em> data. </p>\n<p>For example, in your view above, you have 1213 observations. So an epoch concludes when it has finished a training pass over all 1213 of your observations. </p>\n<p><strong>Loss</strong>: A scalar value that we attempt to minimize during our training of the model. The lower the loss, the closer our predictions are to the true labels. </p>\n<p>This is usually Mean Squared Error (MSE) as David Maust said above, or often in Keras, <a href=\"http://keras.io/backend/#categorical_crossentropy\" rel=\"noreferrer\" title=\"Categorical Cross-Entropy\">Categorical Cross Entropy</a></p>\n<hr/>\n<p>What you'd expect to see from running fit on your Keras model, is a decrease in loss over n number of epochs. Your training run is rather abnormal, as your loss is actually increasing. This <em>could</em> be due to a learning rate that is too large, which is causing you to overshoot optima. </p>\n<p>As jaycode mentioned, you will want to look at your model's performance on unseen data, as this is the general use case of Machine Learning. </p>\n<p>As such, you should include a list of metrics in your compile method, which could look like:</p>\n<pre><code>model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n</code></pre>\n<p>As well as run your model on validation during the fit method, such as: </p>\n<pre><code>model.fit(data, labels, validation_split=0.2)\n</code></pre>\n<hr/>\n<p>There's a lot more to explain, but hopefully this gets you started.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One epoch ends when your model had run the data through all nodes in your network and ready to update the weights to reach optimal loss value. That is, smaller is better. In your case, as there are higher loss scores on higher epoch, it \"seems\" the model is better on first epoch.</p>\n<p>I said \"seems\" since we can't actually tell for sure yet as the model has not been tested using proper cross validation method i.e. it is evaluated only against its training data.</p>\n<p>Ways to improve your model:</p>\n<ul>\n<li>Use cross validation in your Keras model in order to find out how the model actually perform, does it generalize well when predicting new data it has never seen before?</li>\n<li>Adjust your learning rate, structure of neural network model, number of hidden units / layers, init, optimizer, and activator parameters used in your model among myriad other things.</li>\n</ul>\n<p>Combining sklearn's GridSearchCV with Keras can automate this process.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This has become quite a frustrating question, but I've asked in the Coursera discussions and they won't help. Below is the question:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/OK0L3.png\"/></p>\n<p>I've gotten it wrong 6 times now. How do I normalize the feature? Hints are all I'm asking for.</p>\n<p>I'm assuming x_2^(2) is the value 5184, unless I am adding the x_0 column of 1's, which they don't mention but he certainly mentions in the lectures when talking about creating the design matrix X. In which case x_2^(2) would be the value 72. Assuming one or the other is right (I'm playing a guessing game), what should I use to normalize it? He talks about 3 different ways to normalize in the lectures: one using the maximum value, another with the range/difference between max and mins, and another the standard deviation -- they want an answer correct to the <em>hundredths</em>. Which one am I to use? This is so confusing.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p><em>...use both <strong>feature scaling (dividing by the</strong></em>\n<strong><em>\"max-min\"</em></strong>, or range, of a feature) and <strong>mean normalization.</strong></p>\n</blockquote>\n<p>So for any individual feature f:</p>\n<pre><code>f_norm = (f - f_mean) / (f_max - f_min)\n</code></pre>\n<p>e.g. for x2,(midterm exam)^2 = {7921, 5184, 8836, 4761}</p>\n<pre><code>&gt; x2 &lt;- c(7921, 5184, 8836, 4761)\n&gt; mean(x2)\n 6676\n&gt; max(x2) - min(x2)\n 4075\n&gt; (x2 - mean(x2)) / (max(x2) - min(x2))\n 0.306  -0.366  0.530 -0.470\n</code></pre>\n<p>Hence <strong>norm(5184) = 0.366</strong></p>\n<p>(using R language, which is great at vectorizing expressions like this)</p>\n<p>I agree it's confusing they used the notation <strong><em>x2 (2)</em></strong> to mean <strong><em>x2 (norm)</em></strong> or <strong><em>x2'</em></strong></p>\n<hr/>\n<p>EDIT: in practice everyone calls the builtin <a href=\"http://www.inside-r.org/r-doc/base/scale\" rel=\"noreferrer\"><code>scale(...)</code></a> function, which does the same thing.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It's asking to normalize the second feature under second column using both feature scaling and mean normalization. Therefore,</p>\n<p><code>(5184 - 6675.5) / 4075 = -0.366</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset from <code>sklearn</code> and I plotted the distribution of the <code>load_diabetes.target</code> data (i.e. the values of the regression that the <code>load_diabetes.data</code> are used to predict). </p>\n<p><em>I used this because it has the fewest number of variables/attributes of the regression <code>sklearn.datasets</code>.</em></p>\n<p>Using Python 3, <strong>How can I get the distribution-type and parameters of the distribution this most closely resembles?</strong> </p>\n<p>All I know the <code>target</code> values are all positive and skewed (positve skew/right skew). . . Is there a way in Python to provide a few distributions and then get the best fit for the <code>target</code> data/vector? OR, to actually suggest a fit based on the data that's given? That would be realllllly useful for people who have theoretical statistical knowledge but little experience with applying it to \"real data\". </p>\n<p><strong>Bonus</strong>\nWould it make sense to use this type of approach to figure out what your posterior distribution would be with \"real data\" ? If no, why not?</p>\n<pre><code>from sklearn.datasets import load_diabetes\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport pandas as pd\n\n#Get Data\ndata = load_diabetes()\nX, y_ = data.data, data.target\n\n#Organize Data\nSR_y = pd.Series(y_, name=\"y_ (Target Vector Distribution)\")\n\n#Plot Data\nfig, ax = plt.subplots()\nsns.distplot(SR_y, bins=25, color=\"g\", ax=ax)\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/ol9gk.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ol9gk.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use this approach</p>\n<pre><code>import scipy.stats as st\ndef get_best_distribution(data):\n    dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n    dist_results = []\n    params = {}\n    for dist_name in dist_names:\n        dist = getattr(st, dist_name)\n        param = dist.fit(data)\n\n        params[dist_name] = param\n        # Applying the Kolmogorov-Smirnov test\n        D, p = st.kstest(data, dist_name, args=param)\n        print(\"p value for \"+dist_name+\" = \"+str(p))\n        dist_results.append((dist_name, p))\n\n    # select the best fitted distribution\n    best_dist, best_p = (max(dist_results, key=lambda item: item[1]))\n    # store the name of the best fit and its p value\n\n    print(\"Best fitting distribution: \"+str(best_dist))\n    print(\"Best p value: \"+ str(best_p))\n    print(\"Parameters for the best fit: \"+ str(params[best_dist]))\n\n    return best_dist, best_p, params[best_dist]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To the best of my knowledge, there is no automatic way of obtaining the distribution type and parameters of a sample (as <em>inferring</em> the distribution of a sample is a statistical problem by itself).</p>\n<p>In my opinion, the best you can do is:</p>\n<p>(for each attribute)</p>\n<ul>\n<li><p>Try to fit each attribute to a reasonably large list of possible distributions \n(e.g. see <a href=\"https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python?lq=1\">Fitting empirical distribution to theoretical ones with Scipy (Python)?</a> for an example with Scipy)</p></li>\n<li><p>Evaluate all your fits and pick the best one. This can be done by performing a Kolmogorov-Smirnov test between your sample and each of the distributions of the fit (you have an implementation in Scipy, again), and picking the one that minimises D, the test statistic (a.k.a. the difference between the sample and the fit).</p></li>\n</ul>\n<p>Bonus: It would make sense - as you'll be building a model on each of the variables as you pick a fit for each one - although the goodness of your prediction would depend on the quality of your data and the distributions you are using for fitting. You are building a model, after all.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use that code to fit (according to the maximum likelihood) different distributions with your datas: </p>\n<pre><code>import matplotlib.pyplot as plt\nimport scipy\nimport scipy.stats\n\ndist_names = ['gamma', 'beta', 'rayleigh', 'norm', 'pareto']\n\nfor dist_name in dist_names:\n    dist = getattr(scipy.stats, dist_name)\n    param = dist.fit(y)\n    # here's the parameters of your distribution, scale, location\n</code></pre>\n<p>You can see a sample snippet about how to use the parameters obtained here: <a href=\"https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python?lq=1\">Fitting empirical distribution to theoretical ones with Scipy (Python)?</a></p>\n<p>Then, you can <strong>pick the distribution with the best log likelihood</strong> (there are also other criteria to match the \"best\" distribution, such as Bayesian posterior probability, AIC, BIC or BICc values, ...). </p>\n<p>For your bonus question, there's I think no generic answer. If your set of data is significant and obtained <strong>under the same conditions</strong> as the real word datas, you can do it. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code> outputs random values from a normal distribution.</p>\n<p><code>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code> outputs random values from a truncated normal distribution.</p>\n<p>I tried googling 'truncated normal distribution'. But didn't understand much.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <a href=\"https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal\" rel=\"nofollow noreferrer\">documentation</a> says it all:\nFor truncated normal distribution:</p>\n<blockquote>\n<p>The values are drawn from a normal distribution with specified mean and standard deviation, discarding and re-drawing any samples that are more than two standard deviations from the mean.</p>\n</blockquote>\n<p>Most probably it is easy to understand the difference by plotting the graph for yourself (%magic is because I use jupyter notebook):</p>\n<pre><code>import tensorflow as tf\nimport matplotlib.pyplot as plt\n\n%matplotlib inline  \n\nn = 500000\nA = tf.truncated_normal((n,))\nB = tf.random_normal((n,))\nwith tf.Session() as sess:\n    a, b = sess.run([A, B])\n</code></pre>\n<p>And now</p>\n<pre><code>plt.hist(a, 100, (-4.2, 4.2));\nplt.hist(b, 100, (-4.2, 4.2));\n</code></pre>\n<p><a href=\"https://i.sstatic.net/fXqXs.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/fXqXs.png\"/></a></p>\n<hr/>\n<p>The point for using truncated normal is to overcome saturation of tome functions like sigmoid (where if the value is too big/small, the neuron stops learning).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>tf.truncated_normal()</code> selects random numbers from a normal distribution whose mean is close to 0 and values are close to 0. For example, from -0.1 to 0.1. It's called truncated because your cutting off the tails from a normal distribution.</p>\n<p><code>tf.random_normal()</code> selects random numbers from a normal distribution whose mean is close to 0, but values can be a bit further apart. For example, from -2 to 2.</p>\n<p>In machine learning, in practice, you usually want your weights to be close to 0.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <a href=\"https://www.tensorflow.org/api_docs/python/tf/truncated_normal\" rel=\"nofollow noreferrer\">API documentation for tf.truncated_normal()</a> describes the function as:</p>\n<blockquote>\n<p>Outputs random values from a truncated normal distribution.</p>\n<p>The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is\nmore than 2 standard deviations from the mean are dropped and\nre-picked.</p>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I trained a model to classify images from 2 classes and saved it using <code>model.save()</code>. Here is the code I used:</p>\n<pre><code>from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\n\n\n# dimensions of our images.\nimg_width, img_height = 320, 240\n\ntrain_data_dir = 'data/train'\nvalidation_data_dir = 'data/validation'\nnb_train_samples = 200  #total\nnb_validation_samples = 10  # total\nepochs = 6\nbatch_size = 10\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary')\n\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples // batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=5)\n\nmodel.save('model.h5')\n</code></pre>\n<p>It successfully trained with 0.98 accuracy which is pretty good. To load and test this model on new images, I used the below code:</p>\n<pre><code>from keras.models import load_model\nimport cv2\nimport numpy as np\n\nmodel = load_model('model.h5')\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nimg = cv2.imread('test.jpg')\nimg = cv2.resize(img,(320,240))\nimg = np.reshape(img,[1,320,240,3])\n\nclasses = model.predict_classes(img)\n\nprint classes\n</code></pre>\n<p>It outputs:</p>\n<blockquote>\n<p>[[0]]</p>\n</blockquote>\n<p>Why wouldn't it give out the actual name of the class and why <code>[[0]]</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If someone is still struggling to make predictions on images, here is the optimized code to load the saved model and make predictions:</p>\n<pre><code># Modify 'test1.jpg' and 'test2.jpg' to the images you want to predict on\n\nfrom keras.models import load_model\nfrom keras.preprocessing import image\nimport numpy as np\n\n# dimensions of our images\nimg_width, img_height = 320, 240\n\n# load the model we saved\nmodel = load_model('model.h5')\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# predicting images\nimg = image.load_img('test1.jpg', target_size=(img_width, img_height))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\n\nimages = np.vstack([x])\nclasses = model.predict_classes(images, batch_size=10)\nprint classes\n\n# predicting multiple images at once\nimg = image.load_img('test2.jpg', target_size=(img_width, img_height))\ny = image.img_to_array(img)\ny = np.expand_dims(y, axis=0)\n\n# pass the list of multiple images np.vstack()\nimages = np.vstack([x, y])\nclasses = model.predict_classes(images, batch_size=10)\n\n# print the classes, the images belong to\nprint classes\nprint classes[0]\nprint classes[0][0]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <code>model.predict()</code> to predict the class of a single image as follows <a href=\"https://keras.io/models/sequential/\" rel=\"noreferrer\">[doc]</a>:</p>\n<pre><code># load_model_sample.py\nfrom keras.models import load_model\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n\ndef load_image(img_path, show=False):\n\n    img = image.load_img(img_path, target_size=(150, 150))\n    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n\n    if show:\n        plt.imshow(img_tensor[0])                           \n        plt.axis('off')\n        plt.show()\n\n    return img_tensor\n\n\nif __name__ == \"__main__\":\n\n    # load model\n    model = load_model(\"model_aug.h5\")\n\n    # image path\n    img_path = '/media/data/dogscats/test1/3867.jpg'    # dog\n    #img_path = '/media/data/dogscats/test1/19.jpg'      # cat\n\n    # load a single image\n    new_image = load_image(img_path)\n\n    # check prediction\n    pred = model.predict(new_image)\n</code></pre>\n<p>In this example, a image is loaded as a <code>numpy</code> array with shape <code>(1, height, width, channels)</code>. Then, we load it into the model and predict its class, returned as a real value in the range [0, 1] (binary classification in this example).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>keras predict_classes (<a href=\"https://keras.io/models/sequential/#predict_classes\" rel=\"noreferrer\">docs</a>) outputs A numpy array of class predictions. Which in your model case, the index of neuron of highest activation from your last(softmax) layer. <code>[[0]]</code> means that your model predicted that your test data is class 0. (usually you will be passing multiple image, and the result will look like <code>[[0], [1], [1], [0]]</code> )</p>\n<p>You must convert your actual label (e.g. <code>'cancer', 'not cancer'</code>) into binary encoding (<code>0</code> for 'cancer', <code>1</code> for 'not cancer') for binary classification. Then you will interpret your sequence output of <code>[[0]]</code> as having class label <code>'cancer'</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can't get the dtypes to match, either the loss wants long or the model wants float if I change my tensors to long. The shape of the tensors are 42000, 1, 28, 28 and 42000. I'm not sure where I can change what dtypes are required for the model or loss. </p>\n<p>I'm not sure if dataloader is required, using Variable didn't work either.</p>\n<pre><code>dataloaders_train = torch.utils.data.DataLoader(Xt_train, batch_size=64)\n\ndataloaders_test = torch.utils.data.DataLoader(Yt_train, batch_size=64)\n\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n\n        self.hidden = nn.Linear(42000, 256)\n\n        self.output = nn.Linear(256, 10)\n\n\n        self.sigmoid = nn.Sigmoid()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n\n        x = self.hidden(x)\n        x = self.sigmoid(x)\n        x = self.output(x)\n        x = self.softmax(x)\n\n        return x\n\nmodel = Network()\n\ninput_size = 784\nhidden_sizes = [28, 64]\noutput_size = 10 \nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[1], output_size),\n                      nn.Softmax(dim=1))\nprint(model)\n\ncriterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.003)\n\nepochs = 5\n\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in zip(dataloaders_train, dataloaders_test):\n\n        images = images.view(images.shape[0], -1)\n        #images, labels = Variable(images), Variable(labels)\n        print(images.dtype)\n        print(labels.dtype)\n\n        optimizer.zero_grad()\n\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    else:\n        print(f\"Training loss: {running_loss}\")\n</code></pre>\n<p>Which gives</p>\n<pre><code>RuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-128-68109c274f8f&gt; in &lt;module&gt;\n     11 \n     12         output = model(images)\n---&gt; 13         loss = criterion(output, labels)\n     14         loss.backward()\n     15         optimizer.step()\n\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    530             result = self._slow_forward(*input, **kwargs)\n    531         else:\n--&gt; 532             result = self.forward(*input, **kwargs)\n    533         for hook in self._forward_hooks.values():\n    534             hook_result = hook(self, input, result)\n\n/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py in forward(self, input, target)\n    202 \n    203     def forward(self, input, target):\n--&gt; 204         return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)\n    205 \n    206 \n\n/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n   1836                          .format(input.size(0), target.size(0)))\n   1837     if dim == 2:\n-&gt; 1838         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n   1839     elif dim == 4:\n   1840         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n\nRuntimeError: expected scalar type Long but found Float\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>LongTensor</code> is synonymous with integer. PyTorch won't accept a <code>FloatTensor</code> as categorical target, so it's telling you to cast your tensor to <code>LongTensor</code>. This is how you should change your target dtype:</p>\n<pre><code>Yt_train = Yt_train.type(torch.LongTensor)\n</code></pre>\n<p>This is very well <a href=\"https://pytorch.org/docs/stable/tensors.html\" rel=\"noreferrer\">documented</a> on the PyTorch website, you definitely won't regret spending a minute or two reading this page. PyTorch essentially defines nine CPU tensor types and nine GPU tensor types:</p>\n<pre><code>╔══════════════════════════╦═══════════════════════════════╦════════════════════╦═════════════════════════╗\n║        Data type         ║             dtype             ║     CPU tensor     ║       GPU tensor        ║\n╠══════════════════════════╬═══════════════════════════════╬════════════════════╬═════════════════════════╣\n║ 32-bit floating point    ║ torch.float32 or torch.float  ║ torch.FloatTensor  ║ torch.cuda.FloatTensor  ║\n║ 64-bit floating point    ║ torch.float64 or torch.double ║ torch.DoubleTensor ║ torch.cuda.DoubleTensor ║\n║ 16-bit floating point    ║ torch.float16 or torch.half   ║ torch.HalfTensor   ║ torch.cuda.HalfTensor   ║\n║ 8-bit integer (unsigned) ║ torch.uint8                   ║ torch.ByteTensor   ║ torch.cuda.ByteTensor   ║\n║ 8-bit integer (signed)   ║ torch.int8                    ║ torch.CharTensor   ║ torch.cuda.CharTensor   ║\n║ 16-bit integer (signed)  ║ torch.int16 or torch.short    ║ torch.ShortTensor  ║ torch.cuda.ShortTensor  ║\n║ 32-bit integer (signed)  ║ torch.int32 or torch.int      ║ torch.IntTensor    ║ torch.cuda.IntTensor    ║\n║ 64-bit integer (signed)  ║ torch.int64 or torch.long     ║ torch.LongTensor   ║ torch.cuda.LongTensor   ║\n║ Boolean                  ║ torch.bool                    ║ torch.BoolTensor   ║ torch.cuda.BoolTensor   ║\n╚══════════════════════════╩═══════════════════════════════╩════════════════════╩═════════════════════════╝\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to get an agent to learn the mouse movements necessary to best perform some task in a reinforcement learning setting (i.e. the reward signal is the only feedback for learning).</p>\n<p>I'm hoping to use the Q-learning technique, but while I've found <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf\">a way to extend this method to continuous state spaces</a>, I can't seem to figure out how to accommodate a problem with a continuous action space.</p>\n<p>I could just force all mouse movement to be of a certain magnitude and in only a certain number of different directions, but any reasonable way of making the actions discrete would yield a huge action space. Since standard Q-learning requires the agent to evaluate <em>all</em> possible actions, such an approximation doesn't solve the problem in any practical sense.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The common way of dealing with this problem is with <a href=\"http://incompleteideas.net/book/first/ebook/node66.html\" rel=\"noreferrer\">actor-critic methods</a>. These naturally extend to continuous action spaces. Basic Q-learning could diverge when working with approximations, however, if you still want to use it, you can try combining it with a self-organizing map, as done in <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9850&amp;rep=rep1&amp;type=pdf\" rel=\"noreferrer\">\"Applications of the self-organising map to reinforcement learning\"</a>. The paper also contains some further references you might find useful.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Fast forward to this year, folks from DeepMind proposes a deep reinforcement learning actor-critic method for dealing with <strong>both</strong> continuous state and action space. It is based on a technique called deterministic policy gradient. See the paper <a href=\"http://arxiv.org/abs/1509.02971\" rel=\"noreferrer\">Continuous control with deep reinforcement learning</a> and some <a href=\"https://www.reddit.com/r/MachineLearning/comments/3ytnqc/deep_q_learning_with_continuous_actions_question/\" rel=\"noreferrer\">implementations</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are numerous ways to extend reinforcement learning to continuous actions. One way is to use actor-critic methods. Another way is to use policy gradient methods.</p>\n<p>A rather extensive explanation of different methods can be found in the following paper, which is available online:\n<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.7658&amp;rep=rep1&amp;type=pdf\" rel=\"noreferrer\">Reinforcement Learning in Continuous State and Action Spaces</a> (by Hado van Hasselt and Marco A. Wiering).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://i.sstatic.net/be1s4.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/be1s4.png\"/></a></p>\n<p>What is the meaning of the (None, 100) in Output Shape?\nIs this(\"None\") the Sample number or the hidden dimension?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>None</code> means this dimension is variable. </p>\n<p>The first dimension in a keras model is always the batch size. You don't need fixed batch sizes, unless in very specific cases (for instance, when working with <code>stateful=True</code> LSTM layers).    </p>\n<p>That's why this dimension is often ignored when you define your model. For instance, when you define <code>input_shape=(100,200)</code>, actually you're ignoring the batch size and defining the shape of \"each sample\". Internally the shape will be <code>(None, 100, 200)</code>, allowing a variable batch size, each sample in the batch having the shape <code>(100,200)</code>.     </p>\n<p>The batch size will be then automatically defined in the <code>fit</code> or <code>predict</code> methods.</p>\n<hr/>\n<p><strong>Other <code>None</code> dimensions:</strong></p>\n<p>Not only the batch dimension can be <code>None</code>, but many others as well.   </p>\n<p>For instance, in a 2D convolutional network, where the expected input is <code>(batchSize, height, width, channels)</code>, you can have shapes like <code>(None, None, None, 3)</code>, allowing variable image sizes. </p>\n<p>In recurrent networks and in 1D convolutions, you can also make the <code>length/timesteps</code> dimension variable, with shapes like <code>(None, None, featuresOrChannels)</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, <code>None</code> in summary means a dynamic dimension of a batch (mini batch). \nThis is why you can set any batch size to your model.</p>\n<p>The <code>summary()</code> method is part of TF that incorporates Keras method <a href=\"https://github.com/keras-team/keras/blob/b0bfd5201da2bfced84028bcc5bda05bdfd75af7/keras/utils/layer_utils.py\" rel=\"noreferrer\"><code>print_summary()</code></a>. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Given a vector of scores and a vector of actual class labels, how do you calculate a single-number AUC metric for a binary classifier in the R language or in simple English? </p>\n<p>Page 9 of <a href=\"http://www.site.uottawa.ca/~stan/csi7162/presentations/William-presentation.pdf\">\"AUC: a Better Measure...\"</a> seems to require knowing the class labels, and here is <a href=\"http://dwinnell.com/SampleError.m\">an example in MATLAB</a> where I don't understand </p>\n<pre><code>R(Actual == 1))\n</code></pre>\n<p>Because R (not to be confused with the R language) is defined a vector but used as a function?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>With the package <code>pROC</code> you can use the function <code>auc()</code> like this example from the help page:</p>\n<pre><code>&gt; data(aSAH)\n&gt; library(pROC) \n&gt; # Syntax (response, predictor):\n&gt; auc(aSAH$outcome, aSAH$s100b)\nArea under the curve: 0.7314\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://rocr.bioinf.mpi-sb.mpg.de/\" rel=\"noreferrer\">The ROCR package</a> will calculate the AUC among other statistics:</p>\n<pre><code>auc.tmp &lt;- performance(pred,\"auc\"); auc &lt;- as.numeric(<a class=\"__cf_email__\" data-cfemail=\"8eeffbeda0fae3fecef7a0f8efe2fbebfd\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As mentioned by others, you can compute the AUC using the <a href=\"http://rocr.bioinf.mpi-sb.mpg.de/\" rel=\"noreferrer\">ROCR</a> package.  With the ROCR package you can also plot the ROC curve, lift curve and other model selection measures. </p>\n<p>You can compute the AUC directly without using any package by using the fact that the AUC is equal to the probability that a true positive is scored greater than a true negative.</p>\n<p>For example, if <code>pos.scores</code> is a vector containing a score of the positive examples, and <code>neg.scores</code> is a vector containing the negative examples then the AUC is approximated by:</p>\n<pre><code>&gt; mean(sample(pos.scores,1000,replace=T) &gt; sample(neg.scores,1000,replace=T))\n[1] 0.7261\n</code></pre>\n<p>will give an approximation of the AUC.  You can also estimate the variance of the AUC by bootstrapping:</p>\n<pre><code>&gt; aucs = replicate(1000,mean(sample(pos.scores,1000,replace=T) &gt; sample(neg.scores,1000,replace=T)))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've working on a CNN over several hundred GBs of images. I've created a training function that bites off 4Gb chunks of these images and calls <code>fit</code> over each of these pieces. I'm worried that I'm only training on the last piece on not the entire dataset.</p>\n<p>Effectively, my pseudo-code looks like this:</p>\n<pre><code>DS = lazy_load_400GB_Dataset()\nfor section in DS:\n    X_train = section.images\n    Y_train = section.classes\n\n    model.fit(X_train, Y_train, batch_size=16, nb_epoch=30)\n</code></pre>\n<p>I know that the API and the Keras forums say that this will train over the entire dataset, but I can't intuitively understand why the network wouldn't relearn over just the last training chunk.</p>\n<p>Some help understanding this would be much appreciated.</p>\n<p>Best,\nJoe</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This question was raised at the <a href=\"https://github.com/keras-team/keras\" rel=\"noreferrer\">Keras github repository</a> in <a href=\"https://github.com/keras-team/keras/issues/4446\" rel=\"noreferrer\">Issue #4446: Quick Question: can a model be fit for multiple times?</a>  It was closed by <a href=\"https://github.com/fchollet\" rel=\"noreferrer\">François Chollet</a> with the following <a href=\"https://github.com/keras-team/keras/issues/4446#issuecomment-261804574\" rel=\"noreferrer\">statement</a>:</p>\n<blockquote>\n<p>Yes, successive calls to <code>fit</code> will incrementally train the model.</p>\n</blockquote>\n<p>So, yes, you can call fit multiple times.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For datasets that do not fit into memory, there is an answer in the <a href=\"https://keras.io/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory\" rel=\"noreferrer\">Keras Documentation FAQ section</a></p>\n<blockquote>\n<p>You can do batch training using <code>model.train_on_batch(X, y)</code> and\n  <code>model.test_on_batch(X, y)</code>. See the <a href=\"https://keras.io/models/sequential\" rel=\"noreferrer\">models documentation</a>.</p>\n<p>Alternatively, you can write a generator that yields batches of\n  training data and use the method <code>model.fit_generator(data_generator, samples_per_epoch, nb_epoch)</code>.</p>\n<p>You can see batch training in action in our <a href=\"https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\" rel=\"noreferrer\">CIFAR10 example</a>.</p>\n</blockquote>\n<p>So if you want to iterate your dataset the way you are doing, you should probably use <code>model.train_on_batch</code> and take care of the batch sizes and iteration yourself.</p>\n<p>One more thing to note is that you should make sure the order in which the samples you train your model with is shuffled after each epoch. The way you have written the example code seems to not shuffle the dataset. You can read a bit more about shuffling <a href=\"https://stackoverflow.com/questions/8101925/effects-of-randomizing-the-order-of-inputs-to-a-neural-network\">here</a> and <a href=\"https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks\" rel=\"noreferrer\">here</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using DBSCAN to cluster some data using Scikit-Learn (Python 2.7):</p>\n<pre><code>from sklearn.cluster import DBSCAN\ndbscan = DBSCAN(random_state=0)\ndbscan.fit(X)\n</code></pre>\n<p>However, I found that there was no built-in function (aside from \"fit_predict\") that could assign the new data points, Y, to the clusters identified in the original data, X. The K-means method has a \"predict\" function but I want to be able to do the same with DBSCAN. Something like this:</p>\n<pre><code>dbscan.predict(X, Y)\n</code></pre>\n<p>So that the density can be inferred from X but the return values (cluster assignments/labels) are only for Y. From what I can tell, this capability is available in R so I assume that it is also somehow available in Python. I just can't seem to find any documentation for this.</p>\n<p>Also, I have tried searching for reasons as to why DBSCAN may not be used for labeling new data but I haven't found any justifications.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While Anony-Mousse has some good points (Clustering is indeed not classifying) I think the ability of assigning new points has it's usefulness. <sup>*</sup></p>\n<p>Based on the original paper on <a href=\"http://www.dbs.informatik.uni-muenchen.de/Publikationen/Papers/KDD-96.final.frame.pdf\" rel=\"noreferrer\">DBSCAN</a> and robertlaytons ideas on <a href=\"https://github.com/scikit-learn/scikit-learn/issues/901#issuecomment-6286281\" rel=\"noreferrer\">github.com/scikit-learn</a>, I suggest running through core points and assigning to the cluster of the first core point that is within <code>eps</code> of you new point. \nThen it is guaranteed that your point will at least be a border point of the assigned cluster according to the definitions used for the clustering. \n(Be aware that your point might be deemed noise and not assigned to a cluster)</p>\n<p>I've done a quick implementation:</p>\n<pre><code>import numpy as np\nimport scipy as sp\n\ndef dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):\n    # Result is noise by default\n    y_new = np.ones(shape=len(X_new), dtype=int)*-1 \n\n    # Iterate all input samples for a label\n    for j, x_new in enumerate(X_new):\n        # Find a core sample closer than EPS\n        for i, x_core in enumerate(dbscan_model.components_): \n            if metric(x_new, x_core) &lt; dbscan_model.eps:\n                # Assign label of x_core to x_new\n                y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]\n                break\n\n    return y_new\n</code></pre>\n<p>The labels obtained by clustering (<code>dbscan_model = DBSCAN(...).fit(X)</code> and the labels obtained from the same model on the same data  (<code>dbscan_predict(dbscan_model, X)</code>) sometimes differ. I'm not quite certain if this is a bug somewhere or a result of randomness. </p>\n<p><strong>EDIT:</strong> I Think the above problem of differing prediction outcomes could stem from the possibility that a border point can be close to multiple clusters. Please update if you test this and find an answer. Ambiguity might be solved by shuffling core points every time or by picking the closest instead of the first core point. </p>\n<p>*) Case at hand: I'd like to evaluate if the clusters obtained from a subset of my data makes sense for other subset or is simply a special case.\nIf it generalises it supports the validity of the clusters and the earlier steps of pre-processing applied.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Clustering is not classification.</p>\n<p>Clustering is unlabeled. If you want to squeeze it into a prediction mindset (which is not the best idea), then it essentially <em>predicts without learning</em>. Because there is no labeled training data available for clustering. It has to make up new labels for the data, based on what it sees. But you can't do this on a single instance, you can only \"bulk predict\".</p>\n<p>But there is something wrong with scipys DBSCAN:</p>\n<blockquote>\n<p><code>random_state</code> : numpy.RandomState, optional :</p>\n<blockquote>\n<p>The generator used to initialize the centers. Defaults to numpy.random.</p>\n</blockquote>\n</blockquote>\n<p>DBSCAN does not \"initialize the centers\", because there are no centers in DBSCAN.</p>\n<p>Pretty much the <em>only</em> clustering algorithm where you can assign new points to the old clusters is k-means (and its many variations). Because it performs a \"1NN classification\" using the previous iterations cluster centers, then updates the centers. But most algorithms don't work like k-means, so you can't copy this.</p>\n<h3>If you want to classify new points, it is best to train a classifier on your clustering result.</h3>\n<p>What the R version maybe is doing, is using a 1NN classificator for prediction; maybe with the extra rule that points are assigned the noise label, if their 1NN distance is larger than epsilon, mabye also using the core points only. Maybe not.</p>\n<p>Get the DBSCAN paper, it does not discuss \"prediction\" IIRC.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here a slightly different and more efficient implementation. Also, instead of taking the first best core point that is within the eps radius, the core point that is closest to the sample is taken.</p>\n<pre><code>def dbscan_predict(model, X):\n\n    nr_samples = X.shape[0]\n\n    y_new = np.ones(shape=nr_samples, dtype=int) * -1\n\n    for i in range(nr_samples):\n        diff = model.components_ - X[i, :]  # NumPy broadcasting\n\n        dist = np.linalg.norm(diff, axis=1)  # Euclidean distance\n\n        shortest_dist_idx = np.argmin(dist)\n\n        if dist[shortest_dist_idx] &lt; model.eps:\n            y_new[i] = model.labels_[model.core_sample_indices_[shortest_dist_idx]]\n\n    return y_new\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to evaluate multiple machine learning algorithms with sklearn for a couple of metrics (accuracy, recall, precision and maybe more).</p>\n<p>For what I understood from the documentation <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#\" rel=\"noreferrer\">here</a> and from the source code(I'm using sklearn 0.17), the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score\" rel=\"noreferrer\">cross_val_score</a> function only receives one scorer for each execution. So for calculating multiple scores, I have to :</p>\n<ol>\n<li>Execute multiple times</li>\n<li><p>Implement my (time consuming and error prone) scorer</p>\n<p>I've executed multiple times with this code :</p>\n<pre><code>from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cross_validation import  cross_val_score\nimport time\nfrom sklearn.datasets import  load_iris\n\niris = load_iris()\n\nmodels = [GaussianNB(), DecisionTreeClassifier(), SVC()]\nnames = [\"Naive Bayes\", \"Decision Tree\", \"SVM\"]\nfor model, name in zip(models, names):\n    print name\n    start = time.time()\n    for score in [\"accuracy\", \"precision\", \"recall\"]:\n        print score,\n        print \" : \",\n        print cross_val_score(model, iris.data, iris.target,scoring=score, cv=10).mean()\n    print time.time() - start\n</code></pre></li>\n</ol>\n<p>And I get this output: </p>\n<pre><code>Naive Bayes\naccuracy  :  0.953333333333\nprecision  :  0.962698412698\nrecall  :  0.953333333333\n0.0383198261261\nDecision Tree\naccuracy  :  0.953333333333\nprecision  :  0.958888888889\nrecall  :  0.953333333333\n0.0494720935822\nSVM\naccuracy  :  0.98\nprecision  :  0.983333333333\nrecall  :  0.98\n0.063080072403\n</code></pre>\n<p>Which is ok, but it's slow for my own data. How can I measure all scores ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Since the time of writing this post scikit-learn has updated and made my answer obsolete, see the much cleaner solution below</strong></p>\n<hr/>\n<p><strike>You can write your own scoring function to capture all three pieces of information, however a scoring function for cross validation must only return a single number in <code>scikit-learn</code> (this is likely for compatibility reasons). Below is an example where each of the scores for each cross validation slice prints to the console, and the returned value is just the sum of the three metrics. If you want to return all these values, you're going to have to make some changes to <code>cross_val_score</code> (line 1351 of cross_validation.py) and <code>_score</code> (line 1601 or the same file).</strike></p>\n<pre><code>from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cross_validation import  cross_val_score\nimport time\nfrom sklearn.datasets import  load_iris\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\niris = load_iris()\n\nmodels = [GaussianNB(), DecisionTreeClassifier(), SVC()]\nnames = [\"Naive Bayes\", \"Decision Tree\", \"SVM\"]\n\ndef getScores(estimator, x, y):\n    yPred = estimator.predict(x)\n    return (accuracy_score(y, yPred), \n            precision_score(y, yPred, pos_label=3, average='macro'), \n            recall_score(y, yPred, pos_label=3, average='macro'))\n\ndef my_scorer(estimator, x, y):\n    a, p, r = getScores(estimator, x, y)\n    print a, p, r\n    return a+p+r\n\nfor model, name in zip(models, names):\n    print name\n    start = time.time()\n    m = cross_val_score(model, iris.data, iris.target,scoring=my_scorer, cv=10).mean()\n    print '\\nSum:',m, '\\n\\n'\n    print 'time', time.time() - start, '\\n\\n'\n</code></pre>\n<p><strike>Which gives:</strike></p>\n<pre><code>Naive Bayes\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n0.866666666667 0.904761904762 0.866666666667\n1.0 1.0 1.0\n1.0 1.0 1.0\n1.0 1.0 1.0\n\nSum: 2.86936507937 \n\n\ntime 0.0249638557434 \n\n\nDecision Tree\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n0.866666666667 0.866666666667 0.866666666667\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n1.0 1.0 1.0\n\nSum: 2.86555555556 \n\n\ntime 0.0237860679626 \n\n\nSVM\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n1.0 1.0 1.0\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n1.0 1.0 1.0\n1.0 1.0 1.0\n\nSum: 2.94333333333 \n\n\ntime 0.043044090271 \n</code></pre>\n<hr/>\n<p>As of scikit-learn 0.19.0 the solution becomes <strong>much</strong> easier</p>\n<pre><code>from sklearn.model_selection import cross_validate\nfrom sklearn.datasets import  load_iris\nfrom sklearn.svm import SVC\n\niris = load_iris()\nclf = SVC()\nscoring = {'acc': 'accuracy',\n           'prec_macro': 'precision_macro',\n           'rec_micro': 'recall_macro'}\nscores = cross_validate(clf, iris.data, iris.target, scoring=scoring,\n                         cv=5, return_train_score=True)\nprint(scores.keys())\nprint(scores['test_acc'])  \n</code></pre>\n<p>Which gives:</p>\n<pre><code>['test_acc', 'score_time', 'train_acc', 'fit_time', 'test_rec_micro', 'train_rec_micro', 'train_prec_macro', 'test_prec_macro']\n[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I ran over the same problem and I created a module that can support multiple metrics in <code>cross_val_score</code>.<br/>\nIn order to accomplish what you want with this module, you can write:</p>\n<pre><code>from multiscorer import MultiScorer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score          \nfrom sklearn.model_selection import cross_val_score\nfrom numpy import average\n\nscorer = MultiScorer({\n    'Accuracy'  : (accuracy_score , {}),\n    'Precision' : (precision_score, {'pos_label': 3, 'average':'macro'}),\n    'Recall'    : (recall_score   , {'pos_label': 3, 'average':'macro'})\n})\n\nfor model, name in zip(models, names):\n    print name\n    start = time.time()\n\n    _ = cross_val_score(model, iris.data, iris.target,scoring=scorer, cv=10) # Added assignment of the result to `_` in order to illustrate that the return value will not be used\n    results = scorer.get_results()\n\n    for metric_name in results.keys():\n        average_score = np.average(results[metric_name])\n        print('%s : %f' % (metric_name, average_score))\n\n    print 'time', time.time() - start, '\\n\\n'\n</code></pre>\n<p>You can check and download this module from <a href=\"https://github.com/StKyr/multiscorer/\" rel=\"noreferrer\">GitHub</a>.\nHope it helps.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>from sklearn import model_selection\n\ndef error_metrics(model, train_data, train_targ, kfold):\n    scoring = [\"accuracy\",\"roc_auc\",\"neg_log_loss\",\"r2\",\n             \"neg_mean_squared_error\",\"neg_mean_absolute_error\"] \n\n    error_metrics = pd.DataFrame()\n    error_metrics[\"model\"] = model\n    for scor in scoring:\n        score = []\n        for mod in model:\n           \n            result = model_selection.cross_val_score(estimator= mod, X=train_data, y=train_targ,cv=kfold,scoring=scor )\n            score.append(result.mean())\n            \n        error_metrics[scor] =pd.Series(score)\n        \n    return error_metrics\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I understand neural networks with any number of hidden layers can approximate nonlinear functions, however, can it approximate:</p>\n<pre><code>f(x) = x^2\n</code></pre>\n<p>I can't think of how it could. It seems like a very obvious limitation of neural networks that can potentially limit what it can do. For example, because of this limitation, neural networks probably can't properly approximate many functions used in statistics like Exponential Moving Average, or even variance.</p>\n<p>Speaking of moving average, can recurrent neural networks properly approximate that? I understand how a feedforward neural network or even a single linear neuron can output a moving average using the sliding window technique, but how would recurrent neural networks do it without X amount of hidden layers (X being the moving average size)?</p>\n<p>Also, let us assume we don't know the original function <strong>f</strong>, which happens to get the average of the last 500 inputs, and then output a 1 if it's higher than 3, and 0 if it's not. But for a second, pretend we don't know that, it's a black box.</p>\n<p>How would a recurrent neural network approximate that? We would first need to know how many timesteps it should have, which we don't. Perhaps a LSTM network could, but even then, what if it's not a simple moving average, it's an exponential moving average? I don't think even LSTM can do it.</p>\n<p>Even worse still, what if <strong>f(x,x1)</strong> that we are trying to learn is simply</p>\n<pre><code>f(x,x1) = x * x1\n</code></pre>\n<p>That seems very simple and straightforward. Can a neural network learn it? I don't see how.</p>\n<p>Am I missing something huge here or are machine learning algorithms extremely limited? Are there other learning techniques besides neural networks that can actually do any of this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The key point to understand is <strong>compact</strong>: </p>\n<p>Neural networks (as any other approximation structure like, polynomials, splines, or Radial Basis Functions) can approximate any continuous function only within a <strong>compact set</strong>. </p>\n<p>In other words the theory states that, given:</p>\n<ol>\n<li>A continuous function <strong>f(x)</strong>,</li>\n<li>A finite range for the input <strong>x</strong>, <strong>[a,b]</strong>, and </li>\n<li>A desired approximation accuracy <strong>ε&gt;0</strong>, </li>\n</ol>\n<p>then there exists a neural network that approximates <strong>f(x)</strong> with an approximation error less than <strong>ε</strong>, everywhere within <strong>[a,b]</strong>. </p>\n<p>Regarding your example of <strong>f(x) = x<sup>2</sup></strong>, yes you can approximate it with a neural network within any finite range: <strong>[-1,1]</strong>, <strong>[0, 1000]</strong>, etc. To visualise this, imagine that you approximate <strong>f(x)</strong> within <strong>[-1,1]</strong> with a <a href=\"http://en.wikipedia.org/wiki/Step_function\" rel=\"noreferrer\">Step Function</a>. Can you do it on paper? Note that if you make the steps narrow enough you can achieve any desired accuracy. The way neural networks approximate <strong>f(x)</strong> is not much different than this. </p>\n<p>But again, there is no neural network (or any other approximation structure) with a finite number of parameters that can approximate <strong>f(x) = x<sup>2</sup></strong> for all <strong>x</strong> in <strong>[-∞, +∞]</strong>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Unfortunately many of the answers show how little practitioners seem to know about the theory of neural networks. The only rigorous theorem that exists about the ability of neural networks to approximate different kinds of functions is the Universal Approximation Theorem.</p>\n<p>The UAT states that any continuous function on a compact domain can be approximated by a neural network with only one hidden layer provided the activation functions used are BOUNDED, continuous and monotonically increasing. Now, a finite sum of bounded functions is bounded by definition.</p>\n<p>A polynomial is not bounded so the best we can do is provide a neural network approximation of that polynomial over a compact subset of R^n. Outside of this compact subset, the approximation will fail miserably as the polynomial will grow without bound. In other words, the neural network will work well on the training set but will not generalize!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am not sure why there is such a visceral reaction, I think it is a legitimate question that is hard to find by googling it, even though I think it is widely appreciated and repeated outloud. I think in this case you are looking for the actually citations showing that a neural net can approximate any function. <a href=\"http://jmlr.org/proceedings/papers/v32/andoni14.pdf\">This recent paper</a> explains it nicely, in my opinion. They also cite the original paper by Barron from 1993 that proved a less general result. The conclusion: a two-layer neural network can represent any bounded degree polynomial, under certain (seemingly non-restrictive) conditions.</p>\n<p>Just in case the link does not work, it is called \"Learning Polynomials with Neural Networks\" by Andoni et al., 2014.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to implement multivariate linear regression in Python using TensorFlow, but have run into some logical and implementation issues. My code throws the following error:</p>\n<pre><code>Attempting to use uninitialized value Variable\nCaused by op u'Variable/read'\n</code></pre>\n<p>Ideally the <code>weights</code> output should be <code>[2, 3]</code></p>\n<pre><code>def hypothesis_function(input_2d_matrix_trainingexamples,\n                        output_matrix_of_trainingexamples,\n                        initial_parameters_of_hypothesis_function,\n                        learning_rate, num_steps):\n    # calculate num attributes and num examples\n    number_of_attributes = len(input_2d_matrix_trainingexamples[0])\n    number_of_trainingexamples = len(input_2d_matrix_trainingexamples)\n\n    #Graph inputs\n    x = []\n    for i in range(0, number_of_attributes, 1):\n        x.append(tf.placeholder(\"float\"))\n    y_input = tf.placeholder(\"float\")\n\n    # Create Model and Set Model weights\n    parameters = []\n    for i in range(0, number_of_attributes, 1):\n        parameters.append(\n            tf.Variable(initial_parameters_of_hypothesis_function[i]))\n\n    #Contruct linear model\n    y = tf.Variable(parameters[0], \"float\")\n    for i in range(1, number_of_attributes, 1):\n        y = tf.add(y, tf.multiply(x[i], parameters[i]))\n\n    # Minimize the mean squared errors\n    loss = tf.reduce_mean(tf.square(y - y_input))\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    train = optimizer.minimize(loss)\n\n    #Initialize the variables\n    init = tf.initialize_all_variables()\n\n    # launch the graph\n    session = tf.Session()\n    session.run(init)\n    for step in range(1, num_steps + 1, 1):\n        for i in range(0, number_of_trainingexamples, 1):\n            feed = {}\n            for j in range(0, number_of_attributes, 1):\n                array = [input_2d_matrix_trainingexamples[i][j]]\n                feed[j] = array\n            array1 = [output_matrix_of_trainingexamples[i]]\n            feed[number_of_attributes] = array1\n            session.run(train, feed_dict=feed)\n\n    for i in range(0, number_of_attributes - 1, 1):\n        print (session.run(parameters[i]))\n\narray = [[0.0, 1.0, 2.0], [0.0, 2.0, 3.0], [0.0, 4.0, 5.0]]\nhypothesis_function(array, [8.0, 13.0, 23.0], [1.0, 1.0, 1.0], 0.01, 200)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Run this:</p>\n<pre><code>init = tf.global_variables_initializer()\nsess.run(init)\n</code></pre>\n<p>Or (depending on the version of TF that you have):</p>\n<pre><code>init = tf.initialize_all_variables()\nsess.run(init)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It's not 100% clear from the code example, but if the list <code>initial_parameters_of_hypothesis_function</code> is a list of <code>tf.Variable</code> objects, then the line <code>session.run(init)</code> will fail because TensorFlow isn't (yet) smart enough to figure out the dependencies in variable initialization. To work around this, you should change the loop that creates <code>parameters</code> to use <a href=\"https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#Variable.initialized_value\" rel=\"noreferrer\"><code>initial_parameters_of_hypothesis_function[i].initialized_value()</code></a>, which adds the necessary dependency:</p>\n<pre><code>parameters = []\nfor i in range(0, number_of_attributes, 1):\n    parameters.append(tf.Variable(\n        initial_parameters_of_hypothesis_function[i].initialized_value()))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is another the error happening which related to the order when calling initializing global variables. I've had the sample of code has similar error <strong>FailedPreconditionError (see above for traceback): Attempting to use uninitialized value W</strong></p>\n<pre><code>def linear(X, n_input, n_output, activation = None):\n    W = tf.Variable(tf.random_normal([n_input, n_output], stddev=0.1), name='W')\n    b = tf.Variable(tf.constant(0, dtype=tf.float32, shape=[n_output]), name='b')\n    if activation != None:\n        h = tf.nn.tanh(tf.add(tf.matmul(X, W),b), name='h')\n    else:\n        h = tf.add(tf.matmul(X, W),b, name='h')\n    return h\n\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\ng = tf.get_default_graph()\nprint([op.name for op in g.get_operations()])\nwith tf.Session() as sess:\n    # RUN INIT\n    sess.run(tf.global_variables_initializer())\n    # But W hasn't in the graph yet so not know to initialize \n    # EVAL then error\n    print(linear(np.array([[1.0,2.0,3.0]]).astype(np.float32), 3, 3).eval())\n</code></pre>\n<p>You should change to following</p>\n<pre><code>from tensorflow.python.framework import ops\nops.reset_default_graph()\ng = tf.get_default_graph()\nprint([op.name for op in g.get_operations()])\nwith tf.Session() as \n    # NOT RUNNING BUT ASSIGN\n    l = linear(np.array([[1.0,2.0,3.0]]).astype(np.float32), 3, 3)\n    # RUN INIT\n    sess.run(tf.global_variables_initializer())\n    print([op.name for op in g.get_operations()])\n    # ONLY EVAL AFTER INIT\n    print(l.eval(session=sess))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using sklearn for multi-classification task. I need to split alldata into train_set and test_set. I want to take randomly the same sample number from each class.\nActually, I amusing this function</p>\n<pre><code>X_train, X_test, y_train, y_test = cross_validation.train_test_split(Data, Target, test_size=0.3, random_state=0)\n</code></pre>\n<p>but it gives unbalanced dataset! Any suggestion.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Although Christian's suggestion is correct, technically <code>train_test_split</code> should give you stratified results by using the <code>stratify</code> param.</p>\n<p>So you could do:</p>\n<pre><code>X_train, X_test, y_train, y_test = cross_validation.train_test_split(Data, Target, test_size=0.3, random_state=0, stratify=Target)\n</code></pre>\n<p>The trick here is that <strong>it starts from version</strong> <code>0.17</code> in <code>sklearn</code>.</p>\n<p>From the documentation about the parameter <code>stratify</code>:</p>\n<blockquote>\n<p>stratify : array-like or None (default is None)\n  If not None, data is split in a stratified fashion, using this as the labels array.\n  New in version 0.17: stratify splitting</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\" rel=\"noreferrer\">StratifiedShuffleSplit</a> to create datasets featuring the same percentage of classes as the original one:</p>\n<pre><code>import numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nX = np.array([[1, 3], [3, 7], [2, 4], [4, 8]])\ny = np.array([0, 1, 0, 1])\nstratSplit = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=42)\nfor train_idx, test_idx in stratSplit:\n    X_train=X[train_idx]\n    y_train=y[train_idx]\n\nprint(X_train)\n# [[3 7]\n#  [2 4]]\nprint(y_train)\n# [1 0]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If the classes are not balanced but you want the split to be balanced, then stratifying isn't going to help. There doesn't seem to be a method for doing balanced sampling in sklearn but it's kind of easy using basic numpy, for example a function like this might help you:</p>\n<pre><code>def split_balanced(data, target, test_size=0.2):\n\n    classes = np.unique(target)\n    # can give test_size as fraction of input data size of number of samples\n    if test_size&lt;1:\n        n_test = np.round(len(target)*test_size)\n    else:\n        n_test = test_size\n    n_train = max(0,len(target)-n_test)\n    n_train_per_class = max(1,int(np.floor(n_train/len(classes))))\n    n_test_per_class = max(1,int(np.floor(n_test/len(classes))))\n\n    ixs = []\n    for cl in classes:\n        if (n_train_per_class+n_test_per_class) &gt; np.sum(target==cl):\n            # if data has too few samples for this class, do upsampling\n            # split the data to training and testing before sampling so data points won't be\n            #  shared among training and test data\n            splitix = int(np.ceil(n_train_per_class/(n_train_per_class+n_test_per_class)*np.sum(target==cl)))\n            ixs.append(np.r_[np.random.choice(np.nonzero(target==cl)[0][:splitix], n_train_per_class),\n                np.random.choice(np.nonzero(target==cl)[0][splitix:], n_test_per_class)])\n        else:\n            ixs.append(np.random.choice(np.nonzero(target==cl)[0], n_train_per_class+n_test_per_class,\n                replace=False))\n\n    # take same num of samples from all classes\n    ix_train = np.concatenate([x[:n_train_per_class] for x in ixs])\n    ix_test = np.concatenate([x[n_train_per_class:(n_train_per_class+n_test_per_class)] for x in ixs])\n\n    X_train = data[ix_train,:]\n    X_test = data[ix_test,:]\n    y_train = target[ix_train]\n    y_test = target[ix_test]\n\n    return X_train, X_test, y_train, y_test\n</code></pre>\n<p>Note that if you use this and sample more points per class than in the input data, then those will be upsampled (sample with replacement). As a result, some data points will appear multiple times and this may have an effect on the accuracy measures etc. And if some class has only one data point, there will be an error. You can easily check the numbers of points per class for example with <code>np.unique(target, return_counts=True)</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here in this code <code>UpSampling2D</code> and <code>Conv2DTranspose</code> seem to be used interchangeably. I want to know why this is happening. </p>\n<pre><code># u-net model with up-convolution or up-sampling and weighted binary-crossentropy as loss func\n\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\ndef unet_model(n_classes=5, im_sz=160, n_channels=8, n_filters_start=32, growth_factor=2, upconv=True,\n               class_weights=[0.2, 0.3, 0.1, 0.1, 0.3]):\n    droprate=0.25\n    n_filters = n_filters_start\n    inputs = Input((im_sz, im_sz, n_channels))\n    #inputs = BatchNormalization()(inputs)\n    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    #pool1 = Dropout(droprate)(pool1)\n\n    n_filters *= growth_factor\n    pool1 = BatchNormalization()(pool1)\n    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    pool2 = Dropout(droprate)(pool2)\n\n    n_filters *= growth_factor\n    pool2 = BatchNormalization()(pool2)\n    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    pool3 = Dropout(droprate)(pool3)\n\n    n_filters *= growth_factor\n    pool3 = BatchNormalization()(pool3)\n    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)\n    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)\n    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)\n    pool4_1 = Dropout(droprate)(pool4_1)\n\n    n_filters *= growth_factor\n    pool4_1 = BatchNormalization()(pool4_1)\n    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)\n    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)\n    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n    pool4_2 = Dropout(droprate)(pool4_2)\n\n    n_filters *= growth_factor\n    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)\n    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)\n\n    n_filters //= growth_factor\n    if upconv:\n        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])\n    else:\n        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])\n    up6_1 = BatchNormalization()(up6_1)\n    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)\n    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)\n    conv6_1 = Dropout(droprate)(conv6_1)\n\n    n_filters //= growth_factor\n    if upconv:\n        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])\n    else:\n        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])\n    up6_2 = BatchNormalization()(up6_2)\n    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)\n    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)\n    conv6_2 = Dropout(droprate)(conv6_2)\n\n    n_filters //= growth_factor\n    if upconv:\n        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])\n    else:\n        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])\n    up7 = BatchNormalization()(up7)\n    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)\n    conv7 = Dropout(droprate)(conv7)\n\n    n_filters //= growth_factor\n    if upconv:\n        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n    else:\n        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])\n    up8 = BatchNormalization()(up8)\n    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)\n    conv8 = Dropout(droprate)(conv8)\n\n    n_filters //= growth_factor\n    if upconv:\n        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n    else:\n        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])\n    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(n_classes, (1, 1), activation='sigmoid')(conv9)\n\n    model = Model(inputs=inputs, outputs=conv10)\n\n    def weighted_binary_crossentropy(y_true, y_pred):\n        class_loglosses = K.mean(K.binary_crossentropy(y_true, y_pred), axis=[0, 1, 2])\n        return K.sum(class_loglosses * K.constant(class_weights))\n\n    model.compile(optimizer=Adam(), loss=weighted_binary_crossentropy)\n    return model\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>UpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling, so nothing smart. Advantage is it's cheap.</p>\n<p>Conv2DTranspose is a convolution operation whose kernel is learnt (just like normal conv2d operation) while training your model. Using Conv2DTranspose will also upsample its input but the key difference is the model should learn what is the best upsampling for the job.</p>\n<p>EDIT: Link to nice visualization of transposed convolution: <a href=\"https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\" rel=\"noreferrer\">https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How <code>train_on_batch()</code> is different from <code>fit()</code>? What are the cases when we should use <code>train_on_batch()</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For this question, it's a <a href=\"https://github.com/keras-team/keras/issues/2708#issuecomment-218803413\" rel=\"noreferrer\">simple answer from the primary author</a>:</p>\n<blockquote>\n<p>With <code>fit_generator</code>, you can use a generator for the validation data as\n  well. In general, I would recommend using <code>fit_generator</code>, but using\n  <code>train_on_batch</code> works fine too. These methods only exist for the sake of\n  convenience in different use cases, there is no \"correct\" method.</p>\n</blockquote>\n<p><code>train_on_batch</code> allows you to expressly update weights based on a collection of samples you provide, without regard to any fixed batch size. You would use this in cases when that is what you want: to train on an explicit collection of samples. You could use that approach to maintain your own iteration over multiple batches of a traditional training set but allowing <code>fit</code> or <code>fit_generator</code> to iterate batches for you is likely simpler.</p>\n<p>One case when it might be nice to use <code>train_on_batch</code> is for updating a pre-trained model on a single new batch of samples. Suppose you've already trained and deployed a model, and sometime later you've received a new set of training samples previously never used. You could use <code>train_on_batch</code> to directly update the existing model only on those samples. Other methods can do this too, but it is rather explicit to use <code>train_on_batch</code> for this case.</p>\n<p>Apart from special cases like this (either where you have some pedagogical reason to maintain your own cursor across different training batches, or else for some type of semi-online training update on a special batch), it is probably better to just always use <code>fit</code> (for data that fits in memory) or <code>fit_generator</code> (for streaming batches of data as a generator).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>train_on_batch()</code> gives you greater control of the state of the LSTM, for example, when using a stateful LSTM and controlling calls to <code>model.reset_states()</code> is needed. You may have multi-series data and need to reset the state after each series, which you can do with <code>train_on_batch()</code>, but if you used <code>.fit()</code> then the network would be trained on all the series of data without resetting the state.  There's no right or wrong, it depends on what data you're using, and how you want the network to behave.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Train_on_batch will also see a performance increase over fit and fit generator if youre using large datasets and don't have easily serializable data (like high rank numpy arrays), to write to tfrecords.</p>\n<p>In this case you can save the arrays as numpy files and load up smaller subsets of them (traina.npy, trainb.npy etc) in memory, when the whole set won't fit in memory. You can then use tf.data.Dataset.from_tensor_slices and then using train_on_batch with your subdataset, then loading up another dataset and calling train on batch again, etc, now you've trained on your entire set and can control exactly how much and what of your dataset trains your model. You can then define your own epochs, batch sizes, etc with simple loops and functions to grab from your dataset.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools.</p>\n<p>Additionally, I will be grateful if you point any Python based solution / library for this.</p>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document.</p>\n<p>To identify keywords like this, you could use the <a href=\"http://en.wikipedia.org/wiki/Pointwise_mutual_information\" rel=\"noreferrer\">point-wise mutual information</a> of the keyword and the document. This is given by <code>PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]</code>. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection.</p>\n<p>To identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score. </p>\n<p>If you want to extract <strong>multiword tags</strong>, see the StackOverflow question <a href=\"https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries\">How to extract common / significant phrases from a series of text entries</a>. </p>\n<p>Borrowing from my answer to that question, the <a href=\"http://www.nltk.org/howto/collocations.html\" rel=\"noreferrer\">NLTK collocations how-to</a> covers how to do \nextract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.:</p>\n<pre><code>import nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n   nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3) \n\n# return the 5 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 5)  \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First, the key python library for computational linguistics is <a href=\"http://nltk.sourceforge.net/index.php/Main_Page\" rel=\"noreferrer\">NLTK</a> (\"<strong>Natural Language Toolkit</strong>\"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">collection</a> of tutorials, FAQs, etc. I recommend it highly.</p>\n<p>Below is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic.</p>\n<p>Keywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a <em>priori</em> knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency.</p>\n<pre><code>text = \"\"\" The intensity of the feeling makes up for the disproportion of the objects.  Things are equal to the imagination, which have the power of affecting the mind with an equal degree of terror, admiration, delight, or love.  When Lear calls upon the heavens to avenge his cause, \"for they are old like him,\" there is nothing extravagant or impious in this sublime identification of his age with theirs; for there is no other image which could do justice to the agonising sense of his wrongs and his despair! \"\"\"\n\nBAD_CHARS = \".!?,\\'\\\"\"\n\n# transform text into a list words--removing punctuation and filtering small words\nwords = [ word.strip(BAD_CHARS) for word in text.strip().split() if len(word) &gt; 4 ]\n\nword_freq = {}\n\n# generate a 'word histogram' for the text--ie, a list of the frequencies of each word\nfor word in words :\n  word_freq[word] = word_freq.get(word, 0) + 1\n\n# sort the word list by frequency \n# (just a DSU sort, there's a python built-in for this, but i can't remember it)\ntx = [ (v, k) for (k, v) in word_freq.items()]\ntx.sort(reverse=True)\nword_freq_sorted = [ (k, v) for (v, k) in tx ]\n\n# eg, what are the most common words in that text?\nprint(word_freq_sorted)\n# returns: [('which', 4), ('other', 4), ('like', 4), ('what', 3), ('upon', 3)]\n# obviously using a text larger than 50 or so words will give you more meaningful results\n\nterm_importance = lambda word : 1.0/word_freq[word]\n\n# select document keywords from the words at/near the top of this list:\nmap(term_importance, word_freq.keys())\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a> tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.  </p>\n<p>I had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.  </p>\n<p>I recommend you try playing with <a href=\"http://mallet.cs.umass.edu/\" rel=\"noreferrer\">http://mallet.cs.umass.edu/</a> and seeing if this model fits your needs..  </p>\n<p>LDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is \"epoch\" in <code>keras.models.Model.fit</code>? Is it one gradient update? If it is more than one gradient update, then what is defining an epoch?</p>\n<p>Suppose I am feeding my own batches to <code>fit</code>. I would regard \"epoch\" as finishing to process entire training set (is this correct)? Then how to control keras for this way? Can I set <code>batch_size</code> equal to <code>x</code> and <code>y</code> size and <code>epochs</code> to 1?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is how Keras <a href=\"https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean\" rel=\"noreferrer\">documentation</a> defines an epoch:</p>\n<blockquote>\n<p><strong>Epoch</strong>: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.</p>\n</blockquote>\n<p>So, in other words, a number of epochs means how many times you go through your training set. </p>\n<p>The model is updated each time a batch is processed, which means that it can be updated multiple times during one epoch. If <code>batch_size</code> is set equal to the length of <code>x</code>, then the model will be updated once per epoch.   </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've gone through the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Fold.html\" rel=\"noreferrer\">official doc</a>. I'm having a hard time understanding what this function is used for and how it works. Can someone explain this in layman's terms?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>unfold</code> imagines a tensor as a longer tensor with repeated columns/rows of values 'folded' on top of each other, which is then \"unfolded\":</p>\n<ul>\n<li><code>size</code> determines how large the folds are</li>\n<li><code>step</code> determines how often it is folded</li>\n</ul>\n<p>E.g. for a 2x5 tensor, unfolding it with <code>step=1</code>, and patch <code>size=2</code> across <code>dim=1</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = torch.tensor([[1,2,3,4,5],\n                  [6,7,8,9,10]])\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; x.unfold(1,2,1)\ntensor([[[ 1,  2], [ 2,  3], [ 3,  4], [ 4,  5]],\n        [[ 6,  7], [ 7,  8], [ 8,  9], [ 9, 10]]])\n</code></pre>\n<p><a href=\"https://i.sstatic.net/uCrOg.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/uCrOg.png\"/></a></p>\n<p><code>fold</code> is roughly the opposite of this operation, but \"overlapping\" values are summed in the output.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <a href=\"https://pytorch.org/docs/stable/nn.html?highlight=unfold#torch.nn.functional.unfold\" rel=\"noreferrer\"><code>unfold</code></a> and <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.functional.fold\" rel=\"noreferrer\"><code>fold</code></a> are used to facilitate \"sliding window\" operations (like convolutions). Suppose you want to apply a function <code>foo</code> to every <code>5x5</code> window in a feature map/image:</p>\n<pre><code>from torch.nn import functional as f\nwindows = f.unfold(x, kernel_size=5)\n</code></pre>\n<p>Now <code>windows</code> has <code>size</code> of batch-(5<em>5</em><code>x.size(1)</code>)-num_windows, you can apply <code>foo</code> on <code>windows</code>:</p>\n<pre><code>processed = foo(windows)\n</code></pre>\n<p>Now you need to \"fold\" <code>processed</code> back to the original size of <code>x</code>:</p>\n<pre><code>out = f.fold(processed, x.shape[-2:], kernel_size=5)\n</code></pre>\n<p>You need to take care of <code>padding</code>, and <code>kernel_size</code> that may affect your ability to \"fold\" back <code>processed</code> to the size of <code>x</code>. Moreover, <code>fold</code> <em>sums</em> over overlapping elements, so you might want to divide the output of <code>fold</code> by patch size.</p>\n<hr/>\n<p>Please note that <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html\" rel=\"noreferrer\"><code>torch.unfold</code></a> performs a different operation than <code>nn.Unfold</code>. See <a href=\"https://stackoverflow.com/a/73276874/1714410\">this thread</a> for details.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>One dimensional unfolding is easy:</h2>\n<pre><code>x = torch.arange(1, 9).float()\nprint(x)\n# dimension, size, step\nprint(x.unfold(0, 2, 1))\nprint(x.unfold(0, 3, 2))\n</code></pre>\n<p>Out:</p>\n<pre><code>tensor([1., 2., 3., 4., 5., 6., 7., 8.])\ntensor([[1., 2.],\n        [2., 3.],\n        [3., 4.],\n        [4., 5.],\n        [5., 6.],\n        [6., 7.],\n        [7., 8.]])\ntensor([[1., 2., 3.],\n        [3., 4., 5.],\n        [5., 6., 7.]])\n</code></pre>\n<h2>Two dimensional unfolding (also called <em>patching</em>)</h2>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\npatch=(3,3)\nx=torch.arange(16).float()\nprint(x, x.shape)\nx2d = x.reshape(1,1,4,4)\nprint(x2d, x2d.shape)\nh,w = patch\nc=x2d.size(1)\nprint(c) # channels\n# unfold(dimension, size, step)\nr = x2d.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1, c, h, w)\nprint(r.shape)\nprint(r) # result\n</code></pre>\n<pre><code>tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15.]) torch.Size([16])\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]]) torch.Size([1, 1, 4, 4])\n1\ntorch.Size([4, 1, 3, 3])\n\ntensor([[[[ 0.,  1.,  2.],\n          [ 4.,  5.,  6.],\n          [ 8.,  9., 10.]]],\n\n\n        [[[ 4.,  5.,  6.],\n          [ 8.,  9., 10.],\n          [12., 13., 14.]]],\n\n\n        [[[ 1.,  2.,  3.],\n          [ 5.,  6.,  7.],\n          [ 9., 10., 11.]]],\n\n\n        [[[ 5.,  6.,  7.],\n          [ 9., 10., 11.],\n          [13., 14., 15.]]]])\n</code></pre>\n<p><a href=\"https://i.sstatic.net/ZYIe3.png\" rel=\"noreferrer\"><img alt=\"patching\" src=\"https://i.sstatic.net/ZYIe3.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/1148727/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2014-02-24 14:58:37Z\">10 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/1148727/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm wondering how hard it would be to implement a <strong>chess engine</strong>. Are there already open-source implementations? </p>\n<p>It seems that you'd need a scoring function for a given board constellation, and a very fast way of exploring several likely future board constellations. Exploring all possible future moves is of course impossible, so one could greedily follow the most promising moves, or use approximate techniques like <a href=\"http://en.wikipedia.org/wiki/Simulated_annealing\" rel=\"noreferrer\">simulated annealing</a> to follow likely moves probabilistically.</p>\n<p>Do you think that is within the scope of a <a href=\"http://en.wikipedia.org/wiki/Machine_learning\" rel=\"noreferrer\">machine learning</a> graduate student project -- assuming there was an open-source implementation that the students could use, that does the basic things like returning the next possible moves for a given figure? Probably too hard? </p>\n<p>It would be a fun project to have different teams work on chess engines and then let them play against each other ... </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have spent the last year building my own chess engine in C#.  It was not all that difficult.  During my work I have made mistakes, I have found that information on the internet was just not presented clearly, and much of it was simply copied from other sites.  </p>\n<p>In order to make life easier for someone else going through this process, I have been documenting the development of my chess engine and posted much of the source code on my blog:</p>\n<p><a href=\"http://www.chessbin.com\" rel=\"noreferrer\">http://www.chessbin.com</a></p>\n<p>I have even created a <a href=\"http://www.chessbin.com/page/Chess-Game-Starer-Kit.aspx\" rel=\"noreferrer\">Chess Game Development Kit</a> that will get you started in developing your own chess engine, which contains:</p>\n<ol>\n<li>All the code necessary to represent a chess board and chess pieces</li>\n<li>Code related to validating chess piece movement</li>\n<li>Graphical User Interface that displays the chess position and allows you to move pieces around the board</li>\n</ol>\n<p>My site is basically dedicated for people just like you; people that want to get started on building their own chess engine.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, this is definitely within the scope of a student project. Here are some links from my archive to get you started:</p>\n<ul>\n<li>This is a useful <a href=\"https://www.chessprogramming.org/Main_Page\" rel=\"nofollow noreferrer\">chess programming wiki</a>.</li>\n<li>This is a <a href=\"http://www.gamedev.net/reference/programming/features/chess1/\" rel=\"nofollow noreferrer\">simple introduction</a> to chess programming.</li>\n<li>This is a <a href=\"http://www.frayn.net/beowulf/theory.html\" rel=\"nofollow noreferrer\">more advanced introduction</a>.</li>\n<li>This is a <a href=\"http://www.cs.vu.nl/~aske/mtdf.html\" rel=\"nofollow noreferrer\">good analysis of MTD</a>, a sophisticated search algorithm.</li>\n<li>This is a good guide to <a href=\"http://mediocrechess.blogspot.com/2007/01/guide-perft-scores.html\" rel=\"nofollow noreferrer\">validation of move generation</a>.</li>\n<li>This describes the <a href=\"http://www.npac.syr.edu/copywrite/pcw/node341.html\" rel=\"nofollow noreferrer\">basic architecture of a chess program</a>.</li>\n<li>This is lots of <a href=\"http://people.csail.mit.edu/heinz/dt/\" rel=\"nofollow noreferrer\">good information on the Dark Thought</a> chess program.</li>\n<li>These are <a href=\"http://www.maths.nott.ac.uk/personal/anw/G13GT1/compch.html\" rel=\"nofollow noreferrer\">more notes on chess programming</a>.</li>\n<li>A reasonable <a href=\"http://www.cis.uab.edu/hyatt/bitmaps.html\" rel=\"nofollow noreferrer\">introduction to rotated bitboards</a>.</li>\n<li>A reasonable <a href=\"http://chessprogramming.wikispaces.com/Magic+Bitboards\" rel=\"nofollow noreferrer\">introduction to magic bitboards</a>.</li>\n<li>Here is an old report from <a href=\"http://www.csbruce.com/~csbruce/chess/\" rel=\"nofollow noreferrer\">2 students who wrote a chess program</a>.</li>\n<li>Finally, here is <a href=\"http://en.wikipedia.org/wiki/Computer_chess\" rel=\"nofollow noreferrer\">Wikipedia's take on computer chess</a>.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://www.craftychess.com/\" rel=\"noreferrer\">Crafty</a> is one of the top chess engines and completely open source. However I would discourage you from using it for a student project it's written in C, very complex and very hard to understand because it is highly optimized.</p>\n<p>For educational purposes I would recommend taking a look at <a href=\"http://www.chessbin.com/\" rel=\"noreferrer\">Adam Berents site</a> where he describes the process he went through when he implemented a chess engine in C#. The source code is available as well of course. It's an excellent point to start from, in my opinion.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I know that principal component analysis does a SVD on a matrix and then generates an eigen value matrix. To select the principal components we have to take only the first few eigen values. Now, how do we decide on the number of eigen values that we should take from the eigen value matrix?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To decide how many eigenvalues/eigenvectors to keep, you should consider your reason for doing PCA in the first place.  Are you doing it for reducing storage requirements, to reduce dimensionality for a classification algorithm, or for some other reason?  If you don't have any strict constraints, I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order).  If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is no correct answer, it is somewhere between 1 and n.</p>\n<p>Think of a principal component as a street in a town you have never visited before. How many streets should you take to get to know the town?</p>\n<p>Well, you should obviously visit the main street (the first component), and maybe some of the other big streets too. Do you need to visit every street to know the town well enough? Probably not.</p>\n<p>To know the town perfectly, you should visit all of the streets. But what if you could visit, say 10 out of the 50 streets, and have a 95% understanding of the town? Is that good enough? </p>\n<p>Basically, you should select enough components to explain enough of the variance that you are comfortable with. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As others said, it doesn't hurt to plot the explained variance.</p>\n<p>If you use PCA as a preprocessing step for a supervised learning task, you should cross validate the whole data processing pipeline and treat the number of PCA dimension as an hyperparameter to select using a grid search on the final supervised score (e.g. F1 score for classification or RMSE for regression).</p>\n<p>If cross-validated grid search on the whole dataset is too costly try on a 2 sub samples, e.g. one with 1% of the data and the second with 10% and see if you come up with the same optimal value for the PCA dimensions.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am starting to work with TensorFlow library for deep learning, <a href=\"https://www.tensorflow.org/\">https://www.tensorflow.org/</a>. </p>\n<p>I found a explicit guide to work on it on linux and Mac but I did not find how to work with it under Windows. I try over the net, but the information are lacking. </p>\n<p>I use Visual Studio 2015 for my projects, and I am trying to compile the library with Visual studio Compiler VC14.</p>\n<p>How to install it and to use it under Windows?</p>\n<p>Can I use <a href=\"https://github.com/dslomov/bazel-windows\">Bazel for Windows </a> for production use?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>How to install TensorFlow and to use it under Windows?</p>\n</blockquote>\n<p><strong>Updated on 8/4/16</strong> </p>\n<p>Windows 10 now has a <a href=\"http://www.ubuntu.com/\" rel=\"noreferrer\">Ubuntu</a> Bash environment, AKA <a href=\"https://msdn.microsoft.com/en-us/commandline/wsl/about\" rel=\"noreferrer\">Bash on Ubuntu on Windows</a>, available as a standard option (as opposed to <a href=\"https://developer.microsoft.com/en-us/windows/downloads/windows-10-developer-preview\" rel=\"noreferrer\">Insider Preview updates for developers</a>). (StackOverflow tag <a href=\"https://stackoverflow.com/questions/tagged/wsl\">wsl</a>) This option came with the <a href=\"https://support.microsoft.com/en-us/help/12387/windows-10-update-history\" rel=\"noreferrer\">Windows 10 anniversary update</a> (Version 1607) released on 8/2/2016. This allows the use of <a href=\"https://help.ubuntu.com/community/AptGet/Howto\" rel=\"noreferrer\">apt-get</a> to install software packages such as <a href=\"https://www.python.org/\" rel=\"noreferrer\">Python</a> and <a href=\"https://www.tensorflow.org/\" rel=\"noreferrer\">TensorFlow</a>.</p>\n<p>Note: Bash on Ubuntu on Windows does not have access to the GPU, so all of the GPU options for installing TensorFlow will not work.</p>\n<p>The dated <a href=\"https://msdn.microsoft.com/en-us/commandline/wsl/install_guide\" rel=\"noreferrer\">installation instructions</a> for Bash on Ubuntu on Windows are basically correct, but only these steps are necessary:<br/>\nPrerequisites<br/>\nEnable the Windows Subsystem for Linux feature (GUI)<br/>\nReboot when prompted<br/>\nRun Bash on Windows  </p>\n<p>Steps no longer needed:<br/>\nTurn on Developer Mode<br/>\nEnable the Windows Subsystem for Linux feature (command-line)  </p>\n<p>Then install TensorFlow using apt-get</p>\n<pre><code>sudo apt-get install python3-pip python3-dev\nsudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl \n</code></pre>\n<p>and now test TensorFlow</p>\n<pre><code>$ python3\n...\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')\n&gt;&gt;&gt; sess = tf.Session()\n&gt;&gt;&gt; print(sess.run(hello))\nHello, TensorFlow!\n&gt;&gt;&gt; a = tf.constant(10)\n&gt;&gt;&gt; b = tf.constant(32)\n&gt;&gt;&gt; print(sess.run(a + b))\n42\n&gt;&gt;&gt; exit()\n</code></pre>\n<p>and run an actual neural network</p>\n<pre><code>python3 -m tensorflow.models.image.mnist.convolutional\n</code></pre>\n<h2>Earlier Answer</h2>\n<p>After learning about the developer preview of Bash on Windows.</p>\n<p>See <a href=\"http://www.hanselman.com/blog/PlayingWithTensorFlowOnWindows.aspx\" rel=\"noreferrer\">Playing with TensorFlow on Windows</a> by Scott Hanselman which uses <a href=\"http://www.hanselman.com/blog/DevelopersCanRunBashShellAndUsermodeUbuntuLinuxBinariesOnWindows10.aspx\" rel=\"noreferrer\">Bash on Windows 10</a> </p>\n<h2>Original Answer</h2>\n<p><strong>Bazel is the problem</strong></p>\n<p>TensorFlow is not made with <a href=\"https://en.wikipedia.org/wiki/List_of_build_automation_software\" rel=\"noreferrer\">build automation tools</a> such as <a href=\"https://en.wikipedia.org/wiki/Make_(software)\" rel=\"noreferrer\">make</a>, but with Google's in-house build tool <a href=\"http://bazel.io/faq.html\" rel=\"noreferrer\">Bazel</a>. Bazel only works on systems based on <a href=\"http://www.unix.org/\" rel=\"noreferrer\">Unix</a> such as <a href=\"https://en.wikipedia.org/wiki/Linux\" rel=\"noreferrer\">Linux</a> and <a href=\"http://www.apple.com/osx/what-is/\" rel=\"noreferrer\">OS X</a>.  </p>\n<p>Since the current published/known means to build TensorFlow uses <a href=\"http://bazel.io/\" rel=\"noreferrer\">Bazel</a> and Bazel does not work on Windows, one can not install or run TensorFlow natively on Windows.  </p>\n<p>From <a href=\"http://bazel.io/faq.html\" rel=\"noreferrer\">Bazel FAQ</a></p>\n<blockquote>\n<p>What about Windows?</p>\n<p>Due to its UNIX heritage, porting Bazel to Windows is significant\n  work. For example, Bazel uses symlinks extensively, which has varying\n  levels of support across Windows versions.</p>\n<p>We are currently actively working on improving Windows support, but\n  it's still ways from being usable.</p>\n</blockquote>\n<p><em>Status</em></p>\n<p>See: <a href=\"https://github.com/tensorflow/tensorflow/issues/17\" rel=\"noreferrer\">TensorFlow issue #17</a><br/>\nSee: <a href=\"https://github.com/bazelbuild/bazel/issues/276\" rel=\"noreferrer\">Bazel issue #276</a></p>\n<h2>Solutions</h2>\n<p>The solutions are listed in the order of complexity and work needed; from about an hour to may not even work.</p>\n<ol>\n<li><em><a href=\"https://www.docker.com/what-docker\" rel=\"noreferrer\">Docker</a></em><br/>\n~ 1 hour</li>\n</ol>\n<p><a href=\"https://www.tensorflow.org/versions/master/get_started/os_setup.html#docker-installation\" rel=\"noreferrer\">Docker installation</a></p>\n<p>Docker is a system to build self contained versions of a Linux operating system running on your machine. When you install and run TensorFlow via Docker it completely isolates the installation from pre-existing packages on your machine.</p>\n<p>Also look at <a href=\"https://stackoverflow.com/q/34694701/1243762\">TensorFlow - which Docker image to use?</a></p>\n<ol start=\"2\">\n<li><em>OS X</em><br/>\n~ 1 hour</li>\n</ol>\n<p>If you have a current Mac running OS X then see: <a href=\"https://www.tensorflow.org/versions/master/get_started/os_setup.html#installation-for-mac-os-x\" rel=\"noreferrer\">Installation for Mac OS X</a></p>\n<ol start=\"3\">\n<li><em>Linux</em></li>\n</ol>\n<p>The <a href=\"https://stackoverflow.com/questions/34664602/building-new-tensorflow-op-is-there-a-build-system-standard\">recommend Linux system</a> tends to be <a href=\"http://www.ubuntu.com/\" rel=\"noreferrer\">Ubuntu 14.04 LTS</a> (<a href=\"http://www.ubuntu.com/download\" rel=\"noreferrer\">Download page</a>).</p>\n<p>a. Virtual Machine - <a href=\"https://en.wikipedia.org/wiki/Hardware_virtualization#Full_virtualization\" rel=\"noreferrer\">Hardware Virtualization - Full Virtualization</a><br/>\n     ~ 3 hours</p>\n<p>Download and install a virtual machine such as the commercial <a href=\"http://www.vmware.com/\" rel=\"noreferrer\">VMware</a> or the free <a href=\"https://www.virtualbox.org/wiki/Downloads\" rel=\"noreferrer\">Virtual Box</a>, after which you can install Linux and then install TensorFlow.  </p>\n<p>When you go to install TensorFlow you will be using <a href=\"https://en.wikipedia.org/wiki/Pip_(package_manager)\" rel=\"noreferrer\">Pip</a> - Python's package management system. Visual Studio users should think NuGet. The packages are known as <a href=\"http://pythonwheels.com/\" rel=\"noreferrer\">wheels</a>.</p>\n<p>See: <a href=\"https://www.tensorflow.org/versions/master/get_started/os_setup.html#overview\" rel=\"noreferrer\">Pip Installation</a></p>\n<p>If you need to build from the source then see: <a href=\"https://www.tensorflow.org/versions/master/get_started/os_setup.html#installing-from-sources\" rel=\"noreferrer\">Installing From Sources</a><br/>\n~ 4 hours</p>\n<p>Note: If you plan on using a Virtual Machine and have never done so before, consider using the Docker option instead, since Docker is the Virtual Machine, OS and TensorFlow all packaged together.</p>\n<p>b. <a href=\"https://en.wikipedia.org/wiki/Multi-booting\" rel=\"noreferrer\">Dual boot</a><br/>\n     ~ 3 hours</p>\n<p>If you want to run TensorFlow on the same machine that you have Windows and make use of the GPU version then you will most likely have to use this option as running on a hosted Virtual Machine, <a href=\"https://en.wikipedia.org/wiki/Hypervisor\" rel=\"noreferrer\">Type 2 hypervisor</a>, will not allow you access to the GPU. </p>\n<ol start=\"4\">\n<li><em>Remote machine</em><br/>\n~ 4 hours</li>\n</ol>\n<p>If you have <a href=\"https://en.wikipedia.org/wiki/Remote_desktop_software\" rel=\"noreferrer\">remote access</a> to another machine that you can install the Linux OS and TensorFlow software on and allow remote connections to, then you can use your Windows machine to present the remote machine as an application running on Windows.</p>\n<ol start=\"5\">\n<li><em>Cloud Service</em><br/>\nI have no experience with this. Please edit answer if you know.</li>\n</ol>\n<p><a href=\"https://en.wikipedia.org/wiki/Cloud_computing\" rel=\"noreferrer\">Cloud</a> services such as <a href=\"https://aws.amazon.com/\" rel=\"noreferrer\">AWS</a> are being used.  </p>\n<p>From <a href=\"https://www.tensorflow.org/\" rel=\"noreferrer\">TensorFlow Features</a></p>\n<blockquote>\n<p>Want to run the model as a service in the cloud?\n  Containerize with Docker and TensorFlow just works.</p>\n</blockquote>\n<p>From <a href=\"https://www.docker.com/\" rel=\"noreferrer\">Docker</a></p>\n<blockquote>\n<p>Running Docker on AWS provides a highly reliable, low-cost way to\n  quickly build, ship, and run distributed applications at scale. Deploy\n  Docker using AMIs from the AWS Marketplace.</p>\n</blockquote>\n<ol start=\"6\">\n<li>Wait for Bazel to work on Windows.</li>\n</ol>\n<p>Currently it appears the only hold up is Bazel, however <a href=\"http://bazel.io/roadmap.html\" rel=\"noreferrer\">Bazel's roadmap</a> list working on Windows should be available this year.</p>\n<p>There are two features listed for Windows:</p>\n<pre><code>2016‑02  Bazel can bootstrap itself on Windows without requiring admin privileges.  \n\n2016‑12  Full Windows support for Android: Android feature set is identical for Windows and Linux/OS X.\n</code></pre>\n<ol start=\"7\">\n<li>Build TensorFlow by hand.<br/>\nA few days or more depending on you skill level. I gave up on this one; too many subprojects to build and files to locate.</li>\n</ol>\n<p>Remember that Bazel is only used to build TensorFlow. If you get the commands Bazel runs and the correct source code and libraries you should be able to build TensorFlow on Windows. See: <a href=\"https://stackoverflow.com/q/33983711/1243762\">How do I get the commands executed by Bazel</a>.  </p>\n<p>While I have not researched this more, you can look at the <a href=\"https://en.wikipedia.org/wiki/Continuous_integration\" rel=\"noreferrer\">continuous integration</a> info for needed files and info on how to they build it for testing. (<a href=\"https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/tools/ci_build/README.md\" rel=\"noreferrer\">Readme</a>) (<a href=\"http://ci.tensorflow.org/\" rel=\"noreferrer\">site</a>)</p>\n<ol start=\"8\">\n<li>Build Bazel on Windows<br/>\nA few days or more depending on you skill level. I gave up on this one also; could not find the necessary source files needed for Windows.</li>\n</ol>\n<p>There is a public experimental source code version of <a href=\"https://github.com/bazelbuild/bazel/blob/master/site/docs/windows.md\" rel=\"noreferrer\">Bazel that boots on Windows</a>. You may be able to leverage this into getting Bazel to work on Windows, etc.  </p>\n<p>Also these solutions require the use of <a href=\"https://cygwin.com/\" rel=\"noreferrer\">Cygwin</a> or <a href=\"http://www.mingw.org/\" rel=\"noreferrer\">MinGW</a> which adds another layer of complexity.</p>\n<ol start=\"9\">\n<li>Use alternative build system such as Make<br/>\nIf you get this one to work I would like to see in on GitHub.</li>\n</ol>\n<p>This currently does not exist for TensorFlow. It is a feature request.</p>\n<p>See: <a href=\"https://github.com/tensorflow/tensorflow/issues/380\" rel=\"noreferrer\">TensorFlow issue 380</a></p>\n<ol start=\"10\">\n<li>Cross Build<br/>\nIf you get this one to work I would like to see in on GitHub.</li>\n</ol>\n<p>You build TensorFlow on Linux using Bazel but change the build process to output a wheel that can be installed on Windows. This will require detailed knowledge of Bazel to change the configuration, and locating the source code and libraries that work with Windows. An option I would only suggest as a last resort. It may not even be possible.</p>\n<ol start=\"11\">\n<li>Run on the new Windows Subsystem for Linux.</li>\n</ol>\n<p>See: <a href=\"https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/\" rel=\"noreferrer\">Windows Subsystem for Linux Overview</a></p>\n<p>You will know as much as I do by reading the referenced article. </p>\n<blockquote>\n<p>Can I use Bazel for Windows for production use?</p>\n</blockquote>\n<p>Since it is experimental software I would not use on a production machine.</p>\n<p>Remember that you only need Bazel to build TensorFlow. So use the experimental code on a non production machine to build the <a href=\"http://pythonwheels.com/\" rel=\"noreferrer\">wheel</a>, then install the wheel on a production machine. See: <a href=\"https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation\" rel=\"noreferrer\">Pip Installation</a></p>\n<p>TLDR;</p>\n<p>Currently I have several versions for learning. Most use a VMWare 7.1 Workstation to host Ubuntu 14.04 LTS or Ubuntu 15 or Debian. I also have one dual boot of Ubuntu 14.04 LTS on my Windows machine to access the GPU as the machine with VMware does not have the proper GPU. I would recommend that you give these machines at least 8G of memory either as RAM or RAM and swap space as I have run out of memory a few times.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can confirm that it works in the Windows Subsystem for Linux!\nAnd it is also very straightforward.</p>\n<p>In the Ubuntu Bash on Windows 10, first update the package index:</p>\n<pre><code>apt-get update\n</code></pre>\n<p>Then install pip for Python 2:</p>\n<pre><code>sudo apt-get install python-pip python-dev\n</code></pre>\n<p>Install tensorflow:</p>\n<pre><code>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n</code></pre>\n<p>The package is now installed an you can run the CNN sample on the MNIST set:</p>\n<pre><code>cd /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist\n\npython convolutional.py\n</code></pre>\n<p>I just tested the CPU package for now.</p>\n<p>I blogged about it: <a href=\"http://blog.mosthege.net/2016/05/11/running-tensorflow-with-native-linux-binaries-in-the-windows-subsystem-for-linux/\" rel=\"noreferrer\">http://blog.mosthege.net/2016/05/11/running-tensorflow-with-native-linux-binaries-in-the-windows-subsystem-for-linux/</a></p>\n<p>cheers</p>\n<p>~michael</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>Sorry for the excavation, but this question is quite popular, and now it has a different answer.</em></p>\n<p>Google officially announced the addition of Windows (7, 10, and Server 2016) support for TensorFlow:\n<a href=\"https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">developers.googleblog.com</a></p>\n<p>The Python module can be installed using pip with a single command:</p>\n<pre><code>C:\\&gt; pip install tensorflow\n</code></pre>\n<p>And if you need GPU support:</p>\n<pre><code> C:\\&gt; pip install tensorflow-gpu\n</code></pre>\n<p><a href=\"https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows\" rel=\"nofollow noreferrer\">TensorFlow manual - How to install pip on windows</a></p>\n<p>Another useful information are included in release notes:\n<a href=\"https://github.com/tensorflow/tensorflow/releases\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/releases</a></p>\n<p><strong>UPD:</strong> As @m02ph3u5 right mentioned in the comments TF for windows supports only Python 3.5.x <a href=\"https://www.tensorflow.org/install/install_windows#installing_with_native_pip\" rel=\"nofollow noreferrer\">Installing TensorFlow on Windows with native pip</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/18441779/how-to-specify-upper-and-lower-limits-when-using-numpy-random-normal\">How to specify upper and lower limits when using numpy.random.normal</a>\n<span class=\"question-originals-answer-count\">\n                                (10 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2020-05-14 19:24:27Z\">4 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>In machine learning task. We should get a group of random w.r.t normal distribution with bound. We can get a normal distribution number with <code>np.random.normal()</code> but it does't offer any bound parameter. I want to know how to do that?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <strong>parametrization of <code>truncnorm</code> is complicated</strong>, so here is a function that translates the parametrization to something more intuitive:</p>\n<pre><code>from scipy.stats import truncnorm\n\ndef get_truncated_normal(mean=0, sd=1, low=0, upp=10):\n    return truncnorm(\n        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n</code></pre>\n<hr/>\n<hr/>\n<h1>How to use it?</h1>\n<ol>\n<li><p>Instance the generator with the parameters: <em>mean</em>, <em>standard deviation</em>, and <em>truncation range</em>:</p>\n<pre><code>&gt;&gt;&gt; X = get_truncated_normal(mean=8, sd=2, low=1, upp=10)\n</code></pre></li>\n<li><p>Then, you can use X to generate a value:</p>\n<pre><code>&gt;&gt;&gt; X.rvs()\n6.0491227353928894\n</code></pre></li>\n<li><p>Or, a numpy array with N generated values:</p>\n<pre><code>&gt;&gt;&gt; X.rvs(10)\narray([ 7.70231607,  6.7005871 ,  7.15203887,  6.06768994,  7.25153472,\n        5.41384242,  7.75200702,  5.5725888 ,  7.38512757,  7.47567455])\n</code></pre></li>\n</ol>\n<hr/>\n<h1>A Visual Example</h1>\n<p>Here is the plot of three different truncated normal distributions:</p>\n<pre><code>X1 = get_truncated_normal(mean=2, sd=1, low=1, upp=10)\nX2 = get_truncated_normal(mean=5.5, sd=1, low=1, upp=10)\nX3 = get_truncated_normal(mean=8, sd=1, low=1, upp=10)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(3, sharex=True)\nax[0].hist(X1.rvs(10000), normed=True)\nax[1].hist(X2.rvs(10000), normed=True)\nax[2].hist(X3.rvs(10000), normed=True)\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/dtObZ.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/dtObZ.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're looking for the <a href=\"https://en.wikipedia.org/wiki/Truncated_normal_distribution\" rel=\"noreferrer\">Truncated normal distribution</a>, SciPy has a function for it called <a href=\"http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.stats.truncnorm.html\" rel=\"noreferrer\"><code>truncnorm</code></a></p>\n<blockquote>\n<p>The standard form of this distribution is a standard normal truncated\n  to the range [a, b] — notice that a and b are defined over the domain\n  of the standard normal. To convert clip values for a specific mean and\n  standard deviation, use:</p>\n<p>a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std</p>\n<p>truncnorm takes a and b as shape parameters.</p>\n</blockquote>\n<pre><code>&gt;&gt;&gt; from scipy.stats import truncnorm\n&gt;&gt;&gt; truncnorm(a=-2/3., b=2/3., scale=3).rvs(size=10)\narray([-1.83136675,  0.77599978, -0.01276925,  1.87043384,  1.25024188,\n        0.59336279, -0.39343176,  1.9449987 , -1.97674358, -0.31944247])\n</code></pre>\n<p>The above example is bounded by -2 and 2 and returns 10 random variates (using the <code>.rvs()</code> method)</p>\n<pre><code>&gt;&gt;&gt; min(truncnorm(a=-2/3., b=2/3., scale=3).rvs(size=10000))\n-1.9996074381484044\n&gt;&gt;&gt; max(truncnorm(a=-2/3., b=2/3., scale=3).rvs(size=10000))\n1.9998486576228549\n</code></pre>\n<p>Here's a histogram plot for -6, 6:</p>\n<p><a href=\"https://i.sstatic.net/78HpD.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/78HpD.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Besides @bakkal suggestion (+1) you might also want to take a look into <a href=\"http://miv.u-strasbg.fr/mazet/rtnorm/\" rel=\"nofollow\">Vincent Mazet</a> recipe for achieving this, rewritten as <a href=\"https://github.com/classner/py-rtnorm\" rel=\"nofollow\">py-rtnorm</a> module by <a href=\"http://www.christophlassner.de/blog/2013/08/12/Generation-of-Truncated-Gaussian-Samples/\" rel=\"nofollow\">Christoph Lassner</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset and I want to train my model on that data. After training, I need to know the features that are major contributors in the classification for a SVM classifier. </p>\n<p>There is something called feature importance for forest algorithms, is there anything similar?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, there is attribute <code>coef_</code> for SVM classifier but it only works for SVM with <strong>linear kernel</strong>. For other kernels it is not possible because data are transformed by kernel method to another space, which is not related to input space, check the <a href=\"https://stackoverflow.com/questions/21260691/scikits-learn-how-to-obtain-features-weight\">explanation</a>.</p>\n<pre><code>from matplotlib import pyplot as plt\nfrom sklearn import svm\n\ndef f_importances(coef, names):\n    imp = coef\n    imp,names = zip(*sorted(zip(imp,names)))\n    plt.barh(range(len(names)), imp, align='center')\n    plt.yticks(range(len(names)), names)\n    plt.show()\n\nfeatures_names = ['input1', 'input2']\nsvm = svm.SVC(kernel='linear')\nsvm.fit(X, Y)\nf_importances(svm.coef_, features_names)\n</code></pre>\n<p>And the output of the function looks like this:\n<a href=\"https://i.sstatic.net/zWjMz.png\" rel=\"noreferrer\"><img alt=\"Feature importances\" src=\"https://i.sstatic.net/zWjMz.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're using <strong>rbf</strong> (Radial basis function) kernal, you can use <code>sklearn.inspection.permutation_importance</code> as follows to get feature importance. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\" rel=\"noreferrer\">[doc]</a></p>\n<pre><code>from sklearn.inspection import permutation_importance\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\n\nperm_importance = permutation_importance(svc, X_test, y_test)\n\nfeature_names = ['feature1', 'feature2', 'feature3', ...... ]\nfeatures = np.array(feature_names)\n\nsorted_idx = perm_importance.importances_mean.argsort()\nplt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\nplt.xlabel(\"Permutation Importance\")\n</code></pre>\n<p><a href=\"https://i.sstatic.net/FrDo0.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/FrDo0.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In only one line of code:</p>\n<p>fit an SVM model:</p>\n<pre><code>from sklearn import svm\nsvm = svm.SVC(gamma=0.001, C=100., kernel = 'linear')\n</code></pre>\n<p>and implement the plot as follows:</p>\n<pre><code>pd.Series(abs(svm.coef_[0]), index=features.columns).nlargest(10).plot(kind='barh')\n</code></pre>\n<p>The resuit will be:</p>\n<p><a href=\"https://i.sstatic.net/NHJQ3.png\" rel=\"noreferrer\">the most contributing features of the SVM model in absolute values</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to perform GridSearchCV in a SVC model, but that uses the one-vs-all strategy. For the latter part, I can just do this:</p>\n<pre><code>model_to_set = OneVsRestClassifier(SVC(kernel=\"poly\"))\n</code></pre>\n<p>My problem is with the parameters. Let's say I want to try the following values:</p>\n<pre><code>parameters = {\"C\":[1,2,4,8], \"kernel\":[\"poly\",\"rbf\"],\"degree\":[1,2,3,4]}\n</code></pre>\n<p>In order to perform GridSearchCV, I should do something like:</p>\n<pre><code> cv_generator = StratifiedKFold(y, k=10)\n model_tunning = GridSearchCV(model_to_set, param_grid=parameters, score_func=f1_score, n_jobs=1, cv=cv_generator)\n</code></pre>\n<p>However, then I execute it I get:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/.../main.py\", line 66, in &lt;module&gt;\n    argclass_sys.set_model_parameters(model_name=\"SVC\", verbose=3, file_path=PATH_ROOT_MODELS)\n  File \"/.../base.py\", line 187, in set_model_parameters\n    model_tunning.fit(self.feature_encoder.transform(self.train_feats), self.label_encoder.transform(self.train_labels))\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py\", line 354, in fit\n    return self._fit(X, y)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py\", line 392, in _fit\n    for clf_params in grid for train, test in cv)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 473, in __call__\n    self.dispatch(function, args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 296, in dispatch\n    job = ImmediateApply(func, args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 124, in __init__\n    self.results = func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py\", line 85, in fit_grid_point\n    clf.set_params(**clf_params)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/base.py\", line 241, in set_params\n    % (key, self.__class__.__name__))\nValueError: Invalid parameter kernel for estimator OneVsRestClassifier\n</code></pre>\n<p>Basically, since the SVC is inside a OneVsRestClassifier and that's the estimator I send to the GridSearchCV, the SVC's parameters can't be accessed. </p>\n<p>In order to accomplish what I want, I see two solutions:</p>\n<ol>\n<li>When creating the SVC, somehow tell it not to use the one-vs-one strategy but the one-vs-all.</li>\n<li>Somehow indicate the GridSearchCV that the parameters correspond to the estimator inside the OneVsRestClassifier. </li>\n</ol>\n<p>I'm yet to find a way to do any of the mentioned alternatives. Do you know if there's a way to do any of them? Or maybe you could suggest another way to get to the same result?</p>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you use nested estimators with grid search you can scope the parameters with <code>__</code> as a separator. In this case the SVC model is stored as an attribute named <code>estimator</code> inside the <code>OneVsRestClassifier</code> model:</p>\n<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import f1_score\n\niris = load_iris()\n\nmodel_to_set = OneVsRestClassifier(SVC(kernel=\"poly\"))\n\nparameters = {\n    \"estimator__C\": [1,2,4,8],\n    \"estimator__kernel\": [\"poly\",\"rbf\"],\n    \"estimator__degree\":[1, 2, 3, 4],\n}\n\nmodel_tunning = GridSearchCV(model_to_set, param_grid=parameters,\n                             score_func=f1_score)\n\nmodel_tunning.fit(iris.data, iris.target)\n\nprint model_tunning.best_score_\nprint model_tunning.best_params_\n</code></pre>\n<p>That yields:</p>\n<pre><code>0.973290762737\n{'estimator__kernel': 'poly', 'estimator__C': 1, 'estimator__degree': 2}\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For Python 3, the following code should be used</p>\n<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\n\niris = load_iris()\n\nmodel_to_set = OneVsRestClassifier(SVC(kernel=\"poly\"))\n\nparameters = {\n    \"estimator__C\": [1,2,4,8],\n    \"estimator__kernel\": [\"poly\",\"rbf\"],\n    \"estimator__degree\":[1, 2, 3, 4],\n}\n\nmodel_tunning = GridSearchCV(model_to_set, param_grid=parameters,\n                             scoring='f1_weighted')\n\nmodel_tunning.fit(iris.data, iris.target)\n\nprint(model_tunning.best_score_)\nprint(model_tunning.best_params_)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>param_grid  = {\"estimator__alpha\": [10**-5, 10**-3, 10**-1, 10**1, 10**2]}\n\nclf = OneVsRestClassifier(SGDClassifier(loss='log',penalty='l1'))\n\nmodel = GridSearchCV(clf,param_grid, scoring = 'f1_micro', cv=2,n_jobs=-1)\n\nmodel.fit(x_train_multilabel, y_train)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://xgboost.readthedocs.org/en/latest/python/python_intro.html\">http://xgboost.readthedocs.org/en/latest/python/python_intro.html</a></p>\n<p>On the homepage of xgboost(above link), it says:\nTo install XGBoost, do the following steps:</p>\n<ol>\n<li><p>You need to run <code>make</code> in the root directory of the project</p></li>\n<li><p>In the python-package directory run</p>\n<p>python setup.py install</p></li>\n</ol>\n<p>However, when I did it, for step 1 the following error appear:\nmake : The term 'make' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the\n spelling of the name, or if a path was included, verify that the path is correct and try again.</p>\n<p>then I skip step1 and did step 2 directly, another error appear:</p>\n<pre><code>Traceback (most recent call last):\n  File \"setup.py\", line 19, in &lt;module&gt;\n    LIB_PATH = libpath['find_lib_path']()\n  File \"xgboost/libpath.py\", line 44, in find_lib_path\n    'List of candidates:\\n' + ('\\n'.join(dll_path)))\n__builtin__.XGBoostLibraryNotFound: Cannot find XGBoost Libarary in the candicate path, did you install compilers and run build.sh in root path?\n</code></pre>\n<p>Does anyone know how to install xgboost for python on Windows10 platform? Thanks for your help!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In case anyone's looking for a simpler solution that doesn't require compiling it yourself:</p>\n<ol>\n<li>download xgboost whl file from <a href=\"http://www.lfd.uci.edu/~gohlke/pythonlibs/\" rel=\"noreferrer\">here</a> (make sure to match your python version and system architecture, e.g. \"xgboost-0.6-cp35-cp35m-win_amd64.whl\" for python 3.5 on 64-bit machine)</li>\n<li>open command prompt</li>\n<li><code>cd</code> to your Downloads folder (or wherever you saved the whl file)</li>\n<li><code>pip install xgboost-0.6-cp35-cp35m-win_amd64.whl</code> (or whatever your whl file is named)</li>\n</ol>\n<p>If you find it won't install because of a missing dependency, download and install the dependency first and retry. </p>\n<p>If it complains about access permissions, try opening your command prompt as Administrator and retry.</p>\n<p>This gives you xgboost and the scikit-learn wrapper, and saves you from having to go through the pain of compiling it yourself. :)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Note that as of the most recent release the Microsoft Visual Studio instructions no longer seem to apply as this link returns a 404 error:</p>\n<p><a href=\"https://github.com/dmlc/xgboost/tree/master/windows\">https://github.com/dmlc/xgboost/tree/master/windows</a></p>\n<p>You can read more about the removal of the MSVC build from Tianqi Chen's comment <a href=\"https://github.com/dmlc/xgboost/issues/928#issuecomment-193065994\">here</a>.</p>\n<p>So here's what I did to finish a 64-bit build on Windows:</p>\n<ol>\n<li>Download and install MinGW-64:  <a href=\"http://sourceforge.net/projects/mingw-w64/\">http://sourceforge.net/projects/mingw-w64/</a></li>\n<li>On the first screen of the install prompt make sure you set the Architecture to <strong>x86_64</strong> and the Threads to <strong>win32</strong></li>\n<li>I installed to C:\\mingw64 (to avoid spaces in the file path) so I added this to my PATH environment variable:  C:\\mingw64\\mingw64\\bin</li>\n<li>I also noticed that the make utility that is included in bin\\mingw64 is called <strong>mingw32-make</strong> so to simplify things I just renamed this to <strong>make</strong></li>\n<li>Open a Windows command prompt and type gcc.  You should see something like \"fatal error: no input file\"</li>\n<li>Next type make.  You should see something like \"No targets specified and no makefile found\"</li>\n<li>Type git.  If you don't have git, install it and add it to your\nPATH.</li>\n</ol>\n<p>These should be all the tools you need to build the xgboost project.  To get the source code run these lines:</p>\n<ol>\n<li>cd c:\\</li>\n<li>git clone --recursive <a href=\"https://github.com/dmlc/xgboost\">https://github.com/dmlc/xgboost</a></li>\n<li>cd xgboost</li>\n<li>git submodule init</li>\n<li>git submodule update</li>\n<li>cp make/mingw64.mk config.mk</li>\n<li>make -j4</li>\n</ol>\n<p>Note that I ran this part from a Cygwin shell.  If you are using the Windows command prompt you should be able to change cp to copy and arrive at the same result.  However, if the build fails on you for any reason I would recommend trying again using cygwin.</p>\n<p>If the build finishes successfully, you should have a file called xgboost.exe located in the project root.  To install the Python package, do the following:  </p>\n<ol>\n<li>cd python-package</li>\n<li>python setup.py install</li>\n</ol>\n<p>Now you should be good to go.  Open up Python, and you can import the package with:</p>\n<pre><code>import xgboost as xgb\n</code></pre>\n<p>To test the installation, I went ahead and ran the basic_walkthrough.py file that was included in the demo/guide-python folder of the project and didn't get any errors.  </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I installed XGBoost successfully in Windows 8 64bit, Python 2.7 with Visual Studio 2013 (don't need mingw64)</p>\n<p><strong>Updated 15/02/2017</strong></p>\n<p>With newer version of XGBoost, here are my steps</p>\n<p><strong>Step 1.</strong> Install cmake <a href=\"https://cmake.org/download/\" rel=\"noreferrer\">https://cmake.org/download/</a></p>\n<p>Verify <code>cmake</code> have been installed successfully</p>\n<pre><code>$ cmake\nUsage\n\ncmake [options] &lt;path-to-source&gt;\ncmake [options] &lt;path-to-existing-build&gt;\n...\n</code></pre>\n<p><strong>Step 2.</strong> Clone xgboost source</p>\n<pre><code>$ git clone https://github.com/dmlc/xgboost xgboost_dir\n</code></pre>\n<p><strong>Step 3.</strong> Create Visual Studio Project</p>\n<pre><code>$ cd xgboost_dir\n$ mkdir build\n$ cd build\n$ cmake .. -G\"Visual Studio 12 2013 Win64\"\n</code></pre>\n<p><strong>Step 4.</strong> Build Visual Studio 2013 project</p>\n<ul>\n<li>Open file <code>xgboost_dir/build/ALL_BUILD.vcxproj</code> with Visual Studio 2013</li>\n<li>In Visual Studio 2013, open <code>BUILD &gt; Configuration Manager...</code>\n<ul>\n<li>choose Release in Active solution configuration</li>\n<li>choose x64 in Active solution platform</li>\n</ul></li>\n<li>Click BUILD &gt; Build Solution (Ctrl + Shift +B)</li>\n</ul>\n<p>After build solution, two new files <code>libxgboost.dll</code> and <code>xgboost.exe</code> are created in folder <code>xgboost_dir/lib</code></p>\n<p><strong>Step 5.</strong> Build python package</p>\n<ul>\n<li>Copy file <code>libxgboost.dll</code> to <code>xgboost_dir/python-package</code></li>\n<li>Change directory to <code>xgboost_dir/python-package</code> folder</li>\n<li>Run command <code>python setup.py install</code></li>\n</ul>\n<p>Verify xgboost have been installed successfully</p>\n<pre><code>$ python -c \"import xgboost\"\n</code></pre>\n<p><strong>Old Answer</strong></p>\n<p>Here are my steps:</p>\n<ol>\n<li>git clone <a href=\"https://github.com/dmlc/xgboost\" rel=\"noreferrer\">https://github.com/dmlc/xgboost</a></li>\n<li>git checkout 9bc3d16</li>\n<li>Open project in <code>xgboost/windows</code> with Visual Studio 2013</li>\n<li>In Visual Studio 2013, open <code>BUILD &gt; Configuration Manager...</code>,\n\n<ul>\n<li>choose <code>Release</code> in <code>Active solution configuration</code></li>\n<li>choose <code>x64</code> in <code>Active solution platform</code></li>\n</ul></li>\n<li>Rebuild <code>xgboost</code>, <code>xgboost_wrapper</code></li>\n<li>Copy all file in <code>xgboost/windows/x64/Release</code> folder to <code>xgboost/wrapper</code></li>\n<li>Go to <code>xgboost/python-package</code>, run command <code>python setup.py install</code></li>\n<li>Check xgboost by running command <code>python -c \"import xgboost\"</code></li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was using the Decision Tree and this error was raised. The same situation appeared when I used Back Propagation. How can I solve it?</p>\n<pre><code>import pandas as pd\nimport numpy as np\na = np.test()\nf = open('E:/lgdata.csv')\ndata = pd.read_csv(f,index_col = 'id')\n\nx = data.iloc[:,10:12].as_matrix().astype(int)\ny = data.iloc[:,9].as_matrix().astype(int)\n\nfrom sklearn.tree import DecisionTreeClassifier as DTC\ndtc = DTC(criterion='entropy')\ndtc.fit(x,y)\nx=pd.DataFrame(x) \n\nfrom sklearn.tree import export_graphviz\nwith open('tree.dot','w') as f1:\n    f1 = export_graphviz(dtc, feature_names = x.columns, out_file = f1)\n</code></pre>\n<blockquote>\n<p>Traceback (most recent call last):<br/>\n  File \"&lt;ipython-input-40-4359c06ae1f0&gt;\", line 1, in &lt;module&gt;<br/>\n    runfile('C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py', wdir='C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib')<br/>\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 710, in runfile<br/>\n    execfile(filename, namespace)<br/>\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile<br/>\n    exec(compile(f.read(), filename, 'exec'), namespace)<br/>\n  File \"C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py\", line 9, in &lt;module&gt;<br/>\n    from numpy.testing.nosetester import import_nose</p>\n<p>ModuleNotFoundError: No module named 'numpy.testing.nosetester'</p>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is happening due to a version incompatibility between <code>numpy</code> and <code>scipy</code>. <code>numpy</code> in its latest versions have deprecated <code>numpy.testing.nosetester</code>.</p>\n<h2>Replicating the issue</h2>\n<pre><code>pip install numpy==1.18 # &gt; 1.18\npip install scipy&lt;=0.19.0 # &lt;= 0.19 \n</code></pre>\n<p>and</p>\n<pre><code>from sklearn.tree import DecisionTreeClassifier as DTC\n</code></pre>\n<p>Triggers the error.</p>\n<h2>Fixing the error</h2>\n<p>Upgrade your <code>scipy</code> to a higher version.</p>\n<pre><code>pip install numpy==1.18\npip install scipy==1.1.0\npip install scikit-learn==0.21.3\n</code></pre>\n<p>But not limited to this. By upgrading the above libraries to the latest stable, you should be able to get rid of this error.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I needed to upgrade scipy</p>\n<p><code>pip3 install -U scipy</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was facing the same error while using lexnlp package \nGot it fixed by installing:</p>\n<pre><code>scipy==1.4.1\npandas==0.23.4    \nnumpy==1.18.1\nlexnlp==0.2.7.1 \n</code></pre>\n<p>(Only install lexnlp if know you're explicitly using it in your project and you know what you're doing)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm slightly confused in regard to how I save a trained classifier. As in, re-training a classifier each time I want to use it is obviously really bad and slow, how do I save it and the load it again when I need it? Code is below, thanks in advance for your help. I'm using Python with NLTK Naive Bayes Classifier. </p>\n<pre><code>classifier = nltk.NaiveBayesClassifier.train(training_set)\n# look inside the classifier train method in the source code of the NLTK library\n\ndef train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):\n    # Create the P(label) distribution\n    label_probdist = estimator(label_freqdist)\n    # Create the P(fval|label, fname) distribution\n    feature_probdist = {}\n    return NaiveBayesClassifier(label_probdist, feature_probdist)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To save:</p>\n<pre><code>import pickle\nf = open('my_classifier.pickle', 'wb')\npickle.dump(classifier, f)\nf.close()\n</code></pre>\n<p>To load later:</p>\n<pre><code>import pickle\nf = open('my_classifier.pickle', 'rb')\nclassifier = pickle.load(f)\nf.close()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I went thru the same problem, and you cannot save the object since is a ELEFreqDistr NLTK class. Anyhow NLTK is hell slow. Training took 45 mins on a decent set and I decided to implement my own version of the algorithm (run it with pypy or rename it .pyx and install cython). It takes about 3 minutes with the same set and it can simply save data as json (I'll implement pickle which is faster/better). </p>\n<p>I started a simple github project, check out the code <a href=\"https://github.com/luke14free/sentipy\" rel=\"nofollow\">here</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To Retrain the Pickled Classifer :</p>\n<pre><code>f = open('originalnaivebayes5k.pickle','rb')\nclassifier = pickle.load(f)\nclassifier.train(training_set)\nprint('Accuracy:',nltk.classify.accuracy(classifier,testing_set)*100)\nf.close()\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is it possible to train a model by <a aria-label=\"show questions tagged 'xgboost'\" aria-labelledby=\"tag-xgboost-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/xgboost\" rel=\"tag\" title=\"show questions tagged 'xgboost'\">xgboost</a> that has multiple continuous outputs (multi-regression)?\nWhat would be the objective of training such a model?</p>\n<p>Thanks in advance for any suggestions</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My suggestion is to use <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html\" rel=\"nofollow noreferrer\">sklearn.multioutput.MultiOutputRegressor</a> as a wrapper of <code>xgb.XGBRegressor</code>. <code>MultiOutputRegressor</code> trains one regressor per target and only requires that the regressor implements <code>fit</code> and <code>predict</code>, which <a aria-label=\"show questions tagged 'xgboost'\" aria-labelledby=\"tag-xgboost-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/xgboost\" rel=\"tag\" title=\"show questions tagged 'xgboost'\">xgboost</a> happens to support.</p>\n<pre class=\"lang-py prettyprint-override\"><code># get some noised linear data\nX = np.random.random((1000, 10))\na = np.random.random((10, 3))\ny = np.dot(X, a) + np.random.normal(0, 1e-3, (1000, 3))\n\n# fitting\nmultioutputregressor = MultiOutputRegressor(xgb.XGBRegressor(objective='reg:linear')).fit(X, y)\n\n# predicting\nprint(np.mean((multioutputregressor.predict(X) - y)**2, axis=0))  # 0.004, 0.003, 0.005\n</code></pre>\n<p>This is probably the easiest way to regress multi-dimension targets using <a aria-label=\"show questions tagged 'xgboost'\" aria-labelledby=\"tag-xgboost-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/xgboost\" rel=\"tag\" title=\"show questions tagged 'xgboost'\">xgboost</a> as you would not need to change any other part of your code (if you were using the <a aria-label=\"show questions tagged 'sklearn'\" aria-labelledby=\"tag-sklearn-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/sklearn\" rel=\"tag\" title=\"show questions tagged 'sklearn'\">sklearn</a> API originally).</p>\n<p>However, this method does not leverage any possible relation between targets. But you can try to design a <a href=\"https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py\" rel=\"nofollow noreferrer\">customized objective</a> function to achieve that.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Multiple output regression is now available in the nightly build of XGBoost, and will be included in XGBoost 1.6.0.</p>\n<p>See <a href=\"https://github.com/dmlc/xgboost/blob/master/demo/guide-python/multioutput_regression.py\" rel=\"noreferrer\">https://github.com/dmlc/xgboost/blob/master/demo/guide-python/multioutput_regression.py</a> for an example.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It generates warnings:</p>\n<blockquote>\n<p><code>reg:linear is now deprecated in favor of reg:squarederror</code></p>\n</blockquote>\n<p>, so I updated an answer based on @ComeOnGetMe's</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np \nimport pandas as pd \nimport xgboost as xgb\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# get some noised linear data\nX = np.random.random((1000, 10))\na = np.random.random((10, 3))\ny = np.dot(X, a) + np.random.normal(0, 1e-3, (1000, 3))\n\n# fitting\nmultioutputregressor = MultiOutputRegressor(xgb.XGBRegressor(objective='reg:squarederror')).fit(X, y)\n\n# predicting\nprint(np.mean((multioutputregressor.predict(X) - y)**2, axis=0))\n</code></pre>\n<p>Out:</p>\n<pre><code>[2.00592697e-05 1.50084441e-05 2.01412247e-05]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have two numpy arrays:</p>\n<ul>\n<li>One that contains captcha images</li>\n<li>Another that contains the corresponding labels (in one-hot vector format)</li>\n</ul>\n<p><strong>I want to load these into TensorFlow so I can classify them using a neural network. How can this be done?</strong></p>\n<p>What shape do the numpy arrays need to have? </p>\n<p>Additional Info - My images are 60 (height) by 160 (width) pixels each and each of them have 5 alphanumeric characters. Here is a sample image:</p>\n<p><img alt=\"sample image.\" src=\"https://i.sstatic.net/lSZYb.png\"/></p>\n<p>Each label is a 5 by 62 array. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <code>tf.convert_to_tensor()</code>:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndata = [[1,2,3],[4,5,6]]\ndata_np = np.asarray(data, np.float32)\n\ndata_tf = tf.convert_to_tensor(data_np, np.float32)\n\nsess = tf.InteractiveSession()  \nprint(data_tf.eval())\n\nsess.close()\n</code></pre>\n<p>Here's a link to the documentation for this method:</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\" rel=\"noreferrer\">https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#pack\" rel=\"noreferrer\">tf.pack</a> (<a href=\"https://www.tensorflow.org/api_docs/python/tf/stack\" rel=\"noreferrer\">tf.stack</a> in TensorFlow 1.0.0) method for this purpose. Here is how to pack a random image of type <code>numpy.ndarray</code> into a <code>Tensor</code>:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nrandom_image = np.random.randint(0,256, (300,400,3))\nrandom_image_tensor = tf.pack(random_image)\ntf.InteractiveSession()\nevaluated_tensor = random_image_tensor.eval()\n</code></pre>\n<p>UPDATE: to convert a Python object to a Tensor you can use <a href=\"https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\" rel=\"noreferrer\">tf.convert_to_tensor</a> function.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use placeholders and feed_dict.</p>\n<p>Suppose we have numpy arrays like these:</p>\n<pre><code>trX = np.linspace(-1, 1, 101) \ntrY = 2 * trX + np.random.randn(*trX.shape) * 0.33 \n</code></pre>\n<p>You can declare two placeholders:</p>\n<pre><code>X = tf.placeholder(\"float\") \nY = tf.placeholder(\"float\")\n</code></pre>\n<p>Then, use these placeholders (X, and Y) in your model, cost, etc.:\n    model = tf.mul(X, w) ... Y ...\n    ...</p>\n<p>Finally, when you run the model/cost, feed the numpy arrays using feed_dict:</p>\n<pre><code>with tf.Session() as sess:\n.... \n    sess.run(model, feed_dict={X: trY, Y: trY})\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n                                As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, <a href=\"/help/reopen-questions\">visit the help center</a> for guidance.\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2012-09-09 22:22:44Z\">12 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>I'm looking for a practical application to use a genetic algorithm for. Some things that have thought of are:</p>\n<ul>\n<li>Website interface optimization</li>\n<li>Vehicle optimization with a physics simulator</li>\n<li>Genetic programming</li>\n<li>Automatic test case generation</li>\n</ul>\n<p>But none have really popped out at me. So if you had some free time (a few months) to spend on a genetic algorithms project, what would you choose to tackle?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One topic with lots of possibilities is to use evolutionary algorithms to evolve strategies for game playing.  People have used evolution to generate strategies for poker, checkers/draughts, Go and many other games.  The <a href=\"http://jgap.sourceforge.net/\" rel=\"noreferrer\">J-GAP</a> people have used genetic programming to evolve bots for <a href=\"http://robocode.sourceforge.net/\" rel=\"noreferrer\">Robocode</a>.</p>\n<p>I recently posted an <a href=\"http://blog.uncommons.org/2009/01/20/practical-evolutionary-computation-an-introduction/\" rel=\"noreferrer\">introductory article</a> about evolutionary computation.  It includes details of some of the things evolutionary algorithms have been used for.  <a href=\"http://www.talkorigins.org/faqs/genalg/genalg.html\" rel=\"noreferrer\">Adam Marczyk</a> has also written an excellent article with lots of examples.  The <a href=\"http://geneticargonaut.blogspot.com/\" rel=\"noreferrer\">Genetic Argonaut blog</a> contains dozens of links to interesting evolutionary projects.</p>\n<p>A less common type of evolutionary algorithm is the <a href=\"http://en.wikipedia.org/wiki/Learning_classifier_system\" rel=\"noreferrer\">learning classifier system</a>.  This evolves a set of rules for classifying inputs.  It can be applied to the same kind of problems that neural networks are used for.  It could be interesting to develop an LCS for a particular problem, such as attempting to predict sports results based on form.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You might be interested in something like Roger Alsing's <a href=\"http://rogeralsing.com/2008/12/07/genetic-programming-evolution-of-mona-lisa/\" rel=\"noreferrer\">Mona Lisa</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I consider <a href=\"http://www.thinkartificial.org/artificial-intelligence/evolving-lego-brick-structures/\" rel=\"noreferrer\">evolving lego structures</a> by far the most interesting toy project for GA.</p>\n<p><a href=\"http://www.demo.cs.brandeis.edu/pr/buildable/\" rel=\"noreferrer\">http://static.23.nu/md/Pictures/ZZ099735B6.jpg</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Update: a better formulation of the issue.</p>\n<p>I'm trying to understand the backpropagation algorithm with an XOR neural network as an example.  For this case there are 2 input neurons + 1 bias, 2 neurons in the hidden layer + 1 bias, and 1 output neuron.</p>\n<pre><code> A   B  A XOR B\n 1    1   -1\n 1   -1    1\n-1    1    1\n-1   -1   -1\n</code></pre>\n<p><a href=\"https://i.sstatic.net/h1BHt.png\" rel=\"noreferrer\"><img alt=\"A sample XOR neural network\" src=\"https://i.sstatic.net/h1BHt.png\"/></a><br/>\n<sub>(source: <a href=\"https://upload.wikimedia.org/wikipedia/en/7/7b/XOR_perceptron_net.png\" rel=\"noreferrer\">wikimedia.org</a>)</sub> </p>\n<p>I'm using <a href=\"http://pandamatak.com/people/anand/771/html/node37.html\" rel=\"noreferrer\">stochastic backpropagation</a>.</p>\n<p>After reading a bit more I have found out that the error of the output unit is propagated to the hidden layers... initially this was confusing, because when you get to the input layer of the neural network, then each neuron gets an error adjustment from both of the neurons in the hidden layer.  In particular, the way the error is distributed is difficult to grasp at first.</p>\n<p><strong>Step 1</strong> calculate the output for each instance of input.<br/>\n<strong>Step 2</strong> calculate the error between the output neuron(s) (in our case there is only one) and the target value(s):<br/>\n<a href=\"https://i.sstatic.net/YJTHr.gif\" rel=\"noreferrer\"><img alt=\"Step 2\" src=\"https://i.sstatic.net/YJTHr.gif\"/></a><br/>\n<strong>Step 3</strong> we use the error from Step 2 to calculate the error for each hidden unit h:<br/>\n<a href=\"https://i.sstatic.net/bFVPz.gif\" rel=\"noreferrer\"><img alt=\"Step 3\" src=\"https://i.sstatic.net/bFVPz.gif\"/></a></p>\n<p>The 'weight kh' is the weight between the hidden unit h and the output unit k, well this is confusing because the input unit does not have a direct weight associated with the output unit.  After staring at the formula for a few hours I started to think about what the summation means, and I'm starting to come to the conclusion that each input neuron's weight that connects to the hidden layer neurons is multiplied by the output error and summed up.  This is a logical conclusion, but the formula seems a little confusing since it clearly says the 'weight kh' (between the output layer k and hidden layer h).</p>\n<p>Am I understanding everything correctly here? Can anybody confirm this?  </p>\n<p>What's O(h) of the input layer? My understanding is that each input node has two outputs: one that goes into the the first node of the hidden layer and one that goes into the second node hidden layer. Which of the two outputs should be plugged into the <code>O(h)*(1 - O(h))</code> part of the formula?<br/>\n<a href=\"https://i.sstatic.net/bFVPz.gif\" rel=\"noreferrer\"><img alt=\"Step 3\" src=\"https://i.sstatic.net/bFVPz.gif\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The tutorial you posted here is actually doing it wrong. I double checked it against Bishop's two standard books and two of my working implementations. I will point out below where exactly.</p>\n<p>An important thing to keep in mind is that you are always searching for derivatives of the error function with respect to a unit or weight. The former are the deltas, the latter is what you use to update your weights.</p>\n<p>If you want to understand backpropagation, you have to understand the chain rule. It's all about the chain rule here. If you don't know how it works exactly, check up at wikipedia - it's not that hard. But as soon as you understand the derivations, everything falls into place. Promise! :)</p>\n<p>∂E/∂W can be composed into ∂E/∂o ∂o/∂W via the chain rule. ∂o/∂W is easily calculated, since it's just the derivative of the activation/output of a unit with respect to the weights. ∂E/∂o is actually what we call the deltas. (I am assuming that E, o and W are vectors/matrices here)</p>\n<p>We do have them for the output units, since that is where we can calculate the error. (Mostly we have an error function that comes down to delta of (t_k - o_k), eg for quadratic error function in the case of linear outputs and cross entropy in case for logistic outputs.) </p>\n<p>The question now is, how do we get the derivatives for the internal units? Well, we know that the output of a unit is the sum of all incoming units weighted by their weights and the application of a transfer function afterwards. So o_k = f(sum(w_kj * o_j, for all j)). </p>\n<p>So what we do is, derive o_k with respect to o_j. Since delta_j = ∂E/∂o_j = ∂E/∂o_k ∂o_k/∂o_j = delta_k ∂o_k/o_j. So given delta_k, we can calculate delta_j!</p>\n<p>Let's do this. o_k = f(sum(w_kj * o_j, for all j)) =&gt; ∂o_k/∂o_j = f'(sum(w_kj * o_j, for all j)) * w_kj = f'(z_k) * w_kj.</p>\n<p>For the case of the sigmoidal transfer function, this becomes z_k(1 - z_k) * w_kj. (<strong>Here is the error in the tutorial, the author says o_k(1 - o_k) * w_kj!</strong>)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm not sure what your question is but I actually went through that tutorial myself and I can assure you, other than a one obvious typo, there is nothing incorrect about it.</p>\n<p>I will make the assumption that your question is because you are confused about how the backpropagation <strong>hidden</strong> delta is derived. If this is indeed your question then please consider</p>\n<p><a href=\"https://i.sstatic.net/Aa1MD.gif\" rel=\"nofollow noreferrer\"><img alt=\"alt text\" src=\"https://i.sstatic.net/Aa1MD.gif\"/></a><br/>\n<sub>(source: <a href=\"http://pandamatak.com/people/anand/771/html/img334.gif\" rel=\"nofollow noreferrer\">pandamatak.com</a>)</sub> </p>\n<p>You are probably confused as to how the author derived this equation. This is actually a straightforward application of the multivariate chain rule. Namely, (what follows is taken from <a href=\"http://en.wikipedia.org/wiki/Chain_rule\" rel=\"nofollow noreferrer\">wikipedia</a>)</p>\n<p>\"Suppose that each argument of z = f(u, v) is a two-variable function such that u = h(x, y) and v = g(x, y), and that these functions are all differentiable. Then the chain rule would look like:</p>\n<p><img alt=\"alt text\" src=\"https://upload.wikimedia.org/math/4/8/2/4828144e41e012df450d1a30ecf3159d.png\"/></p>\n<p><img alt=\"alt text\" src=\"https://upload.wikimedia.org/math/5/e/4/5e4928e3aa49acf7583e00ae55c538c0.png\"/>\n\"</p>\n<p>Now imagine extending the chain rule by an induction argument to</p>\n<p>E(z'<sub>1</sub>,z'<sub>2</sub>,..,z'<sub>n</sub>)\nwhere z'<sub>k</sub> is the output of the  kth output layer pre-activation,\nand z'<sub>k</sub>(w<sub>ji</sub>) that is to say that E is a function of the z' and z' itself is a function of w<sub>ji</sub> (if this doesn't make sense to you at first <strong>think</strong> very carefully about how a NN is setup.) Applying the chain rule directly extended to n variables:</p>\n<p><sup>δE(z'<sub>1</sub>,z'<sub>2</sub>,..,z'<sub>n</sub>)</sup>/<sub>δw<sub>ji</sub></sub> = Σ<sub>k</sub> <sup>δE</sup>/<sub>δz'<sub>k</sub></sub> <sup>δz'<sub>k</sub></sup>/<sub>δw<sub>ji</sub></sub> </p>\n<p>that is the most important step, the author then applies the chain rule again, this time within the sum to expand the <sup>δz'<sub>k</sub></sup>/<sub>δw<sub>ji</sub></sub> term, that is </p>\n<p><sup>δz'<sub>k</sub></sup>/<sub>δw<sub>ji</sub></sub> = <sup>δz'<sub>k</sub></sup>/<sub>δo<sub>j</sub></sub>\n<sup>δo<sub>j</sub></sup>/<sub>δz<sub>j</sub></sub>\n<sup>δz<sub>j</sub></sup>/<sub>δw<sub>ji</sub></sub>.</p>\n<p>If you have difficulties understanding the chain rule, you may need to take a course on multivariate calculus, or read such a section in a textbook.</p>\n<p>Good luck.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What I read from Step 3's equation is:</p>\n<ol>\n<li>O_h = last output of this hidden unit (O_h on the input layer is the actual input value)</li>\n<li>w_kh = weight of connection between this hidden unit and a unit of the next layer (towards output)</li>\n<li>delta_k = error of unit of the next layer (towards output, same unit as previous bullet)</li>\n</ol>\n<p>Each unit has only one output, but each link between the output and the next layer is weighted. So the output is the same, but on the receiving end, each unit will receive a different value if the weight of the links is different. O_h always refers to the value of this neuron for the last iteration. Error does not apply to the input layer, as by definition, the input has no 'error' per se.</p>\n<p>The error needs to be calculated layer by layer, starting at the output side, since we need the error values of layer N+1 to calculate layer N. You are right, there is no direct connection between input and output in backpropagation. </p>\n<p>I believe the equation is correct, if counterintuitive. What is probably confusing is that in forward propagation for each unit we have to consider all the units and links on the left of the unit (input values), but for error propagation (backpropagation) was have to consider the units on the right (output value) of the unit being processed. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to do image classification with the Inception V3 model. Does <code>ImageDataGenerator</code> from Keras create new images which are added onto my dataset? If I have 1000 images, will using this function double it to 2000 images which are used for training? Is there a way to know how many images were created and now fed into the model?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Short answer:</strong> 1) All the original images are just transformed (i.e. rotation, zooming, etc.) <strong>every epoch</strong> and then used for training, and 2) [Therefore] the number of images in each epoch is equal to the number of original images you have.</p>\n<p><strong>Long answer:</strong> In each epoch, the <a href=\"https://keras.io/preprocessing/image/#imagedatagenerator-class\" rel=\"noreferrer\"><code>ImageDataGenerator</code></a> applies a transformation on the images you have and use the transformed images for training. The set of transformations includes rotation, zooming, etc. By doing this you're somehow creating new data (i.e. also called <em>data augmentation</em>), but obviously the generated images are not totally different from the original ones. This way the learned model may be more robust and accurate as it is trained on different variations of the same image.</p>\n<p>You need to set the <code>steps_per_epoch</code> argument of <code>fit</code> method to <code>n_samples / batch_size</code>, where <code>n_samples</code> is the total number of training data you have (i.e. 1000 in your case). This way in each epoch, each training sample is augmented only one time and therefore 1000 transformed images will be generated in each epoch.</p>\n<p>Further, I think it's worth clarifying the meaning of \"augmentation\" in this context: basically we are augmenting the images when we use <code>ImageDataGenerator</code> and enabling its augmentation capabilities. But the word \"augmentation\" here does not mean, say, if we have 100 original training images we end up having 1000 images <strong>per epoch</strong> after augmentation (i.e. the number of training images does not increase per epoch). Instead, it means we use a different transformation of each image in each epoch; hence, if we train our model for, say, 5 epochs, we have used 5 different versions of each original image in training (or 100 * 5 = 500 different images in the whole training, instead of using just the 100 original images in the whole training). To put it differently, the total number of <strong>unique images</strong> increases in the whole training from start to finish, and not per epoch.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is my attempt to answer as I also had this question on my mind.</p>\n<p><code>ImageDataGenerator</code> will NOT add new images to your data set in a sense that it will not make your epochs bigger. Instead, in each epoch it will provide slightly altered images (depending on your configuration). It will always generate new images, no matter how many epochs you have.</p>\n<p>So in each epoch model will train on different images, but not too different. This should prevent overfitting and in some way simulates online learning.</p>\n<p>All these alterations happen in memory, but if you want to see these images you can save them to disc, inspect them, see how many of them were generated and get the sense of how <code>ImageDataGenerator</code> works. To do this pass <code>save_to_dir=/tmp/img-data-gen-outputs</code> to function <code>flow_from_directory</code>. See <a href=\"https://keras.io/preprocessing/image/\" rel=\"noreferrer\">docs</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As is officially written <a href=\"https://keras.io/preprocessing/image/\" rel=\"nofollow noreferrer\">here</a> <code>ImageDataGenerator</code> is a  batches Generator of tensor image data with real-time data augmentation. The data will be looped over (in batches). This means that will on the fly apply transformations to batch of images <strong>randomly</strong>. For instance:</p>\n<pre><code>train_datagen = ImageDataGenerator(\n    rescale=1./255, #scale images from integers 0-255 to floats 0-1.\n    shear_range=0.2,\n    zoom_range=0.2, # zoom in or out in images\n    horizontal_flip=True) #horizontal flip of images\n</code></pre>\n<p>At every new epoch new random transformations will be applied and in this way we train with a little different set of images each time. Obtaining more data is not always achievable or possible, using <code>ImageDataGenerator</code> is helpful this way.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Hi I have been trying to make a custom loss function in keras for dice_error_coefficient. It has its implementations in <strong>tensorboard</strong> and I tried using the same function in keras with tensorflow but it keeps returning a <strong>NoneType</strong> when I used <strong>model.train_on_batch</strong> or <strong>model.fit</strong> where as it gives proper values when used in metrics in the model. Can please someone help me out with what should i do? I have tried following libraries like Keras-FCN by ahundt where he has used custom loss functions but none of it seems to work. The target and output in the code are y_true and y_pred respectively as used in the losses.py file in keras.</p>\n<pre><code>def dice_hard_coe(target, output, threshold=0.5, axis=[1,2], smooth=1e-5):\n    \"\"\"References\n    -----------\n    - `Wiki-Dice &lt;https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient&gt;`_\n    \"\"\"\n\n    output = tf.cast(output &gt; threshold, dtype=tf.float32)\n    target = tf.cast(target &gt; threshold, dtype=tf.float32)\n    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)\n    l = tf.reduce_sum(output, axis=axis)\n    r = tf.reduce_sum(target, axis=axis)\n    hard_dice = (2. * inse + smooth) / (l + r + smooth)\n    hard_dice = tf.reduce_mean(hard_dice)\n    return hard_dice\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are two steps in implementing a parameterized custom loss function in Keras. First, writing a method for the coefficient/metric. Second, writing a wrapper function to format things the way Keras needs them to be.</p>\n<ol>\n<li><p>It's actually quite a bit cleaner to use the Keras backend instead of tensorflow directly for simple custom loss functions like DICE. Here's an example of the coefficient implemented that way:</p>\n<pre><code>import keras.backend as K\ndef dice_coef(y_true, y_pred, smooth, thresh):\n    y_pred = y_pred &gt; thresh\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n</code></pre></li>\n<li><p>Now for the tricky part. Keras loss functions must only take (y_true, y_pred) as parameters. So we need a separate function that returns another function. </p>\n<pre><code>def dice_loss(smooth, thresh):\n  def dice(y_true, y_pred)\n    return -dice_coef(y_true, y_pred, smooth, thresh)\n  return dice\n</code></pre></li>\n</ol>\n<p>Finally, you can use it as follows in Keras compile.</p>\n<pre><code># build model \nmodel = my_model()\n# get the loss function\nmodel_dice = dice_loss(smooth=1e-5, thresh=0.5)\n# compile model\nmodel.compile(loss=model_dice)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>According to the <a href=\"https://keras.io/api/losses/#creating-custom-losses\" rel=\"noreferrer\">documentation</a>, you can use a custom loss function like this:</p>\n<blockquote>\n<p>Any callable with the signature <code>loss_fn(y_true, y_pred)</code> that returns an array of losses (one of sample in the input batch) can be passed to compile() as a loss. Note that sample weighting is automatically supported for any such loss.</p>\n</blockquote>\n<p>As a simple example:</p>\n<pre><code>def my_loss_fn(y_true, y_pred):\n    squared_difference = tf.square(y_true - y_pred)\n    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n\nmodel.compile(optimizer='adam', loss=my_loss_fn)\n</code></pre>\n<p>Complete example:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef my_loss_fn(y_true, y_pred):\n    squared_difference = tf.square(y_true - y_pred)\n    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1)])\n\nmodel.compile(optimizer='adam', loss=my_loss_fn)\n\nx = np.random.rand(1000)\ny = x**2\n\nhistory = model.fit(x, y, epochs=10)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In addition, you can extend an existing loss function by inheriting from it. For example masking the <code>BinaryCrossEntropy</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>class MaskedBinaryCrossentropy(tf.keras.losses.BinaryCrossentropy):\n    def call(self, y_true, y_pred):\n        mask = y_true != -1\n        y_true = y_true[mask]\n        y_pred = y_pred[mask]\n        return super().call(y_true, y_pred)\n</code></pre>\n<p>A good starting point is the <code>custom log</code> guide: <a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.</p>\n<p>My question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A <a href=\"https://www.aclweb.org/anthology/P02-1053.pdf\" rel=\"nofollow noreferrer\">classic paper by Peter Turney</a> (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words <em>excellent</em> and <em>poor</em> as a seed set. Turney uses the <a href=\"http://en.wikipedia.org/wiki/Pointwise_mutual_information\" rel=\"nofollow noreferrer\">mutual information</a> of other words with these two adjectives to achieve an accuracy of 74%.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I haven't tried doing untrained sentiment analysis such as you are describing, but off the top of my head I'd say you're oversimplifying the problem.  Simply analyzing adjectives is not enough to get a good grasp of the sentiment of a text; for example, consider the word 'stupid.'  Alone, you would classify that as negative, but if a product review were to have '... [x] product makes their competitors look stupid for not thinking of this feature first...' then the sentiment in there would definitely be positive. The greater context in which words appear definitely matters in something like this.  This is why an untrained bag-of-words approach alone (let alone an even more limited bag-of-adjectives) is not enough to tackle this problem adequately.</p>\n<p>The pre-classified data ('training data') helps in that the problem shifts from trying to determine whether a text is of positive or negative sentiment from scratch, to trying to determine if the text is more similar to positive texts or negative texts, and classify it that way.  The other big point is that textual analyses such as sentiment analysis are often affected greatly by the differences of the characteristics of texts depending on domain.  This is why having a good set of data to train on (that is, accurate data from within the domain in which you are working, and is hopefully representative of the texts you are going to have to classify) is as important as building a good system to classify with.</p>\n<p>Not exactly an article, but hope that helps.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The paper of Turney (2002) mentioned by larsmans is a good basic one. In a newer research, <a href=\"http://web.archive.org/web/20111215183843/http://people.kmi.open.ac.uk/yulan/PAPERS/JST_CIKM09.pdf\" rel=\"noreferrer\">Li and He [2009]</a> introduce an approach using <a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"noreferrer\">Latent Dirichlet Allocation</a> (LDA) to train a model that can classify an article's overall sentiment and topic simultaneously in a totally unsupervised manner. The accuracy they achieve is 84.6%.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n                                As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, <a href=\"/help/reopen-questions\">visit the help center</a> for guidance.\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2012-08-27 11:48:37Z\">12 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>I have been trying to grasp the basics of Support Vector Machines, and downloaded and read many online articles. But still am not able to grasp it.</p>\n<p>I would like to know, if there are some</p>\n<ul>\n<li>nice tutorial</li>\n<li>sample code which can be used for understanding</li>\n</ul>\n<p>or something, that you can think of, and that will enable me to learn SVM Basics easily.</p>\n<p>PS: I somehow managed to learn PCA (Principal Component Analysis).\nBTW, you guys would have guessed that I am working on Machine Learning.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The standard recommendation for a tutorial in SVMs is <a href=\"http://research.microsoft.com/pubs/67119/svmtutorial.pdf\" rel=\"noreferrer\">A Tutorial on Support Vector Machines for Pattern Recognition</a> by Christopher Burges. Another good place to learn about SVMs is the <a href=\"http://see.stanford.edu/see/courseInfo.aspx?coll=348ca38a-3a6d-4052-937d-cb017338d7b1\" rel=\"noreferrer\">Machine Learning Course</a> at Stanford (SVMs are covered in lectures 6-8). Both these are quite theoretical and heavy on the maths.</p>\n<p>As for source code; <a href=\"http://svmlight.joachims.org/\" rel=\"noreferrer\">SVMLight</a>, <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\" rel=\"noreferrer\">libsvm</a> and <a href=\"http://chasen.org/~taku/software/TinySVM/\" rel=\"noreferrer\">TinySVM</a> are all open-source, but the code is not very easy to follow. I haven't looked at each of them very closely, but the source for TinySVM is probably the is easiest to understand. There is also a pseudo-code implementation of the SMO algorithm in <a href=\"http://research.microsoft.com/apps/pubs/?id=68391\" rel=\"noreferrer\">this paper</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a very good beginner's tutorial on SVM:</p>\n<p><a href=\"https://static1.squarespace.com/static/58851af9ebbd1a30e98fb283/t/58902fbae4fcb5398aeb7505/1485844411772/SVM+Explained.pdf\" rel=\"nofollow noreferrer\">SVM explained</a></p>\n<p>I always thought StompChicken's recommended tutorial was a bit confusing in the way that they jump right into talking about bounds and VC statistics and trying to find the optimal machine and such.  It's good if you already understand the basics, though.  </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Lots of video lectures on SVM:<br/>\n<a href=\"http://videolectures.net/Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines/\" rel=\"noreferrer\">http://videolectures.net/Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines/</a></p>\n<p>I found the one by <em>Colin Campbell</em> to be very useful.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is common practice to augment data (add samples programmatically, such as random crops, etc. in the case of a dataset consisting of images) on both training and test set, or just the training data set?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Only on training. Data augmentation is used to increase the size of the training set and to get more different images.\nTechnically, you could use data augmentation on the test set to see how the model behaves on such images, but usually, people don't do it.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would argue that, in some cases, using data augmentation for the <em>validation</em> set can be helpful.</p>\n<p>For example, I train a lot of CNNs for medical image segmentation. Many of <a href=\"https://torchio.readthedocs.io/transforms/augmentation.html\" rel=\"noreferrer\">the augmentation transforms that I use</a> are meant to reduce the image quality so that the network is trained to be robust against such data. If the training set looks bad and the validation set looks nice, it will be hard to compare the losses during training and therefore assessing overfit will be complicated.</p>\n<p>I would never use augmentation for the test set unless I'm using test-time augmentation to improve results or <a href=\"https://www.sciencedirect.com/science/article/pii/S0925231219301961\" rel=\"noreferrer\">estimate aleatoric uncertainty</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Data augmentation is done only on training set as it helps the model become more generalize and robust. So there's no point of augmenting the test set. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-08-10 13:41:44Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/44260217/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is the best way to perform hyperparameter optimization for a Pytorch model? Implement e.g. Random Search myself? Use Skicit Learn? Or is there anything else I am not aware of?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Many researchers use <a href=\"http://ray.readthedocs.io/en/latest/tune.html\" rel=\"noreferrer\">RayTune</a>. It's a scalable hyperparameter tuning framework, specifically for deep learning. You can easily use it with any deep learning framework (2 lines of code below), and it provides most state-of-the-art algorithms, including HyperBand, Population-based Training, Bayesian Optimization, and BOHB.</p>\n<pre><code>import torch.optim as optim\nfrom ray import tune\nfrom ray.tune.examples.mnist_pytorch import get_data_loaders, ConvNet, train, test\n\n\ndef train_mnist(config):\n    train_loader, test_loader = get_data_loaders()\n    model = ConvNet()\n    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n    for i in range(10):\n        train(model, optimizer, train_loader)\n        acc = test(model, test_loader)\n        tune.report(mean_accuracy=acc)\n\n\nanalysis = tune.run(\n    train_mnist, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n\nprint(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n\n# Get a dataframe for analyzing trial results.\ndf = analysis.dataframe()\n</code></pre>\n<p>[Disclaimer: I contribute actively to this project!]</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What I found is following:</p>\n<ul>\n<li><a href=\"https://github.com/hyperopt/hyperopt\" rel=\"noreferrer\">HyperOpt</a></li>\n<li><a href=\"https://optuna.org/\" rel=\"noreferrer\">Optuna</a></li>\n<li><a href=\"https://ray.readthedocs.io/en/latest/tune.html\" rel=\"noreferrer\">tune</a></li>\n</ul>\n<p><strong>More young projects</strong>:</p>\n<ul>\n<li><a href=\"https://github.com/kevinzakka/hypersearch\" rel=\"noreferrer\">hypersearch</a>\nlimited only to FC layers.</li>\n<li><a href=\"https://github.com/skorch-dev/skorch\" rel=\"noreferrer\">skorch</a> Just grid search available</li>\n<li><a href=\"https://github.com/automl/Auto-PyTorch\" rel=\"noreferrer\">Auto-PyTorch</a></li>\n</ul>\n<p><strong>UPDATE</strong>\nsomething new:</p>\n<ul>\n<li><p><a href=\"https://ax.dev/\" rel=\"noreferrer\">Ax: Adaptive Experimentation Platform by facebook</a> </p></li>\n<li><p><a href=\"https://botorch.org/\" rel=\"noreferrer\">BoTorch: Bayesian Optimization in PyTorch</a></p></li>\n</ul>\n<p>Also, I found a useful table at <a href=\"https://towardsdatascience.com/fast-hyperparameter-tuning-at-scale-d428223b081c\" rel=\"noreferrer\">post by @Richard Liaw</a>:</p>\n<p><a href=\"https://i.sstatic.net/53T1bl.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/53T1bl.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://github.com/fmfn/BayesianOptimization\" rel=\"noreferrer\">Bayesian optimization</a> (full disclosure, I've contributed to this package) or <a href=\"https://github.com/zygmuntz/hyperband\" rel=\"noreferrer\">Hyperband</a>.  Both of these methods attempt to automate the hyperparameter tuning stage.  Hyperband is supposedly the state of the art in this space.  Hyperband is the only parameter-free method that I've heard of other than random search.  You can also look into using reinforcement learning to learn the optimal hyperparameters if you prefer.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When doing fitting, I always come across code like</p>\n<pre><code>clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n</code></pre>\n<p>(from <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html#k-fold\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/cross_validation.html#k-fold</a>)</p>\n<p>What does <code>clf</code> stand for? I googled around but didn't find any clues.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the <a href=\"http://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting\" rel=\"noreferrer\"><code>scikit-learn</code> tutorial</a>, it's short for classifier.:</p>\n<blockquote>\n<p>We call our estimator instance <code>clf</code>, as it is a classifier.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In the link you provided, clf refers to classifier.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I run a python program that calls <code>sklearn.metrics</code>'s methods to calculate precision and F1 score. Here is the output when there is no predicted sample:</p>\n<pre><code>/xxx/py2-scikit-learn/0.15.2-comp6/lib/python2.6/site-packages/sklearn/metr\\\nics/metrics.py:1771: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n\n/xxx/py2-scikit-learn/0.15.2-comp6/lib/python2.6/site-packages/sklearn/metr\\\nics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n</code></pre>\n<p>When there is no predicted sample, it means that TP+FP is 0, so </p>\n<ul>\n<li>precision (defined as TP/(TP+FP)) is 0/0, not defined, </li>\n<li>F1 score (defined as 2TP/(2TP+FP+FN)) is 0 if FN is not zero.</li>\n</ul>\n<p>In my case, <code>sklearn.metrics</code> also returns the accuracy as 0.8, and recall as 0. So FN is not zero.</p>\n<p>But why does scikilearn says F1 is ill-defined?</p>\n<p>What is the definition of F1 used by Scikilearn?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py\" rel=\"noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py</a></p>\n<blockquote>\n<p>F1 = 2 * (precision * recall) / (precision + recall)</p>\n</blockquote>\n<p>precision = TP/(TP+FP) as you've just said if predictor doesn't predicts positive class at all - precision is 0.</p>\n<p>recall = TP/(TP+FN), in case if predictor doesn't predict positive class - TP is 0 - recall is 0.</p>\n<p>So now you are dividing 0/0.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Precision, Recall, F1-score</strong> and <strong>Accuracy</strong> calculation  </p>\n<pre><code>- In a given image of Dogs and Cats\n\n  * Total Dogs - 12  D = 12\n  * Total Cats - 8   C = 8\n\n- Computer program predicts\n\n  * Dogs - 8  \n    5 are actually Dogs   T.P = 5\n    3 are not             F.P = 3    \n  * Cats - 12\n    6 are actually Cats   T.N = 6 \n    6 are not             F.N = 6\n\n- Calculation\n\n  * Precision = T.P / (T.P + F.P) =&gt; 5 / (5 + 3)\n  * Recall    = T.P / D           =&gt; 5 / 12\n\n  * F1 = 2 * (Precision * Recall) / (Precision + Recall)\n  * F1 = 0.5\n\n  * Accuracy = T.P + T.N / P + N\n  * Accuracy = 0.55\n</code></pre>\n<p>Wikipedia <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">reference</a> </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Scikit-learn utilizes a very convenient approach based on <code>fit</code> and <code>predict</code> methods. I have time-series data in the format suited for <code>fit</code> and <code>predict</code>.</p>\n<p>For example I have the following <code>Xs</code>:</p>\n<pre><code>[[1.0, 2.3, 4.5], [6.7, 2.7, 1.2], ..., [3.2, 4.7, 1.1]]\n</code></pre>\n<p>and the corresponding <code>ys</code>:</p>\n<pre><code>[[1.0], [2.3], ..., [7.7]]\n</code></pre>\n<p>These data have the following meaning. The values stored in <code>ys</code> form a time series. The values in <code>Xs</code> are corresponding time dependent \"factors\" that are known to have some influence on the values in <code>ys</code> (for example: temperature, humidity and atmospheric pressure).</p>\n<p>Now, of course, I can use <code>fit(Xs,ys)</code>. But then I get a model in which future values in <code>ys</code> depend only on factors and do not dependend on the previous <code>Y</code> values (at least directly) and this is a limitation of the model. I would like to have a model in which <code>Y_n</code> depends also on <code>Y_{n-1}</code> and <code>Y_{n-2}</code> and so on. For example I might want to use an exponential moving average as a model. What is the most elegant way to do it in scikit-learn</p>\n<p><strong>ADDED</strong></p>\n<p>As it has been mentioned in the comments, I can extend <code>Xs</code> by adding <code>ys</code>. But this way has some limitations. For example, if I add the last 5 values of <code>y</code> as 5 new columns to <code>X</code>, the information about time ordering of <code>ys</code> is lost. For example, there is no indication in <code>X</code> that values in the 5th column follows value in the 4th column and so on. As a model, I might want to have a linear fit of the last five <code>ys</code> and use the found linear function to make a prediction. But if I have 5 values in 5 columns it is not so trivial.</p>\n<p><strong>ADDED 2</strong></p>\n<p>To make my problem even more clear, I would like to give one concrete example. I would like to have a \"linear\" model in which <code>y_n = c + k1*x1 + k2*x2 + k3*x3 + k4*EMOV_n</code>, where EMOV_n is just an exponential moving average. How, can I implement this simple model in scikit-learn?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>According to Wikipedia, EWMA works well with stationary data, but it does not work as expected in the presence of trends, or seasonality. In those cases you should use a second or third order EWMA method, respectively. I decided to look at the pandas <code>ewma</code> function to see how it handled trends, and this is what I came up with:</p>\n<pre><code>import pandas, numpy as np\newma = pandas.stats.moments.ewma\n\n# make a hat function, and add noise\nx = np.linspace(0,1,100)\nx = np.hstack((x,x[::-1]))\nx += np.random.normal( loc=0, scale=0.1, size=200 )\nplot( x, alpha=0.4, label='Raw' )\n\n# take EWMA in both directions with a smaller span term\nfwd = ewma( x, span=15 )          # take EWMA in fwd direction\nbwd = ewma( x[::-1], span=15 )    # take EWMA in bwd direction\nc = np.vstack(( fwd, bwd[::-1] )) # lump fwd and bwd together\nc = np.mean( c, axis=0 )          # average  \n\n# regular EWMA, with bias against trend\nplot( ewma( x, span=20 ), 'b', label='EWMA, span=20' )\n\n# \"corrected\" (?) EWMA\nplot( c, 'r', label='Reversed-Recombined' )\n\nlegend(loc=8)\nsavefig( 'ewma_correction.png', fmt='png', dpi=100 )\n</code></pre>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/T9ZSm.png\"/></p>\n<p>As you can see, the EWMA bucks the trend uphill and downhill. We can correct for this (without having to implement a second-order scheme ourselves) by taking the EWMA in both directions and then averaging. I hope your data was stationary!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This <em>might</em> be what you're looking for, with regard to the exponentially weighted moving average:</p>\n<pre><code>import pandas, numpy\newma = pandas.stats.moments.ewma\nEMOV_n = ewma( ys, com=2 )\n</code></pre>\n<p>Here, <code>com</code> is a parameter that you can read about <a href=\"https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.ewma.html\" rel=\"nofollow noreferrer\">here</a>. Then you can combine <code>EMOV_n</code> to <code>Xs</code>, using something like:</p>\n<pre><code>Xs = numpy.vstack((Xs,EMOV_n))\n</code></pre>\n<p>And then you can look at various linear models, <a href=\"http://scikit-learn.org/stable/modules/linear_model.html\" rel=\"nofollow noreferrer\">here</a>, and do something like:</p>\n<pre><code>from sklearn import linear_model\nclf = linear_model.LinearRegression()\nclf.fit ( Xs, ys )\nprint clf.coef_\n</code></pre>\n<p>Best of luck!    </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2017-09-19 17:27:32Z\">7 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/46260775/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I've seen such words as:</p>\n<blockquote>\n<p>A policy defines the learning agent's way of behaving at a given time. Roughly\n  speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.</p>\n</blockquote>\n<p>But still didn't fully understand. What exactly is a policy in reinforcement learning?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The definition is correct, though not instantly obvious if you see it for the first time. Let me put it this way: <em>a policy is an agent's strategy</em>.</p>\n<p>For example, imagine a world where a robot moves across the room and the task is to get to the target point (x, y), where it gets a reward. Here:</p>\n<ul>\n<li>A room is an <em>environment</em></li>\n<li>Robot's current position is a <em>state</em></li>\n<li><p>A <em>policy</em> is what an agent does to accomplish this task: </p>\n<ul>\n<li>dumb robots just wander around randomly until they accidentally end up in the right place (policy #1)</li>\n<li>others may, for some reason, learn to go along the walls most of the route (policy #2)</li>\n<li>smart robots plan the route in their \"head\" and go straight to the goal (policy #3)</li>\n</ul></li>\n</ul>\n<p>Obviously, some policies are better than others, and there are multiple ways to assess them, namely <em>state-value function</em> and <em>action-value function</em>. The goal of RL is to learn the best policy. Now the definition should make more sense (note that in the context time is better understood as a state):</p>\n<p><em>A policy defines the learning agent's way of behaving at a given time.</em></p>\n<h3>Formally</h3>\n<p>More formally, we should first define <em>Markov Decision Process</em> (MDP) as a tuple (<code>S</code>, <code>A</code>, <code>P</code>, <code>R</code>, <code>y</code>), where:</p>\n<ul>\n<li><code>S</code> is a finite set of states</li>\n<li><code>A</code> is a finite set of actions</li>\n<li><code>P</code> is a state transition probability matrix (probability of ending up in a state for each current state and each action)</li>\n<li><code>R</code> is a reward function, given a state and an action</li>\n<li><code>y</code> is a discount factor, between 0 and 1</li>\n</ul>\n<p>Then, a policy <code>π</code> is a probability distribution over actions given states. That is the likelihood of every action when an agent is in a particular state (of course, I'm skipping a lot of details here). This definition corresponds to the second part of your definition.</p>\n<p>I highly recommend <a href=\"https://www.youtube.com/playlist?list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB\" rel=\"noreferrer\">David Silver's RL course</a> available on YouTube. The first two lectures focus particularly on MDPs and policies.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In plain words, in the simplest case, a policy <code>π</code> is a function that takes as input a state <code>s</code> and returns an action <code>a</code>. That is: <code>π(s) → a</code></p>\n<p>In this way, the policy is typically used by the agent to decide what action <code>a</code> should be performed when it is in a given state <code>s</code>.</p>\n<p>Sometimes, the policy can be <em>stochastic</em> instead of <em>deterministic</em>. In such a case, instead of returning a unique action <code>a</code>, the policy returns a probability distribution over a set of actions.</p>\n<p>In general, the goal of any RL algorithm is to learn an optimal policy that achieve a specific goal.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a succinct answer: a policy is the 'thinking' of the agent. It's the mapping of when you are in some state <code>s</code>, which action <code>a</code> should the agent take now? You can think of policies as a lookup table:</p>\n<pre><code>state----action----probability/'goodness' of taking the action\n  1         1                     0.6\n  1         2                     0.4\n  2         1                     0.3\n  2         2                     0.7\n</code></pre>\n<p>If you are in state 1, you'd (assuming a greedy strategy) pick action 1. If you are in state 2, you'd pick action 2.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>We are writing a small ANN which is supposed to categorize 7000 products into 7 classes based on 10 input variables.</p>\n<p>In order to do this we have to use k-fold cross validation but we are kind of confused.</p>\n<p>We have this excerpt from the presentation slide:</p>\n<p><a href=\"https://i.sstatic.net/aDaSc.png\" rel=\"noreferrer\"><img alt=\"k-fold cross validation diagram\" src=\"https://i.sstatic.net/aDaSc.png\"/></a></p>\n<p><strong>What are exactly the validation and test sets?</strong> </p>\n<p>From what we understand is that we run through the 3 training sets and adjust the weights (single epoch). Then what do we do with the validation? Because from what I understand is that the test set is used to get the error of the network.</p>\n<p>What happens next is also confusing to me. When does the crossover take place?</p>\n<p>If it's not too much to ask, a bullet list of step would be appreciated</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You seem to be a bit confused (I remember I was too) so I am going to simplify things for you. ;)</p>\n<h1>Sample Neural Network Scenario</h1>\n<p>Whenever you are given a task such as devising a neural network you are often also given a sample dataset to use for training purposes. Let us assume you are training a simple neural network system <code>Y = W · X</code> where <code>Y</code> is the output computed from calculating the scalar product (·) of the weight vector <code>W</code> with a given sample vector <code>X</code>. Now, the naive way to go about this would be using the entire dataset of, say, 1000 samples to train the neural network. Assuming that the training converges and your weights stabilise you can then safely say that you network will correctly classify the training data. <em>But what happens to the network if presented with previously unseen data?</em> Clearly the purpose of such systems is to be able to generalise and correctly classify data other than the one used for training. </p>\n<h2>Overfitting Explained</h2>\n<p>In any real-world situation, however, previously-unseen/new data is only available once your neural network is deployed in a, let's call it, production environment. But since you have not tested it adequately you are probably going to have a bad time. :) The phenomenon by which any learning system matches its training set almost perfectly but constantly fails with unseen data is called <a href=\"http://en.wikipedia.org/wiki/Overfitting\" rel=\"noreferrer\">overfitting</a>.</p>\n<h1>The Three Sets</h1>\n<p>Here come in the validation and testing parts of the algorithm. Let's go back to the original dataset of 1000 samples. What you do is you split it into three sets -- <strong>training</strong>, <strong>validation</strong> and <strong>testing</strong> (<code>Tr</code>, <code>Va</code> and <code>Te</code>) -- using carefully selected proportions. (80-10-10)% is usually a good proportion, where:</p>\n<ul>\n<li><code>Tr = 80%</code></li>\n<li><code>Va = 10%</code></li>\n<li><code>Te = 10%</code></li>\n</ul>\n<h2>Training and Validation</h2>\n<p>Now what happens is that the neural network is trained on the <code>Tr</code> set and its weights are correctly updated. The validation set <code>Va</code> is then used to compute the classification error <code>E = M - Y</code> using the weights resulting from the training, where <code>M</code> is the expected output vector taken from the validation set and <code>Y</code> is the computed output resulting from the classification (<code>Y = W * X</code>). If the error is higher than a user-defined threshold then the whole <a href=\"https://stackoverflow.com/questions/25887205/what-is-an-epoch-in-anns-and-how-does-it-translate-into-code-in-matlab/25887889#25887889\">training-validation epoch</a> is repeated. This training phase ends when the error computed using the validation set is deemed low enough.</p>\n<h3>Smart Training</h3>\n<p>Now, a smart ruse here is to <strong>randomly select which samples to use for training and validation</strong> from the total set <code>Tr + Va</code> at each epoch iteration. This ensures that the network will not over-fit the training set.</p>\n<h2>Testing</h2>\n<p>The testing set <code>Te</code> is then  used to measure the performance of the network. This data is perfect for this purpose as it was never used throughout the training and validation phase. It is effectively a small set of previously unseen data, which is supposed to mimic what would happen once the network is deployed in the production environment.</p>\n<p>The performance is again measured in term of classification error as explained above. The performance can also (or maybe even should) be measured in terms of <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">precision and recall</a> so as to know where and how the error occurs, but that's the topic for another Q&amp;A.</p>\n<h1>Cross-Validation</h1>\n<p>Having understood this training-validation-testing mechanism, one can further strengthen the network against over-fitting by performing <a href=\"https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation\" rel=\"noreferrer\">K-fold cross-validation</a>. This is somewhat an evolution of the smart ruse I explained above. This technique involves <strong>performing K rounds of training-validation-testing on, different, non-overlapping, equally-proportioned <code>Tr</code>, <code>Va</code> and <code>Te</code> sets</strong>.</p>\n<p>Given <code>k = 10</code>, for each value of K you will split your dataset into <code>Tr+Va = 90%</code> and <code>Te = 10%</code> and you will run the algorithm, recording the testing performance.</p>\n<pre class=\"lang-cs prettyprint-override\"><code>k = 10\nfor i in 1:k\n     # Select unique training and testing datasets\n     KFoldTraining &lt;-- subset(Data)\n     KFoldTesting &lt;-- subset(Data)\n\n     # Train and record performance\n     KFoldPerformance[i] &lt;-- SmartTrain(KFoldTraining, KFoldTesting)\n\n# Compute overall performance\nTotalPerformance &lt;-- ComputePerformance(KFoldPerformance)\n</code></pre>\n<h2>Overfitting Shown</h2>\n<p>I am taking the world-famous plot below from <a href=\"http://en.wikipedia.org/wiki/Overfitting\" rel=\"noreferrer\">wikipedia</a> to show how the validation set helps prevent overfitting. The training error, in blue, tends to decrease as the number of epochs increases: the network is therefore attempting to match the training set exactly. The validation error, in red, on the other hand follows a different, u-shaped profile. The minimum of the curve is when ideally the training should be stopped as this is the point at which the training and validation error are lowest.</p>\n<p><img alt=\"Overfitting reduced by validating neural network\" src=\"https://i.sstatic.net/8Kguj.png\"/></p>\n<h2>References</h2>\n<p>For more references <a href=\"http://research.microsoft.com/en-us/um/people/cmbishop/prml/\" rel=\"noreferrer\">this excellent book</a> will give you both a sound knowledge of machine learning as well as several migraines. Up to you to decide if it's worth it. :)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ol>\n<li><p>Divide your data into K non-overlapping folds. Have each fold K contain an equal number of items from each of the m classes (stratified cross-validation; if you have 100 items from class A and 50 from class B and you do 2 fold validation, each fold should contain a random 50 items from A and 25 from B).</p>\n<ol start=\"2\">\n<li><p>For i in 1..k:</p>\n<ul>\n<li>Designate fold i the test fold</li>\n<li>Designate one of the remaining k-1 folds the validation fold (this can either be random or a function of i, doesn't really matter)</li>\n<li>Designate all remaining folds the training fold</li>\n<li>Do a grid search for all free parameters (e.g. learning rate, # of neurons in hidden layer) training on your training data and computing loss on your validation data. Pick parameters minimising loss</li>\n<li>Use the classifier with the winning parameters to evaluate test loss. Accumulate results</li>\n</ul></li>\n</ol></li>\n</ol>\n<p>You have now collected aggregate results across all the folds. This is your final performance. If you're going to apply this for real, in the wild, use the best parameters from the grid search to train on all the data.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My code is follow the class of machine learning of google.The two code are same.I don't know why it show error.May be the type of variable is error.But google's code is same to me.Who has ever had this problem?</p>\n<p>This is error</p>\n<pre><code>[0 1 2]\n[0 1 2]\nTraceback (most recent call last):\n  File \"/media/joyce/oreo/python/machine_learn/VisualizingADecisionTree.py\", line 34, in &lt;module&gt;\n    graph.write_pdf(\"iris.pdf\")\nAttributeError: 'list' object has no attribute 'write_pdf'\n[Finished in 0.4s with exit code 1]\n[shell_cmd: python -u \"/media/joyce/oreo/python/machine_learn/VisualizingADecisionTree.py\"]\n[dir: /media/joyce/oreo/python/machine_learn]\n[path: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games]\n</code></pre>\n<p>This is code</p>\n<pre><code>import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\niris = load_iris()\ntest_idx = [0, 50, 100]\n\n# training data\ntrain_target = np.delete(iris.target, test_idx)\ntrain_data = np.delete(iris.data, test_idx, axis=0)\n\n# testing data\ntest_target = iris.target[test_idx]\ntest_data = iris.data[test_idx]\n\nclf = tree.DecisionTreeClassifier()\nclf.fit(train_data, train_target)\n\nprint test_target\nprint clf.predict(test_data) \n\n# viz code\nfrom sklearn.externals.six import StringIO\nimport pydot\ndot_data = StringIO()\ntree.export_graphviz(clf,\n        out_file=dot_data,\n        feature_names=iris.feature_names,\n        class_names=iris.target_names,\n        filled=True, rounded=True,\n        impurity=False)\n\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\ngraph.write_pdf(\"iris.pdf\")\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you are using newer version of python. Please try with pydotplus.</p>\n<pre><code>import pydotplus\n...\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_pdf(\"iris.pdf\")\n</code></pre>\n<p>This should do it.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>pydot.graph_from_dot_data()</code> returns a list, so try:</p>\n<pre><code>graph = pydot.graph_from_dot_data(dot_data.getvalue())\ngraph[0].write_pdf(\"iris.pdf\") \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had exactly the same issue. Turned out that I hadn't installed graphviz. Once i did that it started to work. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between back-propagation and feed-forward neural networks?</p>\n<p>By googling and reading, I found that in feed-forward there is only forward direction, but in back-propagation once we need to do a forward-propagation and then back-propagation. I referred to <a href=\"http://www.nnwj.de/backpropagation.html\" rel=\"noreferrer\">this link</a></p>\n<ol>\n<li>Any other difference other than the direction of flow? What about the weight calculation? The outcome? </li>\n<li>Say I am implementing back-propagation, i.e. it contains forward and backward flow. So is back-propagation enough for showing feed-forward?</li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ul>\n<li><p>A <a href=\"http://en.wikipedia.org/wiki/Feedforward_neural_network\" rel=\"noreferrer\">Feed-Forward Neural Network</a> is a type of Neural Network <strong>architecture</strong> where the connections are \"fed forward\", i.e. do not form cycles (like in recurrent nets).</p></li>\n<li><p>The term \"Feed forward\" is also used when you input something at the input layer and it <em>travels</em> from input to hidden and from hidden to output layer. <br/><strong>The values are \"fed forward\"</strong>.</p></li>\n</ul>\n<p>Both of these uses of the phrase \"feed forward\" are in a context that has nothing to do with training per se.</p>\n<ul>\n<li>Backpropagation is a <strong>training algorithm</strong> consisting of 2 steps: 1) <strong>Feed forward</strong> the values 2) calculate the error and <strong>propagate it back</strong> to the earlier layers. So to be precise, forward-propagation is part of the backpropagation <strong>algorithm</strong> but comes before back-propagating.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is no pure backpropagation or pure feed-forward neural network.</p>\n<p>Backpropagation is algorithm to train (adjust weight) of neural network.\nInput for backpropagation is output_vector, target_output_vector,\noutput is adjusted_weight_vector.</p>\n<p>Feed-forward is algorithm to calculate output vector from input vector.\nInput for feed-forward is input_vector,\noutput is output_vector.</p>\n<p>When you are training neural network, you need to use both algorithms.</p>\n<p>When you are using neural network (which have been trained), you are using only feed-forward.</p>\n<p>Basic type of neural network is multi-layer perceptron, which is Feed-forward backpropagation neural network.</p>\n<p>There are also more advanced types of neural networks, using modified algorithms.</p>\n<p>Also good source to study : <a href=\"ftp://ftp.sas.com/pub/neural/FAQ.html\" rel=\"noreferrer\">ftp://ftp.sas.com/pub/neural/FAQ.html</a>\nBest to understand principle is to program it (tutorial in this video) <a href=\"https://www.youtube.com/watch?v=KkwX7FkLfug\" rel=\"noreferrer\">https://www.youtube.com/watch?v=KkwX7FkLfug</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To be simple: </p>\n<p>Feed-foward is an architecture. The contrary one is Recurrent Neural Networks. </p>\n<p>Back Propagation (BP) is a solving method. BP can solve both feed-foward and Recurrent Neural Networks.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When we have a high degree linear polynomial that is used to fit a set of points in a linear regression setup, to prevent overfitting, we use regularization, and we include a lambda parameter in the cost function. This lambda is then used to update the theta parameters in the gradient descent algorithm.</p>\n<p>My question is how do we calculate this lambda regularization parameter?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The regularization parameter (lambda) is an input to your model so what you probably want to know is how do you <em>select</em> the value of lambda.  The regularization parameter reduces overfitting, which reduces the variance of your estimated regression parameters; however, it does this at the expense of adding bias to your estimate.  Increasing lambda results in less overfitting but also greater bias.  So the real question is \"How much bias are you willing to tolerate in your estimate?\"</p>\n<p>One approach you can take is to randomly subsample your data a number of times and look at the variation in your estimate.  Then repeat the process for a slightly larger value of lambda to see how it affects the variability of your estimate.  Keep in mind that whatever value of lambda you decide is appropriate for your subsampled data, you can likely use a smaller value to achieve comparable regularization on the full data set.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h3>CLOSED FORM (TIKHONOV) VERSUS GRADIENT DESCENT</h3>\n<p>Hi! nice explanations for the intuitive and top-notch mathematical approaches there. I just wanted to add some specificities that, where not \"problem-solving\", may definitely help to speed up and give some consistency to the process of finding a good regularization hyperparameter.</p>\n<p>I assume that you are talking about the <strong>L2</strong> (a.k. \"weight decay\") regularization, linearly weighted by the <em>lambda</em> term, and that you are optimizing the weights of your model either with the <strong>closed-form <a href=\"https://en.wikipedia.org/wiki/Tikhonov_regularization\" rel=\"nofollow noreferrer\">Tikhonov</a> equation</strong> (highly recommended for low-dimensional  linear regression models), or with some variant of <strong>gradient descent with backpropagation</strong>. And that in this context, you want to choose the value for <em>lambda</em> that provides best generalization ability.</p>\n<hr/>\n<h3>CLOSED FORM (TIKHONOV)</h3>\n<p>If you are able to go the Tikhonov way with your model (<a href=\"https://www.youtube.com/watch?v=NN7mBupK-8o#t=12m45s\" rel=\"nofollow noreferrer\">Andrew Ng</a> says under 10k dimensions, but this suggestion is at least 5 years old) <a href=\"https://en.wikipedia.org/wiki/Tikhonov_regularization#Determination_of_the_Tikhonov_factor\" rel=\"nofollow noreferrer\">Wikipedia - determination of the Tikhonov factor</a> offers an interesting <strong>closed-form solution, which has been proven to provide the optimal value</strong>. But this solution probably raises some kind of implementation issues (time complexity/numerical stability) I'm not aware of, because there is no mainstream algorithm to perform it. This <a href=\"https://arxiv.org/abs/1610.01952\" rel=\"nofollow noreferrer\">2016 paper</a> looks very promising though and may be worth a try if you really have to optimize your linear model to its best.</p>\n<ul>\n<li>For a quicker prototype implementation, this <a href=\"https://pypi.python.org/pypi/InverseProblem/1.0\" rel=\"nofollow noreferrer\">2015</a> Python package seems to deal with it iteratively, you could let it optimize and then extract the final value for the lambda:</li>\n</ul>\n<blockquote>\n<p>In this new innovative method, we have derived an iterative approach to solving the general Tikhonov regularization problem, which converges to the noiseless solution, does not depend strongly on the choice of lambda, and yet still avoids the inversion problem.</p>\n</blockquote>\n<p>And from the <a href=\"https://github.com/kathrynthegreat/InverseProblem\" rel=\"nofollow noreferrer\">GitHub README</a> of the project:\n<code>InverseProblem.invert(A, be, k, l) #this will invert your A matrix, where be is noisy be, k is the no. of iterations, and lambda is your dampening effect (best set to 1)</code></p>\n<hr/>\n<h3>GRADIENT DESCENT</h3>\n<p><em>All links of this part are from Michael Nielsen's amazing online book \"Neural Networks and Deep Learning\", recommended reading!</em></p>\n<p>For this approach it seems to be even less to be said: the cost function is usually non-convex, the optimization is performed numerically and the performance of the model is measured by some form of cross validation (see <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization\" rel=\"nofollow noreferrer\">Overfitting and Regularization</a> and <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#why_does_regularization_help_reduce_overfitting\" rel=\"nofollow noreferrer\">why does regularization help reduce overfitting</a> if you haven't had enough of that). But even when cross-validating, Nielsen suggests something: you may want to take a look at <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#regularization\" rel=\"nofollow noreferrer\">this detailed explanation</a>  on how does the L2 regularization provide a weight decaying effect, but the summary is that it is <strong>inversely proportional to the number of samples <code>n</code></strong>, so when calculating the gradient descent equation with the L2 term,</p>\n<blockquote>\n<p>just use backpropagation, as usual, and then add <code>(λ/n)*w</code> to the partial derivative of all the weight terms.</p>\n</blockquote>\n<p>And his conclusion is that, when wanting a similar regularization effect with a different number of samples, lambda has to be changed proportionally:</p>\n<blockquote>\n<p>we need to modify the regularization parameter. The reason is because the size <code>n</code> of the training set has changed from <code>n=1000</code> to <code>n=50000</code>, and this changes the weight decay factor <code>1−learning_rate*(λ/n)</code>. If we continued to use <code>λ=0.1</code> that would mean much less weight decay, and thus much less of a regularization effect. We compensate by changing to <code>λ=5.0</code>.</p>\n</blockquote>\n<p>This is only useful when applying the same model to different amounts of the same data, but I think it opens up the door for some intuition on how it should work, and, more importantly, speed up the hyperparametrization process by allowing you to finetune lambda in smaller subsets and then scale up.</p>\n<p>For choosing the exact values, he suggests in his conclusions on <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network%27s_hyper-parameters\" rel=\"nofollow noreferrer\">how to choose a neural network's hyperparameters</a> the purely empirical approach: start with 1 and then progressively multiply&amp;divide by 10 until you find the proper order of magnitude, and then do a local search within that region. In the comments of <a href=\"https://scicomp.stackexchange.com/a/10673\">this SE related question</a>, the user Brian Borchers suggests also a very well known method that may be useful for that local search:</p>\n<ol>\n<li>Take small subsets of the training and validation sets (to be able to make many of them in a reasonable amount of time)</li>\n<li>Starting with <code>λ=0</code> and increasing by small amounts within some region, perform a quick training&amp;validation of the model and plot both loss functions</li>\n<li>You will observe three things:\n<ul>\n<li>The CV loss function will be consistently higher than the training one, since your model is optimized for the training data exclusively (<em>EDIT: After some time I've seen a MNIST case where adding L2 helped the CV loss decrease faster than the training one until convergence. Probably due to the ridiculous consistency of the data and a suboptimal hyperparametrization though</em>).</li>\n<li>The training loss function will have its minimum for <code>λ=0</code>, and then increase with the regularization, since preventing the model from optimally fitting the training data is exactly what regularization does.</li>\n<li>The CV loss function will start high at <code>λ=0</code>, then decrease, and then start increasing again at some point (<em>EDIT: this assuming that the setup is able to overfit for <code>λ=0</code>, i.e. the model has enough power and no other regularization means are heavily applied</em>).</li>\n</ul>\n</li>\n<li>The optimal value for <code>λ</code> will be probably somewhere around the minimum of the CV loss function, it also may depend a little on how does the training loss function look like. See the picture for a possible (but not the only one) representation of this: instead of \"model complexity\" you should interpret the x axis <strong>as <code>λ</code> being zero at the right and increasing towards the left</strong>.</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/ZTQSP.png\" rel=\"nofollow noreferrer\"><img alt='L2 diagnostics: instead of \"model complexity\" one should interpret the x axis **as λ being zero at the right and increasing towards the left' src=\"https://i.sstatic.net/ZTQSP.png\"/></a></p>\n<p>Hope this helps! Cheers,<br/>\nAndres</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The cross validation described above is a method used often in Machine Learning. However, choosing a reliable and safe regularization parameter is still a very hot topic of research in mathematics. \nIf you need some ideas (and have access to a decent university library) you can have a look at this paper:\n<a href=\"http://www.sciencedirect.com/science/article/pii/S0378475411000607\" rel=\"noreferrer\">http://www.sciencedirect.com/science/article/pii/S0378475411000607</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Occasionally I see some models are using <code>SpatialDropout1D</code> instead of <code>Dropout</code>. For example, in the Part of speech tagging neural network, they use:</p>\n<pre class=\"lang-python prettyprint-override\"><code>model = Sequential()\nmodel.add(Embedding(s_vocabsize, EMBED_SIZE,\n                    input_length=MAX_SEQLEN))\nmodel.add(SpatialDropout1D(0.2)) ##This\nmodel.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(RepeatVector(MAX_SEQLEN))\nmodel.add(GRU(HIDDEN_SIZE, return_sequences=True))\nmodel.add(TimeDistributed(Dense(t_vocabsize)))\nmodel.add(Activation(\"softmax\"))\n</code></pre>\n<p>According to Keras' documentation, it says:</p>\n<blockquote>\n<p>This version performs the same function as Dropout, however it drops\n  entire 1D feature maps instead of individual elements.</p>\n</blockquote>\n<p>However, I am unable to understand the meaning of <strong>entrie 1D feature</strong>. More specifically, I am unable to visualize <code>SpatialDropout1D</code> in the same model explained in <a href=\"https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network\" rel=\"noreferrer\">quora</a>.\nCan someone explain this concept by using the same model as in quora?</p>\n<p>Also, under what situation we will use <code>SpatialDropout1D</code> instead of <code>Dropout</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To make it simple, I would first note that so-called feature maps (1D, 2D, etc.) is our regular channels. Let's look at examples:</p>\n<ol>\n<li><p><code>Dropout()</code>: Let's define 2D input: [[1, 1, 1], [2, 2, 2]]. Dropout will consider every element independently, and may result in something like [[1, 0, 1], [0, 2, 2]]</p></li>\n<li><p><code>SpatialDropout1D()</code>: In this case result will look like [[1, 0, 1], [2, 0, 2]]. Notice that 2nd element was zeroed along <strong>all</strong> channels.</p></li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>The noise shape</h2>\n<p>In order to understand <code>SpatialDropout1D</code>, you should get used to the notion of the <strong>noise shape</strong>. In plain vanilla dropout, each element is kept or dropped independently. For example, if the tensor is <code>[2, 2, 2]</code>, each of 8 elements can be zeroed out depending on random coin flip (with certain \"heads\" probability); in total, there will be 8 independent coin flips and any number of values may become zero, from <code>0</code> to <code>8</code>.</p>\n<p>Sometimes there is a need to do more than that. For example, one may need to drop the <em>whole slice</em> along <code>0</code> axis. The <code>noise_shape</code> in this case is <code>[1, 2, 2]</code> and the dropout involves only 4 independent random coin flips. The first component will either be kept together or be dropped together. The number of zeroed elements can be <code>0</code>, <code>2</code>, <code>4</code>, <code>6</code> or <code>8</code>. It cannot be <code>1</code> or <code>5</code>.</p>\n<p>Another way to view this is to imagine that input tensor is in fact <code>[2, 2]</code>, but each value is double-precision (or multi-precision). Instead of dropping the bytes in the middle, the layer drops the full multi-byte value.</p>\n<h2>Why is it useful?</h2>\n<p>The example above is just for illustration and isn't common in real applications. More realistic example is this: <code>shape(x) = [k, l, m, n]</code> and <code>noise_shape = [k, 1, 1, n]</code>. In this case, each batch and channel component will be kept independently, but each row and column will be kept or not kept together. In other words, the <em>whole</em> <code>[l, m]</code> <em>feature map</em> will be either kept or dropped.</p>\n<p>You may want to do this to account for adjacent pixels correlation, especially in the early convolutional layers. Effectively, you want to prevent co-adaptation of pixels with its neighbors across the feature maps, and make them learn as if no other feature maps exist. This is exactly what <code>SpatialDropout2D</code> is doing: it promotes independence between feature maps.</p>\n<p>The <code>SpatialDropout1D</code> is very similar: given <code>shape(x) = [k, l, m]</code> it uses <code>noise_shape = [k, 1, m]</code> and drops entire 1-D feature maps.</p>\n<p>Reference: <a href=\"https://arxiv.org/abs/1411.4280\" rel=\"noreferrer\">Efficient Object Localization Using Convolutional Networks</a> \nby Jonathan Tompson at al.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to use SGD to classify a large dataset. As the data is too large to fit into memory, I'd like to use the <em>partial_fit</em> method to train the classifier. I have selected a sample of the dataset (100,000 rows) that fits into memory to test <em>fit</em> vs. <em>partial_fit</em>:</p>\n<pre><code>from sklearn.linear_model import SGDClassifier\n\ndef batches(l, n):\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]\n\nclf1 = SGDClassifier(shuffle=True, loss='log')\nclf1.fit(X, Y)\n\nclf2 = SGDClassifier(shuffle=True, loss='log')\nn_iter = 60\nfor n in range(n_iter):\n    for batch in batches(range(len(X)), 10000):\n        clf2.partial_fit(X[batch[0]:batch[-1]+1], Y[batch[0]:batch[-1]+1], classes=numpy.unique(Y))\n</code></pre>\n<p>I then test both classifiers with an identical test set. In the first case I get an accuracy of 100%. As I understand it, SGD by default passes 5 times over the training data (n_iter = 5).</p>\n<p>In the second case, I have to pass 60 times over the data to reach the same accuracy.</p>\n<p>Why this difference (5 vs. 60)? Or am I doing something wrong?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have finally found the answer. You need to <strong>shuffle the training data between each iteration</strong>, as setting <em>shuffle=True</em> when instantiating the model will NOT shuffle the data when using <em>partial_fit</em> (it only applies to <em>fit</em>). Note: it would have been helpful to find this information on the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.partial_fit\">sklearn.linear_model.SGDClassifier page</a>.</p>\n<p>The amended code reads as follows:</p>\n<pre><code>from sklearn.linear_model import SGDClassifier\nimport random\nclf2 = SGDClassifier(loss='log') # shuffle=True is useless here\nshuffledRange = range(len(X))\nn_iter = 5\nfor n in range(n_iter):\n    random.shuffle(shuffledRange)\n    shuffledX = [X[i] for i in shuffledRange]\n    shuffledY = [Y[i] for i in shuffledRange]\n    for batch in batches(range(len(shuffledX)), 10000):\n        clf2.partial_fit(shuffledX[batch[0]:batch[-1]+1], shuffledY[batch[0]:batch[-1]+1], classes=numpy.unique(Y))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to use Naive Bayes algorithm for my dataset. I'm able to find out the accuracy but trying to find out precision and recall for the same. But, it is throwing the following error:</p>\n<pre class=\"lang-none prettyprint-override\"><code>ValueError: Target is multiclass but average='binary'. Please choose another average setting.\n</code></pre>\n<p>Can anyone please suggest me how to proceed with it. I have tried using <code>average ='micro'</code> in the precision and the recall scores. It worked without any errors but it is giving the same score for accuracy, precision, recall.</p>\n<p>My dataset:</p>\n<p>train_data.csv:</p>\n<pre class=\"lang-none prettyprint-override\"><code>review,label\nColors &amp; clarity is superb,positive\nSadly the picture is not nearly as clear or bright as my 40 inch Samsung,negative\n</code></pre>\n<p>test_data.csv:</p>\n<pre class=\"lang-none prettyprint-override\"><code>review,label\nThe picture is clear and beautiful,positive\nPicture is not clear,negative\n</code></pre>\n<p>My code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\n\nX_train, y_train = pd.read_csv('train_data.csv')\nX_test, y_test = pd.read_csv('test_data.csv')\n\nvec = CountVectorizer() \nX_train_transformed = vec.fit_transform(X_train) \nX_test_transformed = vec.transform(X_test)\n\nclf = MultinomialNB()\nclf.fit(X_train_transformed, y_train)\n\nscore = clf.score(X_test_transformed, y_test)\n\ny_pred = clf.predict(X_test_transformed)\ncm = confusion_matrix(y_test, y_pred)\n\nprecision = precision_score(y_test, y_pred, pos_label='positive')\nrecall = recall_score(y_test, y_pred, pos_label='positive')\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to add the <code>'average'</code> param. According to <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\" rel=\"noreferrer\">the documentation</a>:</p>\n<blockquote>\n<p><strong>average : <em>string, [None, ‘binary’ (default), ‘micro’, ‘macro’,\n  ‘samples’, ‘weighted’]</em></strong></p>\n<p>This parameter is required for multiclass/multilabel targets. If <code>None</code>, the \n      scores for each class are returned. Otherwise, this\n      determines the type of averaging performed on the data:</p>\n</blockquote>\n<p>Do this:</p>\n<pre><code>print(\"Precision Score : \",precision_score(y_test, y_pred, \n                                           pos_label='positive'\n                                           average='micro'))\nprint(\"Recall Score : \",recall_score(y_test, y_pred, \n                                           pos_label='positive'\n                                           average='micro'))\n</code></pre>\n<p>Replace <code>'micro'</code> with any one of the above options except <code>'binary'</code>. Also, in the multiclass setting, there is no need to provide the <code>'pos_label'</code> as it will be anyways ignored.</p>\n<p>Update for comment:</p>\n<p>Yes, they can be equal. Its given in the <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\" rel=\"noreferrer\">user guide here</a>:</p>\n<blockquote>\n<p>Note that for “micro”-averaging in a multiclass setting with all\n  labels included will produce equal precision, recall and F, while\n  “weighted” averaging may produce an F-score that is not between\n  precision and recall.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The error is pretty self-explanatory. It is saying for a problem with more than 2 classes, there needs to be some kind of averaging rule. The valid rules are: <code>'micro'</code>, <code>'macro'</code>, <code>'weighted'</code> and <code>None</code> (the documentation lists <code>'samples'</code> but it's not applicable for multi-class targets).</p>\n<p>If we look at its <a href=\"https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/metrics/_classification.py#L1714-L1740\" rel=\"noreferrer\">source code</a>, multi-class problems are treated like a multi-label problem when it comes to precision and recall score computation because the underlying confusion matrix used (<code>multilabel_confusion_matrix</code>) is the same.<sup>1</sup> This confusion matrix creates a 3D array where each \"sub-matrix\" is the 2x2 confusion matrix where the positive value is one of the labels.</p>\n<h3>What are the differences between each averaging rule?</h3>\n<ul>\n<li><p>With <code>average=None</code>, the precision/recall scores of each class is returned (without any averaging), so we get an array of scores whose length is equal to the number of classes.<sup>2</sup></p>\n</li>\n<li><p>With <code>average='macro'</code>, precision/recall is computed for each class and then the average is taken. Its formula is as follows:</p>\n<p><a href=\"https://i.sstatic.net/e01xN.png\" rel=\"noreferrer\"><img alt=\"macro\" src=\"https://i.sstatic.net/e01xN.png\"/></a></p>\n</li>\n<li><p>With <code>average='micro'</code>, the contributions of all classes are summed up to compute the average precision/recall. Its formula is as follows:</p>\n<p><a href=\"https://i.sstatic.net/fp4td.png\" rel=\"noreferrer\"><img alt=\"micro\" src=\"https://i.sstatic.net/fp4td.png\"/></a></p>\n</li>\n<li><p><code>average='weighted'</code> is really a weighted macro average where the weights are the actual positive classes. Its formula is as follows:</p>\n<p><a href=\"https://i.sstatic.net/JMDrn.png\" rel=\"noreferrer\"><img alt=\"weighted\" src=\"https://i.sstatic.net/JMDrn.png\"/></a></p>\n</li>\n</ul>\n<hr/>\n<p>Let's consider an example.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom sklearn import metrics\ny_true, y_pred = np.random.default_rng(0).choice(list('abc'), size=(2,100), p=[.8,.1,.1])\nmcm = metrics.multilabel_confusion_matrix(y_true, y_pred)\n</code></pre>\n<p>The multilabel confusion matrix computed above looks like the following.</p>\n<p><a href=\"https://i.sstatic.net/JsedR.png\" rel=\"noreferrer\"><img alt=\"confusion matrix\" src=\"https://i.sstatic.net/JsedR.png\"/></a></p>\n<p>The respective precision/recall scores are as follows:</p>\n<ul>\n<li><p><code>average='macro'</code> precision/recall are:</p>\n<pre class=\"lang-py prettyprint-override\"><code>recall_macro = (57 / (57 + 16) + 1 / (1 + 10) + 6 / (6 + 10)) / 3\nprecision_macro = (57 / (57 + 15) + 1 / (1 + 13) + 6 / (6 + 8)) / 3\n\n# verify using sklearn.metrics.precision_score and sklearn.metrics.recall_score\nrecall_macro == metrics.recall_score(y_true, y_pred, average='macro')        # True\nprecision_macro == metrics.precision_score(y_true, y_pred, average='macro')  # True\n</code></pre>\n</li>\n<li><p><code>average='micro'</code> precision/recall are:</p>\n<pre class=\"lang-py prettyprint-override\"><code>recall_micro = (57 + 1 + 6) / (57 + 16 + 1 + 10 + 6 + 10)\nprecision_micro = (57 + 1 + 6) / (57 + 15 + 1 + 13 + 6 + 8)\n\n# verify using sklearn.metrics.precision_score and sklearn.metrics.recall_score\nrecall_micro == metrics.recall_score(y_true, y_pred, average='micro')        # True\nprecision_micro == metrics.precision_score(y_true, y_pred, average='micro')  # True\n</code></pre>\n</li>\n<li><p><code>average='weighted'</code> precision/recall are:</p>\n<pre class=\"lang-py prettyprint-override\"><code>recall_weighted = (57 / (57 + 16) * (57 + 16) + 1 / (1 + 10) * (1 + 10) + 6 / (6 + 10) * (6 + 10)) / (57 + 16 + 1 + 10 + 6 + 10)\nprecision_weighted = (57 / (57 + 15) * (57 + 16) + 1 / (1 + 13) * (1 + 10) + 6 / (6 + 8) * (6 + 10)) / (57 + 16 + 1 + 10 + 6 + 10)\n\n# verify using sklearn.metrics.precision_score and sklearn.metrics.recall_score\nrecall_weighted == metrics.recall_score(y_true, y_pred, average='weighted')        # True\nprecision_weighted == metrics.precision_score(y_true, y_pred, average='weighted')  # True\n</code></pre>\n</li>\n</ul>\n<p>As you can see, the example here is imbalanced (class <code>a</code> has 80% frequency while <code>b</code> and <code>c</code> have 10% each). The main difference between the averaging rules is that <code>'macro'</code>-averaging doesn't account for class imbalance but <code>'micro'</code> and <code>'weighted'</code> do. So <code>'macro'</code> is sensitive to the class imbalance and may result in an \"artificially\" high or low score depending on the imbalance.</p>\n<p>Also, it's very easy to see from the formula that recall scores for <code>'micro'</code> and <code>'weighted'</code> are equal.</p>\n<h3>Why is accuracy == recall == precision == f1-score for <code>average='micro'</code>?</h3>\n<p>It may be easier to understand it visually.</p>\n<p>If we look at the multilabel confusion matrix as constructed above, each sub-matrix corresponds to a One vs Rest classification problem; i.e. in each <em>not</em> column/row of a sub-matrix, the other two labels are accounted for.</p>\n<p>For example, for the first sub-matrix, there are</p>\n<ul>\n<li>57 true positives (<code>a</code>)</li>\n<li>16 false negatives (either <code>b</code> or <code>c</code>)</li>\n<li>15 false positives (either <code>b</code> or <code>c</code>)</li>\n<li>12 true negatives</li>\n</ul>\n<p>For the computation of precision/recall, only TP, FN and FP matter. As detailed above, FN and FP counts could be either <code>b</code> or <code>c</code>; since it is binary, this sub-matrix, by itself, cannot say how many of each is predicted; however, we can determine exactly how many of each were correctly classified by computing a multiclass confusion matrix by simply calling the <code>confusion_matrix()</code> method.</p>\n<pre class=\"lang-py prettyprint-override\"><code>mccm = metrics.confusion_matrix(y_true, y_pred)\n</code></pre>\n<p>The following graph plots the same confusion matrix (<code>mccm</code>) using different background colors (yellow background corresponds to TP, red background corresponds to false negatives in the first sub-matrix, orange correspond to false positives in the third sub-matrix etc.). So these are actually TP, FN and FP in the multilabel confusion matrix \"expanded\" to account for exactly what was the negative class. The color scheme on the left graph matches the colors of TP and FN counts in the multilabel confusion matrix (those used to determine <em>recall</em>) and the color scheme on the right graph matches the colors of TP and FP (those used to determine <em>precision</em>).</p>\n<p><a href=\"https://i.sstatic.net/tqLzR.png\" rel=\"noreferrer\"><img alt=\"confusion matrices\" src=\"https://i.sstatic.net/tqLzR.png\"/></a></p>\n<p>With <code>average='micro'</code>, the ratio of yellow background numbers and all numbers in the left graph determine <em>recall</em> and the yellow background numbers and all numbers in the right graph determine <em>precision</em>. If we look closely the same ratio also determines <em>accuracy</em>. Moreover, since <em>f1-score</em> is the harmonic mean of <em>precision</em> and <em>recall</em> and given they are equal, we have the relationship <code>recall == precision == accuracy == f1-score</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to cluster a simple univariate data set into a preset number of clusters. Technically it would be closer to binning or sorting the data since it is only 1D, but my boss is calling it clustering, so I'm going to stick to that name. \nThe current method used by the system I'm on is K-means, but that seems like overkill.</p>\n<p>Is there a better way of performing this task?</p>\n<p>Answers to some other posts are mentioning KDE (Kernel Density Estimation), but that is a density estimation method, how would that work? </p>\n<p>I see how KDE returns a density, but how do I tell it to split the data into bins? </p>\n<p>How do I have a fixed number of bins independent of the data (that's one of my requirements) ? </p>\n<p>More specifically, how would one pull this off using scikit learn? </p>\n<p>My input file looks like: </p>\n<pre><code> str ID     sls\n 1           10\n 2           11 \n 3            9\n 4           23\n 5           21\n 6           11  \n 7           45\n 8           20\n 9           11\n 10          12\n</code></pre>\n<p>I want to group the sls number into clusters or bins, such that:</p>\n<pre><code>Cluster 1: [10 11 9 11 11 12] \nCluster 2: [23 21 20] \nCluster 3: [45] \n</code></pre>\n<p>And my output file will look like: </p>\n<pre><code> str ID     sls    Cluster ID  Cluster centroid\n    1        10       1               10.66\n    2        11       1               10.66\n    3         9       1               10.66 \n    4        23       2               21.33   \n    5        21       2               21.33\n    6        11       1               10.66\n    7        45       3               45\n    8        20       2               21.33\n    9        11       1               10.66 \n    10       12       1               10.66\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Write code yourself. Then it fits your problem best!</p>\n<p><strong>Boilerplate: Never assume code you download from the net to be correct or optimal... make sure to fully understand it before using it.</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>%matplotlib inline\n\nfrom numpy import array, linspace\nfrom sklearn.neighbors import KernelDensity\nfrom matplotlib.pyplot import plot\n\na = array([10,11,9,23,21,11,45,20,11,12]).reshape(-1, 1)\nkde = KernelDensity(kernel='gaussian', bandwidth=3).fit(a)\ns = linspace(0,50)\ne = kde.score_samples(s.reshape(-1,1))\nplot(s, e)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/LqOxF.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/LqOxF.png\"/></a></p>\n<pre class=\"lang-py prettyprint-override\"><code>from scipy.signal import argrelextrema\nmi, ma = argrelextrema(e, np.less)[0], argrelextrema(e, np.greater)[0]\nprint \"Minima:\", s[mi]\nprint \"Maxima:\", s[ma]\n&gt; Minima: [ 17.34693878  33.67346939]\n&gt; Maxima: [ 10.20408163  21.42857143  44.89795918]\n</code></pre>\n<p>Your clusters therefore are</p>\n<pre class=\"lang-py prettyprint-override\"><code>print a[a &lt; mi[0]], a[(a &gt;= mi[0]) * (a &lt;= mi[1])], a[a &gt;= mi[1]]\n&gt; [10 11  9 11 11 12] [23 21 20] [45]\n</code></pre>\n<p>and visually, we did this split:</p>\n<pre class=\"lang-py prettyprint-override\"><code>plot(s[:mi[0]+1], e[:mi[0]+1], 'r',\n     s[mi[0]:mi[1]+1], e[mi[0]:mi[1]+1], 'g',\n     s[mi[1]:], e[mi[1]:], 'b',\n     s[ma], e[ma], 'go',\n     s[mi], e[mi], 'ro')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/inGb2.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/inGb2.png\"/></a></p>\n<p>We cut at the red markers. The green markers are our best estimates for the cluster centers.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is a little error in the <a href=\"https://stackoverflow.com/a/35151947/11671779\">accepted answer</a> by @Has QUIT--Anony-Mousse (I can't comment nor suggest an edit due my reputation).</p>\n<p>The line:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(a[a &lt; mi[0]], a[(a &gt;= mi[0]) * (a &lt;= mi[1])], a[a &gt;= mi[1]])\n</code></pre>\n<p>Should be edited into:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(a[a &lt; s[mi][0]], a[(a &gt;= s[mi][0]) * (a &lt;= s[mi][1])], a[a &gt;= s[mi][1]])\n</code></pre>\n<p>That's because <code>mi</code> and <code>ma</code> is an index, where <code>s[mi]</code> and <code>s[ma]</code> is the value. If you use <code>mi[0]</code> as the limit, you risk and error splitting if your upper and lower linspace &gt;&gt; your upper and lower data. For example, run this code and see the difference in split result:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom numpy import array, linspace\nfrom sklearn.neighbors import KernelDensity\nfrom matplotlib.pyplot import plot\nfrom scipy.signal import argrelextrema\n\na = array([10,11,9,23,21,11,45,20,11,12]).reshape(-1, 1)\nkde = KernelDensity(kernel='gaussian', bandwidth=3).fit(a)\ns = linspace(0,100)\ne = kde.score_samples(s.reshape(-1,1))\nmi, ma = argrelextrema(e, np.less)[0], argrelextrema(e, np.greater)[0]\n\nprint('Grouping by HAS QUIT:')\nprint(a[a &lt; mi[0]], a[(a &gt;= mi[0]) * (a &lt;= mi[1])], a[a &gt;= mi[1]])\nprint('Grouping by yasirroni:')\nprint(a[a &lt; s[mi][0]], a[(a &gt;= s[mi][0]) * (a &lt; s[mi][1])], a[a &gt;= s[mi][1]])\n</code></pre>\n<p>result:</p>\n<pre><code>Grouping by Has QUIT:\n[] [10 11  9 11 11 12] [23 21 45 20]\nGrouping by yasirroni:\n[10 11  9 11 11 12] [23 21 20] [45]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Further improving <a href=\"https://stackoverflow.com/a/64272244/11671779\">the responses above</a> by @yasirroni, to dynamically print all clusters (not just 3 from the above) the line:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(a[a &lt; s[mi][0]], a[(a &gt;= s[mi][0]) * (a &lt;= s[mi][1])], a[a &gt;= s[mi][1]])\n</code></pre>\n<p>can be changed into:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(a[a &lt; s[mi][0]])  # print most left cluster\n\n# print all middle cluster\nfor i_cluster in range(len(mi)-1):\n    print(a[(a &gt;= s[mi][i_cluster]) * (a &lt;= s[mi][i_cluster+1])])\n\nprint(a[a &gt;= s[mi][-1]])  # print most right cluster\n</code></pre>\n<p>This would ensure that all the clusters are taken into account.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to produce a CNN using Keras, and wrote the following code:</p>\n<pre><code>batch_size = 64\nepochs = 20\nnum_classes = 5\n\ncnn_model = Sequential()\ncnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='linear',\n                     input_shape=(380, 380, 1), padding='same'))\ncnn_model.add(Activation('relu'))\ncnn_model.add(MaxPooling2D((2, 2), padding='same'))\ncnn_model.add(Conv2D(64, (3, 3), activation='linear', padding='same'))\ncnn_model.add(Activation('relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\ncnn_model.add(Conv2D(128, (3, 3), activation='linear', padding='same'))\ncnn_model.add(Activation('relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(128, activation='linear'))\ncnn_model.add(Activation('relu'))\ncnn_model.add(Dense(num_classes, activation='softmax'))\n\ncnn_model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n</code></pre>\n<p>I want to use Keras's <strong>LeakyReLU</strong> activation layer instead of using <code>Activation('relu')</code>. However, I tried using <code>LeakyReLU(alpha=0.1)</code> in place, but this is an activation layer in Keras, and I get an error about using an activation layer and not an activation function.</p>\n<p>How can I use <strong>LeakyReLU</strong> in this example?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>All advanced activations in Keras, including <code>LeakyReLU</code>, are available as <a href=\"https://keras.io/layers/advanced-activations/\" rel=\"noreferrer\">layers</a>, and not as activations; therefore, you should use it as such:</p>\n<pre><code>from keras.layers import LeakyReLU\n\n# instead of cnn_model.add(Activation('relu'))\n# use\ncnn_model.add(LeakyReLU(alpha=0.1))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sometimes you just want a drop-in replacement for a built-in activation layer, and not having to add extra activation layers just for this purpose.</p>\n<p>For that, you can use the fact that the <code>activation</code> argument can be a callable object.</p>\n<pre><code>lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\nmodel.add(Conv2D(..., activation=lrelu, ...)\n</code></pre>\n<p>Since a <code>Layer</code> is also a callable object, you could also simply use</p>\n<pre><code>model.add(Conv2D(..., activation=tf.keras.layers.LeakyReLU(alpha=0.1), ...)\n</code></pre>\n<p>which now works in TF2. This is a better solution as this avoids the need to use a <code>custom_object</code> during loading as @ChristophorusReyhan mentionned.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can import the function to make the code cleaner and then use it like any other activation.</p>\n<p>if you choose not to define alpha, don't forget to add brackets \"LeakyReLU()\"</p>\n<pre><code>from tensorflow.keras.layers import LeakyReLU\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(512, activation=LeakyReLU()))\nmodel.add(tf.keras.layers.Dense(512, activation=LeakyReLU(alpha=0.1)))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between standardscaler and normalizer in sklearn.preprocessing module? \nDon't both do the same thing? i.e remove mean and scale using deviation?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\" rel=\"noreferrer\">Normalizer</a> docs:</p>\n<blockquote>\n<p>Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one.</p>\n</blockquote>\n<p>And <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" rel=\"noreferrer\">StandardScaler</a></p>\n<blockquote>\n<p>Standardize features by removing the mean and scaling to unit variance</p>\n</blockquote>\n<p>In other words Normalizer acts <em>row-wise</em> and StandardScaler <em>column-wise</em>. Normalizer does not remove the mean and scale by deviation but scales the whole row to unit norm.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This <a href=\"https://i.sstatic.net/PZgJ2.png\" rel=\"noreferrer\">visualization</a> and <a href=\"http://benalexkeen.com/feature-scaling-with-scikit-learn/\" rel=\"noreferrer\">article</a> by Ben helps a lot in illustrating the idea.</p>\n<p><a href=\"https://i.sstatic.net/SPq4w.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/SPq4w.png\"/></a></p>\n<p>The StandardScaler assumes your data is normally distributed within each feature. By \"removing the mean and scaling to unit variance\", you can see in the picture now they have the same \"scale\" regardless of its original one.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In addition to the excellent suggestion by @vincentlcy to view <a href=\"http://benalexkeen.com/feature-scaling-with-scikit-learn\" rel=\"noreferrer\">this</a> article, there is now an example in the Scikit-Learn documentation <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\" rel=\"noreferrer\">here</a>. An important difference is that <code>Normalizer()</code> is applied to each sample (i.e., row) rather than column. This may work only for certain datasets that fit its assumption of similar types of data in each column.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>class sklearn.ensemble.RandomForestClassifier(n_estimators=10,\n                                              criterion='gini', \n                                              max_depth=None,\n                                              min_samples_split=2,\n                                              min_samples_leaf=1, \n                                              min_weight_fraction_leaf=0.0, \n                                              max_features='auto', \n                                              max_leaf_nodes=None, \n                                              bootstrap=True, \n                                              oob_score=False,\n                                              n_jobs=1, \n                                              random_state=None,\n                                              verbose=0, \n                                              warm_start=False, \n                                              class_weight=None)\n</code></pre>\n<p>I'm using a random forest model with 9 samples and about 7000 attributes.  Of these samples, there are 3 categories that my classifier recognizes. </p>\n<p>I know this is far from ideal conditions but I'm trying to figure out which attributes are the most important in feature predictions.  Which parameters would be the best to tweak for optimizing feature importance? </p>\n<p>I tried different <code>n_estimators</code> and noticed that the amount of \"significant features\" (i.e. nonzero values in the <code>feature_importances_</code> array) increased dramatically. </p>\n<p>I've read through the documentation but if anyone has any experience in this, I would like to know which parameters are the best to tune and a brief explanation why. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From my experience, there are three features worth exploring with the sklearn RandomForestClassifier, in order of importance:</p>\n<ul>\n<li><p><code>n_estimators</code></p></li>\n<li><p><code>max_features</code></p></li>\n<li><p><code>criterion</code></p></li>\n</ul>\n<p><code>n_estimators</code> is not really worth optimizing. The more estimators you give it, the better it will do. 500 or 1000 is usually sufficient.</p>\n<p><code>max_features</code> is worth exploring for many different values. It may have a large impact on the behavior of the RF because it decides how many features each tree in the RF considers at each split.</p>\n<p><code>criterion</code> may have a small impact, but usually the default is fine. If you have the time, try it out.</p>\n<p>Make sure to use sklearn's <a href=\"http://scikit-learn.org/stable/modules/grid_search.html\" rel=\"noreferrer\">GridSearch</a> (preferably GridSearchCV, but your data set size is too small) when trying out these parameters.</p>\n<p>If I understand your question correctly, though, you only have 9 samples and 3 classes? Presumably 3 samples per class? It's very, very likely that your RF is going to overfit with that little amount of data, unless they are good, representative records.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The crucial parts are usually three elements:</p>\n<ul>\n<li><strong>number of estimators</strong> - usually bigger the forest the better, there is small chance of overfitting here</li>\n<li><strong>max depth of each tree</strong> (default none, leading to full tree) - reduction of the maximum depth helps fighting with overfitting</li>\n<li><strong>max features per split</strong> (default <code>sqrt(d)</code>) - you might one to play around a bit as it significantly alters behaviour of the whole tree. sqrt heuristic is usually a good starting point but an actual sweet spot might be somewhere else</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\" rel=\"noreferrer\">This</a> wonderful article has a detailed explanation of tunable parameters, how to track performance vs speed trade-off, some practical tips, and how to perform grid-search.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am doing research on data mining and more precisely, decision trees.</p>\n<p>I would like to know if there are multiple algorithms to build a decision trees (or just one?), and which is better, based on criteria such as</p>\n<ul>\n<li>Performance</li>\n<li>Complexity</li>\n<li>Errors in decision making</li>\n<li>and more.</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Decision Tree implementations differ primarily along these axes:</p>\n<ul>\n<li><p>the <strong><em>splitting criterion</em></strong> (i.e., how \"variance\" is calculated)</p></li>\n<li><p>whether it builds models for <strong><em>regression</em></strong> (continuous variables, e.g., a\nscore) as well as <strong><em>classification</em></strong> (discrete variables, e.g., a class\nlabel)</p></li>\n<li><p>technique to eliminate/reduce <strong><em>over-fitting</em></strong></p></li>\n<li><p>whether it can handle <strong><em>incomplete data</em></strong></p></li>\n</ul>\n<p><br/>\nThe major Decision Tree implementations are:</p>\n<ul>\n<li><p><strong>ID3</strong>, or Iterative Dichotomizer, was the first of three Decision Tree\nimplementations developed by Ross Quinlan (Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.)</p></li>\n<li><p><strong>CART</strong>, or <em>Classification And Regression Trees</em> is often used as a generic\nacronym for the term Decision Tree, though it apparently has a more specific meaning. In sum, the CART implementation is very similar to C4.5; the one notable difference is that CART constructs the tree based on a numerical splitting criterion recursively applied to the data, whereas C4.5 includes the intermediate step of constructing <em>rule set</em>s.</p></li>\n<li><p><strong>C4.5</strong>, Quinlan's next iteration. The new features (versus ID3) are:\n(i) accepts both continuous and discrete features; (ii) handles\nincomplete data points; (iii) solves over-fitting problem by (very\nclever) bottom-up technique usually known as \"pruning\"; and (iv)\ndifferent weights can be applied the features that comprise the\ntraining data. Of these, the first <em>three</em> are very important--and I would suggest that any DT implementation you choose has all three. The fourth (differential weighting) is much less important</p></li>\n<li><p><strong>C5.0</strong>, the most recent Quinlan iteration. This implementation is\ncovered by a patent and probably, as a result, is rarely implemented\n(outside of commercial software packages). I have never coded a C5.0\nimplementation myself (I have never even seen the source code) so I can't offer an informed comparison of C5.0 versus C4.5. I have always\nbeen skeptical about the improvements claimed by its inventor (Ross\nQuinlan)--for instance, he claims it is \"several orders of magnitude\"\nfaster than C4.5. Other claims are similarly broad (\"significantly more memory efficient\") and so forth. I'll just point you to <a href=\"http://www.rulequest.com/see5-comparison.html\" rel=\"noreferrer\">studies</a>\nwhich report the result of comparison of the two techniques and you can decide for yourself.</p></li>\n<li><p><strong>CHAID</strong> (chi-square automatic interaction detector) actually predates\nthe original ID3 implementation by about six years (published in a\nPh.D. thesis by Gordon Kass in 1980). I know every little about this technique.The R Platform has a Package called <a href=\"http://r-forge.r-project.org/projects/chaid/\" rel=\"noreferrer\">CHAID</a> which\nincludes excellent documentation</p></li>\n<li><p><strong>MARS</strong> (multi-adaptive regression splines) is actually a term trademarked by the original inventor of MARS, Salford Systems. As a\nresult, MARS clones in libraries not sold by Salford are named something other than MARS--e.g., in R, the relevant function is polymars in the poly-spline library. Matlab and Statistica also have\nimplementations with MARS-functionality</p></li>\n</ul>\n<p>I would recommend CART or C4.5 (though again, I have no direct experience with C5.0 or with CHAID, though I am familiar with their feature sets). </p>\n<p>C4.5 is the Decision Tree flavor implemented in <a href=\"http://orange.biolab.si/\" rel=\"noreferrer\">Orange</a>; CART is the flavor in <a href=\"http://scikit-learn.sourceforge.net/dev/index.html\" rel=\"noreferrer\">sklearn</a>--both excellent implementations in excellent ML libraries.</p>\n<p>C4.5 is a major step beyond ID3--both in terms of <em>range</em> (C4.5 has a far broader use case spectrum because it can handle continuous variables in the training data) and in terms of <em>model quality</em>.</p>\n<p>Perhaps the most significant claimed improvement of C5.0 versus C4.5 is support for <strong><em>boosted trees</em></strong>. Ensemble support for DTs--boosted trees and Random Forests--has been included in the DT implementation in Orange; here, ensemble support was added to a C4.5 algorithm. sklearn also features a range of random forest and boosting methods.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is not yet clear for me what <code>metrics</code> are (as given in the code below). What exactly are they evaluating? Why do we need to define them in the <code>model</code>? Why we can have multiple metrics in one model? And more importantly what is the mechanics behind all this? \nAny scientific reference is also appreciated.</p>\n<pre class=\"lang-python prettyprint-override\"><code>model.compile(loss='mean_squared_error',\n              optimizer='sgd',\n              metrics=['mae', 'acc'])\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So in order to understand what <code>metrics</code> are, it's good to start by understanding what a <code>loss</code> function is. Neural networks are mostly trained using gradient methods by an iterative process of decreasing a <code>loss</code> function.</p>\n<p>A <code>loss</code> is designed to have two crucial properties - first, the smaller its value is, the better your model fits your data, and second, it should be differentiable. So, knowing this, we could fully define what a <code>metric</code> is: it's a function that, given predicted values and ground truth values from examples, provides you with a scalar measure of a \"fitness\" of your model, to the data you have. So, as you may see, a <code>loss</code> function is a metric, but the opposite doesn't always hold. To understand these differences, let's look at the most common examples of <code>metrics</code> usage:</p>\n<ol>\n<li><p><strong>Measure a performance of your network using non-differentiable functions:</strong> e.g. accuracy is not differentiable (not even continuous) so you cannot directly optimize your network w.r.t. to it. However, you could use it in order to choose the model with the best accuracy.</p></li>\n<li><p><strong>Obtain values of different loss functions when your final loss is a combination of a few of them:</strong> Let's assume that your loss has a regularization term which measures how your weights differ from <code>0</code>, and a term which measures the fitness of your model. In this case, you could use <code>metrics</code> in order to have a separate track of how the fitness of your model changes across epochs.</p></li>\n<li><p><strong>Track a measure with respect to which you don't want to directly optimize your model:</strong> so - let's assume that you are solving a multidimensional regression problem where you are mostly concerned about <code>mse</code>, but at the same time you are interested in how a <code>cosine-distance</code> of your solution is changing in time. Then, it's the best to use <code>metrics</code>.</p></li>\n</ol>\n<p>I hope that the explanation presented above made obvious what metrics are used for, and why you could use multiple metrics in one model. So now, let's say a few words about mechanics of their usage in <code>keras</code>. There are two ways of computing them while training:</p>\n<ol>\n<li><p><strong>Using <code>metrics</code> defined while compilation</strong>: this is what you directly asked. In this case, <code>keras</code> is defining a separate tensor for each metric you defined, to have it computed while training. This usually makes computation faster, but this comes at a cost of additional compilations, and the fact that metrics should be defined in terms of <code>keras.backend</code> functions.</p></li>\n<li><p><strong>Using <code>keras.callback</code></strong>: It is nice that you can use <a href=\"https://keras.io/callbacks/\" rel=\"noreferrer\"><code>Callbacks</code></a> in order to compute your metrics. As each callback has a default attribute of <code>model</code>, you could compute a variety of metrics using <code>model.predict</code> or model parameters while training. Moreover, it makes it possible to compute it, not only epoch-wise, but also batch-wise, or training-wise. This comes at a cost of slower computations, and more complicated logic - as you need to define metrics on your own.</p></li>\n</ol>\n<p><a href=\"https://keras.io/metrics/\" rel=\"noreferrer\">Here</a> you can find a list of available metrics, as well as an example on how you could define your own.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As in <a href=\"https://keras.io/metrics/\" rel=\"noreferrer\">keras metrics</a> page described: </p>\n<blockquote>\n<p>A metric is a function that is used to judge the performance of your\n  model</p>\n</blockquote>\n<p>Metrics are frequently used with early stopping callback to terminate training and avoid overfitting</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Reference: <a href=\"https://keras.io/metrics/\" rel=\"nofollow noreferrer\">Keras Metrics Documentation</a></p>\n<p>As given in the documentation page of <code>keras metrics</code>, a <code>metric</code> judges the performance of your model. The <code>metrics</code> argument in the <code>compile</code> method holds the list of metrics that needs to be evaluated by the model during its training and testing phases.\nMetrics like:</p>\n<ul>\n<li><p><code>binary_accuracy</code></p></li>\n<li><p><code>categorical_accuracy</code></p></li>\n<li><p><code>sparse_categorical_accuracy</code></p></li>\n<li><p><code>top_k_categorical_accuracy</code> and</p></li>\n<li><p><code>sparse_top_k_categorical_accuracy</code></p></li>\n</ul>\n<p>are the available metric functions that are supplied in the <strong><code>metrics</code></strong> parameter when the model is compiled.</p>\n<p>Metric functions are customizable as well. When multiple metrics need to be evaluated it is passed in the form of a <code>dictionary</code> or a <code>list</code>. </p>\n<p>One important resource you should refer for diving deep into metrics can be found <a href=\"https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\" rel=\"nofollow noreferrer\">here</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using the  <code>LogisticRegression()</code> method in <code>scikit-learn</code> on a highly unbalanced data set. I have even turned the <code>class_weight</code> feature to <code>auto</code>.</p>\n<p>I know that in Logistic Regression it should be possible to know what is the threshold value for a particular pair of classes. </p>\n<p>Is it possible to know what the threshold value is in each of the One-vs-All classes the <code>LogisticRegression()</code> method designs?</p>\n<p>I did not find anything in the documentation page.</p>\n<p>Does it by default apply the <code>0.5</code> value as threshold for all the classes regardless of the parameter values?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is a little trick that I use, instead of using <code>model.predict(test_data)</code> use <code>model.predict_proba(test_data)</code>. Then use a range of values for thresholds to analyze the effects on the prediction;</p>\n<pre><code>pred_proba_df = pd.DataFrame(model.predict_proba(x_test))\nthreshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\nfor i in threshold_list:\n    print ('\\n******** For i = {} ******'.format(i))\n    Y_test_pred = pred_proba_df.applymap(lambda x: 1 if x&gt;i else 0)\n    test_accuracy = metrics.accuracy_score(Y_test.values.reshape(Y_test.values.size,1),\n                                           Y_test_pred.iloc[:,1].values.reshape(Y_test_pred.iloc[:,1].values.size,1))\n    print('Our testing accuracy is {}'.format(test_accuracy))\n\n    print(confusion_matrix(Y_test.values.reshape(Y_test.values.size,1),\n                           Y_test_pred.iloc[:,1].values.reshape(Y_test_pred.iloc[:,1].values.size,1)))\n</code></pre>\n<p>Best!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Logistic regression chooses the class that has the biggest probability. In case of 2 classes, the threshold is 0.5: if P(Y=0) &gt; 0.5 then obviously P(Y=0) &gt; P(Y=1). The same stands for the multiclass setting: again, it chooses the class with the biggest probability (see e.g. <a href=\"http://www.holehouse.org/mlclass/06_Logistic_Regression.html\" rel=\"noreferrer\">Ng's lectures</a>, the bottom lines).</p>\n<p>Introducing special thresholds only affects in the proportion of false positives/false negatives (and thus in precision/recall tradeoff), but it is not the parameter of the LR model. See also <a href=\"https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold\">the similar question</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, Sci-Kit learn is using a threshold of P&gt;=0.5 for binary classifications. I am going to build on some of the answers already posted with two options to check this:</p>\n<p>One simple option is to extract the probabilities of each classification using the output from model.predict_proba(test_x) segment of the code below along with class predictions (output from model.predict(test_x) segment of code below).  Then, append class predictions and their probabilities to your test dataframe as a check.</p>\n<p>As another option, one can graphically view precision vs. recall at various thresholds using the following code.</p>\n<pre><code>### Predict test_y values and probabilities based on fitted logistic \nregression model\n\npred_y=log.predict(test_x) \n\nprobs_y=log.predict_proba(test_x) \n  # probs_y is a 2-D array of probability of being labeled as 0 \n  # (first column of array) vs 1 (2nd column in array)\n\n\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(test_y, probs_y[:, \n1]) \n   #retrieve probability of being 1(in second column of probs_y)\npr_auc = metrics.auc(recall, precision)\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sometimes I run into a problem:</p>\n<pre><code>OOM when allocating tensor with shape\n</code></pre>\n<p>e.g.</p>\n<pre><code>OOM when allocating tensor with shape (1024, 100, 160)\n</code></pre>\n<p>Where 1024 is my batch size and I don't know what's the rest. If I reduce the batch size or the number of neurons in the model, it runs fine.</p>\n<p>Is there a generic way to calculate optimal batch size based on model and GPU memory, so the program doesn't crash?</p>\n<p>In short: I want the largest batch size possible in terms of my model, which will fit into my GPU memory and won't crash the program.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the recent Deep Learning book by Goodfellow et al., <a href=\"http://www.deeplearningbook.org/contents/optimization.html\" rel=\"noreferrer\">chapter 8</a>:</p>\n<blockquote>\n<p>Minibatch sizes are generally driven by the following factors:</p>\n<ul>\n<li>Larger batches provide a more accurate estimate of the gradient, but\nwith less than linear returns.</li>\n<li>Multicore architectures are usually\nunderutilized by extremely small batches. This motivates using some\nabsolute minimum batch size, below which there is no reduction in the\ntime to process a minibatch.</li>\n<li>If all examples in the batch are to be\nprocessed in parallel (as is typically the case), then the amount of\nmemory scales with the batch size. For many hardware setups this is\nthe limiting factor in batch size.</li>\n<li>Some kinds of hardware achieve\nbetter runtime with speciﬁc sizes of arrays. Especially when using\nGPUs, it is common for power of 2 batch sizes to offer better runtime.\nTypical power of 2 batch sizes range from 32 to 256, with 16 sometimes\nbeing attempted for large models.</li>\n<li>Small batches can offer a\nregularizing effect (Wilson and Martinez, 2003), perhaps due to the\nnoise they add to the learning process. Generalization error is often\nbest for a batch size of 1. Training with such a small batch size\nmight require a small learning rate to maintain stability because of\nthe high variance in the estimate of the gradient. The total runtime\ncan be very high as a result of the need to make more steps, both\nbecause of the reduced learning rate and because it takes more steps\nto observe the entire training set.</li>\n</ul>\n</blockquote>\n<p>Which in practice usually means \"<em>in powers of 2 and the larger the better, provided that the batch fits into your (GPU) memory</em>\".</p>\n<p>You might want also to consult several good posts here in Stack Exchange:</p>\n<ul>\n<li><a href=\"https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\">Tradeoff batch size vs. number of iterations to train a neural network</a></li>\n<li><a href=\"https://stackoverflow.com/questions/40535679/selection-of-mini-batch-size-for-neural-network-regression\">Selection of Mini-batch Size for Neural Network Regression</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent\">How large should the batch size be for stochastic gradient descent?</a></li>\n</ul>\n<p>Just keep in mind that the paper by Keskar et al. '<a href=\"https://arxiv.org/abs/1609.04836\" rel=\"noreferrer\">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a>', quoted by several of the posts above, has received <a href=\"https://arxiv.org/abs/1703.04933\" rel=\"noreferrer\">some objections</a> by other respectable researchers of the deep learning community.</p>\n<p><strong>UPDATE</strong> (Dec 2017):</p>\n<p>There is a new paper by Yoshua Bengio &amp; team, <a href=\"https://arxiv.org/abs/1711.04623\" rel=\"noreferrer\">Three Factors Influencing Minima in SGD</a> (Nov 2017); it is worth reading in the sense that it reports new theoretical &amp; experimental results on the interplay between learning rate and batch size.</p>\n<p><strong>UPDATE</strong> (Mar 2021):</p>\n<p>Of interest here is also another paper from 2018, <a href=\"https://arxiv.org/abs/1804.07612\" rel=\"noreferrer\">Revisiting Small Batch Training for Deep Neural Networks</a> (h/t to Nicolas Gervais), which runs contrary to <em>the larger the better</em> advice; quoting from the abstract:</p>\n<blockquote>\n<p>The best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can estimate the largest batch size using:</p>\n<p><em>Max batch size= available GPU memory bytes / 4 / (size of tensors + trainable parameters)</em></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use the summaries provided by pytorchsummary (pip install) or keras (builtin).</p>\n<p>E.g.</p>\n<pre><code>from torchsummary import summary\nsummary(model)\n.....\n.....\n================================================================\nTotal params: 1,127,495\nTrainable params: 1,127,495\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 13.93\nParams size (MB): 4.30\nEstimated Total Size (MB): 18.25\n----------------------------------------------------------------\n</code></pre>\n<p>Each instance you put in the batch will require a full forward/backward pass in memory, your model you only need once. People seem to prefer batch sizes of powers of two, probably because of automatic layout optimization on the GPU.</p>\n<p>As a rule of thumb you may want to double your learning rate when you double your batch size.</p>\n<p>Let's assume we have a Tesla P100 at hand with 16 GB memory.</p>\n<pre><code>(16000 - model_size) / (forward_back_ward_size)\n(16000 - 4.3) / 13.93 = 1148.29\nrounded to powers of 2 results in batch size 1024\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using scickit-learn to tune a model hyper-parameters. I'm using a pipeline to have chain the preprocessing with the estimator. A simple version of my problem would look like this:</p>\n<pre><code>import numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\ngrid = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression()),\n                    param_grid={'logisticregression__C': [0.1, 10.]},\n                    cv=2,\n                    refit=False)\n\n_ = grid.fit(X=np.random.rand(10, 3),\n             y=np.random.randint(2, size=(10,)))\n</code></pre>\n<p>In my case the preprocessing (what would be StandardScale() in the toy example) is time consuming, and I'm not tuning any parameter of it.</p>\n<p>So, when I execute the example, the StandardScaler is executed 12 times. 2 fit/predict * 2 cv * 3 parameters. But every time StandardScaler is executed for a different value of the parameter C, it returns the same output, so it'd be much more efficient, to compute it once, and then just run the estimator part of the pipeline.</p>\n<p>I can manually split the pipeline between the preprocessing (no hyper parameters tuned) and the estimator. But to apply the preprocessing to the data, I should provide the training set only. So, I would have to implement the splits manually, and not use GridSearchCV at all.</p>\n<p>Is there a simple/standard way to avoid repeating the preprocessing while using GridSearchCV?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Update</strong>:\nIdeally, the answer below should not be used as it leads to data leakage as discussed in comments. In this answer, <code>GridSearchCV</code> will tune the hyperparameters on the data already preprocessed by <code>StandardScaler</code>, which is not correct. In most conditions that should not matter much, but algorithms which are too sensitive to scaling will give wrong results.</p>\n<hr/>\n<p>Essentially, GridSearchCV is also an estimator, implementing fit() and predict() methods, used by the pipeline.</p>\n<p>So instead of:</p>\n<pre><code>grid = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression()),\n                    param_grid={'logisticregression__C': [0.1, 10.]},\n                    cv=2,\n                    refit=False)\n</code></pre>\n<p>Do this:</p>\n<pre><code>clf = make_pipeline(StandardScaler(), \n                    GridSearchCV(LogisticRegression(),\n                                 param_grid={'logisticregression__C': [0.1, 10.]},\n                                 cv=2,\n                                 refit=True))\n\nclf.fit()\nclf.predict()\n</code></pre>\n<p>What it will do is, call the StandardScalar() only once, for one call to <code>clf.fit()</code> instead of multiple calls as you described. </p>\n<p><strong>Edit:</strong></p>\n<p>Changed refit to <code>True</code>, when GridSearchCV is used inside a pipeline. As <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" rel=\"noreferrer\">mentioned in documentation</a>:</p>\n<blockquote>\n<p>refit : boolean, default=True\n      Refit the best estimator with the entire dataset. If “False”, it is impossible to make predictions using this GridSearchCV instance\n  after fitting.</p>\n</blockquote>\n<p>If refit=False, <code>clf.fit()</code> will have no effect because the GridSearchCV object inside the pipeline will be reinitialized after <code>fit()</code>.\nWhen <code>refit=True</code>, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in <code>fit()</code>. </p>\n<p>So if you want to make the pipeline, just to see the scores of the grid search, only then the <code>refit=False</code> is appropriate. If you want to call the <code>clf.predict()</code> method, <code>refit=True</code> must be used, else Not Fitted error will be thrown.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For those who stumbled upon a little bit different problem, that I had as well.</p>\n<p>Suppose you have this pipeline:</p>\n<pre><code>classifier = Pipeline([\n    ('vectorizer', CountVectorizer(max_features=100000, ngram_range=(1, 3))),\n    ('clf', RandomForestClassifier(n_estimators=10, random_state=SEED, n_jobs=-1))])\n</code></pre>\n<p>Then, when specifying parameters you need to include this '<strong>clf_</strong>' name that you used for your estimator. So the parameters grid is going to be:</p>\n<pre><code>params={'clf__max_features':[0.3, 0.5, 0.7],\n        'clf__min_samples_leaf':[1, 2, 3],\n        'clf__max_depth':[None]\n        }\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is not possible to do this in the current version of scikit-learn (0.18.1). A fix has been proposed on the github project:</p>\n<p><a href=\"https://github.com/scikit-learn/scikit-learn/issues/8830\" rel=\"noreferrer\">https://github.com/scikit-learn/scikit-learn/issues/8830</a></p>\n<p><a href=\"https://github.com/scikit-learn/scikit-learn/pull/8322\" rel=\"noreferrer\">https://github.com/scikit-learn/scikit-learn/pull/8322</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What exactly does the <code>LogisticRegression.predict_proba</code> function return?</p>\n<p>In my example I get a result like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>array([\n    [4.65761066e-03, 9.95342389e-01],\n    [9.75851270e-01, 2.41487300e-02],\n    [9.99983374e-01, 1.66258341e-05]\n])\n</code></pre>\n<p>From other calculations, using the sigmoid function, I know, that the second column is the probabilities. The <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba\" rel=\"noreferrer\">documentation</a> says that the first column is <code>n_samples</code>, but that can't be, because my samples are reviews, which are texts and not numbers. The documentation also says that the second column is <code>n_classes</code>. That certainly can't be, since I only have two classes (namely, <code>+1</code> and <code>-1</code>) and the function is supposed to be about calculating probabilities of samples really being of a class, but not the classes themselves.</p>\n<p>What is the first column really and why it is there?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>4.65761066e-03 + 9.95342389e-01 = 1\n9.75851270e-01 + 2.41487300e-02 = 1\n9.99983374e-01 + 1.66258341e-05 = 1\n</code></pre>\n<p>The first column is the probability that the entry has the <code>-1</code> label and the second column is the probability that the entry has the <code>+1</code> label. Note that classes are ordered as they are in self.classes_.</p>\n<p>If you would like to get the predicted probabilities for the positive label only, you can use <code>logistic_model.predict_proba(data)[:,1]</code>. This will yield you the <code>[9.95342389e-01, 2.41487300e-02, 1.66258341e-05]</code> result.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As <a href=\"https://stackoverflow.com/a/36681493/19123103\">iulian explained</a>, each row of <code>predict_proba()</code>'s result is the probabilities that the observation in that row is of each class (and the classes are ordered as they are in <code>lr.classes_</code>).</p>\n<p>In fact, it's also intimately tied to <code>predict()</code> in that each row's highest probability class is chosen by <code>predict()</code>. So for any <code>LogisticRegression</code> (or any classifier really), the following is True.</p>\n<pre class=\"lang-py prettyprint-override\"><code>lr = LogisticRegression().fit(X, y)\nhighest_probability_classes = lr.predict_proba(X).argmax(axis=1)\nall(lr.predict(X) == lr.classes_[highest_probability_classes])     # True\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using R package <code>randomForest</code> to do a regression on some biological data. My training data size is <code>38772 X 201</code>.</p>\n<p>I just wondered---what would be a good value for the number of trees <code>ntree</code> and the number of variable per level <code>mtry</code>? Is there an approximate formula to find such parameter values?</p>\n<p>Each row in my input data is a 200 character representing the amino acid sequence, and I want to build a regression model to use such sequence in order to predict the distances between the proteins.   </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The default for mtry is quite sensible so there is not really a need to muck with it. There is a function <code>tuneRF</code> for optimizing this parameter. However, be aware that it may cause bias. </p>\n<p>There is no optimization for the number of bootstrap replicates. I often start with <code>ntree=501</code> and then plot the random forest object. This will show you the error convergence based on the OOB error. You want enough trees to stabilize the error but not so many that you over correlate the ensemble, which leads to overfit. </p>\n<p>Here is the caveat: variable interactions stabilize at a slower rate than error so, if you have a large number of independent variables you need more replicates. I would keep the ntree an odd number so ties can be broken. </p>\n<p>For the dimensions of you problem I would start <code>ntree=1501</code>. I would also recommended looking onto one of the published variable selection approaches to reduce the number of your independent variables.             </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The short answer is no.</p>\n<p>The <code>randomForest</code> function of course has default values for both <code>ntree</code> and <code>mtry</code>. The default for <code>mtry</code> is often (but not always) sensible, while generally people will want to increase <code>ntree</code> from it's default of 500 quite a bit.</p>\n<p>The \"correct\" value for <code>ntree</code> generally isn't much of a concern, as it will be quite apparent with a little tinkering that the predictions from the model won't change much after a certain number of trees.</p>\n<p>You can spend (read: waste) a lot of time tinkering with things like <code>mtry</code> (and <code>sampsize</code> and <code>maxnodes</code> and <code>nodesize</code> etc.), probably to some benefit, but in my experience not a lot. However, every data set will be different. Sometimes you may see a big difference, sometimes none at all.</p>\n<p>The <strong>caret</strong> package has a very general function <code>train</code> that allows you to do a simple grid search over parameter values like <code>mtry</code> for a wide variety of models. My only caution would be that doing this with fairly large data sets is likely to get time consuming fairly quickly, so watch out for that.</p>\n<p>Also, somehow I forgot that the <strong>ranfomForest</strong> package itself has a <code>tuneRF</code> function that is specifically for searching for the \"optimal\" value for <code>mtry</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Could this paper help ? \n<a href=\"http://lisa.ulb.ac.be/image/publifiles/100/Mcs2001_final.pdf\" rel=\"noreferrer\">Limiting the Number of Trees in Random Forests</a> </p>\n<blockquote>\n<p>Abstract. The aim of this paper is to propose a simple procedure that\n  a priori determines a minimum number of classifiers to combine in order\n  to obtain a prediction accuracy level similar to the one obtained with the\n  combination of larger ensembles. The procedure is based on the McNemar\n  non-parametric test of significance. Knowing a priori the minimum\n  size of the classifier ensemble giving the best prediction accuracy, constitutes\n  a gain for time and memory costs especially for huge data bases\n  and real-time applications. Here we applied this procedure to four multiple\n  classifier systems with C4.5 decision tree (Breiman’s Bagging, Ho’s\n  Random subspaces, their combination we labeled ‘Bagfs’, and Breiman’s\n  Random forests) and five large benchmark data bases. It is worth noticing\n  that the proposed procedure may easily be extended to other base\n  learning algorithms than a decision tree as well. The experimental results\n  showed that it is possible to limit significantly the number of trees. We\n  also showed that the minimum number of trees required for obtaining\n  the best prediction accuracy may vary from one classifier combination\n  method to another</p>\n</blockquote>\n<p>They never use more than 200 trees. </p>\n<p><a href=\"https://i.sstatic.net/MlkfP.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/MlkfP.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm new on Keras and have some questions on how to understanding my model results. Here is my result:(for your convenience, I only paste the loss acc val_loss val_acc after each epoch here)</p>\n<p>Train on 4160 samples, validate on 1040 samples as below:</p>\n<pre><code>Epoch 1/20\n4160/4160 - loss: 3.3455 - acc: 0.1560 - val_loss: 1.6047 - val_acc: 0.4721\n\nEpoch 2/20\n4160/4160 - loss: 1.7639 - acc: 0.4274 - val_loss: 0.7060 - val_acc: 0.8019\n\nEpoch 3/20\n4160/4160 - loss: 1.0887 - acc: 0.5978 - val_loss: 0.3707 - val_acc: 0.9087\n\nEpoch 4/20\n4160/4160 - loss: 0.7736 - acc: 0.7067 - val_loss: 0.2619 - val_acc: 0.9442\n\nEpoch 5/20\n4160/4160 - loss: 0.5784 - acc: 0.7690 - val_loss: 0.2058 - val_acc: 0.9433\n\nEpoch 6/20\n4160/4160 - loss: 0.5000 - acc: 0.8065 - val_loss: 0.1557 - val_acc: 0.9750\n\nEpoch 7/20\n4160/4160 - loss: 0.4179 - acc: 0.8296 - val_loss: 0.1523 - val_acc: 0.9606\n\nEpoch 8/20\n4160/4160 - loss: 0.3758 - acc: 0.8495 - val_loss: 0.1063 - val_acc: 0.9712\n\nEpoch 9/20\n4160/4160 - loss: 0.3202 - acc: 0.8740 - val_loss: 0.1019 - val_acc: 0.9798\n\nEpoch 10/20\n4160/4160 - loss: 0.3028 - acc: 0.8788 - val_loss: 0.1074 - val_acc: 0.9644\n\nEpoch 11/20\n4160/4160 - loss: 0.2696 - acc: 0.8923 - val_loss: 0.0581 - val_acc: 0.9856\n\nEpoch 12/20\n4160/4160 - loss: 0.2738 - acc: 0.8894 - val_loss: 0.0713 - val_acc: 0.9837\n\nEpoch 13/20\n4160/4160 - loss: 0.2609 - acc: 0.8913 - val_loss: 0.0679 - val_acc: 0.9740\n\nEpoch 14/20\n4160/4160 - loss: 0.2556 - acc: 0.9022 - val_loss: 0.0599 - val_acc: 0.9769\n\nEpoch 15/20\n4160/4160 - loss: 0.2384 - acc: 0.9053 - val_loss: 0.0560 - val_acc: 0.9846\n\nEpoch 16/20\n4160/4160 - loss: 0.2305 - acc: 0.9079 - val_loss: 0.0502 - val_acc: 0.9865\n\nEpoch 17/20\n4160/4160 - loss: 0.2145 - acc: 0.9185 - val_loss: 0.0461 - val_acc: 0.9913\n\nEpoch 18/20\n4160/4160 - loss: 0.2046 - acc: 0.9183 - val_loss: 0.0524 - val_acc: 0.9750\n\nEpoch 19/20\n4160/4160 - loss: 0.2055 - acc: 0.9120 - val_loss: 0.0440 - val_acc: 0.9885\n\nEpoch 20/20\n4160/4160 - loss: 0.1890 - acc: 0.9236 - val_loss: 0.0501 - val_acc: 0.9827\n</code></pre>\n<p>Here are my understandings:</p>\n<ol>\n<li><p>The two losses (both loss and val_loss) are decreasing and the tow acc (acc and val_acc) are increasing. So this indicates the modeling is trained in a good way.</p></li>\n<li><p>The val_acc is the measure of how good the predictions of your model are. So for my case, it looks like the model was trained pretty well after 6 epochs, and the rest training is not necessary.</p></li>\n</ol>\n<p>My Questions are:</p>\n<ol>\n<li><p>The acc (the acc on training set) is always smaller, actually much smaller, than val_acc. Is this normal? Why this happens?In my mind, acc should usually similar to better than val_acc.</p></li>\n<li><p>After 20 epochs, the acc is still increasing. So should I use more epochs and stop when acc stops increasing? Or I should stop where val_acc stops increasing, regardless of the trends of acc?</p></li>\n<li><p>Is there any other thoughts on my results? </p></li>\n</ol>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Answering your questions:</p>\n<ol>\n<li>As described on official <a href=\"https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss\" rel=\"noreferrer\">keras FAQ</a></li>\n</ol>\n<blockquote>\n<p>the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.</p>\n</blockquote>\n<ol start=\"2\">\n<li><p>Training should be stopped when val_acc stops increasing, otherwise your model will probably overffit. You can use earlystopping callback to stop training.</p>\n</li>\n<li><p>Your model seems to achieve very good results. Keep up the good work.</p>\n</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ol>\n<li>What are <code>loss</code> and <code>val_loss</code>?</li>\n</ol>\n<p>In deep learning, the <strong>loss</strong> is the value that a neural network is trying to minimize: it's the distance between the ground truth and the predictions. In order to minimize this distance, the neural network <em>learns</em> by adjusting weights and biases in a manner that reduces the <em>loss</em>.</p>\n<p>For instance, in <strong>regression</strong> tasks, you have a continuous target, e.g., height. What you want to minimize is the difference between your predictions, and the actual height. You can use <code>mean_absolute_error</code> as loss so the neural network knows this is what it needs to minimize.</p>\n<p>In <strong>classification</strong>, it's a little more complicated, but very similar. Predicted classes are based on probability. The loss is therefore also based on probability. In classification, the neural network minimizes the likelihood to assign a low probability to the actual class. The loss is typically <code>categorical_crossentropy</code>.</p>\n<p><code>loss</code> and <code>val_loss</code> differ because the former is applied to the train set, and the latter the test set. As such, the latter is a good indication of how the model performs on unseen data. You can get a validation set by using <code>validation_data=[x_test, y_test]</code> or <code>validation_split=0.2</code>.</p>\n<p>It's best to rely on the <code>val_loss</code> to prevent <a href=\"https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\" rel=\"noreferrer\">overfitting</a>. <em>Overfitting</em> is when the model fits the training data too closely, and the <code>loss</code> keeps decreasing while the <code>val_loss</code> is stale, or increases.</p>\n<p>In Keras, you can use <code>EarlyStopping</code> to stop training when the <code>val_loss</code> stops decreasing. Read <a href=\"https://keras.io/callbacks/#earlystopping\" rel=\"noreferrer\">here</a>.</p>\n<p><em>Read more about deep learning losses here: <a href=\"https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\" rel=\"noreferrer\">Loss and Loss Functions for Training Deep Learning Neural Networks</a>.</em></p>\n<ol start=\"2\">\n<li>What are <code>acc</code> and <code>val_acc</code>?</li>\n</ol>\n<p>Accuracy is a metric <strong>only for classification</strong>. It makes no sense on a task with a continuous target. It gives the percentage of instances that are correctly classified.</p>\n<p>Once again, <code>acc</code> is on the training data, and <code>val_acc</code> is on the validation data. It's best to rely on <code>val_acc</code> for a fair representation of model performance because a good neural network will end up fitting the training data at 100%, but would perform poorly on unseen data.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was trying to use my current code with an A100 gpu but I get this error:</p>\n<pre><code>---&gt; backend='nccl'\n/home/miranda9/miniconda3/envs/metalearningpy1.7.1c10.2/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: \nA100-SXM4-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\nIf you want to use the A100-SXM4-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n</code></pre>\n<p>which is reather confusing because it points to the usual pytorch installation but doesn't tell me which combination of pytorch version + cuda version to use for my specific hardware (A100). What is the right way to install pytorch for an A100?</p>\n<hr/>\n<p>These are some versions I've tried:</p>\n<pre><code># conda install -y pytorch==1.8.0 torchvision cudatoolkit=10.2 -c pytorch\n# conda install -y pytorch torchvision cudatoolkit=10.2 -c pytorch\n#conda install -y pytorch==1.7.1 torchvision torchaudio cudatoolkit=10.2 -c pytorch -c conda-forge\n# conda install -y pytorch==1.6.0 torchvision cudatoolkit=10.2 -c pytorch\n#conda install -y pytorch==1.7.1 torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n\n# conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch\n# conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n# conda install -y pytorch torchvision cudatoolkit=9.2 -c pytorch # For Nano, CC\n# conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n</code></pre>\n<hr/>\n<p>note that this can be subtle because I've had this error with this machine + pytorch version in the past:</p>\n<p><a href=\"https://stackoverflow.com/questions/66807131/how-to-solve-the-famous-unhandled-cuda-error-nccl-version-2-7-8-error\">How to solve the famous `unhandled cuda error, NCCL version 2.7.8` error?</a></p>\n<hr/>\n<h1>Bonus 1:</h1>\n<p>I still have errors:</p>\n<pre><code>ncclSystemError: System call (socket, malloc, munmap, etc) failed.\nTraceback (most recent call last):\n  File \"/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py\", line 1423, in &lt;module&gt;\n    main()\n  File \"/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py\", line 1365, in main\n    train(args=args)\n  File \"/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py\", line 1385, in train\n    args.opt = move_opt_to_cherry_opt_and_sync_params(args) if is_running_parallel(args.rank) else args.opt\n  File \"/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/torch_uu/distributed.py\", line 456, in move_opt_to_cherry_opt_and_sync_params\n    args.opt = cherry.optim.Distributed(args.model.parameters(), opt=args.opt, sync=syn)\n  File \"/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py\", line 62, in __init__\n    self.sync_parameters()\n  File \"/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py\", line 78, in sync_parameters\n    dist.broadcast(p.data, src=root)\n  File \"/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1090, in broadcast\n    work = default_pg.broadcast([tensor], opts)\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:911, unhandled system error, NCCL version 2.7.8\n</code></pre>\n<p>one of the answers suggested to have nvcca &amp; pytorch.version.cuda to match but they do not:</p>\n<pre><code>(meta_learning_a100) [miranda9@hal-dgx ~]$ python -c \"import torch;print(torch.version.cuda)\"\n\n11.1\n(meta_learning_a100) [miranda9@hal-dgx ~]$ nvcc -V\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\nCuda compilation tools, release 11.0, V11.0.221\nBuild cuda_11.0_bu.TC445_37.28845127_0\n</code></pre>\n<p>How do I match them? I this the error? Can someone display their pip, conda and nvcca version to see what set up works?</p>\n<p>More error messages:</p>\n<pre><code>hal-dgx:21797:21797 [0] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\nhal-dgx:21797:21797 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\nhal-dgx:21797:21797 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\nhal-dgx:21797:21797 [0] NCCL INFO Using network IB\nNCCL version 2.7.8+cuda11.1\nhal-dgx:21805:21805 [2] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\nhal-dgx:21799:21799 [1] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\nhal-dgx:21805:21805 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\nhal-dgx:21799:21799 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\nhal-dgx:21811:21811 [3] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\nhal-dgx:21811:21811 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\nhal-dgx:21811:21811 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\nhal-dgx:21811:21811 [3] NCCL INFO Using network IB\nhal-dgx:21799:21799 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\nhal-dgx:21805:21805 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\nhal-dgx:21799:21799 [1] NCCL INFO Using network IB\nhal-dgx:21805:21805 [2] NCCL INFO Using network IB\n\nhal-dgx:21797:27906 [0] misc/ibvwrap.cc:280 NCCL WARN Call to ibv_create_qp failed\nhal-dgx:21797:27906 [0] NCCL INFO transport/net_ib.cc:360 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO transport/net_ib.cc:437 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO include/net.h:21 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO include/net.h:51 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO init.cc:300 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO init.cc:566 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO init.cc:840 -&gt; 2\nhal-dgx:21797:27906 [0] NCCL INFO group.cc:73 -&gt; 2 [Async thread]\n\nhal-dgx:21811:27929 [3] misc/ibvwrap.cc:280 NCCL WARN Call to ibv_create_qp failed\nhal-dgx:21811:27929 [3] NCCL INFO transport/net_ib.cc:360 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO transport/net_ib.cc:437 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO include/net.h:21 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO include/net.h:51 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO init.cc:300 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO init.cc:566 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO init.cc:840 -&gt; 2\nhal-dgx:21811:27929 [3] NCCL INFO group.cc:73 -&gt; 2 [Async thread]\n</code></pre>\n<p>after putting</p>\n<pre><code>import os\nos.environ[\"NCCL_DEBUG\"] = \"INFO\"\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the link <a href=\"https://pytorch.org/get-started/locally/\" rel=\"noreferrer\">pytorch site</a> from @SimonB 's answer, I did:</p>\n<pre><code>pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>\n<p>This solved the problem for me.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've got an A100 and have had success with</p>\n<pre><code>conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia\n</code></pre>\n<p>Which is now also recommended on the <a href=\"https://pytorch.org/get-started/locally/\" rel=\"noreferrer\">pytorch site</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To me this is what worked:</p>\n<pre><code>conda update conda\npip install --upgrade pip\npip3 install --upgrade pip\n\nconda create -n meta_learning_a100 python=3.9\nconda activate meta_learning_a100\n\npip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>\n<p>then I tested it, asked for the device and did a matrix multiply, no errors is it worked:</p>\n<pre><code>(meta_learning_a100) [miranda9@hal-dgx diversity-for-predictive-success-of-meta-learning]$ python -c \"import uutils; uutils.torch_uu.gpu_test()\"\ndevice name: A100-SXM4-40GB\nSuccess, no Cuda errors means it worked see:\nout=tensor([[ 0.5877],\n        [-3.0269]], device='cuda:0')\n</code></pre>\n<p>gpu pytorch code:</p>\n<pre><code>def gpu_test():\n    \"\"\"\n    python -c \"import uutils; uutils.torch_uu.gpu_test()\"\n    \"\"\"\n    from torch import Tensor\n\n    print(f'device name: {device_name()}')\n    x: Tensor = torch.randn(2, 4).cuda()\n    y: Tensor = torch.randn(4, 1).cuda()\n    out: Tensor = (x @ y)\n    assert out.size() == torch.Size([2, 1])\n    print(f'Success, no Cuda errors means it worked see:\\n{out=}')\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I recieve this error while trying to obtain the recall score. </p>\n<pre><code>X_test = test_pos_vec + test_neg_vec\nY_test = [\"pos\"] * len(test_pos_vec) + [\"neg\"] * len(test_neg_vec)\n\nrecall_average = recall_score(Y_test, y_predict, average=\"binary\")\n\nprint(recall_average)\n</code></pre>\n<p>This will give me:</p>\n<pre><code>    C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1030: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if pos_label not in present_labels:\nTraceback (most recent call last):\n  File \"G:/PyCharmProjects/NB/accuracy/script.py\", line 812, in &lt;module&gt;\n    main()\n  File \"G:/PyCharmProjects/NB/accuracy/script.py\", line 91, in main\n    evaluate_model(model, train_pos_vec, train_neg_vec, test_pos_vec, test_neg_vec, False)\n  File \"G:/PyCharmProjects/NB/accuracy/script.py\", line 648, in evaluate_model\n    recall_average = recall_score(Y_test, y_predict, average=\"binary\")\n  File \"C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 1359, in recall_score\n    sample_weight=sample_weight)\n  File \"C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 1036, in precision_recall_fscore_support\n    (pos_label, present_labels))\nValueError: pos_label=1 is not a valid label: array(['neg', 'pos'],\n      dtype='&lt;U3')\n</code></pre>\n<p>I tried to transform 'pos' in 1 and 'neg' in 0 this way:</p>\n<pre><code>for i in range(len(Y_test)):\n     if 'neg' in Y_test[i]:\n         Y_test[i] = 0\n     else:\n         Y_test[i] = 1\n</code></pre>\n<p>But this is giving me another error:</p>\n<pre><code>    C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:181: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  score = y_true == y_pred\nTraceback (most recent call last):\n  File \"G:/PyCharmProjects/NB/accuracy/script.py\", line 812, in &lt;module&gt;\n    main()\n  File \"G:/PyCharmProjects/NB/accuracy/script.py\", line 91, in main\n    evaluate_model(model, train_pos_vec, train_neg_vec, test_pos_vec, test_neg_vec, False)\n  File \"G:/PyCharmProjects/NB/accuracy/script.py\", line 648, in evaluate_model\n    recall_average = recall_score(Y_test, y_predict, average=\"binary\")\n  File \"C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 1359, in recall_score\n    sample_weight=sample_weight)\n  File \"C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 1026, in precision_recall_fscore_support\n    present_labels = unique_labels(y_true, y_pred)\n  File \"C:\\Users\\anca_elena.moisa\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Mix of label input types (string and number)\")\nValueError: Mix of label input types (string and number)\n</code></pre>\n<p>What I am trying to do is to obtain the metrics: accuracy, precision, recall, f_measure. With <code>average='weighted'</code>, I obtain the same result: accuracy=recall. I guess this is not correct, so I changed the <code>average='binary'</code>, but I have those errors. Any ideas?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>recall_average = recall_score(Y_test, y_predict, average=\"binary\", pos_label=\"neg\")\n</code></pre>\n<p>Use <code>\"neg\"</code> or <code>\"pos\"</code> as <code>pos_label</code> and this error won't raise again.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you face this error it means the values of your <code>target</code> variable are not the expected one for <code>recall_score()</code>, which by default are <strong>1 for positive case</strong> and <strong>0 for negative case</strong> [This also applies to <code>precision_score()</code>]</p>\n<p>From the error you mentioned:</p>\n<pre><code>pos_label=1 is not a valid label: array(['neg', 'pos']\n</code></pre>\n<p>It is clear that values for your positive scenarios is <code>pos</code> instead of <code>1</code> and for the negative <code>neg</code> instead of <code>0</code>.<br/><br/></p>\n<p>Then you have two options to fix this mismatch:</p>\n<ul>\n<li>Changing the value default in the <code>recall_score()</code> to consider positive scenarios when <code>pos</code> appears with:</li>\n</ul>\n<pre><code>recall_average = recall_score(Y_test, y_predict, average=\"binary\", pos_label='pos') \n</code></pre>\n<ul>\n<li>Changing the values of the target variable in your dataset to be <code>1</code> or <code>0</code></li>\n</ul>\n<pre><code>Y_test = Y_test.map({'pos': 1, 'neg': 0}).astype(int)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have been exploring scikit-learn, making decision trees with both entropy and gini splitting criteria, and exploring the differences.</p>\n<p>My question, is how can I \"open the hood\" and find out exactly which attributes the trees are splitting on at each level, along with their associated information values, so I can see where the two criterion make different choices?</p>\n<p>So far, I have explored the 9 methods outlined in the documentation.  They don't appear to allow access to this information.  But surely this information is accessible?  I'm envisioning a list or dict that has entries for node and gain.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Directly from the documentation ( <a href=\"http://scikit-learn.org/0.12/modules/tree.html\" rel=\"noreferrer\">http://scikit-learn.org/0.12/modules/tree.html</a> ):</p>\n<pre><code>from io import StringIO\nout = StringIO()\nout = tree.export_graphviz(clf, out_file=out)\n</code></pre>\n<blockquote>\n<p><code>StringIO</code> module is no longer supported in Python3, instead import <code>io</code> module.</p>\n</blockquote>\n<p>There is also the <code>tree_</code> attribute in your decision tree object, which allows the direct access to the whole structure.</p>\n<p>And you can simply read it</p>\n<pre><code>clf.tree_.children_left #array of left children\nclf.tree_.children_right #array of right children\nclf.tree_.feature #array of nodes splitting feature\nclf.tree_.threshold #array of nodes splitting points\nclf.tree_.value #array of nodes values\n</code></pre>\n<p>for more details look at the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\" rel=\"noreferrer\">source code of export method</a></p>\n<p>In general you can use the <code>inspect</code> module</p>\n<pre><code>from inspect import getmembers\nprint( getmembers( clf.tree_ ) )\n</code></pre>\n<p>to get all the object's elements</p>\n<p><img alt=\"Decision tree visualization from sklearn docs\" src=\"https://i.sstatic.net/QDBCv.png\"/></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you just want a quick look at which what is going on in the tree, try:</p>\n<pre><code>zip(X.columns[clf.tree_.feature], clf.tree_.threshold, clf.tree_.children_left, clf.tree_.children_right)\n</code></pre>\n<p>where X is the data frame of independent variables and clf is the decision tree object. Notice that <code>clf.tree_.children_left</code> and <code>clf.tree_.children_right</code> together contain the order that the splits were made (each one of these would correspond to an arrow in the graphviz visualization).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Scikit learn introduced a delicious new method called <code>export_text</code> in version 0.21 (May 2019) to view all the rules from a tree. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html\" rel=\"nofollow noreferrer\">Documentation here</a>.</p>\n<p>Once you've fit your model, you just need two lines of code. First, import <code>export_text</code>:</p>\n<pre><code>from sklearn.tree import export_text\n</code></pre>\n<p>Second, create an object that will contain your rules. To make the rules look more readable, use the <code>feature_names</code> argument and pass a list of your feature names. For example, if your model is called <code>model</code> and your features are named in a dataframe called <code>X_train</code>, you could create an object called <code>tree_rules</code>:</p>\n<pre><code>tree_rules = export_text(model, feature_names=list(X_train))\n</code></pre>\n<p>Then just print or save <code>tree_rules</code>. Your output will look like this:</p>\n<pre><code>|--- Age &lt;= 0.63\n|   |--- EstimatedSalary &lt;= 0.61\n|   |   |--- Age &lt;= -0.16\n|   |   |   |--- class: 0\n|   |   |--- Age &gt;  -0.16\n|   |   |   |--- EstimatedSalary &lt;= -0.06\n|   |   |   |   |--- class: 0\n|   |   |   |--- EstimatedSalary &gt;  -0.06\n|   |   |   |   |--- EstimatedSalary &lt;= 0.40\n|   |   |   |   |   |--- EstimatedSalary &lt;= 0.03\n|   |   |   |   |   |   |--- class: 1\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There doesn't seem to be too many options for deploying predictive models in production which is surprising given the explosion in Big Data.  </p>\n<p>I understand that the open-source PMML can be used to export models as an XML specification. This can then be used for in-database scoring/prediction.  However it seems that to make this work you need to use the PMML plugin by Zementis which means the solution is not truly open source.  Is there an easier open way to map PMML to SQL for scoring?</p>\n<p>Another option would be to use JSON instead of XML to output model predictions.  But in this case, where would the R model sit? I'm assuming it would always need to be mapped to SQL...unless the R model could sit on the same server as the data and then run against that incoming data using an R script?</p>\n<p>Any other options out there? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The following is a list of the alternatives that I have found so far to deploy an R model in production. Please note that the workflow to use these products varies significantly between each other, but they are all somehow oriented to facilitate the process of exposing a trained R model as a service:</p>\n<ul>\n<li><a href=\"https://www.opencpu.org/\" rel=\"noreferrer\">openCPU</a></li>\n<li><a href=\"http://azure.microsoft.com/en-us/services/machine-learning/\" rel=\"noreferrer\">AzureML</a></li>\n<li><a href=\"https://learn.microsoft.com/en-us/machine-learning-server/deployr/deployr-about\" rel=\"noreferrer\">DeployR</a></li>\n<li><a href=\"https://yhathq.com/\" rel=\"noreferrer\">yhat</a> (already mentioned by @Ramnath)</li>\n<li><a href=\"http://dominodatalab.com\" rel=\"noreferrer\">Domino</a></li>\n<li><a href=\"https://sense.io/\" rel=\"noreferrer\">Sense.io</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The answer really depends on what your production environment is. </p>\n<p>If your \"big data\" are on Hadoop, you can try this relatively new open source PMML \"scoring engine\" called <a href=\"http://www.cascading.org/pattern/\">Pattern</a>.</p>\n<p>Otherwise you have no choice (short of writing custom model-specific code) but to run R on your server. You would use <code>save</code> to save your fitted models in .RData files and then <code>load</code> and run corresponding <code>predict</code> on the server. (That is bound to be slow but you can always try and throw more hardware at it.)</p>\n<p>How you do that really depends on your platform. Usually there is a way to add \"custom\" functions written in R. The term is UDF (user-defined function). In Hadoop you can add such functions to Pig (e.g. <a href=\"https://github.com/cd-wood/pigaddons\">https://github.com/cd-wood/pigaddons</a>) or you can use <a href=\"https://github.com/RevolutionAnalytics/RHadoop/wiki\">RHadoop</a> to write simple map-reduce code that would load the model and call <code>predict</code> in R. If your data are in Hive, you can use <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform\">Hive TRANSFORM</a> to call external R script.</p>\n<p>There are also vendor-specific ways to add functions written in R to various SQL databases. Again look for UDF in the documentation. For instance, PostgreSQL has <a href=\"http://www.joeconway.com/plr/\">PL/R</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can create RESTful APIs for your R scripts using plumber (<a href=\"https://github.com/trestletech/plumber\" rel=\"noreferrer\">https://github.com/trestletech/plumber</a>).</p>\n<p>I wrote a blog post about it (<a href=\"http://www.knowru.com/blog/how-create-restful-api-for-machine-learning-credit-model-in-r/\" rel=\"noreferrer\">http://www.knowru.com/blog/how-create-restful-api-for-machine-learning-credit-model-in-r/</a>) using deploying credit models as an example.</p>\n<p>In general, I do not recommend PMML because the packages you used might not support translation to PMML.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can I use batch normalization layer right after input layer and not normalize my data? May I expect to get similar effect/performance?</p>\n<p>In keras functional it would be something like this:</p>\n<pre><code>x = Input (...)\nx = Batchnorm(...)(x)\n...\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can do it. But the nice thing about batchnorm, in addition to activation distribution stabilization, is that the mean and std deviation are likely migrate as the network learns.</p>\n<p>Effectively, setting the batchnorm right after the input layer is a fancy <em>data pre-processing</em> step. It helps, sometimes a lot (e.g. in linear regression). But it's easier and more efficient to compute the mean and variance of the whole training sample once, than learn it per-batch. Note that batchnorm isn't free in terms of performance and you shouldn't abuse it.</p>\n<hr/>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes this is possible, and I have used it very successfully for vision models. There are some pros and cons of this approach though, the main advantages being:</p>\n<ol>\n<li>You can’t forget the normalization step when integrating the model in production since it’s part of the model itself (this happens more often than you think).</li>\n<li>The normalization is Data augmentation aware this way.</li>\n</ol>\n<p>The main drawbacks are:</p>\n<ol>\n<li>Added runtime cost in case you had normalized inputs already available.</li>\n</ol>\n<p>I’ve also written about this subject in detail here: Replace Manual Normalization with Batch Normalization in Vision AI Models. <a href=\"https://towardsdatascience.com/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm evaluating tools for production ML based applications and one of our options is Spark MLlib , but I have some questions about how to serve a model once its trained? </p>\n<p>For example in Azure ML, once trained, the model is exposed as a web service which can be consumed from any application, and it's a similar case with Amazon ML.</p>\n<p>How do you serve/deploy ML models in Apache Spark ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From one hand, a machine learning model built with spark can't be served the way you serve in Azure ML or Amazon ML in a traditional manner. </p>\n<p>Databricks claims to be able to deploy models using it's notebook but I haven't actually tried that yet. </p>\n<p>On other hand, you can use a model in three ways :</p>\n<ul>\n<li>Training on the fly inside an application then applying prediction. This can be done in a spark application or a notebook. </li>\n<li>Train a model and save it if it implements an <code>MLWriter</code> then load in an application or a notebook and run it against your data. </li>\n<li>Train a model with Spark and export it to PMML format using <a href=\"https://github.com/jpmml/jpmml-spark\" rel=\"noreferrer\">jpmml-spark</a>. PMML allows for different statistical and data mining tools to speak the same language. In this way, a predictive solution can be easily moved among tools and applications without the need for custom coding. e.g from Spark ML to R.</li>\n</ul>\n<p>Those are the three possible ways. </p>\n<p>Of course, you can think of an architecture in which you have RESTful service behind which you can build using spark-jobserver per example to train and deploy but needs some development. It's not a out-of-the-box solution. </p>\n<p>You might also use projects like Oryx 2 to create your full lambda architecture to train, deploy and serve a model.</p>\n<p>Unfortunately, describing each of the mentioned above solution is quite broad and doesn't fit in the scope of SO. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One option is to use <a href=\"https://combust.github.io/mleap-docs/\" rel=\"noreferrer\">MLeap</a> to serve a Spark PipelineModel online with <em>no dependencies on Spark/SparkContext</em>. Not having to use the SparkContext is important as it will drop scoring time for a single record from ~100ms to <strong>single-digit microseconds</strong>.</p>\n<p>In order to use it, you have to:</p>\n<ul>\n<li>Serialize your Spark Model with MLeap utilities</li>\n<li>Load the model in MLeap (does not require a SparkContext or any Spark dependencies)</li>\n<li>Create your input record in JSON (not a DataFrame)</li>\n<li>Score your record with MLeap</li>\n</ul>\n<p>MLeap is well integrated with all the Pipeline Stages available in Spark MLlib (with the exception of LDA at the time of this writing). However, things might get a bit more complicated if you are using custom Estimators/Transformers.</p>\n<p>Take a look at the <a href=\"https://combust.github.io/mleap-docs/faq.html\" rel=\"noreferrer\">MLeap FAQ</a> for more info about custom transformers/estimators, performances, and integration.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are comparing two rather different things. Apache Spark is a computation engine, while mentioned by you Amazon and Microsoft solutions are offering services. These services might as well have Spark with MLlib behind the scene. They save you from the trouble building a web service yourself, but you pay extra.</p>\n<p>Number of companies, like Domino Data Lab, Cloudera or IBM offer products that you can deploy on your own Spark cluster and easily build service around your models (with various degrees of flexibility).</p>\n<p>Naturally you build a service yourself with various open source tools. Which specifically? It all depends on what you are after. How user should interact with the model? Should there be some sort of UI or jest a REST API? Do you need to change some parameters on the model or the model itself? Are the jobs more of a batch or real-time nature? You can naturally build all-in-one solution, but that's going to be a huge effort.</p>\n<p>My personal recommendation would be to take advantage, if you can, of one of the available services from Amazon, Google, Microsoft or whatever. Need on-premises deployment? Check Domino Data Lab, their product is mature and allows easy working with models (from building till deployment). Cloudera is more focused on cluster computing (including Spark), but it will take a while before they have something mature.</p>\n<p>[EDIT] I'd recommend to have a look at <a href=\"http://predictionio.incubator.apache.org/\" rel=\"nofollow noreferrer\">Apache PredictionIO</a>, open source machine learning server  - amazing project with lot's of potential. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I recently went through the <a href=\"https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" rel=\"noreferrer\">Transformer</a> paper from Google Research describing how self-attention layers could completely replace traditional RNN-based sequence encoding layers for machine translation. In Table 1 of the paper, the authors compare the computational complexities of different sequence encoding layers, and state (later on) that self-attention layers are faster than RNN layers when the sequence length <code>n</code> is smaller than the dimension of the vector representations <code>d</code>.</p>\n<p>However, the self-attention layer seems to have an inferior complexity than claimed if my understanding of the computations is correct. Let <code>X</code> be the input to a self-attention layer. Then, <code>X</code> will have shape <code>(n, d)</code> since there are <code>n</code> word-vectors (corresponding to rows) each of dimension <code>d</code>. Computing the output of self-attention requires the following steps (consider single-headed self-attention for simplicity):</p>\n<ol>\n<li>Linearly transforming the rows of <code>X</code> to compute the query <code>Q</code>, key <code>K</code>, and value <code>V</code> matrices, each of which has shape <code>(n, d)</code>. This is accomplished by post-multiplying <code>X</code> with 3 learned matrices of shape <code>(d, d)</code>, amounting to a computational complexity of <code>O(n d^2)</code>.</li>\n<li>Computing the layer output, specified in Equation 1 of the paper as <code>SoftMax(Q Kt / sqrt(d)) V</code>, where the softmax is computed over each row. Computing <code>Q Kt</code> has complexity <code>O(n^2 d)</code>, and post-multiplying the resultant with <code>V</code> has complexity <code>O(n^2 d)</code> as well.</li>\n</ol>\n<p>Therefore, the total complexity of the layer is <code>O(n^2 d + n d^2)</code>, which is worse than that of a traditional RNN layer. I obtained the same result for multi-headed attention too, on considering the appropriate intermediate representation dimensions (<code>dk</code>, <code>dv</code>) and finally multiplying by the number of heads <code>h</code>.</p>\n<p>Why have the authors ignored the cost of computing the Query, Key, and Value matrices while reporting total computational complexity?</p>\n<p>I understand that the proposed layer is fully parallelizable across the <code>n</code> positions, but I believe that Table 1 does not take this into account anyway.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First, you are correct in your complexity calculations. So, what is the source of confusion?</p>\n<p>When the original <a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"noreferrer\">Attention paper</a> was first introduced, it didn't require to calculate <code>Q</code>, <code>V</code> and <code>K</code> matrices, as the values were taken directly from the hidden states of the RNNs, and thus the complexity of Attention layer <strong>is</strong> <code>O(n^2·d)</code>.</p>\n<p>Now, to understand what <code>Table 1</code> contains please keep in mind how most people scan papers: they read title, abstract, then look at figures and tables. Only then if the results were interesting, they read the paper more thoroughly. So, the main idea of the <code>Attention is all you need</code> paper was to replace the RNN layers completely with attention mechanism in seq2seq setting because RNNs were really slow to train. If you look at the <code>Table 1</code> in this context, you see that it compares RNN, CNN and Attention and highlights the motivation for the paper: using Attention should have been beneficial over RNNs and CNNs. It should have been advantageous in 3 aspects: constant amount of calculation steps, constant amount of operations <strong>and</strong> lower computational complexity for usual Google setting, where <code>n ~= 100</code> and <code>d ~= 1000</code>. But as any idea, it hit the hard wall of reality. And in reality in order for that great idea to work, they had to add positional encoding, reformulate the Attention and add multiple heads to it. The result is the Transformer architecture which while has the computational complexity of <code>O(n^2·d + n·d^2)</code> still is much faster then RNN (in a sense of wall clock time), and produces better results.</p>\n<p>So the answer for your question is that attention layer the authors refer to in <code>Table 1</code> is strictly the attention mechanism. It is not the complexity of the Transformer. They are very well aware about the complexity of their model (I quote):</p>\n<blockquote>\n<p>Separable convolutions [6], however, decrease the complexity\nconsiderably, to <code>O(k·n·d + n·d^2)</code>. Even with <code>k = n</code>, however, the\ncomplexity of a separable convolution is equal to the combination of a\nself-attention layer and a point-wise feed-forward layer, the approach\nwe take in our model.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Strictly speaking, when considering the complexity of <em>only</em> the self-attention block (Fig 2 left, equation 1) the projection of <code>x</code> to <code>q</code>, <code>k</code> and <code>v</code> is not included in the self-attention. The complexities shown in table 1 are only for the very core of self-attention layer and thus are <code>O(n^2 d)</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can't give the correct number of parameters of <a href=\"http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf\">AlexNet</a> or <a href=\"http://arxiv.org/abs/1409.1556\">VGG Net</a>.</p>\n<p>For example, to calculate the number of parameters of a <code>conv3-256</code> layer of VGG Net, the answer is 0.59M = (3*3)*(256*256), that is (kernel size) * (product of both number of channels in the joint layers), however in that way, I can't get the <code>138M</code> parameters.</p>\n<p>So could you please show me where is wrong with my calculation, or show me the right calculation procedure?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you refer to VGG Net with 16-layer (table 1, column D) then <code>138M</code> refers to the <strong>total number of parameters</strong> of this network, i.e including all convolutional layers, but also the fully connected ones.</p>\n<p>Looking at the 3rd convolutional stage composed of 3 x <code>conv3-256</code> layers:</p>\n<ul>\n<li>the first one has N=128 input planes and F=256 output planes,</li>\n<li>the two other ones have N=256 input planes and F=256 output planes.</li>\n</ul>\n<p>The convolution kernel is 3x3 for each of these layers. In terms of parameters this gives:</p>\n<ul>\n<li>128x3x3x256 (weights) + 256 (biases) = 295,168 parameters for the 1st one,</li>\n<li>256x3x3x256 (weights) + 256 (biases) = 590,080 parameters for the two other ones.</li>\n</ul>\n<p><em>As explained above you have to do that for all layers, but also the fully-connected ones, and sum these values to obtain the final 138M number.</em></p>\n<p>-</p>\n<p><strong>UPDATE</strong>: the breakdown among layers give:</p>\n<pre><code>conv3-64  x 2       : 38,720\nconv3-128 x 2       : 221,440\nconv3-256 x 3       : 1,475,328\nconv3-512 x 3       : 5,899,776\nconv3-512 x 3       : 7,079,424\nfc1                 : 102,764,544\nfc2                 : 16,781,312\nfc3                 : 4,097,000\nTOTAL               : 138,357,544\n</code></pre>\n<p>In particular for the fully-connected layers (fc):</p>\n<pre><code> fc1 (x): (512x7x7)x4,096 (weights) + 4,096 (biases)\n fc2    : 4,096x4,096     (weights) + 4,096 (biases)\n fc3    : 4,096x1,000     (weights) + 1,000 (biases)\n</code></pre>\n<p>(x) see section 3.2 of the article: <em>the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers).</em></p>\n<p><strong>Details about <code>fc1</code></strong></p>\n<p>As precised above the spatial resolution right before feeding the fully-connected layers is 7x7 pixels. This is because this VGG Net uses <em>spatial padding</em> before convolutions, as detailed within section 2.1 of the paper:</p>\n<p><em>[...] the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3×3 conv. layers.</em></p>\n<p>With such a padding, and working with a 224x224 pixels input image, the resolution decreases as follow along the layers: 112x112, 56x56, 28x28, 14x14 and 7x7 after the last convolution/pooling stage which has 512 feature maps.</p>\n<p>This gives a feature vector passed to <code>fc1</code> with dimension: 512x7x7.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A great breakdown of the calculation for VGG-16 network is also given in <a href=\"http://cs231n.github.io/convolutional-networks/\" rel=\"noreferrer\">CS231n</a> lecture notes.</p>\n<pre><code>INPUT:     [224x224x3]    memory:  224*224*3=150K   weights: 0\nCONV3-64:  [224x224x64]   memory:  224*224*64=3.2M  weights: (3*3*3)*64 = 1,728\nCONV3-64:  [224x224x64]   memory:  224*224*64=3.2M  weights: (3*3*64)*64 = 36,864\nPOOL2:     [112x112x64]   memory:  112*112*64=800K  weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M weights: (3*3*128)*128 = 147,456\nPOOL2:     [56x56x128]    memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]    memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]    memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]    memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2:     [28x28x256]    memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]    memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]    memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]    memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2:     [14x14x512]    memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]    memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]    memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]    memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2:     [7x7x512]      memory:  7*7*512=25K      weights: 0\nFC:        [1x1x4096]     memory:  4096             weights: 7*7*512*4096 = 102,760,448\nFC:        [1x1x4096]     memory:  4096             weights: 4096*4096 = 16,777,216\nFC:        [1x1x1000]     memory:  1000             weights: 4096*1000 = 4,096,000\n\nTOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)\nTOTAL params: 138M parameters\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The below VGG-16 architechture is in the <a href=\"https://arxiv.org/pdf/1409.1556.pdf\" rel=\"noreferrer\">original paper as highlighted by @deltheil in  (table 1, column D)  </a>, and I quote from there</p>\n<blockquote>\n<p>2.1 ARCHITECTURE</p>\n<p>During training, the input to our ConvNets is a fixed-size 224 × 224\n  RGB images. The only preprocessing we do is subtracting the mean RGB\n  value, computed on the training set, from each pixel.</p>\n<p>The image is passed through a stack of convolutional (conv.) layers,\n  where we use filters with a very small receptive field: 3 × 3 (which\n  is the smallest size to capture the notion of left/right, up/down,\n  center). The convolution stride is fixed to 1 pixel; the spatial\n  padding of conv. layer input is such that the spatial resolution is\n  preserved after convolution, i.e. the padding is 1 pixel for 3 × 3\n  conv. layers.  Spatial pooling is carried out by five max-pooling\n  layers, which follow some of the conv. layers (not all the conv.\n  layers are followed by max-pooling). Max-pooling is performed over a 2\n  × 2 pixel window, with stride 2.</p>\n<p>A stack of convolutional layers (which has a different depth in\n  different architectures) is followed by three Fully-Connected (FC)\n  layers: the first two have 4096 channels each, the third performs\n  1000-way ILSVRC classification and thus contains 1000 channels (one\n  for each class). </p>\n<p>The final layer is the soft-max layer.</p>\n</blockquote>\n<p>Using the above, and </p>\n<ul>\n<li>A formula to find activation shape of a layer!</li>\n</ul>\n<p><img src=\"https://i.sstatic.net/oucTkt.jpg\" width=\"200\"/></p>\n<ul>\n<li>A formula to calculate the weights corresponding to every layer:</li>\n</ul>\n<p><img src=\"https://i.sstatic.net/R8F7a.png\" width=\"200\"/></p>\n<p><strong>Note:</strong></p>\n<ul>\n<li><p>you can simply multiply respective activation shape column to get the activation size</p></li>\n<li><p>CONV3: means a filter of 3*3 will convolve on the input!</p></li>\n<li><p>MAXPOOL3-2: means, 3rd pooling layer, with 2*2 filter, stride=2, padding=0(pretty standard in pooling layers)</p></li>\n<li><p>Stage-3 : means it has multiple CONV layer stacked! with same padding=1, , stride=1, and filter 3*3</p></li>\n<li><p>Cin : means the depth a.k.a channel coming from the input layer!</p></li>\n<li><p>Cout: means the depth a.k.a channel outgoing (you configure it differently- to learn more complex features!),</p></li>\n</ul>\n<p>Cin and Cout are the number of filters that you stack together to learn multiple features at different scales such as in the first layer you might want to learn vertical edges, and horizontal edges and edges at say 45degree, blah blah!, 64 possible different filters each of different kind of edges!! </p>\n<ul>\n<li><p>n: input dimension without depth such n=224 in case of INPUT-image!</p></li>\n<li><p>p: padding for each layer</p></li>\n<li><p>s: stride used for each layer</p></li>\n<li><p>f: filter size i.e 3*3 for CONV and 2*2 for MAXPOOL layers!  </p></li>\n<li><p>After MAXPOOL5-2, you simply flatten the volume and interface it with the first FC layer.!</p></li>\n</ul>\n<p><strong><em>We get the table:</em></strong>\n<a href=\"https://i.sstatic.net/zCwzX.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zCwzX.jpg\"/></a></p>\n<p><strong><em>Finally, if you add all the weights calculated in the last column, you end up with 138,357,544(138 million) parameters to train for VGG-15!</em></strong></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> Questions asking us to <b>recommend or find a tool, library or favorite off-site resource</b> are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, <a href=\"http://meta.stackoverflow.com/q/254394/\">describe the problem</a> and what has been done so far to solve it.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2014-06-24 16:24:48Z\">10 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/2233435/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Assume you know a student who wants to study Machine Learning and Natural Language Processing.</p>\n<p>What specific computer science subjects should they focus on and which programming languages are specifically designed to solve these types of problems?</p>\n<p>I am not looking for your favorite subjects and tools, but rather industry standards.</p>\n<p><b>Example</b>: I'm guessing that knowing Prolog and Matlab might help them.  They also might want to study Discrete Structures*, Calculus, and Statistics.</p>\n<p>*Graphs and trees. Functions: properties, recursive definitions, solving recurrences. Relations: properties, equivalence, partial order. Proof techniques, inductive proof. Counting techniques and discrete probability.  Logic: propositional calculus, first-order predicate calculus. Formal reasoning: natural deduction, resolution. Applications to program correctness and automatic reasoning. Introduction to algebraic structures in computing.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This related stackoverflow question has some nice answers: <a href=\"https://stackoverflow.com/questions/212219/what-are-good-starting-points-for-someone-interested-in-natural-language-processi\">What are good starting points for someone interested in natural language processing?</a></p>\n<p>This is a very big field.  The prerequisites mostly consist of probability/statistics, linear algebra, and basic computer science, although Natural Language Processing requires a more intensive computer science background to start with (frequently covering some basic AI).  Regarding specific langauges: Lisp was created <a href=\"http://nathanmarz.com/blog/john-mccarthy/\" rel=\"nofollow noreferrer\">\"as an afterthought\" for doing AI research</a>, while Prolog (with it's roots in formal logic) is especially aimed at Natural Language Processing, and many courses will use Prolog, Scheme, Matlab, R, or another functional language (e.g. <a href=\"http://courses.cit.cornell.edu/ling4424/\" rel=\"nofollow noreferrer\">OCaml is used for this course at Cornell</a>) as they are very suited to this kind of analysis.  </p>\n<p>Here are some more specific pointers:</p>\n<p>For Machine Learning, <a href=\"http://www.stanford.edu/class/cs229/\" rel=\"nofollow noreferrer\"><strong>Stanford CS 229: Machine Learning</strong></a> is great: it includes everything, including full videos of the lectures (also up on iTunes), course notes, problem sets, etc., and it was very well taught by <a href=\"http://www.cs.stanford.edu/people/ang/\" rel=\"nofollow noreferrer\">Andrew Ng</a>.</p>\n<p>Note the prerequisites:</p>\n<blockquote>\n<p>Students are expected to have the following background: Knowledge of\n  basic computer science principles and skills, at a level sufficient to write\n  a reasonably non-trivial computer program. Familiarity with the basic probability theory. \n  Familiarity with the basic linear algebra.</p>\n</blockquote>\n<p>The course uses Matlab and/or Octave.  It also recommends the following readings (although the course notes themselves are very complete):</p>\n<ul>\n<li>Christopher Bishop, <a href=\"https://rads.stackoverflow.com/amzn/click/com/0387310738\" rel=\"nofollow noreferrer\">Pattern Recognition and Machine Learning</a>. Springer, 2006.</li>\n<li>Richard Duda, Peter Hart and David Stork, <a href=\"https://rads.stackoverflow.com/amzn/click/com/0471056693\" rel=\"nofollow noreferrer\">Pattern Classification</a>, 2nd ed. John Wiley &amp; Sons, 2001.</li>\n<li>Tom Mitchell, <a href=\"https://rads.stackoverflow.com/amzn/click/com/0070428077\" rel=\"nofollow noreferrer\">Machine Learning</a>. McGraw-Hill, 1997.</li>\n<li>Richard Sutton and Andrew Barto, <a href=\"https://rads.stackoverflow.com/amzn/click/com/0262193981\" rel=\"nofollow noreferrer\">Reinforcement Learning: An introduction</a>. MIT Press, 1998</li>\n</ul>\n<p>For Natural Language Processing, the <a href=\"http://nlp.stanford.edu/\" rel=\"nofollow noreferrer\">NLP group at Stanford</a> provides many good resources.  The introductory course <a href=\"http://www.stanford.edu/class/cs224n/\" rel=\"nofollow noreferrer\"><strong>Stanford CS 224: Natural Language Processing</strong></a> includes <a href=\"http://www.stanford.edu/class/cs224n/syllabus.html\" rel=\"nofollow noreferrer\">all the lectures online</a> and has the following prerequisites:</p>\n<blockquote>\n<p>Adequate experience with programming\n  and formal structures.  Programming\n  projects will be written in Java 1.5,\n  so knowledge of Java (or a willingness\n  to learn on your own) is required. \n  Knowledge of standard concepts in\n  artificial intelligence and/or\n  computational linguistics.  Basic\n  familiarity with logic, vector spaces,\n  and probability.</p>\n</blockquote>\n<p>Some recommended texts are:</p>\n<ul>\n<li>Daniel Jurafsky and James H. Martin. 2008. <a href=\"http://www.cs.colorado.edu/~martin/slp.html\" rel=\"nofollow noreferrer\">Speech and Language Processing: An Introduction to Natural Language Processing</a>, Computational Linguistics and Speech Recognition. Second Edition. Prentice Hall. </li>\n<li>Christopher D. Manning and Hinrich Schütze. 1999. <a href=\"http://nlp.stanford.edu/fsnlp/\" rel=\"nofollow noreferrer\">Foundations of Statistical Natural Language Processing</a>. MIT Press.</li>\n<li>James Allen. 1995. <a href=\"https://rads.stackoverflow.com/amzn/click/com/0805303340\" rel=\"nofollow noreferrer\">Natural Language Understanding</a>. Benjamin/Cummings, 2ed.</li>\n<li>Gerald Gazdar and Chris Mellish. 1989. <a href=\"http://www.informatics.susx.ac.uk/research/groups/nlp/gazdar/nlp-in-prolog/\" rel=\"nofollow noreferrer\">Natural Language Processing in Prolog</a>. Addison-Wesley. (this is available online for <strong>free</strong>)</li>\n<li>Frederick Jelinek. 1998. <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=7447\" rel=\"nofollow noreferrer\">Statistical Methods for Speech Recognition</a>. MIT Press.</li>\n</ul>\n<p>The prerequisite <a href=\"http://www.stanford.edu/class/linguist180/\" rel=\"nofollow noreferrer\">computational linguistics course</a> requires basic computer programming and data structures knowledge, and uses the same text books.  The required articificial intelligence course <a href=\"http://www.stanford.edu/class/cs221/\" rel=\"nofollow noreferrer\">is also available online</a> along with <a href=\"http://www.stanford.edu/class/cs221/handouts.html\" rel=\"nofollow noreferrer\">all the lecture notes</a> and uses:</p>\n<ul>\n<li>S. Russell and P. Norvig, <a href=\"http://aima.cs.berkeley.edu/\" rel=\"nofollow noreferrer\">Artificial Intelligence: A Modern Approach</a>. Second Edition</li>\n</ul>\n<p>This is the standard Artificial Intelligence text and is also worth reading.</p>\n<p>I use <a href=\"http://www.r-project.org\" rel=\"nofollow noreferrer\"><strong>R</strong></a> for machine learning myself and really recommend it.  For this, I would suggest looking at <a href=\"http://www-stat.stanford.edu/~tibs/ElemStatLearn/\" rel=\"nofollow noreferrer\"><strong>The Elements of Statistical Learning</strong></a>, for which the full text is available online for free.  You may want to refer to the <a href=\"http://cran.r-project.org/web/views/MachineLearning.html\" rel=\"nofollow noreferrer\">Machine Learning</a> and <a href=\"http://cran.r-project.org/web/views/NaturalLanguageProcessing.html\" rel=\"nofollow noreferrer\">Natural Language Processing</a> views on CRAN for specific functionality.  </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My recommendation would be either or all (depending on his amount and area of interest) of these:</p>\n<p><a href=\"http://ukcatalogue.oup.com/product/9780199276349.do\" rel=\"nofollow noreferrer\">The Oxford Handbook of Computational Linguistics</a>:</p>\n<p><a href=\"https://i.sstatic.net/KXnwR.jpg\" rel=\"nofollow noreferrer\"><img alt=\"The Oxford Handbook of Computational Linguistics\" src=\"https://i.sstatic.net/KXnwR.jpg\" width=\"100\"/></a><br/>\n<sub>(source: <a href=\"http://ukcatalogue.oup.com/images/en_US/covers/medium/9780199276349_140.jpg\" rel=\"nofollow noreferrer\">oup.com</a>)</sub> </p>\n<p><a href=\"http://nlp.stanford.edu/fsnlp/promo/\" rel=\"nofollow noreferrer\">Foundations of Statistical Natural Language Processing</a>:</p>\n<p><a href=\"https://i.sstatic.net/4j54w.gif\" rel=\"nofollow noreferrer\"><img alt=\"Foundations of Statistical Natural Language Processing\" src=\"https://i.sstatic.net/4j54w.gif\" width=\"100\"/></a></p>\n<p><a href=\"http://nlp.stanford.edu/IR-book/information-retrieval-book.html\" rel=\"nofollow noreferrer\">Introduction to Information Retrieval</a>:</p>\n<p><a href=\"https://i.sstatic.net/CDAk3.jpg\" rel=\"nofollow noreferrer\"><img alt=\"Introduction to Information Retrieval\" src=\"https://i.sstatic.net/CDAk3.jpg\" width=\"100\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>String algorithms, including suffix trees. Calculus and linear algebra. Varying varieties of statistics. Artificial intelligence optimization algorithms. Data clustering techniques... and a million other things. This is a very active field right now, depending on what you intend to do.</p>\n<p>It doesn't really matter what language you choose to operate in. Python, for instance has the NLTK, which is a pretty nice free package for tinkering with computational linguistics.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Working with Sklearn stratified kfold split, and when I attempt to split using multi-class, I received on error (see below).  When I tried and split using binary, it works no problem.</p>\n<pre class=\"lang-python prettyprint-override\"><code>num_classes = len(np.unique(y_train))\ny_train_categorical = keras.utils.to_categorical(y_train, num_classes)\nkf=StratifiedKFold(n_splits=5, shuffle=True, random_state=999)\n\n# splitting data into different folds\nfor i, (train_index, val_index) in enumerate(kf.split(x_train, y_train_categorical)):\n    x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]\n    y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n\nValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>keras.utils.to_categorical</code> produces a one-hot encoded class vector, i.e. the <code>multilabel-indicator</code> mentioned in the error message. <code>StratifiedKFold</code> is not designed to work with such input; from the <code>split</code> method <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold.split\" rel=\"noreferrer\">docs</a>:</p>\n<blockquote>\n<p><strong><code>split</code><em></em></strong>(X, y, groups=None)</p>\n<p>[...]</p>\n<p><strong>y</strong> : array-like, shape (n_samples,)</p>\n<p>The target variable for supervised learning problems. Stratification is done based on the y labels.</p>\n</blockquote>\n<p>i.e. your <code>y</code> must be a 1-D array of your class labels.</p>\n<p>Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial <code>y_train</code>), and convert <code>to_categorical</code> afterwards.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If your target variable is continuous then use simple KFold cross validation instead of StratifiedKFold.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.model_selection import KFold\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Call to <code>split()</code> like this:</p>\n<pre><code>for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train_categorical.argmax(1))):\n    x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]\n    y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the meaning of <em><strong>forward pass</strong></em> and <em><strong>backward pass</strong></em> in neural networks?</p>\n<p>Everybody is mentioning these expressions when talking about backpropagation and epochs.</p>\n<p>I understood that forward pass and backward pass together form an epoch.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <strong>\"forward pass\"</strong> refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.</p>\n<p>A loss function is calculated from the output values.</p>\n<p>And then <strong>\"backward pass\"</strong> refers to process of counting changes in weights (de facto <em>learning</em>), using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.</p>\n<p>Backward and forward pass makes together one <strong>\"iteration\"</strong>.</p>\n<hr/>\n<p>During one iteration, you usually pass a subset of the data set, which is called <strong>\"mini-batch\"</strong> or <strong>\"batch\"</strong> (however, \"batch\" can also mean an entire set, hence the prefix \"mini\")</p>\n<p><strong>\"Epoch\"</strong> means passing the entire data set in batches.<br/> One epoch contains <em>(number_of_items / batch_size)</em> iterations</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've trained a Linear Regression model with R caret. I'm now trying to generate a confusion matrix and keep getting the following error:</p>\n<p>Error in confusionMatrix.default(pred, testing$Final) : \nthe data and reference factors must have the same number of levels</p>\n<pre><code>EnglishMarks &lt;- read.csv(\"E:/Subject Wise Data/EnglishMarks.csv\", \nheader=TRUE)\ninTrain&lt;-createDataPartition(y=EnglishMarks$Final,p=0.7,list=FALSE)\ntraining&lt;-EnglishMarks[inTrain,]\ntesting&lt;-EnglishMarks[-inTrain,]\npredictionsTree &lt;- predict(treeFit, testdata)\nconfusionMatrix(predictionsTree, testdata$catgeory)\nmodFit&lt;-train(Final~UT1+UT2+HalfYearly+UT3+UT4,method=\"lm\",data=training)\npred&lt;-format(round(predict(modFit,testing)))              \nconfusionMatrix(pred,testing$Final)\n</code></pre>\n<p>The error occurs when generating the confusion matrix. The levels are the same on both objects. I cant figure out what the problem is. Their structure and levels are given below. They should be the same. Any help would be greatly appreciated as its making me cracked!!</p>\n<pre><code>&gt; str(pred)\nchr [1:148] \"85\" \"84\" \"87\" \"65\" \"88\" \"84\" \"82\" \"84\" \"65\" \"78\" \"78\" \"88\" \"85\"  \n\"86\" \"77\" ...\n&gt; str(testing$Final)\nint [1:148] 88 85 86 70 85 85 79 85 62 77 ...\n\n&gt; levels(pred)\nNULL\n&gt; levels(testing$Final)\nNULL\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had the same issue. \nI guess it happened because data argument was not casted as factor as I expected.\nTry: </p>\n<pre><code>confusionMatrix(pred,as.factor(testing$Final))\n</code></pre>\n<p>hope it helps</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>confusionMatrix(pred,testing$Final)\n</code></pre>\n<p>Whenever you try to build a confusion matrix, make sure that both the true values and prediction values are of factor datatype. </p>\n<p>Here both pred and <code>testing$Final</code> must be of type <code>factor</code>. Instead of check for levels, check the type of both the variables and convert them to factor if they are not.</p>\n<p>Here <code>testing$final</code> is of type <code>int</code>. conver it to factor and then build the confusion matrix.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Do <code>table(pred)</code> and <code>table(testing$Final)</code>. You will see that there is at least one number in the testing set that is never predicted (i.e. never present in <code>pred</code>). This is what is meant why \"different number of levels\". There is an example of a custom made function to get around this problem <a href=\"https://stackoverflow.com/questions/19871043/r-package-caret-confusionmatrix-with-missing-categories\">here</a>.</p>\n<p>However, I found that this trick works fine:</p>\n<pre><code>table(factor(pred, levels=min(test):max(test)), \n      factor(test, levels=min(test):max(test)))\n</code></pre>\n<p>It should give you exactly the same confusion matrix as with the function. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there any noticeable difference in TensorFlow performance if using Quadro GPUs vs GeForce GPUs? </p>\n<p><strong>e.g. does it use double precision operations or something else that would cause a drop in GeForce cards?</strong></p>\n<p>I am about to buy a GPU for TensorFlow, and wanted to know if a GeForce would be ok. Thanks and appreciate your help</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think GeForce TITAN is great and is widely used in Machine Learning (ML). In ML, single precision is enough in most of cases. </p>\n<p>More detail on the performance of the GTX line (currently GeForce 10) can be found in Wikipedia, <a href=\"https://en.wikipedia.org/wiki/GeForce_10_series\" rel=\"noreferrer\">here</a>.</p>\n<p>Other sources around the web support this claim. Here is a quote <a href=\"http://doc-ok.org/?p=304\" rel=\"noreferrer\">from doc-ok in 2013</a> (<a href=\"https://perma.cc/GH7A-8UUX\" rel=\"noreferrer\">permalink</a>).</p>\n<blockquote>\n<p>For comparison, an “entry-level” $700 Quadro 4000 is significantly slower than a $530 high-end GeForce GTX 680, at least according to my measurements using several Vrui applications, and the closest performance-equivalent to a GeForce GTX 680 I could find was a Quadro 6000 for a whopping $3660. </p>\n</blockquote>\n<p>Specific to ML, including deep learning, there is a <a href=\"https://www.kaggle.com/general/11332\" rel=\"noreferrer\">Kaggle forum discussion dedicated to this subject</a> (Dec 2014, <a href=\"https://perma.cc/RBZ6-YX9F\" rel=\"noreferrer\">permalink</a>), which goes over comparisons between the Quadro, GeForce, and Tesla series:</p>\n<blockquote>\n<p>Quadro GPUs aren't for scientific computation, Tesla GPUs are.  Quadro\n  cards are designed for accelerating CAD, so they won't help you to\n  train neural nets. They can probably be used for that purpose just\n  fine, but it's a waste of money.</p>\n<p>Tesla cards are for scientific computation, but they tend to be pretty\n  expensive. The good news is that many of the features offered by Tesla\n  cards over GeForce cards are not necessary to train neural networks.</p>\n<p>For example, Tesla cards usually have ECC memory, which is nice to\n  have but not a requirement. They also have much better support for\n  double precision computations, but single precision is plenty for\n  neural network training, and they perform about the same as GeForce\n  cards for that.</p>\n<p>One useful feature of Tesla cards is that they tend to have is a lot\n  more RAM than comparable GeForce cards. More RAM is always welcome if\n  you're planning to train bigger models (or use RAM-intensive\n  computations like FFT-based convolutions).</p>\n<p>If you're choosing between Quadro and GeForce, definitely pick\n  GeForce. If you're choosing between Tesla and GeForce, pick GeForce,\n  unless you have a lot of money and could really use the extra RAM.</p>\n</blockquote>\n<p><strong>NOTE:</strong> Be careful what platform you are working on and what the default precision is in it. For example, <a href=\"https://devtalk.nvidia.com/default/topic/959375/cuda-programming-and-performance/performance-issues-with-cuda-and-python-r/1\" rel=\"noreferrer\">here in the CUDA forums</a> (August 2016), one developer owns two Titan X's (GeForce series) and doesn't see a performance gain in any of their R or Python scripts. This is diagnosed as a result of R being defaulted to double precision, and has a worse performance on new GPU than their CPU (a Xeon processor). Tesla GPUs are cited as the best performance for double precision. In this case, converting all numbers to float32 increases performance from 12.437s with nvBLAS 0.324s with gmatrix+float32s on one TITAN X (see first benchmark). Quoting from this forum discussion:</p>\n<blockquote>\n<p>Double precision performance of Titan X is pretty low.</p>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been attempting to develop a means of synthesizing human-like mouse movement in an application of mine for the past few weeks. At the start I used simple techniques like polynomial and spline interpolation, however even with a little noise the result still failed to appear sufficiently human-like.</p>\n<p>In an effort to remedy this issue, I've been researching into ways of applying machine learning algorithms on real human mouse movement biometrics in order to synthesize mouse movements by learning from recorded <em>real</em> human ones. Users would be compiling a profile of recorded movements that would trainh= the program for synthesis purposes.</p>\n<p>I've been searching for a few weeks and read several articles on application of inverse biometrics in generating mouse dynamics, such as <a href=\"http://www.isot.ece.uvic.ca/publications/behavioral-biometricsx/IJPRAI2203_P461.pdf\" rel=\"nofollow\">Inverse Biometrics for Mouse Dynamics</a>; they tend to focus, however, on generating realistic time from <strong>randomly-generated</strong> dynamics, while I was <em>hoping</em> to generate a path from <strong>specifically</strong> A to B. Plus, I still need to actually need to come up with a path, not just a few dynamics measured from one.</p>\n<p>Does anyone have a few pointers to help a noob?</p>\n<p>Currently, testing is done by recording movements and having I and several other developers watch the playback. Ideally the movement will be able to trick both an automatic biometric classifier, <em>as well as</em> a real, live, breathing Homo sapien, too.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://en.wikipedia.org/wiki/Fitts%27s_law\" rel=\"noreferrer\">Fitt's law</a> gives a very good estimation of the time needed to position the mouse pointer. In the <a href=\"http://en.wikipedia.org/wiki/Fitts%27s_law#Derivation\" rel=\"noreferrer\">derivation</a> section there is a simple explanation I think you could use this as one of the basic building blocks of your app. Start with big movements, put some inacurracy both in the direction and the length of the movement, then do a smaller correction movement and so on...</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First, i guess you record human mouse movements from A to B. Because otherwise, trying to synthesize a model for such movement does not seem possible to me.</p>\n<p>Second, how about measuring the deviations from the \"direct\" path, maybe in relation to time. I actually suspect that movements look different for different angles, path lengths etc., but maybe you can try a normalized model first, that you just stretch (in space and time) and rotate like you need it.</p>\n<p>Third, the learning. The easiest thing would be to just have a collection of real moves (in the form i discussed above), and sample from that collection. Evaluate how that looks like. If you really want a probabilistic model, then you have to evaluate what kind of models fit. is it enough to blurr the direct path with gaussian noise whose parameters you learn from your training set? Or some (sin-)wavy deviation? Or seperate models for \"getting near the button\" and \"final corrections\". Fitts law might be useful for evaluation.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This question reminded me of a website I knew about years ago, so I visited it and found <a href=\"https://www.moparisthebest.com/smf/index.php/topic,509860.0.html\" rel=\"nofollow\">this in-depth discussion on the topic</a>.</p>\n<p>The timing is so similar as to make me think this question is related in some way. In fact, someone in the thread linked to the same article you did. If it's not related, well, there's a link to a lot of people discussing exactly what you're thinking about.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been exploring the <code>xgboost</code> package in R and went through several demos as well as tutorials but this still confuses me: after using <code>xgb.cv</code> to do cross validation, how does the optimal parameters get passed to <code>xgb.train</code>? Or should I calculate the ideal parameters (such as <code>nround</code>, <code>max.depth</code>) based on the output of <code>xgb.cv</code>?</p>\n<pre><code>param &lt;- list(\"objective\" = \"multi:softprob\",\n              \"eval_metric\" = \"mlogloss\",\n              \"num_class\" = 12)\ncv.nround &lt;- 11\ncv.nfold &lt;- 5\nmdcv &lt;-xgb.cv(data=dtrain,params = param,nthread=6,nfold = cv.nfold,nrounds = cv.nround,verbose = T)\n\nmd &lt;-xgb.train(data=dtrain,params = param,nround = 80,watchlist = list(train=dtrain,test=dtest),nthread=6)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Looks like you misunderstood <code>xgb.cv</code>, it is not a parameter searching function. It does k-folds cross validation, nothing more.</p>\n<p>In your code, it does not change the value of <code>param</code>. </p>\n<p>To find best parameters in R's XGBoost, there are some methods. These are 2 methods, </p>\n<p>(1) Use <code>mlr</code> package, <a href=\"http://mlr-org.github.io/mlr-tutorial/release/html/\">http://mlr-org.github.io/mlr-tutorial/release/html/</a></p>\n<p>There is a XGBoost + mlr <a href=\"https://www.kaggle.com/casalicchio/prudential-life-insurance-assessment/use-the-mlr-package-scores-0-649\">example code</a> in the Kaggle's Prudential challenge, </p>\n<p>But that code is for regression, not classification. As far as I know, there is no <code>mlogloss</code> metric yet in <code>mlr</code> package, so you must code the mlogloss measurement from scratch by yourself. CMIIW. </p>\n<p>(2) Second method, by manually setting the parameters then repeat, example, </p>\n<pre><code>param &lt;- list(objective = \"multi:softprob\",\n      eval_metric = \"mlogloss\",\n      num_class = 12,\n      max_depth = 8,\n      eta = 0.05,\n      gamma = 0.01, \n      subsample = 0.9,\n      colsample_bytree = 0.8, \n      min_child_weight = 4,\n      max_delta_step = 1\n      )\ncv.nround = 1000\ncv.nfold = 5\nmdcv &lt;- xgb.cv(data=dtrain, params = param, nthread=6, \n                nfold=cv.nfold, nrounds=cv.nround,\n                verbose = T)\n</code></pre>\n<p>Then, you find the best (minimum) mlogloss, </p>\n<pre><code>min_logloss = min(mdcv[, test.mlogloss.mean])\nmin_logloss_index = which.min(mdcv[, test.mlogloss.mean])\n</code></pre>\n<p><code>min_logloss</code> is the minimum value of mlogloss, while <code>min_logloss_index</code> is the index (round). </p>\n<p>You must repeat the process above several times, each time change the parameters manually (<code>mlr</code> does the repeat for you). Until finally you get best global minimum <code>min_logloss</code>. </p>\n<p><strong>Note:</strong> You can do it in a loop of 100 or 200 iterations, in which for each iteration you set the parameters value randomly. This way, you must save the best <code>[parameters_list, min_logloss, min_logloss_index]</code> in  variables or in a file. </p>\n<p><strong>Note:</strong> better to set random seed by <code>set.seed()</code> for <strong>reproducible</strong> result. Different random seed yields different result. So, you must save <code>[parameters_list, min_logloss, min_logloss_index, seednumber]</code> in the  variables or file. </p>\n<p>Say that finally you get 3 results in 3 iterations/repeats: </p>\n<pre><code>min_logloss = 2.1457, min_logloss_index = 840\nmin_logloss = 2.2293, min_logloss_index = 920\nmin_logloss = 1.9745, min_logloss_index = 780\n</code></pre>\n<p>Then you must use the third parameters (it has global minimum <code>min_logloss</code> of <code>1.9745</code>). Your best index (nrounds) is <code>780</code>. </p>\n<p>Once you get best parameters, use it in the training, </p>\n<pre><code># best_param is global best param with minimum min_logloss\n# best_min_logloss_index is the global minimum logloss index\nnround = 780\nmd &lt;- xgb.train(data=dtrain, params=best_param, nrounds=nround, nthread=6)\n</code></pre>\n<p>I don't think you need <code>watchlist</code> in the training, because you have done the cross validation. But if you still want to use <code>watchlist</code>, it is just okay. </p>\n<p>Even better you can use early stopping in <code>xgb.cv</code>. </p>\n<pre><code>mdcv &lt;- xgb.cv(data=dtrain, params=param, nthread=6, \n                nfold=cv.nfold, nrounds=cv.nround,\n                verbose = T, early.stop.round=8, maximize=FALSE)\n</code></pre>\n<p>With this code, when <code>mlogloss</code> value is not decreasing in 8 steps, the <code>xgb.cv</code> will stop. You can save time. You must set <code>maximize</code> to <code>FALSE</code>, because you expect minimum mlogloss. </p>\n<p>Here is an example code, with 100 iterations loop, and random chosen parameters. </p>\n<pre><code>best_param = list()\nbest_seednumber = 1234\nbest_logloss = Inf\nbest_logloss_index = 0\n\nfor (iter in 1:100) {\n    param &lt;- list(objective = \"multi:softprob\",\n          eval_metric = \"mlogloss\",\n          num_class = 12,\n          max_depth = sample(6:10, 1),\n          eta = runif(1, .01, .3),\n          gamma = runif(1, 0.0, 0.2), \n          subsample = runif(1, .6, .9),\n          colsample_bytree = runif(1, .5, .8), \n          min_child_weight = sample(1:40, 1),\n          max_delta_step = sample(1:10, 1)\n          )\n    cv.nround = 1000\n    cv.nfold = 5\n    seed.number = sample.int(10000, 1)[[1]]\n    set.seed(seed.number)\n    mdcv &lt;- xgb.cv(data=dtrain, params = param, nthread=6, \n                    nfold=cv.nfold, nrounds=cv.nround,\n                    verbose = T, early.stop.round=8, maximize=FALSE)\n\n    min_logloss = min(mdcv[, test.mlogloss.mean])\n    min_logloss_index = which.min(mdcv[, test.mlogloss.mean])\n\n    if (min_logloss &lt; best_logloss) {\n        best_logloss = min_logloss\n        best_logloss_index = min_logloss_index\n        best_seednumber = seed.number\n        best_param = param\n    }\n}\n\nnround = best_logloss_index\nset.seed(best_seednumber)\nmd &lt;- xgb.train(data=dtrain, params=best_param, nrounds=nround, nthread=6)\n</code></pre>\n<p>With this code, you run cross validation 100 times, each time with random parameters. Then you get best parameter set, that is in the iteration with minimum <code>min_logloss</code>. </p>\n<p>Increase the value of <code>early.stop.round</code> in case you find out that it's too small (too early stopping). You need also to change the random parameter values' limit based on your data characteristics. </p>\n<p>And, for 100 or 200 iterations, I think you want to change <code>verbose</code> to FALSE. </p>\n<p><strong>Side note:</strong> That is example of random method, you can adjust it e.g. by Bayesian optimization for better method. If you have Python version of XGBoost, there is a good hyperparameter script for XGBoost, <a href=\"https://github.com/mpearmain/BayesBoost\">https://github.com/mpearmain/BayesBoost</a> to search for best parameters set using Bayesian optimization. </p>\n<p>Edit: I want to add 3rd manual method, posted by \"Davut Polat\" a Kaggle master, in the <a href=\"https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19083/best-practices-for-parameter-tuning-on-models/108783\">Kaggle forum</a>.</p>\n<p>Edit: If you know Python and sklearn, you can also use <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\">GridSearchCV</a> along with xgboost.XGBClassifier or xgboost.XGBRegressor</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a good question and great reply from <em>silo</em> with lots of details! I found it very helpful for someone new to <code>xgboost</code> like me. Thank you. The method to randomize and compared to boundary is very inspiring. Good to use and good to know. Now in 2018 some slight revise are needed, for example, <code>early.stop.round</code> should be <code>early_stopping_rounds</code>. The output <code>mdcv</code> is organized slightly differently:</p>\n<pre><code>  min_rmse_index  &lt;-  mdcv$best_iteration\n  min_rmse &lt;-  mdcv$evaluation_log[min_rmse_index]$test_rmse_mean\n</code></pre>\n<p>And depends on the application (linear, logistic,etc...), the <code>objective</code>, <code>eval_metric</code> and parameters shall be adjusted accordingly. </p>\n<p>For the convenience of anyone who is running a regression, here is the slightly adjusted version of code (most are the same as above).  </p>\n<pre><code>library(xgboost)\n# Matrix for xgb: dtrain and dtest, \"label\" is the dependent variable\ndtrain &lt;- xgb.DMatrix(X_train, label = Y_train)\ndtest &lt;- xgb.DMatrix(X_test, label = Y_test)\n\nbest_param &lt;- list()\nbest_seednumber &lt;- 1234\nbest_rmse &lt;- Inf\nbest_rmse_index &lt;- 0\n\nset.seed(123)\nfor (iter in 1:100) {\n  param &lt;- list(objective = \"reg:linear\",\n                eval_metric = \"rmse\",\n                max_depth = sample(6:10, 1),\n                eta = runif(1, .01, .3), # Learning rate, default: 0.3\n                subsample = runif(1, .6, .9),\n                colsample_bytree = runif(1, .5, .8), \n                min_child_weight = sample(1:40, 1),\n                max_delta_step = sample(1:10, 1)\n  )\n  cv.nround &lt;-  1000\n  cv.nfold &lt;-  5 # 5-fold cross-validation\n  seed.number  &lt;-  sample.int(10000, 1) # set seed for the cv\n  set.seed(seed.number)\n  mdcv &lt;- xgb.cv(data = dtrain, params = param,  \n                 nfold = cv.nfold, nrounds = cv.nround,\n                 verbose = F, early_stopping_rounds = 8, maximize = FALSE)\n\n  min_rmse_index  &lt;-  mdcv$best_iteration\n  min_rmse &lt;-  mdcv$evaluation_log[min_rmse_index]$test_rmse_mean\n\n  if (min_rmse &lt; best_rmse) {\n    best_rmse &lt;- min_rmse\n    best_rmse_index &lt;- min_rmse_index\n    best_seednumber &lt;- seed.number\n    best_param &lt;- param\n  }\n}\n\n# The best index (min_rmse_index) is the best \"nround\" in the model\nnround = best_rmse_index\nset.seed(best_seednumber)\nxg_mod &lt;- xgboost(data = dtest, params = best_param, nround = nround, verbose = F)\n\n# Check error in testing data\nyhat_xg &lt;- predict(xg_mod, dtest)\n(MSE_xgb &lt;- mean((yhat_xg - Y_test)^2))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I found silo's answer is very helpful. \nIn addition to his approach of random research, you may want to use Bayesian optimization to facilitate the process of hyperparameter search, e.g. <a href=\"ftp://cran.r-project.org/pub/R/web/packages/rBayesianOptimization/rBayesianOptimization.pdf\" rel=\"nofollow noreferrer\">rBayesianOptimization library</a>. \nThe following is my code with rbayesianoptimization library.</p>\n<pre><code>cv_folds &lt;- KFold(dataFTR$isPreIctalTrain, nfolds = 5, stratified = FALSE, seed = seedNum)\nxgb_cv_bayes &lt;- function(nround,max.depth, min_child_weight, subsample,eta,gamma,colsample_bytree,max_delta_step) {\nparam&lt;-list(booster = \"gbtree\",\n            max_depth = max.depth,\n            min_child_weight = min_child_weight,\n            eta=eta,gamma=gamma,\n            subsample = subsample, colsample_bytree = colsample_bytree,\n            max_delta_step=max_delta_step,\n            lambda = 1, alpha = 0,\n            objective = \"binary:logistic\",\n            eval_metric = \"auc\")\ncv &lt;- xgb.cv(params = param, data = dtrain, folds = cv_folds,nrounds = 1000,early_stopping_rounds = 10, maximize = TRUE, verbose = verbose)\n\nlist(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration],\n     Pred=cv$best_iteration)\n# we don't need cross-validation prediction and we need the number of rounds.\n# a workaround is to pass the number of rounds(best_iteration) to the Pred, which is a default parameter in the rbayesianoptimization library.\n}\nOPT_Res &lt;- BayesianOptimization(xgb_cv_bayes,\n                              bounds = list(max.depth =c(3L, 10L),min_child_weight = c(1L, 40L),\n                                            subsample = c(0.6, 0.9),\n                                            eta=c(0.01,0.3),gamma = c(0.0, 0.2),\n                                            colsample_bytree=c(0.5,0.8),max_delta_step=c(1L,10L)),\n                              init_grid_dt = NULL, init_points = 10, n_iter = 10,\n                              acq = \"ucb\", kappa = 2.576, eps = 0.0,\n                              verbose = verbose)\nbest_param &lt;- list(\nbooster = \"gbtree\",\neval.metric = \"auc\",\nobjective = \"binary:logistic\",\nmax_depth = OPT_Res$Best_Par[\"max.depth\"],\neta = OPT_Res$Best_Par[\"eta\"],\ngamma = OPT_Res$Best_Par[\"gamma\"],\nsubsample = OPT_Res$Best_Par[\"subsample\"],\ncolsample_bytree = OPT_Res$Best_Par[\"colsample_bytree\"],\nmin_child_weight = OPT_Res$Best_Par[\"min_child_weight\"],\nmax_delta_step = OPT_Res$Best_Par[\"max_delta_step\"])\n# number of rounds should be tuned using CV\n#https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/\n# However, nrounds can not be directly derivied from the bayesianoptimization function\n# Here, OPT_Res$Pred, which was supposed to be used for cross-validation, is used to record the number of rounds\nnrounds=OPT_Res$Pred[[which.max(OPT_Res$History$Value)]]\nxgb_model &lt;- xgb.train (params = best_param, data = dtrain, nrounds = nrounds)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is it possible to delete or insert a step in a <code>sklearn.pipeline.Pipeline</code> object?</p>\n<p>I am trying to do a grid search with or without one step in the Pipeline object. And wondering whether I can insert or delete a step in the pipeline. I saw in the <code>Pipeline</code> source code, there is a <code>self.steps</code> object holding all the steps. We can get the steps by <code>named_steps()</code>.  Before modifying it, I want to make sure, I do not cause unexpected effects. </p>\n<p>Here is a example code:</p>\n<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nestimators = [('reduce_dim', PCA()), ('svm', SVC())]\nclf = Pipeline(estimators)\nclf \n</code></pre>\n<p>Is it possible that we do something like <code>steps = clf.named_steps()</code>, then insert or delete in this list? Does this cause undesired effect on the clf object?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I see that everyone mentioned only the delete step. In case you want to also insert a step in the pipeline:</p>\n<pre><code>pipe.steps.append(['step name',transformer()])\n</code></pre>\n<p><code>pipe.steps</code> works in the same way as lists do, so you can also insert an item into a specific location:</p>\n<pre><code>pipe.steps.insert(1,['estimator',transformer()]) #insert as second step\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Based on rudimentary testing you can safely remove a step from a scikit-learn pipeline just like you would any list item, with a simple</p>\n<pre><code>clf_pipeline.steps.pop(n)\n</code></pre>\n<p>where n is the position of the individual estimator you are trying to remove.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just chiming in because I feel like the other answers answered the question of adding steps to a pipeline really well, <strong>but didn't really cover how to delete a step from a pipeline.</strong></p>\n<p>Watch out with my approach though. Slicing lists in this instance is a bit weird. </p>\n<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\n\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n</code></pre>\n<p>If you want to create a pipeline with just steps PCA/Polynomial you can just slice the list step by indexes and pass it to Pipeline</p>\n<pre><code>clf1 = Pipeline(clf.steps[0:2])\n</code></pre>\n<p>Want to just use steps 2/3?\nWatch out these slices don't always make the most amount of sense</p>\n<pre><code>clf2 = Pipeline(clf.steps[1:3])\n</code></pre>\n<p>Want to just use steps 1/3? \nI can't seem to do using this approach</p>\n<pre><code>clf3 = Pipeline(clf.steps[0] + clf.steps[2]) # errors\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I got an keras(h5) file. I need to convert it to tflite??\nI researched, First i need to go via h5 -&gt; pb -&gt; tflite\n(because h5 - tflite sometimes results in some issue)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>from tensorflow.contrib import lite\nconverter = lite.TFLiteConverter.from_keras_model_file( 'model.h5')\ntfmodel = converter.convert()\nopen (\"model.tflite\" , \"wb\") .write(tfmodel)\n</code></pre>\n<p>You can use the TFLiteConverter to directly convert .h5 files to .tflite file.\n<strong>This does not work on Windows.</strong></p>\n<p>For Windows, use this <a href=\"https://drive.google.com/file/d/1IUIn9ffk5ICKujqPyuGaHL2irQ9Wmtpm/view?usp=drivesdk\" rel=\"noreferrer\">Google Colab notebook</a> to convert. Upload the .h5 file and it will convert it .tflite file.</p>\n<p>Follow, if you want to try it yourself :</p>\n<ol>\n<li>Create a Google Colab Notebook. In the left top corner, click the \"UPLOAD\" button and upload your .h5 file.</li>\n<li><p>Create a code cell and insert this code.</p>\n<pre><code>from tensorflow.contrib import lite\nconverter = lite.TFLiteConverter.from_keras_model_file( 'model.h5' ) # Your model's name\nmodel = converter.convert()\nfile = open( 'model.tflite' , 'wb' ) \nfile.write( model )\n</code></pre></li>\n<li><p>Run the cell. You will get a model.tflite file. Right click on the file and select \"DOWNLOAD\" option.</p></li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This worked for me on Windows 10 using Tensorflow 2.1.0 and Keras 2.3.1</p>\n<pre><code>import tensorflow as tf\n\nmodel = tf.keras.models.load_model('model.h5')\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just did this from CoLab using this code in a notebook:</p>\n<pre><code>import tensorflow as tf\nmodel = tf.keras.models.load_model('yourmodel.h5')\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflmodel = converter.convert()\nfile = open( 'yourmodel.tflite' , 'wb' ) \nfile.write( tflmodel )\n</code></pre>\n<p>I had difficulty uploading the h5 model via CoLab so I mounted my Google Drive, uploaded it there, and then moved it over to the notebook content folder.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am beginner in neural networks. I am learning about perceptrons.\nMy question is Why is weight vector perpendicular to decision boundary(Hyperplane)?\nI referred many books but all are mentioning that weight vector is perpendicular to decision boundary but none are saying why?</p>\n<p>Can anyone give me an explanation or reference to a book?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The weights are simply the coefficients that define a separating plane.  For the moment, forget about neurons and just consider the geometric definition of a plane in N dimensions:</p>\n<pre><code>w1*x1 + w2*x2 + ... + wN*xN - w0 = 0\n</code></pre>\n<p>You can also think of this as being a dot product:</p>\n<pre><code>w*x - w0 = 0\n</code></pre>\n<p>where <code>w</code> and <code>x</code> are both length-N vectors.  This equation holds for all points on the plane. Recall that we can multiply the above equation by a constant and it still holds so we can define the constants such that the vector <code>w</code> has unit length.  Now, take out a piece of paper and draw your <code>x-y</code> axes (<code>x1</code> and <code>x2</code> in the above equations).  Next, draw a line (a plane in <code>2D</code>) somewhere near the origin. <code>w0</code> is simply the perpendicular distance from the origin to the plane and <code>w</code> is the unit vector that points from the origin along that perpendicular.  If you now draw a vector from the origin to any point on the plane, the dot product of that vector with the unit vector <code>w</code> will always be equal to <code>w0</code> so the equation above holds, right?  This is simply the geometric definition of a plane: a unit vector defining the perpendicular to the plane (<code>w</code>) and the distance (<code>w0</code>) from the origin to the plane.</p>\n<p>Now our neuron is simply representing the same plane as described above but we just describe the variables a little differently.  We'll call the components of <code>x</code> our \"inputs\", the components of <code>w</code> our \"weights\", and we'll call the distance <code>w0</code> a bias.  That's all there is to it.</p>\n<p>Getting a little beyond your actual question, we don't really care about points on the plane.  We really want to know which side of the plane a point falls on.  While <code>w*x - w0</code> is exactly zero on the plane, it will have positive values for points on one side of the plane and negative values for points on the other side.  That's where the neuron's activation function comes in but that's beyond your actual question.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Intuitively, in a binary problem the weight vector points in the direction of the '1'-class, while the '0'-class is found when pointing away from the weight vector. The decision boundary should thus be drawn perpendicular to the weight vector.</p>\n<p>See the image for a simplified example: You have a neural network with only 1 input which thus has 1 weight. If the weight is -1 (the blue vector), then all negative inputs will become positive, so the whole negative spectrum will be assigned to the '1'-class, while the positive spectrum will be the '0'-class. The decision boundary in a 2-axis plane is thus a vertical line through the origin (the red line). Simply said it is the line perpendicular to the weight vector.</p>\n<p>Lets go through this example with a few values. The output of the perceptron is class 1 if the sum of all <code>inputs * weights</code> is larger than 0 (the default threshold), otherwise if the output is smaller than the threshold of 0 then the class is 0. Your input has value 1. The weight applied to this single input is -1, so <code>1 * -1 = -1</code> which is less than 0. The input is thus assigned class 0 (NOTE: class 0 and class 1 could have just been called class A or class B, don't confuse them with the input and weight values). Conversely, if the input is -1, then <code>input * weight</code> is <code>-1 * -1 = 1</code>, which is larger than 0, so the input is assigned to class 1. If you try every input value then you will see that all the negative values in this example have an output larger than 0, so all of them belong to class 1. All positive values will have an output of smaller than 0 and therefore will be classified as class 0. Draw the line which separates all positive and negative input values (the red line) and you will see that this line is perpendicular to the weight vector.</p>\n<p>Also note that the weight vector is only used to modify the inputs to fit the wanted output. What would happen without a weight vector? An input of 1, would result in an output of 1, which is larger than the threshold of 0, so the class is '1'.</p>\n<p><img alt=\"image\" src=\"https://i.sstatic.net/DybSR.png\"/></p>\n<p>The second image on <a href=\"http://www.mathworks.nl/help/toolbox/nnet/ug/bss4hat-2.html\" rel=\"noreferrer\">this page</a> shows a perceptron with 2 inputs and a bias. The first input has the same weight as my example, while the second input has a weight of 1. The corresponding weight vector together with the decision boundary are thus changed as seen in the image. Also the decision boundary has been translated to the right due to an added bias of 1.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a viewpoint from a more fundamental linear algebra/calculus standpoint:</p>\n<p>The general equation of a plane is Ax + By + Cz = D (can be extended for higher dimensions). The normal vector can be extracted from this equation: [A B C]; it is the vector orthogonal to every other vector that lies on the plane.  </p>\n<p>Now if we have a weight vector [w1 w2 w3], then when do w^T * x &gt;= 0 (to get positive classification) and w^T * x &lt; 0 (to get negative classification). WLOG, we can also do w^T * x &gt;= d. Now, do you see where I am going with this? </p>\n<p>The weight vector is the same as the normal vector from the first section. And as we know, this normal vector (and a point) define a plane: which is exactly the decision boundary. Hence, because the normal vector is orthogonal to the plane, then so too is the weight vector orthogonal to the decision boundary.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using Linear regression to predict data. But, I am getting totally contrasting results when I Normalize (Vs) Standardize variables. </p>\n<p>Normalization               = x -xmin/ xmax – xmin\n \nZero Score Standardization  = x - xmean/ xstd\n </p>\n<pre><code>a) Also, when to Normalize (Vs) Standardize ?\nb) How Normalization affects Linear Regression?\nc) Is it okay if I don't normalize all the attributes/lables in the linear regression?\n</code></pre>\n<p>Thanks,\nSantosh</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Note that the results might not necessarily be so different. You might simply need different hyperparameters for the two options to give similar results.</p>\n<p>The ideal thing is to test what works best for your problem. If you can't afford this for some reason, most algorithms will probably benefit from standardization more so than from normalization.</p>\n<p>See <a href=\"http://spartanideas.msu.edu/2014/07/11/about-feature-scaling-and-normalization/\" rel=\"noreferrer\">here</a> for some examples of when one should be preferred over the other:</p>\n<blockquote>\n<p>For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; but more about PCA in my previous article).</p>\n<p>However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.</p>\n</blockquote>\n<p>One disadvantage of normalization over standardization is that it loses some information in the data, especially about outliers.</p>\n<p>Also on the linked page, there is this picture:</p>\n<p><a href=\"https://i.sstatic.net/hcP4l.png\" rel=\"noreferrer\"><img alt=\"Plots of a standardized and normalized data set\" src=\"https://i.sstatic.net/hcP4l.png\"/></a></p>\n<p>As you can see, scaling clusters all the data very close together, which may not be what you want. It might cause algorithms such as gradient descent to take longer to converge to the same solution they would on a standardized data set, or it might even make it impossible.</p>\n<p>\"Normalizing variables\" doesn't really make sense. The correct terminology is \"normalizing / scaling the features\". If you're going to normalize or scale one feature, you should do the same for the rest.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>That makes sense because normalization and standardization do different things.</p>\n<p>Normalization transforms your data into a range between 0 and 1</p>\n<p>Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1</p>\n<p>Normalization/standardization are designed to achieve a similar goal, which is to create features that have similar ranges to each other. We want that so we can be sure we are capturing the true information in a feature, and that we dont over weigh a particular feature just because its values are much larger than other features.</p>\n<p>If all of your features are within a similar range of each other then theres no real need to standardize/normalize. If, however, some features naturally take on values that are much larger/smaller than others then normalization/standardization is called for</p>\n<p>If you're going to be normalizing at least one variable/feature, I would do the same thing to all of the others as well</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First question is why we need Normalisation/Standardisation?</p>\n<p>=&gt; We take a example of dataset where we have salary variable and age variable.\nAge can take range from 0 to 90 where salary can be from 25,000 to 250,000.</p>\n<p>We compare difference for 2 person then age difference will be in range of below 100 where salary difference will in range of thousands.</p>\n<p>So if we don't want one variable to dominate other then we use either Normalisation or Standardization. Now both age and salary will be in same scale\nbut when we use standardiztion or normalisation, we lose original values and it is transformed to some values. So loss of interpretation but extremely important when we want to draw inference from our data.</p>\n<p>Normalization rescales the values into a range of [0,1]. also called min-max scaled.</p>\n<p>Standardization rescales data to have a mean (μ) of 0 and standard deviation (σ) of 1.So it gives a normal graph.</p>\n<p><a href=\"https://i.sstatic.net/XmyWR.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/XmyWR.png\"/></a></p>\n<p>Example below:</p>\n<p><a href=\"https://i.sstatic.net/8ZFPL.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/8ZFPL.png\"/></a></p>\n<p>Another example:</p>\n<p><a href=\"https://i.sstatic.net/dFDoW.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/dFDoW.png\"/></a></p>\n<p>In above image, you can see that our actual data(in green) is spread b/w 1 to 6, standardised data(in red) is spread around -1 to 3 whereas normalised data(in blue) is spread around 0 to 1.</p>\n<p>Normally many algorithm required you to first standardise/normalise data before passing as parameter. Like in PCA, where we do dimension reduction by plotting our 3D data into 1D(say).Here we required standardisation.</p>\n<p>But in Image processing, it is required to normalise pixels before processing.\nBut during normalisation, we lose outliers(extreme datapoints-either too low or too high) which is slight disadvantage.</p>\n<p>So it depends on our preference what we chose but standardisation is most recommended as it gives a normal curve.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't understand which accuracy in the output to use to compare my 2 Keras models to see which one is better. </p>\n<p>Do I use the \"acc\" (from the training data?) one or the \"val acc\" (from the validation data?) one?</p>\n<p>There are different accs and val accs for each epoch. How do I know the acc or val acc for my model as a whole? Do I average all of the epochs accs or val accs to find the acc or val acc of the model as a whole?</p>\n<p><strong>Model 1 Output</strong></p>\n<pre><code>Train on 970 samples, validate on 243 samples\nEpoch 1/20\n0s - loss: 0.1708 - acc: 0.7990 - val_loss: 0.2143 - val_acc: 0.7325\nEpoch 2/20\n0s - loss: 0.1633 - acc: 0.8021 - val_loss: 0.2295 - val_acc: 0.7325\nEpoch 3/20\n0s - loss: 0.1657 - acc: 0.7938 - val_loss: 0.2243 - val_acc: 0.7737\nEpoch 4/20\n0s - loss: 0.1847 - acc: 0.7969 - val_loss: 0.2253 - val_acc: 0.7490\nEpoch 5/20\n0s - loss: 0.1771 - acc: 0.8062 - val_loss: 0.2402 - val_acc: 0.7407\nEpoch 6/20\n0s - loss: 0.1789 - acc: 0.8021 - val_loss: 0.2431 - val_acc: 0.7407\nEpoch 7/20\n0s - loss: 0.1789 - acc: 0.8031 - val_loss: 0.2227 - val_acc: 0.7778\nEpoch 8/20\n0s - loss: 0.1810 - acc: 0.8010 - val_loss: 0.2438 - val_acc: 0.7449\nEpoch 9/20\n0s - loss: 0.1711 - acc: 0.8134 - val_loss: 0.2365 - val_acc: 0.7490\nEpoch 10/20\n0s - loss: 0.1852 - acc: 0.7959 - val_loss: 0.2423 - val_acc: 0.7449\nEpoch 11/20\n0s - loss: 0.1889 - acc: 0.7866 - val_loss: 0.2523 - val_acc: 0.7366\nEpoch 12/20\n0s - loss: 0.1838 - acc: 0.8021 - val_loss: 0.2563 - val_acc: 0.7407\nEpoch 13/20\n0s - loss: 0.1835 - acc: 0.8041 - val_loss: 0.2560 - val_acc: 0.7325\nEpoch 14/20\n0s - loss: 0.1868 - acc: 0.8031 - val_loss: 0.2573 - val_acc: 0.7407\nEpoch 15/20\n0s - loss: 0.1829 - acc: 0.8072 - val_loss: 0.2581 - val_acc: 0.7407\nEpoch 16/20\n0s - loss: 0.1878 - acc: 0.8062 - val_loss: 0.2589 - val_acc: 0.7407\nEpoch 17/20\n0s - loss: 0.1833 - acc: 0.8072 - val_loss: 0.2613 - val_acc: 0.7366\nEpoch 18/20\n0s - loss: 0.1837 - acc: 0.8113 - val_loss: 0.2605 - val_acc: 0.7325\nEpoch 19/20\n0s - loss: 0.1906 - acc: 0.8010 - val_loss: 0.2555 - val_acc: 0.7407\nEpoch 20/20\n0s - loss: 0.1884 - acc: 0.8062 - val_loss: 0.2542 - val_acc: 0.7449\n</code></pre>\n<p><strong>Model 2 Output</strong></p>\n<pre><code>Train on 970 samples, validate on 243 samples\nEpoch 1/20\n0s - loss: 0.1735 - acc: 0.7876 - val_loss: 0.2386 - val_acc: 0.6667\nEpoch 2/20\n0s - loss: 0.1733 - acc: 0.7825 - val_loss: 0.1894 - val_acc: 0.7449\nEpoch 3/20\n0s - loss: 0.1781 - acc: 0.7856 - val_loss: 0.2028 - val_acc: 0.7407\nEpoch 4/20\n0s - loss: 0.1717 - acc: 0.8021 - val_loss: 0.2545 - val_acc: 0.7119\nEpoch 5/20\n0s - loss: 0.1757 - acc: 0.8052 - val_loss: 0.2252 - val_acc: 0.7202\nEpoch 6/20\n0s - loss: 0.1776 - acc: 0.8093 - val_loss: 0.2449 - val_acc: 0.7490\nEpoch 7/20\n0s - loss: 0.1833 - acc: 0.7897 - val_loss: 0.2272 - val_acc: 0.7572\nEpoch 8/20\n0s - loss: 0.1827 - acc: 0.7928 - val_loss: 0.2376 - val_acc: 0.7531\nEpoch 9/20\n0s - loss: 0.1795 - acc: 0.8062 - val_loss: 0.2445 - val_acc: 0.7490\nEpoch 10/20\n0s - loss: 0.1746 - acc: 0.8103 - val_loss: 0.2491 - val_acc: 0.7449\nEpoch 11/20\n0s - loss: 0.1831 - acc: 0.8082 - val_loss: 0.2477 - val_acc: 0.7449\nEpoch 12/20\n0s - loss: 0.1831 - acc: 0.8113 - val_loss: 0.2496 - val_acc: 0.7490\nEpoch 13/20\n0s - loss: 0.1920 - acc: 0.8000 - val_loss: 0.2459 - val_acc: 0.7449\nEpoch 14/20\n0s - loss: 0.1945 - acc: 0.7928 - val_loss: 0.2446 - val_acc: 0.7490\nEpoch 15/20\n0s - loss: 0.1852 - acc: 0.7990 - val_loss: 0.2459 - val_acc: 0.7449\nEpoch 16/20\n0s - loss: 0.1800 - acc: 0.8062 - val_loss: 0.2495 - val_acc: 0.7449\nEpoch 17/20\n0s - loss: 0.1891 - acc: 0.8000 - val_loss: 0.2469 - val_acc: 0.7449\nEpoch 18/20\n0s - loss: 0.1891 - acc: 0.8041 - val_loss: 0.2467 - val_acc: 0.7531\nEpoch 19/20\n0s - loss: 0.1853 - acc: 0.8072 - val_loss: 0.2511 - val_acc: 0.7449\nEpoch 20/20\n0s - loss: 0.1905 - acc: 0.8062 - val_loss: 0.2460 - val_acc: 0.7531\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>Do I use the \"acc\" (from the training data?) one or the \"val acc\" (from the validation data?) one?</p>\n</blockquote>\n<p>If you want to estimate the ability of your model to generalize to new data (which is probably what you want to do), then you look at the validation accuracy, because the validation split contains only data that the model never sees during the training and therefor cannot just memorize.</p>\n<p>If your training data accuracy (\"acc\") keeps improving while your validation data accuracy (\"val_acc\") gets worse, you are likely in an <a href=\"https://en.wikipedia.org/wiki/Overfitting\">overfitting</a> situation, i.e. your model starts to basically just memorize the data.</p>\n<blockquote>\n<p>There are different accs and val accs for each epoch. How do I know the acc or val acc for my model as a whole? Do I average all of the epochs accs or val accs to find the acc or val acc of the model as a whole?</p>\n</blockquote>\n<p>Each epoch is a training run over all of your data. During that run the parameters of your model are adjusted according to your loss function. The result is a set of parameters which have a certain ability to generalize to new data. That ability is reflected by the validation accuracy. So think of every epoch as its own model, which can get better or worse if it is trained for another epoch. Whether it got better or worse is judged by the change in validation accuracy (better = validation accuracy increased). Therefore pick the model of the epoch with the highest validation accuracy. Don't average the accuracies over different epochs, that wouldn't make much sense. You can use the Keras callback <code>ModelCheckpoint</code> to automatically save the model with the highest validation accuracy (see <a href=\"http://keras.io/callbacks/\">callbacks documentation</a>).</p>\n<p>The highest accuracy in model 1 is <code>0.7737</code> and the highest one in model 2 is <code>0.7572</code>. Therefore you should view model 1 (at epoch 3) as better. Though it is possible that the <code>0.7737</code> was just a random outlier.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to key on decreasing val_loss or increasing val_acc, ultimately it doesn't matter much.   The differences are well within random/rounding errors.</p>\n<p>In practice, the training loss can drop significantly due to over-fitting, which is why you want to look at validation loss.</p>\n<p>In your case, you can see that your training loss is not dropping - which means you are learning nothing after each epoch.   It look like there's nothing to learn in this model, aside from some trivial linear-like fit or cutoff value.</p>\n<p>Also, when learning nothing, or a trivial linear thing, you should a similar performance on training and validation (trivial learning is always generalizable).  You should probably shuffle your data before using the validation_split feature.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between cross-entropy and log loss error? The formulae for both seem to be very similar.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>They are essentially the same; usually, we use the term <em>log loss</em> for binary classification problems, and the more general <em>cross-entropy (loss)</em> for the general case of multi-class classification, but even this distinction is not consistent, and you'll often find the terms used interchangeably as synonyms.</p>\n<p>From the <a href=\"https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression\" rel=\"noreferrer\">Wikipedia entry for cross-entropy</a>:</p>\n<blockquote>\n<p>The logistic loss is sometimes called cross-entropy loss. It is also known as log loss</p>\n</blockquote>\n<p>From the <a href=\"http://wiki.fast.ai/index.php/Log_Loss#Log_Loss_vs_Cross-Entropy\" rel=\"noreferrer\">fast.ai wiki entry on log loss</a> [link is now dead]:</p>\n<blockquote>\n<p>Log loss and cross-entropy are slightly different depending on the context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.</p>\n</blockquote>\n<p>From the <a href=\"http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\" rel=\"noreferrer\">ML Cheatsheet</a>:</p>\n<blockquote>\n<p>Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.</p>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can the Keras deal with input images with different size? For example, in the fully convolutional neural network, the input images can have any size. However, we need to specify the input shape when we create a network by Keras. Therefore, how can we use Keras to deal with different input size without resizing the input images to the same size? Thanks for any help.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes.\nJust change your input shape to shape=(n_channels, <strong>None</strong>, <strong>None</strong>).\nWhere n_channels is the number of channels in your input image.</p>\n<p><strong>I'm using Theano backend</strong> though, so if you are using tensorflow you might have to change it to (None,None,n_channels)</p>\n<blockquote>\n<p>You should use:</p>\n<p>input_shape=(1, None, None)</p>\n<p>None in a shape denotes a variable dimension. Note that not all layers\n  will work with such variable dimensions, since some layers require\n  shape information (such as Flatten).\n  <a href=\"https://github.com/fchollet/keras/issues/1920\" rel=\"noreferrer\">https://github.com/fchollet/keras/issues/1920</a></p>\n</blockquote>\n<p>For example, using keras's functional API your input layer would be:</p>\n<p>For a RGB dataset</p>\n<pre><code>inp = Input(shape=(3,None,None))\n</code></pre>\n<p>For a Gray dataset</p>\n<pre><code>inp = Input(shape=(1,None,None))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Implementing arbitrarily sized input arrays with the same computational kernels can pose many challenges - e.g. on a GPU, you need to know how big buffers to reserve, and more weakly how much to unroll your loops, etc.  This is the main reason that Keras requires constant input shapes, variable-sized inputs are too painful to deal with.</p>\n<p>This more commonly occurs when processing variable-length sequences like sentences in NLP. The common approach is to <em>establish an upper bound</em> on the size (and crop longer sequences), and then <em>pad</em> the sequences with zeros up to this size.</p>\n<p>(You could also include masking on zero values to skip computations on the padded areas, except that the convolutional layers in Keras might still not support masked inputs...)</p>\n<p>I'm not sure if for 3D data structures, the overhead of padding is not prohibitive - if you start getting memory errors, the easiest workaround is to reduce the batch size.  Let us know about your experience with applying this trick on images!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From my research, I found three conflicting results:</p>\n<ol>\n<li><a href=\"https://stackoverflow.com/questions/33843981/under-what-parameters-are-svc-and-linearsvc-in-scikit-learn-equivalent\"><code>SVC(kernel=\"linear\")</code> is better</a></li>\n<li><a href=\"https://gist.github.com/denzilc/987394\" rel=\"noreferrer\"><code>LinearSVC</code> is better</a></li>\n<li><a href=\"https://stackoverflow.com/questions/34811770/linearsvc-differs-from-svckernel-linear\">Doesn't matter</a></li>\n</ol>\n<p>Can someone explain when to use <code>LinearSVC</code> vs. <code>SVC(kernel=\"linear\")</code>?</p>\n<p>It seems like LinearSVC is marginally better than SVC and is usually more finicky. But if <code>scikit</code> decided to spend time on implementing a specific case for linear classification, why wouldn't <code>LinearSVC</code> outperform <code>SVC</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Mathematically, optimizing an SVM is a convex optimization problem, usually with a unique minimizer. This means that there is only one solution to this mathematical optimization problem.</p>\n<p>The differences in results come from several aspects: <code>SVC</code> and <code>LinearSVC</code> are supposed to optimize the same problem, but in fact all <code>liblinear</code> estimators penalize the intercept, whereas <code>libsvm</code> ones don't (IIRC). This leads to a different mathematical optimization problem and thus different results. There may also be other subtle differences such as scaling and default loss function (edit: make sure you set <code>loss='hinge'</code> in <code>LinearSVC</code>). Next, in multiclass classification, <code>liblinear</code> does one-vs-rest by default whereas <code>libsvm</code> does one-vs-one.</p>\n<p><code>SGDClassifier(loss='hinge')</code> is different from the other two in the sense that it uses stochastic gradient descent and not exact gradient descent and may not converge to the same solution. However the obtained solution may generalize better.</p>\n<p>Between <code>SVC</code> and <code>LinearSVC</code>, one important decision criterion is that <code>LinearSVC</code> tends to be faster to converge the larger the number of samples is. This is due to the fact that the linear kernel is a special case, which is optimized for in Liblinear, but not in Libsvm.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The actual problem is in the <strong>problem with scikit approach</strong>, where they call SVM something <strong>which is not SVM</strong>. LinearSVC is actually minimizing squared hinge loss, instead of just hinge loss, furthermore, it penalizes size of the bias (which <strong>is not SVM</strong>), for more details refer to other question:\n<a href=\"https://stackoverflow.com/questions/33843981/under-what-parameters-are-svc-and-linearsvc-in-scikit-learn-equivalent/33844092#33844092\">Under what parameters are SVC and LinearSVC in scikit-learn equivalent?</a></p>\n<p>So which one to use? It is purely <strong>problem specific</strong>. As due to no free lunch theorem it is impossible to say \"this loss function is best, period\". Sometimes squared loss will work better, sometimes normal hinge.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can't understand why dropout works like this in tensorflow. The blog of <a href=\"http://cs231n.github.io/neural-networks-2/\">CS231n</a> says that, <code>\"dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise.\"</code> Also you can see this from picture(Taken from the same site)\n<a href=\"https://i.sstatic.net/SbXq1.jpg\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/SbXq1.jpg\"/></a></p>\n<p>From tensorflow site, <code>With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.</code></p>\n<p>Now, why the input element is scaled up by <code>1/keep_prob</code>? Why not keep the input element as it is with probability and not scale it with <code>1/keep_prob</code>? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This scaling enables the same network to be used for training (with <code>keep_prob &lt; 1.0</code>) and evaluation (with <code>keep_prob == 1.0</code>). From the <a href=\"http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\" rel=\"noreferrer\">Dropout paper</a>:</p>\n<blockquote>\n<p>The idea is to use a single neural net at test time without dropout. The weights of this network are scaled-down versions of the trained weights. If a unit is retained with probability <em>p</em> during training, the outgoing weights of that unit are multiplied by <em>p</em> at test time as shown in Figure 2.</p>\n</blockquote>\n<p>Rather than adding ops to scale down the weights by <code>keep_prob</code> at test time, the TensorFlow implementation adds an op to scale up the weights by <code>1. / keep_prob</code> at training time. The effect on performance is negligible, and the code is simpler (because we use the same graph and treat <code>keep_prob</code> as a <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#placeholder\" rel=\"noreferrer\"><code>tf.placeholder()</code></a> that is fed a different value depending on whether we are training or evaluating the network).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's say the network had <code>n</code> neurons and we applied dropout rate <code>1/2</code></p>\n<p><strong>Training phase</strong>, we would be left with <code>n/2</code> neurons. So if you were expecting output <code>x</code> with all the neurons, now you will get on <code>x/2</code>. So for every batch, the network weights are trained according to this x/2 </p>\n<p><strong>Testing/Inference/Validation phase</strong>, we dont apply any dropout so the output is x. So, in this case, the output would be with x and not x/2, which would give you the incorrect result. So what you can do is scale it to x/2 during testing. </p>\n<p>Rather than the above scaling specific to Testing phase. What Tensorflow's dropout layer does is that whether it is with dropout or without (Training or testing), it scales the output so that the sum is constant.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am new to the field of neural networks and I would like to know the difference between Deep Belief Networks and Convolutional Networks. \nAlso, is there a Deep Convolutional Network which is the combination of Deep Belief and Convolutional Neural Nets?</p>\n<p>This is what I have gathered till now. Please correct me if I am wrong.</p>\n<p>For an image classification problem, <strong>Deep Belief networks</strong> have many layers, each of which is trained using a greedy layer-wise strategy. \nFor example, if my image size is 50 x 50, and I want a Deep Network with 4 layers namely</p>\n<ol>\n<li>Input Layer</li>\n<li>Hidden Layer 1 (HL1)</li>\n<li>Hidden Layer 2 (HL2)</li>\n<li>Output Layer</li>\n</ol>\n<p>My input layer will have 50 x 50 = 2500 neurons, HL1 = 1000 neurons (say) , HL2 = 100 neurons (say) and output layer = 10 neurons,\n in order to train the weights (W1) between Input Layer and HL1, I use an AutoEncoder (2500 - 1000 - 2500) and learn W1 of size 2500 x 1000 (This is unsupervised learning). Then I feed forward all images through the first hidden layers to obtain a set of features and then use another autoencoder ( 1000 - 100 - 1000) to get the next set of features and finally use a softmax layer (100 - 10) for classification. (only learning the weights of the last layer (HL2 - Output which is the softmax layer) is supervised learning).</p>\n<p>(I could use RBM instead of autoencoder).</p>\n<p>If the same problem was solved using <strong>Convolutional Neural Networks</strong>, then for 50x50 input images, I would develop a network using only 7 x 7 patches (say). My layers would be</p>\n<ol>\n<li>Input Layer (7 x 7 = 49 neurons)</li>\n<li>HL1 (25 neurons for 25 different features) - (convolution layer)</li>\n<li>Pooling Layer</li>\n<li>Output Layer (Softmax)</li>\n</ol>\n<p>And for learning the weights, I take 7 x 7 patches from images of size 50 x 50, and feed forward through convolutional layer, so I will have 25 different feature maps each of size (50 - 7 + 1) x (50 - 7 + 1) = 44 x 44.</p>\n<p>I then use a window of say 11x11 for pooling hand hence get 25 feature maps of size (4 x 4) for as the output of the pooling layer. I use these feature maps for classification.</p>\n<p>While learning the weights, I don't use the layer wise strategy as in Deep Belief Networks (Unsupervised Learning), but instead use supervised learning and learn the weights of all the layers simultaneously. Is this correct or is there any other way to learn the weights?</p>\n<p>Is what I have understood correct? </p>\n<p>So if I want to use DBN's for image classification, I should resize all my images to a particular size (say 200x200) and have that many neurons in the input layer, whereas in case of CNN's, I train only on a smaller patch of the input (say 10 x 10 for an image of size 200x200) and convolve the learnt weights over the entire image?</p>\n<p>Do DBNs provide better results than CNNs or is it purely dependent on the dataset?</p>\n<p>Thank You.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Generally speaking, DBNs are generative neural networks that stack Restricted Boltzmann Machines (RBMs) . You can think of RBMs as being generative autoencoders; if you want a deep belief net you should be stacking RBMs and not plain autoencoders as Hinton and his student Yeh proved that stacking RBMs results in sigmoid belief nets.</p>\n<p>Convolutional neural networks have performed better than DBNs by themselves in current literature on benchmark computer vision datasets such as MNIST. If the dataset is not a computer vision one, then DBNs can most definitely perform better. In theory, DBNs should be the best models but it is very hard to estimate joint probabilities accurately at the moment. You may be interested in Lee et. al's (2009) work on Convolutional Deep Belief Networks which looks to combine the two.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I will try to explain the situation through learning shoes.</p>\n<p>If you use DBN to learn those images here is the bad thing that will happen in your learning algorithm </p>\n<ul>\n<li><p>there will be shoes on different places. </p></li>\n<li><p>all the neurons will try to learn not only shoes but also the place of the shoes in the images because it will not have the concept of 'local image patch' inside weights.</p></li>\n<li><p>DBN makes sense if all your images are aligned by means of size, translation and rotation.</p></li>\n</ul>\n<p>the idea of convolutional networks is that, there is a concept called weight sharing. If I try to extend this 'weight sharing' concept </p>\n<ul>\n<li><p>first you looked at 7x7 patches, and according to your example - as an example of 3 of your neurons in the first layer you can say that they learned shoes 'front', 'back-bottom' and 'back-upper' parts as these would look alike for a 7x7 patch through all shoes.</p>\n<ul>\n<li><p>Normally the idea is to have multiple convolution layers one after another to learn </p>\n<ul>\n<li>lines/edges in the first layer, </li>\n<li>arcs, corners in the second layer,</li>\n<li>higher concepts in higher layers like shoes front, eye in a face, wheel in a car or rectangles cones triangles as primitive but yet combinations of previous layers outputs.</li>\n</ul></li>\n<li><p>You can think of these 3 different things I told you as 3 different neurons. And such areas/neurons in your images will fire when there are shoes in some part of the image. </p></li>\n<li><p>Pooling will protect your higher activations while sub-sampling your images and creating a lower-dimensional space to make things computationally easier and feasible. </p></li>\n<li><p>So at last layer when you look at your 25X4x4, in other words 400 dimensional vector, if there is a shoe somewhere in the picture your 'shoe neuron(s)' will be active whereas non-shoe neurons will be close to zero. </p></li>\n<li><p>And to understand which neurons are for shoes and which ones are not you will put that 400 dimensional vector to another supervised classifier(this can be anything like multi-class-SVM or as you said a soft-max-layer)</p></li>\n</ul></li>\n</ul>\n<p>I can advise you to have a glance at Fukushima 1980 paper to understand what I try to say about translation invariance and line -&gt; arc -&gt; semicircle -&gt; shoe front -&gt; shoe idea (<a href=\"http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf\">http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf</a>). Even just looking at the images in the paper will give you some idea.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using a <a aria-label=\"show questions tagged 'scikit-learn'\" aria-labelledby=\"tag-scikit-learn-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/scikit-learn\" rel=\"tag\" title=\"show questions tagged 'scikit-learn'\">scikit-learn</a> custom pipeline (<code>sklearn.pipeline.Pipeline</code>) in conjunction with <code>RandomizedSearchCV</code> for hyper-parameter optimization. This works great.</p>\n<p>Now I would like to insert a <a aria-label=\"show questions tagged 'keras'\" aria-labelledby=\"tag-keras-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/keras\" rel=\"tag\" title=\"show questions tagged 'keras'\">keras</a> model as a first step into the pipeline. The parameters of the model should be optimized. The computed (fitted) <a aria-label=\"show questions tagged 'keras'\" aria-labelledby=\"tag-keras-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/keras\" rel=\"tag\" title=\"show questions tagged 'keras'\">keras</a> model should then be used later on in the pipeline by other steps, so I think I have to store the model as a global variable so that the other pipeline steps can use it. Is this right?</p>\n<p>I know that <a aria-label=\"show questions tagged 'keras'\" aria-labelledby=\"tag-keras-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/keras\" rel=\"tag\" title=\"show questions tagged 'keras'\">keras</a> offers some <em>wrappers</em> for the <a aria-label=\"show questions tagged 'scikit-learn'\" aria-labelledby=\"tag-scikit-learn-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/scikit-learn\" rel=\"tag\" title=\"show questions tagged 'scikit-learn'\">scikit-learn</a>  API, but the problem is that these <em>wrappers</em> already do classification/regression, but I only want to compute the <a aria-label=\"show questions tagged 'keras'\" aria-labelledby=\"tag-keras-tooltip-container\" class=\"post-tag\" href=\"/questions/tagged/keras\" rel=\"tag\" title=\"show questions tagged 'keras'\">keras</a> model and nothing else.</p>\n<p>How can this be done?</p>\n<p>For example, I have a method which returns the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def create_model(file_path, argument2,...):\n  ...\n  return model\n</code></pre>\n<p>The method needs some fixed parameters like a <code>file_path</code> etc. but <code>X</code> and <code>y</code> are not needed (or can be ignored). The parameters of the model should be optimized (number of layers etc.).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to <strong>wrap your Keras model as a Scikit learn model</strong> first and then proceed as usual.</p>\n<p>Here's a quick example (I've omitted the imports for brevity)</p>\n<blockquote>\n<p>Here is a full blog post with this one and many other examples: <a href=\"http://queirozf.com/entries/scikit-learn-pipeline-examples\" rel=\"nofollow noreferrer\">Scikit-learn Pipeline Examples</a></p>\n</blockquote>\n<pre class=\"lang-py prettyprint-override\"><code># create a function that returns a model, taking as parameters things you\n# want to verify using cross-valdiation and model selection\ndef create_model(optimizer='adagrad',\n                  kernel_initializer='glorot_uniform', \n                  dropout=0.2):\n    model = Sequential()\n    model.add(Dense(64,activation='relu',kernel_initializer=kernel_initializer))\n    model.add(Dropout(dropout))\n    model.add(Dense(1,activation='sigmoid',kernel_initializer=kernel_initializer))\n\n    model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])\n\n    return model\n\n# wrap the model using the function you created\nclf = KerasRegressor(build_fn=create_model,verbose=0)\n\n# just create the pipeline\npipeline = Pipeline([\n    ('clf',clf)\n])\n\npipeline.fit(X_train, y_train)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can't find how Keras defines \"accuracy\" and \"loss\". I know I can specify different metrics (e.g. mse, cross entropy) but Keras prints out a standard \"accuracy\". How is that defined? Likewise for loss; I know I can specify different types of regularization; are those in the loss?</p>\n<p>Ideally, I'd like to print out the equation used to define it; if not, I'll settle for an answer here.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Have a look at <a href=\"https://github.com/fchollet/keras/blob/c2e36f369b411ad1d0a40ac096fe35f73b9dffd3/keras/metrics.py\" rel=\"noreferrer\"><code>metrics.py</code></a>, there you can find definition of all available metrics including different types of accuracy. Accuracy is not printed unless you add it to the list of desired metrics when you compile your model.</p>\n<p>Regularizers are by definition added to the loss. For example, see <a href=\"https://github.com/fchollet/keras/blob/f573a86b42e49754386e536358e08e861d40d24c/keras/engine/topology.py#L417\" rel=\"noreferrer\"><code>add_loss</code></a> method of the <code>Layer</code>class.</p>\n<p><strong>Update</strong></p>\n<p>The type of <code>accuracy</code> is determined based on the objective function, see <a href=\"https://github.com/fchollet/keras/blob/d8b226f26b35348d934edb1213061993e7e5a1fa/keras/engine/training.py#L651\" rel=\"noreferrer\"><code>training.py</code></a>. The default choice is <a href=\"https://github.com/fchollet/keras/blob/c2e36f369b411ad1d0a40ac096fe35f73b9dffd3/keras/metrics.py#L13\" rel=\"noreferrer\"><code>categorical_accuracy</code></a>. Other types like <code>binary_accuracy</code> and <code>sparse_categorical_accuracy</code> are selected when the objective function is either binary or sparse. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since <a href=\"https://stackoverflow.com/a/41534323/19123103\">Sergii's answer</a>, Keras library has been cleaned up quite a bit and the source code is pretty readable nowadays. The metrics are defined in <code>tensorflow.keras.metrics</code> (whose documentation can be found <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/metrics\" rel=\"nofollow noreferrer\">here</a>) and the losses are defined in <code>tensorflow.keras.losses</code> (<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses\" rel=\"nofollow noreferrer\">docs</a>). There's a bit of overlap with the metrics module but that's expected given a particular loss function can also be tracked as a metric.</p>\n<p>Also, if we inspect the <a href=\"https://github.com/keras-team/keras/blob/428ed9f03a0a0b2edc22d4ce29001857f617227c/keras/engine/compile_utils.py#L665\" rel=\"nofollow noreferrer\">source code</a>, unless the metric is accuracy, <code>get()</code> method is called on the <code>metrics</code> module to get the particular metric function, i.e. <code>tf.keras.metrics.get('binary_accuracy')</code>. On the other hand, the <code>get()</code> method is always called to <a href=\"https://github.com/keras-team/keras/blob/428ed9f03a0a0b2edc22d4ce29001857f617227c/keras/engine/compile_utils.py#L365\" rel=\"nofollow noreferrer\">fetch</a> the particular loss function.</p>\n<p>Also, the type of accuracy <a href=\"https://github.com/keras-team/keras/blob/428ed9f03a0a0b2edc22d4ce29001857f617227c/keras/engine/compile_utils.py#L677-L683\" rel=\"nofollow noreferrer\">is chosen</a> depending on the the target type (<code>binary_accuracy</code>, <code>categorical_accuracy</code> etc.).</p>\n<p>All metrics/losses can be printed calling <code>dir()</code> on the modules.</p>\n<pre class=\"lang-py prettyprint-override\"><code>metrics_list = [m for m in dir(tf.keras.metrics) if not m.startswith('_')]\n\nlosses_list = [m for m in dir(tf.keras.losses) if not m.startswith('_')]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/51797280/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-08-11 18:45:27Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/51797280/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I've recently started coding with Machine learning techniques and had been going back and forth between Machine learning implemented in different platforms. The frameworks i worked a lot with were <strong>Tensorflow</strong> (Python), <strong>Tensorflow.js</strong> and <strong>Brain.js</strong>. And i've got couple of doubts about them.</p>\n<ol>\n<li>Why do most of them prefer Tensorflow (Python) over Tensorflow.js. What does Tensorflow has that Tensorflow.js doesn't which makes it special?</li>\n<li>Most people i've seen in the internet prefer working with Tensorflow.js than brain.js, even though brain.js uses JSON objects which doesnt put the developer in a hassle to create Tensors and make memory management and stuff. Why do people prefer working with Tensorflow.js even though brain.js is easy to implement?</li>\n<li>If i'm making a web site which uses Node.js as a backend, which would be the preferable library to be implemented for Machine Learning in a long run? Tensorflow.js or Brain.js? or should i use Tensorflow separately for just Machine learning things?</li>\n</ol>\n<p>I've been searching a lot on these topics. And i haven't got a nice explanation for my doubts yet. So expecting a clear and detail exaplanation :)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The speeds are different: Tensorflow &gt; tfjs &gt; brainjs. Python can be directly compiled to machine code and directly use the CPU and GPU, whereas tfjs is a script-language which is being compiled on the client and has to use the <code>&lt;canvas&gt;</code> in the browser to access the GPU the same as brain.js (I am not sure if brain.js is GPU-accelerated)</p>\n<p>Another thing is that tensorflow is a whole ecosystem, which is kept in sync with each different version for the different platforms, so it is really easy to port your python(keras) model to tfjs and if you know how to code a tensorflow-model you can do it in any language.</p>\n<p>And if you're using nodejs the only reason to stay with tfjs and not switch to python is that you like the JavaScript language better or you are forced to use because you are working in a JS backend.</p>\n<p>PS:\nA new library was just released (<a href=\"https://ml5js.org/\" rel=\"noreferrer\">ML5</a>), which is a wrapper for tfjs and adds a lot of stuff, which helps you to build and use models without having a deep machine learning background.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How can you write a python script to read Tensorboard log files, extracting the loss and accuracy and other numerical data, without launching the GUI <code>tensorboard --logdir=...</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use TensorBoard's Python classes or script to extract the data:</p>\n<p><a href=\"https://github.com/tensorflow/tensorboard#how-can-i-export-data-from-tensorboard\" rel=\"noreferrer\">How can I export data from TensorBoard?</a></p>\n<blockquote>\n<p>If you'd like to export data to visualize elsewhere (e.g. iPython Notebook), that's possible too. You can directly depend on the underlying classes that TensorBoard uses for loading data: <code>python/summary/event_accumulator.py</code> (for loading data from a single run) or <code>python/summary/event_multiplexer.py</code> (for loading data from multiple runs, and keeping it organized). These classes load groups of event files, discard data that was \"orphaned\" by TensorFlow crashes, and organize the data by tag.</p>\n<p>As another option, there is a script (<code>tensorboard/scripts/serialize_tensorboard.py</code>) which will load a logdir just like TensorBoard does, but write all of the data out to disk as json instead of starting a server. This script is setup to make \"fake TensorBoard backends\" for testing, so it is a bit rough around the edges.</p>\n</blockquote>\n<p>Using <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/event_accumulator.py#L107\" rel=\"noreferrer\"><code>EventAccumulator</code></a>:</p>\n<pre><code># In [1]: from tensorflow.python.summary import event_accumulator  # deprecated\nIn [1]: from tensorboard.backend.event_processing import event_accumulator\n\nIn [2]: ea = event_accumulator.EventAccumulator('events.out.tfevents.x.ip-x-x-x-x',\n   ...:  size_guidance={ # see below regarding this argument\n   ...:      event_accumulator.COMPRESSED_HISTOGRAMS: 500,\n   ...:      event_accumulator.IMAGES: 4,\n   ...:      event_accumulator.AUDIO: 4,\n   ...:      event_accumulator.SCALARS: 0,\n   ...:      event_accumulator.HISTOGRAMS: 1,\n   ...:  })\n\nIn [3]: ea.Reload() # loads events from file\nOut[3]: &lt;tensorflow.python.summary.event_accumulator.EventAccumulator at 0x7fdbe5ff59e8&gt;\n\nIn [4]: ea.Tags()\nOut[4]: \n{'audio': [],\n 'compressedHistograms': [],\n 'graph': True,\n 'histograms': [],\n 'images': [],\n 'run_metadata': [],\n 'scalars': ['Loss', 'Epsilon', 'Learning_rate']}\n\nIn [5]: ea.Scalars('Loss')\nOut[5]: \n[ScalarEvent(wall_time=1481232633.080754, step=1, value=1.6365480422973633),\n ScalarEvent(wall_time=1481232633.2001867, step=2, value=1.2162202596664429),\n ScalarEvent(wall_time=1481232633.3877788, step=3, value=1.4660096168518066),\n ScalarEvent(wall_time=1481232633.5749283, step=4, value=1.2405034303665161),\n ScalarEvent(wall_time=1481232633.7419815, step=5, value=0.897326648235321),\n ...]\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/event_accumulator.py#L151\" rel=\"noreferrer\"><code>size_guidance</code></a>:</p>\n<pre><code>size_guidance: Information on how much data the EventAccumulator should\n  store in memory. The DEFAULT_SIZE_GUIDANCE tries not to store too much\n  so as to avoid OOMing the client. The size_guidance should be a map\n  from a `tagType` string to an integer representing the number of\n  items to keep per tag for items of that `tagType`. If the size is 0,\n  all events are stored.\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To finish user1501961's answer, you can then just export the list of scalars to a csv file easily with pandas <code>pd.DataFrame(ea.Scalars('Loss)).to_csv('Loss.csv')</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For anyone interested, I've adapted <a href=\"https://stackoverflow.com/users/1501961/user1501961\">user1501961</a>'s answer into a function for parsing tensorboard scalars into a dictionary of pandas dataframes:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from tensorboard.backend.event_processing import event_accumulator\nimport pandas as pd\n\n\ndef parse_tensorboard(path, scalars):\n    \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n    ea = event_accumulator.EventAccumulator(\n        path,\n        size_guidance={event_accumulator.SCALARS: 0},\n    )\n    _absorb_print = ea.Reload()\n    # make sure the scalars are in the event accumulator tags\n    assert all(\n        s in ea.Tags()[\"scalars\"] for s in scalars\n    ), \"some scalars were not found in the event accumulator\"\n    return {k: pd.DataFrame(ea.Scalars(k)) for k in scalars}\n\n\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've fit a <code>Pipeline</code> object with <code>RandomizedSearchCV</code></p>\n<pre><code>pipe_sgd = Pipeline([('scl', StandardScaler()),\n                    ('clf', SGDClassifier(n_jobs=-1))])\n\nparam_dist_sgd = {'clf__loss': ['log'],\n                 'clf__penalty': [None, 'l1', 'l2', 'elasticnet'],\n                 'clf__alpha': np.linspace(0.15, 0.35),\n                 'clf__n_iter': [3, 5, 7]}\n\nsgd_randomized_pipe = RandomizedSearchCV(estimator = pipe_sgd, \n                                         param_distributions=param_dist_sgd, \n                                         cv=3, n_iter=30, n_jobs=-1)\n\nsgd_randomized_pipe.fit(X_train, y_train)\n</code></pre>\n<p>I want to access the <code>coef_</code> attribute of the <code>best_estimator_</code> but I'm unable to do that. I've tried accessing <code>coef_</code> with the code below.</p>\n<p><code>sgd_randomized_pipe.best_estimator_.coef_</code></p>\n<p>However I get the following AttributeError... </p>\n<p>AttributeError: 'Pipeline' object has no attribute 'coef_'</p>\n<p>The scikit-learn docs say that <code>coef_</code> is an attribute of <code>SGDClassifier</code>, which is the class of my <code>base_estimator_</code>. </p>\n<p>What am I doing wrong?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can always use the names you assigned to them while making the pipeline by using the <code>named_steps</code> dict.</p>\n<pre><code>scaler = sgd_randomized_pipe.best_estimator_.named_steps['scl']\nclassifier = sgd_randomized_pipe.best_estimator_.named_steps['clf']\n</code></pre>\n<p>and then access all the attributes like <code>coef_</code>, <code>intercept_</code> etc. which are available to corresponding fitted estimator.</p>\n<p>This is the formal attribute exposed by the Pipeline as <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\" rel=\"noreferrer\">specified in the documentation</a>:</p>\n<blockquote>\n<p><strong>named_steps</strong> : dict</p>\n<p>Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think this should work:</p>\n<pre><code>sgd_randomized_pipe.named_steps['clf'].coef_\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've found one way to do this is by chained indexing with the <code>steps</code> attribute...</p>\n<p><code>sgd_randomized_pipe.best_estimator_.steps[1][1].coef_</code></p>\n<p>Is this best practice, or is there another way?</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have been working with the <code>CountVectorizer</code> class in scikit-learn.</p>\n<p>I understand that if used in the manner shown below, the final output will consist of an array containing counts of features, or tokens.</p>\n<p>These tokens are extracted from a set of keywords, i.e.</p>\n<pre><code>tags = [\n  \"python, tools\",\n  \"linux, tools, ubuntu\",\n  \"distributed systems, linux, networking, tools\",\n]\n</code></pre>\n<p>The next step is:</p>\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(tokenizer=tokenize)\ndata = vec.fit_transform(tags).toarray()\nprint data\n</code></pre>\n<p>Where we get</p>\n<pre><code>[[0 0 0 1 1 0]\n [0 1 0 0 1 1]\n [1 1 1 0 1 0]]\n</code></pre>\n<p>This is fine, but my situation is just a little bit different.  </p>\n<p>I want to extract the features the same way as above, but I don't want the rows in <code>data</code> to be the same documents that the features were extracted from.</p>\n<p>In other words, how can I get counts of another set of documents, say, </p>\n<pre><code>list_of_new_documents = [\n  [\"python, chicken\"],\n  [\"linux, cow, ubuntu\"],\n  [\"machine learning, bird, fish, pig\"]\n]\n</code></pre>\n<p>And get:</p>\n<pre><code>[[0 0 0 1 0 0]\n [0 1 0 0 0 1]\n [0 0 0 0 0 0]]\n</code></pre>\n<p>I have read the documentation for the <code>CountVectorizer</code> class, and came across the <code>vocabulary</code> argument, which is a mapping of terms to feature indices.  I can't seem to get this argument to help me, however.</p>\n<p>Any advice is appreciated.<br/>\nPS:  all credit due to <a href=\"http://blog.mafr.de/2012/04/15/scikit-learn-feature-extractio/\" rel=\"noreferrer\">Matthias Friedrich's Blog</a> for the example I used above.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You're right that <code>vocabulary</code> is what you want.  It works like this:</p>\n<pre><code>&gt;&gt;&gt; cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=['hot', 'cold', 'old'])\n&gt;&gt;&gt; cv.fit_transform(['pease porridge hot', 'pease porridge cold', 'pease porridge in the pot', 'nine days old']).toarray()\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0],\n       [0, 0, 1]], dtype=int64)\n</code></pre>\n<p>So you pass it a dict with your desired features as the keys.</p>\n<p>If you used <code>CountVectorizer</code> on one set of documents and then you want to use the set of features from those documents for a new set, use the <code>vocabulary_</code> attribute of your original CountVectorizer and pass it to the new one.  So in your example, you could do</p>\n<pre><code>newVec = CountVectorizer(vocabulary=vec.vocabulary_)\n</code></pre>\n<p>to create a new tokenizer using the vocabulary from your first one.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should call <code>fit_transform</code> or just <code>fit</code> on your original vocabulary source so that the vectorizer learns a vocab.</p>\n<p>Then you can use this <code>fit</code> vectorizer on any new data source via the <code>transform()</code> method.</p>\n<p>You can obtain the vocabulary produced by the fit (i.e. mapping of word to token ID) via <code>vectorizer.vocabulary_</code> (assuming you name your <code>CountVectorizer</code> the name <code>vectorizer</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>&gt;&gt;&gt; tags = [\n  \"python, tools\",\n  \"linux, tools, ubuntu\",\n  \"distributed systems, linux, networking, tools\",\n]\n\n&gt;&gt;&gt; list_of_new_documents = [\n  [\"python, chicken\"],\n  [\"linux, cow, ubuntu\"],\n  [\"machine learning, bird, fish, pig\"]\n\n]\n\n&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n&gt;&gt;&gt; vect = CountVectorizer()\n&gt;&gt;&gt; tags = vect.fit_transform(tags)\n\n# vocabulary learned by CountVectorizer (vect)\n&gt;&gt;&gt; print(vect.vocabulary_)\n{'python': 3, 'tools': 5, 'linux': 1, 'ubuntu': 6, 'distributed': 0, 'systems': 4, 'networking': 2}\n\n# counts for tags\n&gt;&gt;&gt; tags.toarray()\narray([[0, 0, 0, 1, 0, 1, 0],\n       [0, 1, 0, 0, 0, 1, 1],\n       [1, 1, 1, 0, 1, 1, 0]], dtype=int64)\n\n# to use `transform`, `list_of_new_documents` should be a list of strings \n# `itertools.chain` flattens shallow lists more efficiently than list comprehensions\n\n&gt;&gt;&gt; from itertools import chain\n&gt;&gt;&gt; new_docs = list(chain.from_iterable(list_of_new_documents)\n&gt;&gt;&gt; new_docs = vect.transform(new_docs)\n\n# finally, counts for new_docs!\n&gt;&gt;&gt; new_docs.toarray()\narray([[0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0]])\n</code></pre>\n<p>To verify that <code>CountVectorizer</code> is using the vocabulary learned from <code>tags</code> on <code>new_docs</code>: print <code>vect.vocabulary_</code> again or compare the output of <code>new_docs.toarray()</code> to that of <code>tags.toarray()</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am doing experiments on bert architecture and found out that most of the fine-tuning task takes the final hidden layer as text representation and later they pass it to other models for the further downstream task.</p>\n<p>Bert's last layer looks like this :</p>\n<p><a href=\"https://i.sstatic.net/m0jrg.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/m0jrg.png\"/></a></p>\n<p>Where we take the [CLS] token of each sentence :</p>\n<p><a href=\"https://i.sstatic.net/1OklZ.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/1OklZ.png\"/></a></p>\n<p><a href=\"https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\" rel=\"noreferrer\">Image source</a></p>\n<p>I went through many discussion on this <a href=\"https://github.com/huggingface/transformers/issues/1950\" rel=\"noreferrer\">huggingface issue</a>,  <a href=\"https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-its-encoding-output-is-important\">datascience forum question</a>,  <a href=\"https://github.com/google-research/bert/issues/196\" rel=\"noreferrer\">github issue</a> Most of the data scientist gives this explanation :</p>\n<blockquote>\n<p>BERT is bidirectional, the [CLS] is encoded including all\nrepresentative information of all tokens through the multi-layer\nencoding procedure. The representation of [CLS] is individual in\ndifferent sentences.</p>\n</blockquote>\n<p>My question is, Why the author ignored the other information ( each token's vector ) and taking the average, max_pool or other methods to make use of all information rather than using [CLS] token for classification?</p>\n<p>How does this [CLS] token help compare to the average of all token vectors?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>BERT is designed primarily for transfer learning, i.e., finetuning on task-specific datasets. If you average the states, every state is averaged with the same weight: including stop words or other stuff that are not relevant for the task. The <code>[CLS]</code> vector gets computed using self-attention (like everything in BERT), so it can only collect the relevant information from the rest of the hidden states. So, in some sense the <code>[CLS]</code> vector is also an average over token vectors, only more cleverly computed, specifically for the tasks that you fine-tune on.</p>\n<p>Also, my experience is that when I keep the weights fixed and <em>do not</em> fine-tune BERT, using the token average yields better results.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The use of the <code>[CLS]</code> token to represent the entire sentence comes from the <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">original BERT paper</a>, section 3:</p>\n<blockquote>\n<p>The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.</p>\n</blockquote>\n<p>Your intuition is correct that averaging the vectors of all the tokens may produce superior results. In fact, that is exactly what is mentioned in the Huggingface documentation for <a href=\"https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertmodel\" rel=\"noreferrer\">BertModel</a>:</p>\n<blockquote>\n<p><strong>Returns</strong></p>\n<p>pooler_output (<code>torch.FloatTensor</code>: of shape <code>(batch_size, hidden_size)</code>):</p>\n<p>Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pre-training.</p>\n<p>This output is usually not a good summary of the semantic content of the input, <em>you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence</em>.</p>\n</blockquote>\n<p><strong>Update</strong>: Huggingface removed that statement (\"This output is usually not a good summary of the semantic content ...\") in v3.1.0. You'll have to ask them why.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to classify some data with (I hope) nearest-neighbour algorithm. I've googled this problem and found a lot of libraries (including PyML, mlPy and Orange), but I'm unsure of where to start here. </p>\n<p>How should I go about implementing k-NN using Python?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Particularly given the technique (k-Nearest Neighbors) that you mentioned in your Q, i would strongly recommend <em><strong><a href=\"http://scikit-learn.sourceforge.net/stable/\" rel=\"nofollow noreferrer\">scikit.learn</a></strong></em>. [<em>Note</em>: after this Answer was posted, the lead developer of this Project informed me of a <a href=\"http://scikit-learn.org\" rel=\"nofollow noreferrer\">new homepage</a> for this Project.]</p>\n<p>A few features that i believe distinguish this library from the others (at least the other Python ML libraries that i have used, which is most of them):</p>\n<ul>\n<li><p>an extensive <em>diagnostics &amp; testing library</em> (including plotting\nmodules, via Matplotlib)--includes feature-selection algorithms,\n<a href=\"http://scikit-learn.sourceforge.net/stable/auto_examples/plot_confusion_matrix.html\" rel=\"nofollow noreferrer\">confusion matrix</a>, ROC, precision-recall, etc.;</p>\n</li>\n<li><p>a nice selection of 'batteries-included' <em>data sets</em> (including\nhandwriting digits, facial images, etc.) particularly suited for ML techniques;</p>\n</li>\n<li><p>extensive <em>documentation</em> (a nice surprise given that this Project is\nonly about two years old) including tutorials and step-by-step\nexample code (which use the supplied data sets);</p>\n</li>\n</ul>\n<p>Without exception (at least that i can think of at this moment) the python ML libraries are superb. (See the <a href=\"http://www.pymvpa.org\" rel=\"nofollow noreferrer\">PyMVPA homepag</a>e for a list of the dozen or so most popular python ML libraries.)</p>\n<p>In the past 12 months for instance, i have used <em>ffnet</em> (for MLP), <em>neurolab</em> (also for MLP), <em>PyBrain</em> (Q-Learning), <em>neurolab</em> (MLP), and <em>PyMVPA</em> (SVM) (all available from the <a href=\"http://pypi.python.org/\" rel=\"nofollow noreferrer\">Python Package Index</a>)--these vary significantly from each other w/r/t maturity, scope, and supplied infrastructure, but i found them all to be of very high quality.</p>\n<p>Still, the best of these might be <em><strong>scikit.learn</strong></em>; for instance, i am not aware of any python ML library--other than scikit.learn--that includes any of the three features i mentioned above (though a few have solid example code and/or tutorials, none that i know of integrate these with a library of research-grade data sets and diagnostic algorithms).</p>\n<p>Second, given you the technique you intend to use (<em>k-nearest neighbor</em>) scikit.learn is a particularly good choice. Scikit.learn includes kNN algorithms for both <a href=\"http://scikit-learn.sourceforge.net/stable/auto_examples/plot_neighbors_regression.html\" rel=\"nofollow noreferrer\">regression</a> (returns a score) and <a href=\"http://scikit-learn.sourceforge.net/stable/auto_examples/plot_neighbors.html\" rel=\"nofollow noreferrer\">classification</a> (returns a class label), as well as detailed sample code for each.</p>\n<p>Using the scikit.learn k-nearest neighbor module (literally) couldn't be any easier:</p>\n<pre><code>&gt;&gt;&gt; # import NumPy and the relevant scikit.learn module\n&gt;&gt;&gt; import numpy as NP\n&gt;&gt;&gt; from sklearn import neighbors as kNN\n\n&gt;&gt;&gt; # load one of the sklearn-suppplied data sets\n&gt;&gt;&gt; from sklearn import datasets\n&gt;&gt;&gt; iris = datasets.load_iris()\n&gt;&gt;&gt; # the call to load_iris() loaded both the data and the class labels, so\n&gt;&gt;&gt; # bind each to its own variable\n&gt;&gt;&gt; data = iris.data\n&gt;&gt;&gt; class_labels = iris.target\n\n&gt;&gt;&gt; # construct a classifier-builder by instantiating the kNN module's primary class\n&gt;&gt;&gt; kNN1 = kNN.NeighborsClassifier()\n\n&gt;&gt;&gt; # now construct ('train') the classifier by passing the data and class labels\n&gt;&gt;&gt; # to the classifier-builder\n&gt;&gt;&gt; kNN1.fit(data, class_labels)\n      NeighborsClassifier(n_neighbors=5, leaf_size=20, algorithm='auto')\n</code></pre>\n<p>What's more, unlike nearly all other ML techniques, the crux of k-nearest neighbors is not coding a working classifier builder, rather the difficult step in building a production-grade k-nearest neighbor classifier/regressor is the persistence layer--i.e., <em>storage and fast retrieval of the data points from which the nearest neighbors are selected</em>. For the kNN data storage layer, scikit.learn includes an algorithm for a <em><strong>ball tree</strong></em> (which i know almost nothing about other than is apparently superior to the <em>kd-tree</em> (the traditional data structure for k-NN) because its performance doesn't degrade in higher dimensional features space.</p>\n<p>Additionally, k-nearest neighbors requires an appropriate similarity metric (Euclidean distance is the usual choice, though not always the best one). Scikit.learn includes a stand-along module comprised of various distance metrics as well as testing algorithms for selection of the appropriate one.</p>\n<p>Finally, there are a few libraries that i have not mentioned either because they are out of scope (PyML, Bayesian); they are not primarily 'libraries' for developers but rather applications for end users (e.g., Orange), or they have unusual or difficult-to-install dependencies (e.g., mlpy, which requires the gsl, which in turn must be built from source) at least for my OS, which is Mac OS X.</p>\n<p>(<em>Note</em>: i am not a developer/committer for scikit.learn.)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-05-01 18:37:12Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/11055502/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>It seems like R is really designed to handle datasets that it can pull entirely into memory. What R packages are recommended for signal processing and machine learning on very large datasets that can not be pulled into memory? </p>\n<p>If R is simply the wrong way to do this, I am open to other robust free suggestions (e.g. scipy if there is some nice way to handle very large datasets)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Have a look at the \"Large memory and out-of-memory data\" subsection of the <a href=\"http://cran.r-project.org/web/views/HighPerformanceComputing.html\">high performance computing task view</a> on CRAN. <a href=\"http://cran.r-project.org/web/packages/bigmemory/index.html\">bigmemory</a> and <a href=\"http://cran.r-project.org/web/packages/ff/index.html\">ff</a> are two popular packages. For bigmemory (and the related <a href=\"http://cran.r-project.org/web/packages/biganalytics/index.html\">biganalytics</a>, and <a href=\"http://cran.r-project.org/web/packages/bigtabulate/index.html\">bigtabulate</a>), the <a href=\"http://www.bigmemory.org/\">bigmemory website</a> has a few very good presentations, vignettes, and overviews from Jay Emerson. For ff, I recommend reading Adler Oehlschlägel and colleagues' excellent slide presentations on the <a href=\"http://ff.r-forge.r-project.org/\">ff website</a>. </p>\n<p>Also, consider storing data in a database and reading in smaller batches for analysis. There are likely any number of approaches to consider. To get started, consdier looking through some of the examples in the <a href=\"http://cran.r-project.org/web/packages/biglm/index.html\">biglm</a> package, as well as <a href=\"http://faculty.washington.edu/tlumley/tutorials/user-biglm.pdf\">this presentation</a> from Thomas Lumley.</p>\n<p>And do investigate the other packages on the high-performance computing task view and mentioned in the other answers. The packages I mention above are simply the ones I've happened to have more experience with.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think the amount of data you can process is more limited by ones programming skills than anything else. Although a lot of standard functionality is focused on in memory analysis, cutting your data into chunks already helps a lot. Ofcourse, this takes more time to program than picking up standard R code, but often times it is quite possible. </p>\n<p>Cutting up data can for exale be done using read.table or readBin which support only reading a subset of the data. Alternatively, you can take a look at the high performance computing task view for packages which deliver out of the box out of memory functionality. You could also put your data in a database. For spatial raster data, the excellent raster package provides out of memory analysis.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For machine learning tasks I can recommend using <a href=\"http://cran.r-project.org/web/packages/biglm/index.html\" rel=\"noreferrer\">biglm</a> package, used to do \"Regression for data too large to fit in memory\". For using R with really big data, one can use <a href=\"http://hadoop.apache.org/\" rel=\"noreferrer\">Hadoop</a> as a backend and then use package <a href=\"https://github.com/RevolutionAnalytics/RHadoop/wiki/rmr\" rel=\"noreferrer\">rmr</a> to perform statistical (or other) analysis via MapReduce on a Hadoop cluster.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to calculate NN model certainty/confidence (see <a href=\"http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\" rel=\"noreferrer\">What my deep model doesn't know</a>) - when NN tells me an image represents \"8\", I would like to know how certain it is. Is my model 99% certain it is \"8\" or is it 51% it is \"8\", but it could also be \"6\"? Some digits are quite ambiguous and I would like to know for which images the model is just \"flipping a coin\".</p>\n<p>I have found some theoretical writings about this but I have trouble putting this in code. If I understand correctly, I should evaluate a testing image multiple times while \"killing off\" different neurons (using dropout) and then...?</p>\n<p>Working on MNIST dataset, I am running the following model:</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, Flatten, Dropout\n\nmodel = Sequential()\nmodel.add(Conv2D(128, kernel_size=(7, 7),\n                 activation='relu',\n                 input_shape=(28, 28, 1,)))\nmodel.add(Dropout(0.20))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Dropout(0.20))\nmodel.add(Flatten())\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=10, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\nmodel.fit(train_data, train_labels,  batch_size=100, epochs=30, validation_data=(test_data, test_labels,))\n</code></pre>\n<p><em>How should I predict with this model so that I get its certainty about predictions too?</em> I would appreciate some practical examples (preferably in Keras, but any will do).</p>\n<p>To clarify, I am looking for an example of how to get certainty using <a href=\"http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\" rel=\"noreferrer\">the method outlined by Yurin Gal</a> (or an explanation of why some other method yields better results). </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you want to implement <em>dropout</em> approach to measure uncertainty you should do the following:</p>\n<ol>\n<li><p>Implement function which applies <em>dropout</em> also during the test time:</p>\n<pre><code>import keras.backend as K\nf = K.function([model.layers[0].input, K.learning_phase()],\n               [model.layers[-1].output])\n</code></pre></li>\n<li><p>Use this function as uncertainty predictor e.g. in a following manner:</p>\n<pre><code>def predict_with_uncertainty(f, x, n_iter=10):\n    result = numpy.zeros((n_iter,) + x.shape)\n\n    for iter in range(n_iter):\n        result[iter] = f(x, 1)\n\n    prediction = result.mean(axis=0)\n    uncertainty = result.var(axis=0)\n    return prediction, uncertainty\n</code></pre></li>\n</ol>\n<p>Of course you may use any different function to compute uncertainty. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Made a few changes to the top voted answer. Now it works for me. </p>\n<p>It's a way to estimate model uncertainty. For other source of uncertainty, I found <a href=\"https://eng.uber.com/neural-networks-uncertainty-estimation/\" rel=\"noreferrer\">https://eng.uber.com/neural-networks-uncertainty-estimation/</a> helpful.</p>\n<pre><code>f = K.function([model.layers[0].input, K.learning_phase()],\n               [model.layers[-1].output])\n\n\ndef predict_with_uncertainty(f, x, n_iter=10):\n    result = []\n\n    for i in range(n_iter):\n        result.append(f([x, 1]))\n\n    result = np.array(result)\n\n    prediction = result.mean(axis=0)\n    uncertainty = result.var(axis=0)\n    return prediction, uncertainty\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your model uses a softmax activation, so the simplest way to obtain some kind of uncertainty measure is to look at the output softmax probabilities:</p>\n<pre><code>probs = model.predict(some input data)[0]\n</code></pre>\n<p>The <code>probs</code> array will then be a 10-element vector of numbers in the [0, 1] range that sum to 1.0, so they can be interpreted as probabilities. For example the probability for digit 7 is just <code>probs[7]</code>.</p>\n<p>Then with this information you can do some post-processing, typically the predicted class is the one with highest probability, but you can also look at the class with second highest probability, etc.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since an LSTM RNN uses previous events to predict current sequences, why do we shuffle the training data? Don't we lose the temporal ordering of the training data? How is it still effective at making predictions after being trained on shuffled training data?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In general, when you shuffle the training data (a set of sequences), you shuffle the order in which sequences are fed to the RNN, you don't shuffle the ordering within individual sequences. This is fine to do when your network is stateless:</p>\n<p><strong>Stateless Case:</strong></p>\n<p>The network's memory only persists for the duration of a sequence. Training on sequence B before sequence A doesn't matter because the network's memory state does not persist across sequences.</p>\n<p>On the other hand:</p>\n<p><strong>Stateful Case:</strong></p>\n<p>The network's memory persists across sequences. Here, you cannot blindly shuffle your data and expect optimal results. Sequence A should be fed to the network before sequence B because A comes before B, and we want the network to evaluate sequence B with memory of what was in sequence A.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to build a LSTM autoencoder with the goal of getting a fixed sized vector from a sequence, which represents the sequence as good as possible. This autoencoder consists of two parts:</p>\n<ul>\n<li><code>LSTM</code> Encoder: Takes a sequence and returns an output vector (<code>return_sequences = False</code>)</li>\n<li><code>LSTM</code> Decoder: Takes an output vector and returns a sequence (<code>return_sequences = True</code>)</li>\n</ul>\n<p>So, in the end, the encoder is a <strong>many to one</strong> LSTM and the decoder is a <strong>one to many</strong> LSTM.</p>\n<p><a href=\"https://i.sstatic.net/kwhAP.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/kwhAP.jpg\"/></a>\nImage source: <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"noreferrer\">Andrej Karpathy</a></p>\n<p>On a high level the coding looks like this (similar as described <a href=\"https://github.com/fchollet/keras/issues/5138\" rel=\"noreferrer\">here</a>):</p>\n<pre><code>encoder = Model(...)\ndecoder = Model(...)\n\nautoencoder = Model(encoder.inputs, decoder(encoder(encoder.inputs)))\n\nautoencoder.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nautoencoder.fit(data, data,\n          batch_size=100,\n          epochs=1500)\n</code></pre>\n<p>The shape (number of training examples, sequence length, input dimension) of the <code>data</code> array is <code>(1200, 10, 5)</code> and looks like this:</p>\n<pre><code>array([[[1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        ..., \n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]],\n        ... ]\n</code></pre>\n<p><strong>Problem:</strong> I am not sure how to proceed, especially how to integrate <code>LSTM</code> to <code>Model</code> and how to get the decoder to generate a sequence from a vector.</p>\n<p>I am using <code>keras</code> with <code>tensorflow</code> backend.</p>\n<p><strong>EDIT:</strong> If someone wants to try out, here is my procedure to generate random sequences with moving ones (including padding):</p>\n<pre><code>import random\nimport math\n\ndef getNotSoRandomList(x):\n    rlen = 8\n    rlist = [0 for x in range(rlen)]\n    if x &lt;= 7:\n        rlist[x] = 1\n    return rlist\n\n\nsequence = [[getNotSoRandomList(x) for x in range(round(random.uniform(0, 10)))] for y in range(5000)]\n\n### Padding afterwards\n\nfrom keras.preprocessing import sequence as seq\n\ndata = seq.pad_sequences(\n    sequences = sequence,\n    padding='post',\n    maxlen=None,\n    truncating='post',\n    value=0.\n)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Models can be any way you want. If I understood it right, you just want to know how to create models with LSTM?</p>\n<p><strong>Using LSTMs</strong></p>\n<p>Well, first, you have to define what your encoded vector looks like. Suppose you want it to be an array of 20 elements, a 1-dimension vector. So, shape (None,20). The size of it is up to you, and there is no clear rule to know the ideal one. </p>\n<p>And your input must be three-dimensional, such as your (1200,10,5). In keras summaries and error messages, it will be shown as (None,10,5), as \"None\" represents the batch size, which can vary each time you train/predict. </p>\n<p>There are many ways to do this, but, suppose you want only one LSTM layer:</p>\n<pre><code>from keras.layers import *\nfrom keras.models import Model\n\ninpE = Input((10,5)) #here, you don't define the batch size   \noutE = LSTM(units = 20, return_sequences=False, ...optional parameters...)(inpE)\n</code></pre>\n<p>This is enough for a very very simple encoder resulting in an array with 20 elements (but you can stack more layers if you want). Let's create the model:</p>\n<pre><code>encoder = Model(inpE,outE)   \n</code></pre>\n<p>Now, for the decoder, it gets obscure. You don't have an actual sequence anymore, but a static meaningful vector. You may want to use LTSMs still, they will suppose the vector is a sequence. </p>\n<p>But here, since the input has shape (None,20), you must first reshape it to some 3-dimensional array in order to attach an LSTM layer next.</p>\n<p>The way you will reshape it is entirely up to you. 20 steps of 1 element? 1 step of 20 elements? 10 steps of 2 elements? Who knows?</p>\n<pre><code>inpD = Input((20,))   \noutD = Reshape((10,2))(inpD) #supposing 10 steps of 2 elements    \n</code></pre>\n<p>It's important to notice that if you don't have 10 steps anymore, you won't be able to just enable \"return_sequences\" and have the output you want. You'll have to work a little. Acually, it's not necessary to use \"return_sequences\" or even to use LSTMs, but you may do that. </p>\n<p>Since in my reshape I have 10 timesteps (intentionally), it will be ok to use \"return_sequences\", because the result will have 10 timesteps (as the initial input)</p>\n<pre><code>outD1 = LSTM(5,return_sequences=True,...optional parameters...)(outD)    \n#5 cells because we want a (None,10,5) vector.   \n</code></pre>\n<p>You could work in many other ways, such as simply creating a 50 cell LSTM without returning sequences and then reshaping the result:</p>\n<pre><code>alternativeOut = LSTM(50,return_sequences=False,...)(outD)    \nalternativeOut = Reshape((10,5))(alternativeOut)\n</code></pre>\n<p>And our model goes:</p>\n<pre><code>decoder = Model(inpD,outD1)  \nalternativeDecoder = Model(inpD,alternativeOut)   \n</code></pre>\n<p>After that, you unite the models with your code and train the autoencoder. \nAll three models will have the same weights, so you can make the encoder bring results just by using its <code>predict</code> method.</p>\n<pre><code>encoderPredictions = encoder.predict(data)\n</code></pre>\n<hr/>\n<p>What I often see about LSTMs for generating sequences is something like predicting the next element.</p>\n<p>You take just a few elements of the sequence and try to find the next element. And you take another segment one step forward and so on. This may be helpful in generating sequences. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can find a simple of sequence to sequence autoencoder here: <a href=\"https://blog.keras.io/building-autoencoders-in-keras.html\" rel=\"noreferrer\">https://blog.keras.io/building-autoencoders-in-keras.html</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is an example </p>\n<p>Let's create a synthetic data consisting of a few sequence. The idea is looking into these sequences through the lens of an autoencoder. In other words, lowering the dimension or summarizing them into a fixed length.</p>\n<pre><code># define input sequence\nsequence = np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n                     [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n                     [0.2, 0.4, 0.6, 0.8],\n                     [0.3, 0.6, 0.9, 1.2]])\n\n# prepare to normalize\nx = pd.DataFrame(sequence.tolist()).T.values\nscaler = preprocessing.StandardScaler()\nx_scaled = scaler.fit_transform(x)\nsequence_normalized = [col[~np.isnan(col)] for col in  x_scaled.T]\n\n# make sure to use dtype='float32' in padding otherwise with floating points\nsequence = pad_sequences(sequence, padding='post', dtype='float32')\n\n# reshape input into [samples, timesteps, features]\nn_obs = len(sequence)\nn_in = 9\nsequence = sequence.reshape((n_obs, n_in, 1))\n</code></pre>\n<p>Let's device a simple LSTM</p>\n<pre><code>#define encoder\nvisible = Input(shape=(n_in, 1))\nencoder = LSTM(2, activation='relu')(visible)\n\n# define reconstruct decoder\ndecoder1 = RepeatVector(n_in)(encoder)\ndecoder1 = LSTM(100, activation='relu', return_sequences=True)(decoder1)\ndecoder1 = TimeDistributed(Dense(1))(decoder1)\n\n# tie it together\nmyModel = Model(inputs=visible, outputs=decoder1)\n\n# summarize layers\nprint(myModel.summary())\n\n\n#sequence = tmp\nmyModel.compile(optimizer='adam', loss='mse')\n\nhistory = myModel.fit(sequence, sequence, \n                      epochs=400, \n                      verbose=0, \n                      validation_split=0.1, \n                      shuffle=True)\n\nplot_model(myModel, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n# demonstrate recreation\nyhat = myModel.predict(sequence, verbose=0)\n# yhat\n\nimport matplotlib.pyplot as plt\n\n#plot our loss \nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\n</code></pre>\n<p>                                                 <a href=\"https://i.sstatic.net/De3hEm.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/De3hEm.png\"/></a></p>\n<p><strong>Lets build the autoencoder</strong></p>\n<pre><code># use our encoded layer to encode the training input\ndecoder_layer = myModel.layers[1]\n\nencoded_input = Input(shape=(9, 1))\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\n# we are interested in seeing how the encoded sequences with lenght 2 (same as the dimension of the encoder looks like)\nout = decoder.predict(sequence)\n\nf = plt.figure()\nmyx = out[:,0]\nmyy = out[:,1]\ns = plt.scatter(myx, myy)\n\nfor i, txt in enumerate(out[:,0]):\n    plt.annotate(i+1, (myx[i], myy[i]))\n</code></pre>\n<p>And here is the representation of the sequences </p>\n<p>                                                 <a href=\"https://i.sstatic.net/tvzUxm.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/tvzUxm.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/1793532/how-do-i-determine-k-when-using-k-means-clustering\">How do I determine k when using k-means clustering?</a>\n<span class=\"question-originals-answer-count\">\n                                (20 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2016-10-17 14:18:54Z\">7 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>I am attempting to apply k-means on a set of high-dimensional data points (about 50 dimensions) and was wondering if there are any implementations that find the optimal number of clusters. </p>\n<p>I remember reading somewhere that the way an algorithm generally does this is such that the inter-cluster distance is maximized and intra-cluster distance is minimized but I don't remember where I saw that. It would be great if someone can point me to any resources that discuss this. I am using SciPy for k-means currently but any related library would be fine as well.</p>\n<p>If there are alternate ways of achieving the same or a better algorithm, please let me know.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One approach is <a href=\"http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29\" rel=\"noreferrer\">cross-validation</a>. </p>\n<p>In essence, you pick a subset of your data and cluster it into <em>k</em> clusters, and you ask how well it clusters, compared with the rest of the data: Are you assigning data points to the same cluster memberships, or are they falling into different clusters? </p>\n<p>If the memberships are roughly the same, the data fit well into <em>k</em> clusters. Otherwise, you try a different <em>k</em>.</p>\n<p>Also, you could do PCA (<a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\" rel=\"noreferrer\">principal component analysis</a>) to reduce your 50 dimensions to some more tractable number. If a PCA run suggests that most of your variance is coming from, say, 4 out of the 50 dimensions, then you can pick <em>k</em> on that basis, to explore how the four cluster memberships are assigned.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Take a look at this <a href=\"http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\" rel=\"noreferrer\">wikipedia page on determining the number of clusters in a data set</a>. </p>\n<p>Also you might want to try <a href=\"http://en.wikipedia.org/wiki/Cluster_analysis#Agglomerative_hierarchical_clustering\" rel=\"noreferrer\">Agglomerative hierarchical clustering</a> out. This approach does not need to know the number of clusters, it will incrementally form clusters of cluster till only one exists. This technique also exists in SciPy (<a href=\"http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\" rel=\"noreferrer\">scipy.cluster.hierarchy</a>). </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One interesting approach is that of <a href=\"http://www.cse.msu.edu/prip/ResearchProjects/cluster_research/papers/AFred_AJain_ICPR2002.pdf\" rel=\"nofollow\">evidence accumulation</a> by Fred and Jain. This is based on combining multiple runs of k-means with a large number of clusters, aggregating them into an overall solution. Nice aspects of the approach include that the number of clusters is determined in the process and that the final clusters don't have to be spherical.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As known, modern most popular CNN (convolutional neural network): VGG/ResNet (FasterRCNN), SSD, Yolo, Yolo v2, DenseBox, DetectNet - are not rotate invariant: <a href=\"https://stackoverflow.com/questions/40952163/are-modern-cnn-convolutional-neural-network-as-detectnet-rotate-invariant\">Are modern CNN (convolutional neural network) as DetectNet rotate invariant?</a></p>\n<p>Also known, that there are several neural networks with rotate-invariance object detection:</p>\n<ol>\n<li><p>Rotation-Invariant Neoperceptron 2006 (<a href=\"https://www.researchgate.net/profile/Beat_Fasel/publication/224649475_Rotation-Invariant_Neoperceptron/links/02e7e53859a00a588b000000.pdf\" rel=\"noreferrer\">PDF</a>): <a href=\"https://www.researchgate.net/publication/224649475_Rotation-Invariant_Neoperceptron\" rel=\"noreferrer\">https://www.researchgate.net/publication/224649475_Rotation-Invariant_Neoperceptron</a></p></li>\n<li><p>Learning rotation invariant convolutional filters for texture classification 2016 (<a href=\"https://arxiv.org/pdf/1604.06720v2\" rel=\"noreferrer\">PDF</a>): <a href=\"https://arxiv.org/abs/1604.06720\" rel=\"noreferrer\">https://arxiv.org/abs/1604.06720</a></p></li>\n<li><p>RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection 2016 (<a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.pdf\" rel=\"noreferrer\">PDF</a>): <a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.html\" rel=\"noreferrer\">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.html</a></p></li>\n<li><p>Encoded Invariance in Convolutional Neural Networks 2014 (<a href=\"http://theorycenter.cs.uchicago.edu/REU/2014/final-papers/sauder.pdf\" rel=\"noreferrer\">PDF</a>)</p></li>\n<li><p>Rotation-invariant convolutional neural networks for galaxy morphology prediction (<a href=\"https://arxiv.org/pdf/1503.07077v1\" rel=\"noreferrer\">PDF</a>): <a href=\"https://arxiv.org/abs/1503.07077\" rel=\"noreferrer\">https://arxiv.org/abs/1503.07077</a></p></li>\n<li><p>Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images 2016: <a href=\"http://ieeexplore.ieee.org/document/7560644/\" rel=\"noreferrer\">http://ieeexplore.ieee.org/document/7560644/</a></p></li>\n</ol>\n<p>We know, that in such image-detection competitions as: IMAGE-NET, MSCOCO, PASCAL VOC - used networks ensembles (simultaneously some neural networks). Or networks ensembles in single net such as ResNet (<a href=\"https://arxiv.org/abs/1605.06431\" rel=\"noreferrer\">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a>)</p>\n<p>But are used rotation invariant network ensembles in winners like as MSRA, and if not, then why? Why in ensemble the additional rotation-invariant network does not add accuracy to detect certain objects such as aircraft objects - which images is done at a different angles of rotation? </p>\n<p>It can be:</p>\n<ul>\n<li><p>aircraft objects which are photographed from the ground \n<a href=\"https://i.sstatic.net/s9C7w.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/s9C7w.jpg\"/></a></p></li>\n<li><p>or ground objects which are photographed from the air\n<a href=\"https://i.sstatic.net/ICqUL.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ICqUL.jpg\"/></a></p></li>\n</ul>\n<p>Why rotation-invariant neural networks are not used in winners of the popular object-detection competitions?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The recent progress in image recognition which was mainly made by changing the approach from a classic <em>feature selection - shallow learning algorithm</em> to <em>no feature selection - deep learning algorithm</em> wasn't only caused by mathematical properties of convolutional neural networks. Yes - of course their ability to capture the same information using smaller number of parameters was partially caused by their <em>shift invariance property</em> but the recent <a href=\"https://www.google.pl/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=rethink+generalization+convolutional+neural+networks\" rel=\"nofollow noreferrer\">research</a> has shown that this is not a key in understanding their success.</p>\n<p>In my opinion the main reason behind this success was developing <em>faster</em> learning algorithms than <em>more mathematically accurate</em> ones and that's why less attention is put on developing another <em>property invariant</em> neural nets.</p>\n<p>Of course - rotation invariance is not skipped at all. This is partially made by data augmentation where you put the slightly changed (e.g. rotated or rescaled) image to your dataset - with the same label. As we can read in this <a href=\"http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\" rel=\"nofollow noreferrer\">fantastic book</a> these two approaches (<em>more structure</em> vs <em>less structure + data augmentation</em>) are more or less equivalent. (Chapter 5.5.3, titled: Invariances)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm also wondering why the community or scholar didn't put much attention on ration invariant CNN as @Alex. </p>\n<p>One possible cause, in my opinion, is that many scenarios don't need this property, especially for those popular competitions. Like Rob mentioned, some natural pictures are already taken in a unified horizontal (or vertical) way. For example, in face detection, many works will align the picture to ensure the people are standing on the earth before feeding to any CNN models. To be honest, this is the most cheap and efficient way for this particular task. </p>\n<p>However, there does exist some scenarios in real life, needing rotation invariant property. So I come to another guess: this problem is not difficult from those experts (or researchers)' view. At least we can use data augmentation to obtain some rotate invariant. </p>\n<p>Lastly, thanks so much for your summarization about the papers. I added one more paper <a href=\"http://proceedings.mlr.press/v48/cohenc16.pdf\" rel=\"noreferrer\">Group Equivariant Convolutional Networks_icml2016_GCNN</a> and its <a href=\"https://github.com/tscohen/gconv_experiments\" rel=\"noreferrer\">implementation on github</a>  by other people.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Object detection is mostly driven by the successes of detection algorithms in world-famous object detection benchmarks like PASCAL-VOC and MS-COCO, which are object centric datasets where most objects are vertical (potted plants, humans, horses, etc.) and thus data augmentation with left-right flips is often sufficient (for all we know data augmentation with rotated images like upside-down flips could even hurt detection performance).<br/>\nEvery year the entire community adopts the base algorithmic structure of the winning solution and build on it (I am exaggerating a bit to prove a point but not so much).  </p>\n<p>Interestingly other less widely known topics like oriented text detections and oriented vehicle detections in aerial imagery both need rotation invariant features and rotation equivariant detection pipelines (like in both articles from Cheng you mentioned). </p>\n<p>If you want to find literature and code in this area you need to dive in these two domains. I can already give you a few pointers like the <a href=\"https://arxiv.org/pdf/1711.10398.pdf\" rel=\"noreferrer\">DOTA</a> challenge for aerial imagery or the <a href=\"https://arxiv.org/abs/1506.03184\" rel=\"noreferrer\">ICDAR challenges</a> for oriented text detections.  </p>\n<p>As @Marcin Mozejko said, CNN are by nature translation invariant and not rotation invariant. It is an open problem how to incorporate perfect rotation invariance the few articles that deal with it have yet to become standards even though <a href=\"https://arxiv.org/pdf/1604.06720.pdf\" rel=\"noreferrer\">some</a> <a href=\"https://arxiv.org/abs/1711.07289\" rel=\"noreferrer\">of them</a> seem promising. \nMy personal favorite for detection is the modification of Faster R-CNN recently proposed by <a href=\"https://arxiv.org/abs/1703.01086\" rel=\"noreferrer\">Ma</a>.   </p>\n<p>I hope that this direction of research will be investigated more and more once people will get fed up of MS-COCO and VOC.</p>\n<p>What you could try is take a state-of-the-art detector trained on MS-COCO like <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\" rel=\"noreferrer\">Faster R-CNN with NASNet from TF detection API</a> and see how it performs wrt rotating the test image, in my opinion it would be far from rotation invariant.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to know if there is a way to implement the different score function from the scikit learn package like this one :</p>\n<pre><code>from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_true, y_pred)\n</code></pre>\n<p>into a tensorflow model to get the different score.</p>\n<pre><code>with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\ninit = tf.initialize_all_variables()\nsess.run(init)\nfor epoch in xrange(1):\n        avg_cost = 0.\n        total_batch = len(train_arrays) / batch_size\n        for batch in range(total_batch):\n                train_step.run(feed_dict = {x: train_arrays, y: train_labels})\n                avg_cost += sess.run(cost, feed_dict={x: train_arrays, y: train_labels})/total_batch\n        if epoch % display_step == 0:\n                print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n\nprint \"Optimization Finished!\"\ncorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n# Calculate accuracy\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\nprint \"Accuracy:\", batch, accuracy.eval({x: test_arrays, y: test_labels})\n</code></pre>\n<p>Will i have to run the session again to get the prediction ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You do not really need sklearn to calculate precision/recall/f1 score. You can easily express them in TF-ish way by looking at the formulas:</p>\n<p><a href=\"https://i.sstatic.net/U0hjG.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/U0hjG.png\"/></a></p>\n<p>Now if you have your <code>actual</code> and <code>predicted</code> values as vectors of 0/1, you can calculate TP, TN, FP, FN using <a href=\"https://www.tensorflow.org/api_docs/python/tf/count_nonzero\" rel=\"noreferrer\">tf.count_nonzero</a>:</p>\n<pre><code>TP = tf.count_nonzero(predicted * actual)\nTN = tf.count_nonzero((predicted - 1) * (actual - 1))\nFP = tf.count_nonzero(predicted * (actual - 1))\nFN = tf.count_nonzero((predicted - 1) * actual)\n</code></pre>\n<p>Now your metrics are easy to calculate:</p>\n<pre><code>precision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nf1 = 2 * precision * recall / (precision + recall)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Maybe this example will speak to you :    </p>\n<pre><code>    pred = multilayer_perceptron(x, weights, biases)\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n    with tf.Session() as sess:\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    for epoch in xrange(150):\n            for i in xrange(total_batch):\n                    train_step.run(feed_dict = {x: train_arrays, y: train_labels})\n                    avg_cost += sess.run(cost, feed_dict={x: train_arrays, y: train_labels})/total_batch         \n            if epoch % display_step == 0:\n                    print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n\n    #metrics\n    y_p = tf.argmax(pred, 1)\n    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:test_arrays, y:test_label})\n\n    print \"validation accuracy:\", val_accuracy\n    y_true = np.argmax(test_label,1)\n    print \"Precision\", sk.metrics.precision_score(y_true, y_pred)\n    print \"Recall\", sk.metrics.recall_score(y_true, y_pred)\n    print \"f1_score\", sk.metrics.f1_score(y_true, y_pred)\n    print \"confusion_matrix\"\n    print sk.metrics.confusion_matrix(y_true, y_pred)\n    fpr, tpr, tresholds = sk.metrics.roc_curve(y_true, y_pred)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>Multi-label case</h1>\n<p>Previous answers do not specify how to handle the multi-label case so here is such a version implementing <strong>three types of multi-label f1 score in tensorflow</strong>: micro, macro and weighted (as per scikit-learn)</p>\n<p><strong>Update (06/06/18):</strong> I wrote a <a href=\"http://vict0rsch.github.io/2018/06/06/tensorflow-streaming-multilabel-f1/\" rel=\"noreferrer\">blog post</a> about how to compute the <strong>streaming multilabel f1 score</strong> in case it helps anyone (it's a longer process, don't want to overload this answer)</p>\n<pre><code>f1s = [0, 0, 0]\n\ny_true = tf.cast(y_true, tf.float64)\ny_pred = tf.cast(y_pred, tf.float64)\n\nfor i, axis in enumerate([None, 0]):\n    TP = tf.count_nonzero(y_pred * y_true, axis=axis)\n    FP = tf.count_nonzero(y_pred * (y_true - 1), axis=axis)\n    FN = tf.count_nonzero((y_pred - 1) * y_true, axis=axis)\n\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    f1s[i] = tf.reduce_mean(f1)\n\nweights = tf.reduce_sum(y_true, axis=0)\nweights /= tf.reduce_sum(weights)\n\nf1s[2] = tf.reduce_sum(f1 * weights)\n\nmicro, macro, weighted = f1s\n</code></pre>\n<hr/>\n<h3>Correctness</h3>\n<pre><code>def tf_f1_score(y_true, y_pred):\n    \"\"\"Computes 3 different f1 scores, micro macro\n    weighted.\n    micro: f1 score accross the classes, as 1\n    macro: mean of f1 scores per class\n    weighted: weighted average of f1 scores per class,\n            weighted from the support of each class\n\n\n    Args:\n        y_true (Tensor): labels, with shape (batch, num_classes)\n        y_pred (Tensor): model's predictions, same shape as y_true\n\n    Returns:\n        tuple(Tensor): (micro, macro, weighted)\n                    tuple of the computed f1 scores\n    \"\"\"\n\n    f1s = [0, 0, 0]\n\n    y_true = tf.cast(y_true, tf.float64)\n    y_pred = tf.cast(y_pred, tf.float64)\n\n    for i, axis in enumerate([None, 0]):\n        TP = tf.count_nonzero(y_pred * y_true, axis=axis)\n        FP = tf.count_nonzero(y_pred * (y_true - 1), axis=axis)\n        FN = tf.count_nonzero((y_pred - 1) * y_true, axis=axis)\n\n        precision = TP / (TP + FP)\n        recall = TP / (TP + FN)\n        f1 = 2 * precision * recall / (precision + recall)\n\n        f1s[i] = tf.reduce_mean(f1)\n\n    weights = tf.reduce_sum(y_true, axis=0)\n    weights /= tf.reduce_sum(weights)\n\n    f1s[2] = tf.reduce_sum(f1 * weights)\n\n    micro, macro, weighted = f1s\n    return micro, macro, weighted\n\n\ndef compare(nb, dims):\n    labels = (np.random.randn(nb, dims) &gt; 0.5).astype(int)\n    predictions = (np.random.randn(nb, dims) &gt; 0.5).astype(int)\n\n    stime = time()\n    mic = f1_score(labels, predictions, average='micro')\n    mac = f1_score(labels, predictions, average='macro')\n    wei = f1_score(labels, predictions, average='weighted')\n\n    print('sklearn in {:.4f}:\\n    micro: {:.8f}\\n    macro: {:.8f}\\n    weighted: {:.8f}'.format(\n        time() - stime, mic, mac, wei\n    ))\n\n    gtime = time()\n    tf.reset_default_graph()\n    y_true = tf.Variable(labels)\n    y_pred = tf.Variable(predictions)\n    micro, macro, weighted = tf_f1_score(y_true, y_pred)\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run(session=sess)\n        stime = time()\n        mic, mac, wei = sess.run([micro, macro, weighted])\n        print('tensorflow in {:.4f} ({:.4f} with graph time):\\n    micro: {:.8f}\\n    macro: {:.8f}\\n    weighted: {:.8f}'.format(\n            time() - stime, time()-gtime,  mic, mac, wei\n        ))\n\ncompare(10 ** 6, 10)\n</code></pre>\n<p><strong>outputs</strong>:</p>\n<pre><code>&gt;&gt; rows: 10^6 dimensions: 10\nsklearn in 2.3939:\n    micro: 0.30890287\n    macro: 0.30890275\n    weighted: 0.30890279\ntensorflow in 0.2465 (3.3246 with graph time):\n    micro: 0.30890287\n    macro: 0.30890275\n    weighted: 0.30890279\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I found that scaling in SVM (Support Vector Machine) problems really improve its performance.\nI have read this explanation:</p>\n<blockquote>\n<p>The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges.</p>\n</blockquote>\n<p>Unfortunately this didn't help me. Can somebody provide a better explanation?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Feature scaling is a general trick applied to optimization problems (not just SVM). The underline algorithm to solve the optimization problem of SVM is gradient descend. Andrew Ng has a great explanation in his coursera videos <a href=\"https://class.coursera.org/ml-003/lecture/21\">here</a>.</p>\n<p>I will illustrate the core ideas here (I borrow Andrew's slides). Suppose you have only two parameters and one of the parameters can take a relatively large range of values. Then the contour of the cost function \ncan look like very tall and skinny ovals (see blue ovals below). Your gradients (the path of gradient is drawn in red) could take a long time and go back and forth to find the optimal solution.<br/>\n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/GeAcX.png\"/></p>\n<p>Instead if your scaled your feature, the contour of the cost function might look like circles; then the gradient can take a much more straight path and achieve the optimal point much faster. \n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/30hnH.png\"/></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The true reason behind scaling features in SVM is the fact, that this classifier <strong>is not affine transformation invariant</strong>. In other words, if you multiply one feature by a 1000 than a solution given by SVM will be completely different. It has nearly nothing to do with the underlying optimization techniques (although they are affected by these scales problems, they should still converge to global optimum).</p>\n<p>Consider an example: you have man and a woman, encoded by their sex and height (two features). Let us assume a very simple case with such data:</p>\n<p>0 -&gt; man\n1 -&gt; woman</p>\n<pre><code>╔═════╦════════╗\n║ sex ║ height ║\n╠═════╬════════╣\n║  1  ║  150   ║\n╠═════╬════════╣\n║  1  ║  160   ║\n╠═════╬════════╣\n║  1  ║  170   ║\n╠═════╬════════╣\n║  0  ║  180   ║\n╠═════╬════════╣\n║  0  ║  190   ║\n╠═════╬════════╣\n║  0  ║  200   ║\n╚═════╩════════╝\n</code></pre>\n<p>And let us do something silly. Train it to predict the sex of the person, so we are trying to learn f(x,y)=x (ignoring second parameter).</p>\n<p>It is easy to see, that for such data largest margin classifier will \"cut\" the plane horizontally somewhere around height \"175\", so once we get new sample \"0 178\" (a woman of 178cm height) we get the classification that she is a man. </p>\n<p>However, if we scale down everything to [0,1] we get sth like</p>\n<pre><code>╔═════╦════════╗\n║ sex ║ height ║\n╠═════╬════════╣\n║  1  ║  0.0   ║\n╠═════╬════════╣\n║  1  ║  0.2   ║\n╠═════╬════════╣\n║  1  ║  0.4   ║\n╠═════╬════════╣\n║  0  ║  0.6   ║\n╠═════╬════════╣\n║  0  ║  0.8   ║\n╠═════╬════════╣\n║  0  ║  1.0   ║\n╚═════╩════════╝\n</code></pre>\n<p>and now largest margin classifier \"cuts\" the plane nearly vertically (as expected) and so given new sample \"0 178\" which is also scaled to around \"0 0.56\" we get that it is a woman (correct!)</p>\n<p>So in general - scaling ensures that just because some features are <strong>big</strong> it won't lead to using them as <strong>a main predictor</strong>. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just personal thoughts from another perspective.<br/>\n<strong>1. why feature scaling influence?</strong><br/>\nThere's a word in applying machine learning algorithm, 'garbage in, garbage out'. The more real reflection of your features, the more accuracy your algorithm will get. That applies too for how machine learning algorithms treat relationship between features. Different from human's brain, when machine learning algorithms do the classify for example, all the features are expressed and calculated by the same coordinate system, which in some sense, <strong>establish a priori assumption</strong> between the features(not really reflection of data itself). And also the nature of most algorithms is to find the most appropriate weight percentage between the features to fittest the data. So when these algorithms' input is unscaled features, large scale data has more influence on the weight. Actually it's not the reflection of data iteself.<br/>\n<strong>2. why <em>usually</em> feature scaling improve the accuracy?</strong><br/>\nThe common practice in unsupervised machine learning algorithms about the hyper-parameters(or hyper-hyper parameters) selection(for example, hierachical Dirichlet process, hLDA) is that you should not add any personal subjective assumption about data. The best way is just to assume that they have the equality probability to appear. I think it applies here too. The feature scaling just try to make the assumption that all the features has the equality opportunity to influence the weight, which more really reflects the information/knowledge you know about the data. Commonly also result in better accuracy.</p>\n<p>BTW, about the affine transformation invariant and converge faster, there's are interest link <a href=\"https://stats.stackexchange.com/questions/41704/how-and-why-do-normalization-and-feature-scaling-work?newreg=cb1ec1dda52e45b3be7d1688a57d07ad\">here</a> on stats.stackexchange.com.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/26182980/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2019-02-03 01:15:00Z\">5 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/26182980/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I recently studied about supervised learning and unsupervised learning. From theory, I know that supervised means getting the information from labeled datasets and unsupervised means clustering the data without any labels given.</p>\n<p>But, the problem is I always get confused to identify whether the given example is supervised learning or unsupervised learning during my studies.</p>\n<p>Can anyone please give a real life example?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Supervised learning:</h2>\n<ul>\n<li>You get a bunch of photos <strong>with information about what is on them</strong> and then you train a model to recognize new photos.</li>\n<li>You have a bunch of molecules and <strong>information about which are drugs</strong> and you train a model to answer whether a new molecule is also a drug.</li>\n</ul>\n<hr/>\n<h2>Unsupervised learning:</h2>\n<ul>\n<li>You have a bunch of photos of 6 people but <strong>without information about who is on which one</strong> and you want to <strong>divide</strong> this dataset into 6 piles, each with the photos of one individual.</li>\n<li>You have molecules, part of them are drugs and part are not <strong>but you do not know which are which</strong> and you want the algorithm to discover the drugs.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Supervised Learning:</strong> </p>\n<ul>\n<li>is like learning with a teacher</li>\n<li>training dataset is like a teacher</li>\n<li>the training dataset is used to train the machine</li>\n</ul>\n<blockquote>\n<p><strong>Example:</strong></p>\n<p><strong>Classification:</strong> Machine is trained to classify something into some class. </p>\n<ul>\n<li>classifying whether a patient has disease or not</li>\n<li>classifying whether an email is spam or not</li>\n</ul>\n<p><strong>Regression:</strong> Machine is trained to predict some value like price, weight or height.</p>\n<ul>\n<li>predicting house/property price</li>\n<li>predicting stock market price</li>\n</ul>\n</blockquote>\n<p><strong>Unsupervised Learning:</strong> </p>\n<ul>\n<li>is like learning without a teacher</li>\n<li>the machine learns through observation &amp; find structures in data</li>\n</ul>\n<blockquote>\n<p><strong>Example:</strong></p>\n<p><strong>Clustering:</strong> A clustering problem is where you want to discover the inherent groupings in the data</p>\n<ul>\n<li>such as grouping customers by purchasing behavior</li>\n</ul>\n<p><strong>Association:</strong> An association rule learning problem is where you want to discover rules that describe large portions of your data</p>\n<ul>\n<li>such as people that buy X also tend to buy Y</li>\n</ul>\n</blockquote>\n<p><em>Read more: <a href=\"https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/\" rel=\"noreferrer\">Supervised and Unsupervised Machine Learning Algorithms</a></em></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Supervised Learning</strong></p>\n<p>This is simple and you would have done it a number of times, for example:</p>\n<ol>\n<li>Cortana or any speech automated system in your mobile phone trains your voice and then starts working based on this training.</li>\n<li>Based on various features (past record of head-to-head, pitch, toss, player-vs-player) <a href=\"https://en.wikipedia.org/wiki/WASP_(Winning_and_Score_Predictor)\" rel=\"noreferrer\">WASP</a> predicts the winning % of both teams.</li>\n<li>Train your handwriting to OCR system and once trained, it will be able to convert your hand-writing images into text (till some accuracy obviously)</li>\n<li>Based on some prior knowledge (when its sunny, temperature is higher; when its cloudy, humidity is higher, etc.) weather apps predict the parameters for a given time.</li>\n<li><p>Based on past information about spams, filtering out a new incoming email into <strong>Inbox</strong> (normal) or <strong>Junk folder</strong> (Spam)</p></li>\n<li><p>Biometric attendance or ATM etc systems where you train the machine after couple of inputs (of your biometric identity - be it thumb or iris or ear-lobe, etc.), machine can validate your future input and identify you.</p></li>\n</ol>\n<p><strong>Unsupervised Learning</strong></p>\n<ol start=\"7\">\n<li><p>A friend invites you to his party where you meet totally strangers. Now you will classify them using unsupervised learning (no prior knowledge) and this classification can be on the basis of gender, age group, dressing, educational qualification or whatever way you would like. <strong>Why this learning is different from Supervised Learning? Since you didn't use any past/prior knowledge about people and classified them \"on-the-go\".</strong></p></li>\n<li><p>NASA discovers new heavenly bodies and finds them different from\npreviously known astronomical objects - stars, planets, asteroids,\nblackholes etc. (i.e. it has no knowledge about these new bodies)\nand classifies them the way it would like to (distance from Milky way, intensity, gravitational force, red/blue shift or whatever)</p></li>\n<li><p>Let's suppose you have never seen a Cricket match before and by chance watch a video on internet, now you can classify players on the basis of different criterion: Players wearing same sort of kits are in one class, Players of one style are in one class (batsmen, bowler, fielders), or on the basis of playing hand (RH vs LH) or whatever way you would observe [and classify] it.</p></li>\n<li><p>We are conducting a survey of 500 questions about predicting the IQ level of students in a college. Since this questionnaire is too big, so after 100 students, administration decides to trim the questionnaire down to fewer questions and for it we use some statistical procedure like <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\" rel=\"noreferrer\">PCA</a> to trim it down.</p></li>\n</ol>\n<p>I hope these couple of examples explain the difference in detail.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am looking for a way to graph grid_scores_ from GridSearchCV  in sklearn. In this example I am trying to grid search for best gamma and C parameters for an SVR algorithm. My code looks as follows: </p>\n<pre><code>    C_range = 10.0 ** np.arange(-4, 4)\n    gamma_range = 10.0 ** np.arange(-4, 4)\n    param_grid = dict(gamma=gamma_range.tolist(), C=C_range.tolist())\n    grid = GridSearchCV(SVR(kernel='rbf', gamma=0.1),param_grid, cv=5)\n    grid.fit(X_train,y_train)\n    print(grid.grid_scores_)\n</code></pre>\n<p>After I run the code and print the grid scores I get the following outcome:</p>\n<pre><code>[mean: -3.28593, std: 1.69134, params: {'gamma': 0.0001, 'C': 0.0001}, mean: -3.29370, std: 1.69346, params: {'gamma': 0.001, 'C': 0.0001}, mean: -3.28933, std: 1.69104, params: {'gamma': 0.01, 'C': 0.0001}, mean: -3.28925, std: 1.69106, params: {'gamma': 0.1, 'C': 0.0001}, mean: -3.28925, std: 1.69106, params: {'gamma': 1.0, 'C': 0.0001}, mean: -3.28925, std: 1.69106, params: {'gamma': 10.0, 'C': 0.0001},etc] \n</code></pre>\n<p>I would like to visualize all the scores (mean values) depending on gamma and C parameters. The graph I am trying to obtain should look as follows:</p>\n<p><a href=\"https://i.sstatic.net/BhI4Z.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/BhI4Z.jpg\"/></a></p>\n<p>Where x-axis is gamma, y-axis is mean score (root mean square error in this case), and different lines represent different C values. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The code shown by @sascha is correct. However, the <code>grid_scores_</code> attribute will be soon deprecated. It is better to use the <code>cv_results</code> attribute.</p>\n<p>It can be implemente in a similar fashion to that of @sascha method:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n    # Get Test Scores Mean and std for each grid search\n    scores_mean = cv_results['mean_test_score']\n    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n\n    scores_sd = cv_results['std_test_score']\n    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n\n    # Plot Grid search scores\n    _, ax = plt.subplots(1,1)\n\n    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n    for idx, val in enumerate(grid_param_2):\n        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n\n    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n    ax.set_xlabel(name_param_1, fontsize=16)\n    ax.set_ylabel('CV Average Score', fontsize=16)\n    ax.legend(loc=\"best\", fontsize=15)\n    ax.grid('on')\n\n# Calling Method \nplot_grid_search(pipe_grid.cv_results_, n_estimators, max_features, 'N Estimators', 'Max Features')\n</code></pre>\n<p>The above results in the following plot:</p>\n<p><a href=\"https://i.sstatic.net/2g3ua.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/2g3ua.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>from sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\nclf_ = SVC(kernel='rbf')\nCs = [1, 10, 100, 1000]\nGammas = [1e-3, 1e-4]\nclf = GridSearchCV(clf_,\n            dict(C=Cs,\n                 gamma=Gammas),\n                 cv=2,\n                 pre_dispatch='1*n_jobs',\n                 n_jobs=1)\n\nclf.fit(X, y)\n\nscores = [x[1] for x in clf.grid_scores_]\nscores = np.array(scores).reshape(len(Cs), len(Gammas))\n\nfor ind, i in enumerate(Cs):\n    plt.plot(Gammas, scores[ind], label='C: ' + str(i))\nplt.legend()\nplt.xlabel('Gamma')\nplt.ylabel('Mean score')\nplt.show()\n</code></pre>\n<ul>\n<li>Code is based on <a href=\"http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\">this</a>.</li>\n<li>Only puzzling part: will sklearn always respect the order of C &amp; Gamma -&gt; official example uses this \"ordering\"</li>\n</ul>\n<p>Output:</p>\n<p><a href=\"https://i.sstatic.net/mQR2t.png\"><img alt=\"Example plot\" src=\"https://i.sstatic.net/mQR2t.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For plotting the results when tuning several hyperparameters, what I did was fixed all parameters to their best value except for one and plotted the mean score for the other parameter for each of its values.</p>\n<pre><code>def plot_search_results(grid):\n    \"\"\"\n    Params: \n        grid: A trained GridSearchCV object.\n    \"\"\"\n    ## Results from grid search\n    results = grid.cv_results_\n    means_test = results['mean_test_score']\n    stds_test = results['std_test_score']\n    means_train = results['mean_train_score']\n    stds_train = results['std_train_score']\n\n    ## Getting indexes of values per hyper-parameter\n    masks=[]\n    masks_names= list(grid.best_params_.keys())\n    for p_k, p_v in grid.best_params_.items():\n        masks.append(list(results['param_'+p_k].data==p_v))\n\n    params=grid.param_grid\n\n    ## Ploting results\n    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,5))\n    fig.suptitle('Score per parameter')\n    fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n    pram_preformace_in_best = {}\n    for i, p in enumerate(masks_names):\n        m = np.stack(masks[:i] + masks[i+1:])\n        pram_preformace_in_best\n        best_parms_mask = m.all(axis=0)\n        best_index = np.where(best_parms_mask)[0]\n        x = np.array(params[p])\n        y_1 = np.array(means_test[best_index])\n        e_1 = np.array(stds_test[best_index])\n        y_2 = np.array(means_train[best_index])\n        e_2 = np.array(stds_train[best_index])\n        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n        ax[i].errorbar(x, y_2, e_2, linestyle='-', marker='^',label='train' )\n        ax[i].set_xlabel(p.upper())\n\n    plt.legend()\n    plt.show()\n</code></pre>\n<p><img alt=\"Result\" src=\"https://i.sstatic.net/egdjV.png\"/></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If I want to train a model with train_generator, is there a significant difference between choosing</p>\n<ul>\n<li>10 Epochs with 500 Steps each</li>\n</ul>\n<p>and</p>\n<ul>\n<li>100 Epochs with 50 Steps each</li>\n</ul>\n<p>Currently I am training for 10 epochs, because each epoch takes a long time, but any graph showing improvement looks very \"jumpy\" because I only have 10 datapoints. I figure I can get a smoother graph if I use 100 Epochs, but I want to know first if there is any downside to this</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Based on what you said it sounds like you need a larger <code>batch_size</code>, and of course there are implications with that which could impact the steps_per_epoch and number of epochs.</p>\n<p><strong>To solve for jumping-around</strong></p>\n<ul>\n<li><strong>A larger batch size</strong> will give you a better gradient and will help to prevent jumping around</li>\n<li>You may also want to consider a smaller learning rate, or a learning rate scheduler (or decay) to allow the network to \"settle in\" as it trains</li>\n</ul>\n<p><strong>Implications of a larger batch-size</strong></p>\n<ul>\n<li>Too large of a batch_size can produce memory problems, especially if you are using a GPU. Once you exceed the limit, dial it back until it works. This will help you find the max batch-size that your system can work with.</li>\n<li>Too large of a batch size can get you stuck in a local minima, so if your training get stuck, I would reduce it some. Imagine here you are over-correcting the <em>jumping-around</em> and it's not jumping around enough to further minimize the loss function.</li>\n</ul>\n<p><strong>When to reduce epochs</strong></p>\n<ul>\n<li>If your train error is very low, yet your test/validation is very high, then you have over-fit the model with too many epochs.</li>\n<li>The best way to find the right balance is to use early-stopping with a validation test set. Here you can specify when to stop training, and save the weights for the network that gives you the best validation loss. (I highly recommend using this always)</li>\n</ul>\n<p><strong>When to adjust steps-per-epoch</strong></p>\n<ul>\n<li>Traditionally, the steps per epoch is calculated as train_length // batch_size, since this will use all of the data points, one batch size worth at a time. </li>\n<li>If you are augmenting the data, then you can stretch this a tad (sometimes I multiply that function above by 2 or 3 etc. But, if it's already training for too long, then I would just stick with the traditional approach.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Steps per epoch does not connect to epochs.</p>\n<p>Naturally what you want if to 1 epoch your generator pass through all of your training data one time. To achieve this you should provide steps per epoch equal to number of batches like this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>steps_per_epoch = int( np.ceil(x_train.shape[0] / batch_size) )\n</code></pre>\n<p>as from above equation the largest the <code>batch_size</code>, the lower the <code>steps_per_epoch</code>. </p>\n<p>Next you will choose epoch based on chosen validation. (choose what you think best)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>steps_per_epoch</code> tells the network how many batches to include in an epoch.</p>\n<p>By definition, an <code>epoch</code> is considered complete when the dataset has been run through the model once in its entirety. With other words, it means that all training samples have been run through the model. (For further discussion, let us assume that the size of the training examples is 'm').</p>\n<p>Also by definition, we know that `batch size' is between [1, m].</p>\n<p>Below is what <a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\" rel=\"noreferrer\">TensorFlow page</a> says about <code>steps_per_epoch</code></p>\n<blockquote>\n<p>If you want to run training only on a specific number of batches from this Dataset, you can pass the steps_per_epoch argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch.</p>\n</blockquote>\n<p>Now suppose that your training_size, <code>m = 128</code> and batch_size, <code>b = 16</code>, which means that your data is grouped into 8 batches. According to the above quote, the maximum value you can assign to <code>steps_per_epoch</code> is 8, as computed in <a href=\"https://stackoverflow.com/a/49923476/8314782\">one of the answers</a> by @Ioannis Nasios.</p>\n<p>However, it is not necessary that you set the value to 8 only (as in our example). You can choose any value between 1 and 8. You just need to be aware that the training will be performed only with this number of batches.</p>\n<p>The reason for the jumpy error values could be the size of your batch, as correctly mentioned in <a href=\"https://stackoverflow.com/a/49924566/8314782\">this answer</a> by @Chris Farr.</p>\n<p><a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#training_evaluation_from_tfdata_datasets\" rel=\"noreferrer\">Training &amp; evaluation from tf.data Datasets</a></p>\n<blockquote>\n<p>If you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset).</p>\n</blockquote>\n<p>The advantage of a low value for <code>steps_per_epoch</code> is that different epochs are trained with different data sets (a kind of regularization). However, if you have a limited training size, using only a subset of stacks would not be what we want. It is a decision one has to make.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How is Q-learning different from value iteration in reinforcement learning? </p>\n<p>I know Q-learning is model-free and training samples are transitions <code>(s, a, s', r)</code>. But since we know the transitions and the reward for every transition in Q-learning, is it not the same as model-based learning where we know the reward for a state and action pair, and the transitions for every action from a state (be it stochastic or deterministic)? I do not understand the difference.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are 100% right that if we knew the transition probabilities and reward for every transition in Q-learning, it would be pretty unclear why we would use it instead of model-based learning or how it would even be fundamentally different. After all, transition probabilities and rewards are the two components of the model used in value iteration - if you have them, you have a model.</p>\n<p>The key is that, <strong>in Q-learning, the agent does not know state transition probabilities or rewards</strong>. The agent only discovers that there is a reward for going from one state to another via a given action when it does so and receives a reward. Similarly, it only figures out what transitions are available from a given state by ending up in that state and looking at its options. If state transitions are stochastic, it learns the probability of transitioning between states by observing how frequently different transitions occur.</p>\n<p>A possible source of confusion here is that you, as the programmer, might know exactly how rewards and state transitions are set up. In fact, when you're first designing a system, odds are that you do as this is pretty important to debugging and verifying that your approach works. But you never tell the agent any of this - instead you force it to learn on its own through trial and error. <strong>This is important if you want to create an agent that is capable of entering a new situation that you don't have any prior knowledge about and figuring out what to do.</strong> Alternately, if you don't care about the agent's ability to learn on its own, <strong>Q-learning might also be necessary if the state-space is too large to repeatedly enumerate.</strong> Having the agent explore without any starting knowledge can be more computationally tractable.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Value iteration is used when you have transition probabilities, that means when you know the probability of getting from state x into state x' with action a. In contrast, you might have a black box which allows you to simulate it, but you're not actually given the probability. So you are model-free. This is when you apply Q learning.</p>\n<p>Also what is learned is different. With value iteration, you learn the expected cost when you are given a state x. With q-learning, you get the expected discounted cost when you are in state x and apply action a.</p>\n<p>Here are the algorithms:</p>\n<p><img alt=\"\" src=\"https://martin-thoma.com/images/2016/07/Value-Iteration.png\"/></p>\n<p><img alt=\"\" src=\"https://martin-thoma.com/images/2016/07/q-learning.png\"/></p>\n<p>I'm currently writing down quite a bit about reinforcement learning for an exam. You might also be interested in <a href=\"https://martin-thoma.com/probabilistische-planung/\" rel=\"noreferrer\">my lecture notes</a>. However, they are mostly in German.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't think the accepted answer captured the essential of difference. To quote the newest version of Richard Sutton's book:</p>\n<blockquote>\n<p>\"\n  <em>Having q∗ makes choosing optimal actions even easier. With q∗, the agent does not\n  even have to do a one-step-ahead search: for any state s, it can simply find any action that maximizes q∗(s; a). The action-value function effectively caches the results of all one-step-ahead searches. It provides the optimal expected long-term return as a value that is locally and immediately available for each state{action pair. Hence, at the cost of representing a function of state{action pairs, instead of just of states, the optimal action value function allows optimal actions to be selected without having to know anything about possible successor states and their values, that is, without having to know anything\n  about the environment’s dynamics.</em>\n  \"</p>\n</blockquote>\n<p>Usually in real problems the agent doesn't know the world(or the so called transformation) dynamics but we definitely know the rewards, because those are what the environment gives back during the interaction and the reward function is actually defined by us. </p>\n<p>The <strong>real difference between q-learning and normal value iteration is that</strong>: \nAfter you have V*, you still need to do one step action look-ahead to subsequent states to identify the optimal action for that state. And this look-ahead requires the transition dynamic after the action. But if you have q*, the optimal plan is just choosing <em>a</em> from the max <em>q(s,a)</em> pair.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am having trouble fully understanding the <a href=\"https://en.wikipedia.org/wiki/K-means%2B%2B\" rel=\"nofollow noreferrer\">K-Means++ algorithm</a>.  I am interested exactly how the first <code>k</code> centroids are picked, namely the initialization as the rest is like in the original <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\" rel=\"nofollow noreferrer\">K-Means algorithm</a>.</p>\n<ol>\n<li>Is the probability function used based on distance or Gaussian? </li>\n<li>In the same time the most long distant point (From the other centroids) is picked for a new centroid.</li>\n</ol>\n<p>I will appreciate a step by step explanation and an example.  The one in <a href=\"https://en.wikipedia.org/wiki/K-means%2B%2B\" rel=\"nofollow noreferrer\">Wikipedia</a> is not clear enough.  Also a very well commented source code would also help.  If you are using 6 arrays then please tell us which one is for what.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Interesting question. Thank you for bringing this paper to my attention - <a href=\"http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf\" rel=\"noreferrer\">K-Means++: The Advantages of Careful Seeding</a></p>\n<p>In simple terms, cluster centers are initially chosen at random from the set of input observation vectors, where the probability of choosing vector <code>x</code> is high if <code>x</code> is not near any previously chosen centers.</p>\n<p>Here is a one-dimensional example. Our observations are [0, 1, 2, 3, 4]. Let the first center, <code>c1</code>, be 0. The probability that the next cluster center, <code>c2</code>, is x is proportional to <code>||c1-x||^2</code>. So, P(c2 = 1) = 1a, P(c2 = 2) = 4a, P(c2 = 3) = 9a, P(c2 = 4) = 16a, where a = 1/(1+4+9+16).</p>\n<p>Suppose c2=4. Then, P(c3 = 1) = 1a, P(c3 = 2) = 4a, P(c3 = 3) = 1a, where a = 1/(1+4+1).</p>\n<p>I've coded the initialization procedure in Python; I don't know if this helps you.</p>\n<pre><code>def initialize(X, K):\n    C = [X[0]]\n    for k in range(1, K):\n        D2 = scipy.array([min([scipy.inner(c-x,c-x) for c in C]) for x in X])\n        probs = D2/D2.sum()\n        cumprobs = probs.cumsum()\n        r = scipy.rand()\n        for j,p in enumerate(cumprobs):\n            if r &lt; p:\n                i = j\n                break\n        C.append(X[i])\n    return C\n</code></pre>\n<p>EDIT with clarification: The output of <code>cumsum</code> gives us boundaries to partition the interval [0,1]. These partitions have length equal to the probability of the corresponding point being chosen as a center. So then, since <code>r</code> is uniformly chosen between [0,1], it will fall into exactly one of these intervals (because of <code>break</code>). The <code>for</code> loop checks to see which partition <code>r</code> is in.</p>\n<p>Example: </p>\n<pre><code>probs = [0.1, 0.2, 0.3, 0.4]\ncumprobs = [0.1, 0.3, 0.6, 1.0]\nif r &lt; cumprobs[0]:\n    # this event has probability 0.1\n    i = 0\nelif r &lt; cumprobs[1]:\n    # this event has probability 0.2\n    i = 1\nelif r &lt; cumprobs[2]:\n    # this event has probability 0.3\n    i = 2\nelif r &lt; cumprobs[3]:\n    # this event has probability 0.4\n    i = 3\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One Liner. </p>\n<p>Say we need to select 2 cluster centers, instead of selecting them all randomly{like we do in simple k means}, we will select the first one randomly, then find the points that are farthest to the first center{These points most probably do not belong to the first cluster center as they are far from it} and assign the second cluster center nearby those far points.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have prepared a full source implementation of k-means++ based on the book \"Collective Intelligence\" by Toby Segaran and the k-menas++ initialization provided here.</p>\n<p>Indeed there are two distance functions here. For the initial centroids a standard one is used based numpy.inner and then for the centroids fixation the Pearson one is used. Maybe the Pearson one can be also be used for the initial centroids. They say it is better.    </p>\n<pre><code>from __future__ import division\n\ndef readfile(filename):\n  lines=[line for line in file(filename)]\n  rownames=[]\n  data=[]\n  for line in lines:\n    p=line.strip().split(' ') #single space as separator\n    #print p\n    # First column in each row is the rowname\n    rownames.append(p[0])\n    # The data for this row is the remainder of the row\n    data.append([float(x) for x in p[1:]])\n    #print [float(x) for x in p[1:]]\n  return rownames,data\n\nfrom math import sqrt\n\ndef pearson(v1,v2):\n  # Simple sums\n  sum1=sum(v1)\n  sum2=sum(v2)\n\n  # Sums of the squares\n  sum1Sq=sum([pow(v,2) for v in v1])\n  sum2Sq=sum([pow(v,2) for v in v2])    \n\n  # Sum of the products\n  pSum=sum([v1[i]*v2[i] for i in range(len(v1))])\n\n  # Calculate r (Pearson score)\n  num=pSum-(sum1*sum2/len(v1))\n  den=sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))\n  if den==0: return 0\n\n  return 1.0-num/den\n\nimport numpy\nfrom numpy.random import *\n\ndef initialize(X, K):\n    C = [X[0]]\n    for _ in range(1, K):\n        #D2 = numpy.array([min([numpy.inner(c-x,c-x) for c in C]) for x in X])\n        D2 = numpy.array([min([numpy.inner(numpy.array(c)-numpy.array(x),numpy.array(c)-numpy.array(x)) for c in C]) for x in X])\n        probs = D2/D2.sum()\n        cumprobs = probs.cumsum()\n        #print \"cumprobs=\",cumprobs\n        r = rand()\n        #print \"r=\",r\n        i=-1\n        for j,p in enumerate(cumprobs):\n            if r 0:\n        for rowid in bestmatches[i]:\n          for m in range(len(rows[rowid])):\n            avgs[m]+=rows[rowid][m]\n        for j in range(len(avgs)):\n          avgs[j]/=len(bestmatches[i])\n        clusters[i]=avgs\n\n  return bestmatches\n\nrows,data=readfile('/home/toncho/Desktop/data.txt')\n\nkclust = kcluster(data,k=4)\n\nprint \"Result:\"\nfor c in kclust:\n    out = \"\"\n    for r in c:\n        out+=rows[r] +' '\n    print \"[\"+out[:-1]+\"]\"\n\nprint 'done'\n</code></pre>\n<p>data.txt:\n<code><pre>\np1 1 5 6\np2 9 4 3\np3 2 3 1\np4 4 5 6\np5 7 8 9\np6 4 5 4\np7 2 5 6\np8 3 4 5\np9 6 7 8\n</pre></code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have tried many examples with F1 micro and Accuracy in scikit-learn and in all of them, I see that F1 micro is the same as Accuracy. Is this always true?</p>\n<p>Script</p>\n<pre><code>from sklearn import svm\nfrom sklearn import metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# prepare dataset\niris = load_iris()\nX = iris.data[:, :2]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# svm classification\nclf = svm.SVC(kernel='rbf', gamma=0.7, C = 1.0).fit(X_train, y_train)\ny_predicted = clf.predict(X_test)\n\n# performance\nprint \"Classification report for %s\" % clf\nprint metrics.classification_report(y_test, y_predicted)\n\nprint(\"F1 micro: %1.4f\\n\" % f1_score(y_test, y_predicted, average='micro'))\nprint(\"F1 macro: %1.4f\\n\" % f1_score(y_test, y_predicted, average='macro'))\nprint(\"F1 weighted: %1.4f\\n\" % f1_score(y_test, y_predicted, average='weighted'))\nprint(\"Accuracy: %1.4f\" % (accuracy_score(y_test, y_predicted)))\n</code></pre>\n<p>Output</p>\n<pre><code>Classification report for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma=0.7, kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n             precision    recall  f1-score   support\n\n          0       1.00      0.90      0.95        10\n          1       0.50      0.88      0.64         8\n          2       0.86      0.50      0.63        12\n\navg / total       0.81      0.73      0.74        30\n\nF1 micro: 0.7333\n\nF1 macro: 0.7384\n\nF1 weighted: 0.7381\n\nAccuracy: 0.7333\n</code></pre>\n<p><strong>F1 micro = Accuracy</strong></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In classification tasks for which every test case is guaranteed to be assigned to exactly one class, micro-F is equivalent to accuracy. It won't be the case in multi-label classification.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is because we are dealing with a multi class classification , where every test data should belong to only 1 class and not multi label , in such case where there is no TN , we can call True Negatives as True Positives.</p>\n<p>Formula wise ,</p>\n<p><a href=\"https://i.sstatic.net/JrWbW.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/JrWbW.jpg\"/></a></p>\n<p>correction : F1 score is 2* precision* recall / (precision + recall)</p>\n<p><a href=\"https://i.sstatic.net/sNFLy.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/sNFLy.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Micoaverage precision, recall, f1 and accuracy are all equal for cases in which every instance must be classified into one (and only one) class. A simple way to see this is by looking at the formulas precision=TP/(TP+FP) and recall=TP/(TP+FN). The numerators are the same, and every FN for one class is another classes's FP, which makes the denominators the same as well. If precision = recall, then f1 will also be equal.</p>\n<p>For any inputs should should be able to show that:</p>\n<pre><code>from sklearn.metrics import accuracy_score as acc\nfrom sklearn.metrics import f1_score as f1\nf1(y_true,y_pred,average='micro')=acc(y_true,y_pred)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Restore original text from Keras’s imdb dataset</p>\n<p>I want to restore imdb’s original text from Keras’s imdb dataset.</p>\n<p>First, when I load Keras’s imdb dataset, it returned sequence of word index.</p>\n<p>\n<pre><code>&gt;&gt;&gt; (X_train, y_train), (X_test, y_test) = imdb.load_data()\n&gt;&gt;&gt; X_train[0]\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n</code></pre>\n<p>I found imdb.get_word_index method(), it returns word index dictionary like {‘create’: 984, ‘make’: 94,…}. For converting, I create index word dictionary.\n\n\n<pre><code>&gt;&gt;&gt; word_index = imdb.get_word_index()\n&gt;&gt;&gt; index_word = {v:k for k,v in word_index.items()}\n</code></pre>\n<p>Then, I tried to restore original text like following.</p>\n<p>\n<pre><code>&gt;&gt;&gt; ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n</code></pre>\n<p>I’m not good at English, but I know this sentence is something strange.</p>\n<p>Why is this happened? How can I restore original text?</p>\n</p></p></p></div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your example is coming out as gibberish, it's much worse than just some missing stop words.</p>\n<p>If you re-read the docs for the <code>start_char</code>, <code>oov_char</code>, and <code>index_from</code> parameters of the [<code>keras.datasets.imdb.load_data</code>](<a href=\"https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\" rel=\"noreferrer\">https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification</a>\n) method they explain what is happening:</p>\n<p><code>start_char</code>: int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.</p>\n<p><code>oov_char</code>: int. words that were cut out because of the num_words or skip_top limit will be replaced with this character.</p>\n<p><code>index_from</code>: int. Index actual words with this index and higher.</p>\n<p>That dictionary you inverted assumes the word indices start from <code>1</code>.</p>\n<p>But the indices returned my keras have <code>&lt;START&gt;</code> and <code>&lt;UNKNOWN&gt;</code> as indexes <code>1</code> and <code>2</code>. (And it assumes you will use <code>0</code> for <code>&lt;PADDING&gt;</code>).</p>\n<p>This works for me:</p>\n<pre><code>import keras\nNUM_WORDS=1000 # only use top 1000 words\nINDEX_FROM=3   # word index offset\n\ntrain,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\ntrain_x,train_y = train\ntest_x,test_y = test\n\nword_to_id = keras.datasets.imdb.get_word_index()\nword_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\nword_to_id[\"&lt;PAD&gt;\"] = 0\nword_to_id[\"&lt;START&gt;\"] = 1\nword_to_id[\"&lt;UNK&gt;\"] = 2\nword_to_id[\"&lt;UNUSED&gt;\"] = 3\n\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in train_x[0] ))\n</code></pre>\n<p>The punctuation is missing, but that's all:</p>\n<pre><code>\"&lt;START&gt; this film was just brilliant casting &lt;UNK&gt; &lt;UNK&gt; story\n direction &lt;UNK&gt; really &lt;UNK&gt; the part they played and you could just\n imagine being there robert &lt;UNK&gt; is an amazing actor ...\"\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can get the original dataset without stop words removed using get_file from  keras.utils.data_utils:</p>\n<pre><code>path = get_file('imdb_full.pkl',\n               origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n                md5_hash='d091312047c43cf9e4e38fef92437263')\nf = open(path, 'rb')\n(training_data, training_labels), (test_data, test_labels) = pickle.load(f)\n</code></pre>\n<p>Credit - Jeremy Howards <a href=\"http://course.fast.ai/lessons/lesson5.html\" rel=\"noreferrer\">fast.ai course lesson 5</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This encoding will work along with the labels:</p>\n<pre><code>from keras.datasets import imdb\n(x_train,y_train),(x_test,y_test) = imdb.load_data()\nword_index = imdb.get_word_index() # get {word : index}\nindex_word = {v : k for k,v in word_index.items()} # get {index : word}\n\nindex = 1\nprint(\" \".join([index_word[idx] for idx in x_train[index]]))\nprint(\"positve\" if y_train[index]==1 else \"negetive\")\n</code></pre>\n<p>Upvote if helps. :)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have started using sckikit-learn for my work. So I was going through the <a href=\"http://scikit-learn.org/stable/tutorial/basic/tutorial.html\" rel=\"noreferrer\">tutorial</a> which gives standard procedure to load some datasets:</p>\n<pre><code>$ python\n&gt;&gt;&gt; from sklearn import datasets\n&gt;&gt;&gt; iris = datasets.load_iris()\n&gt;&gt;&gt; digits = datasets.load_digits()\n</code></pre>\n<p>However, for my convenience, I tried loading the data in the following way:</p>\n<pre><code>In [1]: import sklearn\n\nIn [2]: iris = sklearn.datasets.load_iris()\n</code></pre>\n<p>However, this throws following error:</p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-2-db77d2036db5&gt; in &lt;module&gt;()\n----&gt; 1 iris = sklearn.datasets.load_iris()\n\nAttributeError: 'module' object has no attribute 'datasets'\n</code></pre>\n<p>However, if I use the apparently similar method:</p>\n<pre><code>In [3]: from sklearn import datasets\n\nIn [4]: iris = datasets.load_iris()\n</code></pre>\n<p>It works without problem. In fact the following also works:</p>\n<pre><code>In [5]: iris = sklearn.datasets.load_iris()\n</code></pre>\n<p>I am completely confused about this. Am I missing something very trivial? What is the difference between the two approaches?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>sklearn</code> is a <a href=\"https://docs.python.org/3/glossary.html#term-regular-package\" rel=\"noreferrer\">package</a>. <a href=\"https://stackoverflow.com/a/7948672/3642398\">This answer</a> said it very succinctly:</p>\n<blockquote>\n<p>when you import a package, only variables/functions/classes in the <code>__init__.py</code> file of that package are directly visible, not sub-packages or modules.</p>\n</blockquote>\n<p><code>datasets</code> is a sub-package of <code>sklearn</code>. This is why this happens:</p>\n<pre><code>In [1]: import sklearn\n\nIn [2]: sklearn.datasets\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-2-325a2bfc35d0&gt; in &lt;module&gt;()\n----&gt; 1 sklearn.datasets\n\nAttributeError: module 'sklearn' has no attribute 'datasets'\n</code></pre>\n<p>However, the reason why this works: </p>\n<pre><code>In [3]: from sklearn import datasets\n\nIn [4]: sklearn.datasets\nOut[4]: &lt;module 'sklearn.datasets' from '/home/ethan/.virtualenvs/test3/lib/python3.5/site-packages/sklearn/datasets/__init__.py'&gt;\n</code></pre>\n<p>is that when you load the sub-package <code>datasets</code> by doing <code>from sklearn import datasets</code> it is automatically added to the namespace of the package <code>sklearn</code>. This is one of the lesser-known <a href=\"http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-submodules-are-added-to-the-package-namespace-trap\" rel=\"noreferrer\">\"traps\" of the Python import system</a>.</p>\n<p>Also, note that if you look at the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/__init__.py#L60\" rel=\"noreferrer\"><code>__init__.py</code> for <code>sklearn</code></a> you <em>will</em> see <code>'datasets'</code> as a member of <a href=\"https://docs.python.org/3/tutorial/modules.html#importing-from-a-package\" rel=\"noreferrer\"><code>__all__</code></a>, but this only allows you to do:</p>\n<pre><code>In [1]: from sklearn import *\nIn [2]: datasets\nOut[2]: &lt;module 'sklearn.datasets' from '/home/ethan/.virtualenvs/test3/lib/python3.5/site-packages/sklearn/datasets/__init__.py'&gt;\n</code></pre>\n<p>One last point to note is that if you inspect either <code>sklearn</code> or <code>datasets</code> you will see that, although they are packages, their type is <code>module</code>. This is because all packages are considered modules - however, not all modules are packages.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While digging through the topic of neural networks and how to efficiently train them, I came across the method of using very simple activation functions, such as the <strong>rectified linear unit</strong> (ReLU), instead of the classic smooth <strong>sigmoids</strong>. The ReLU-function is not differentiable at the origin, so according to my understanding the backpropagation algorithm (BPA) is not suitable for training a neural network with ReLUs, since the chain rule of multivariable calculus refers to smooth functions only.\nHowever, none of the papers about using ReLUs that I read address this issue. ReLUs seem to be very effective and seem to be used virtually everywhere while not causing any unexpected behavior. Can somebody explain to me why ReLUs can be trained at all via the backpropagation algorithm?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To understand how backpropagation is even possible with functions like ReLU you need to understand what is the most important property of derivative that makes backpropagation algorithm works so well. This property is that :</p>\n<pre><code>f(x) ~ f(x0) + f'(x0)(x - x0)\n</code></pre>\n<p>If you treat <code>x0</code> as actual value of your parameter at the moment - you can tell (knowing value of a cost function and it's derivative)  how the cost function will behave when you change your parameters a little bit. This is most crucial thing in backpropagation.</p>\n<p>Because of the fact that computing cost function is crucial for a cost computation - you will need your cost function to satisfy the property stated above. It's easy to check that ReLU satisfy this property everywhere except a small neighbourhood of <code>0</code>. And this is the only problem with ReLU - the fact that we cannot use this property when we are close to <code>0</code>.</p>\n<p>To overcome that you may choose the value of ReLU derivative in <code>0</code> to either <code>1</code> or <code>0</code>. On the other hand most of researchers don't treat this problem as serious simply because of the fact, that being close to <code>0</code> during ReLU computations is relatively rare. </p>\n<p>From the above - of course - from the pure mathematical point of view it's not plausible to use ReLU with backpropagation algorithm. On the other hand - in practice it usually doesn't make any difference that it has this weird behaviour around <code>0.</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/28064634/random-state-pseudo-random-number-in-scikit-learn\">Random state (Pseudo-random number) in Scikit learn</a>\n<span class=\"question-originals-answer-count\">\n                                (8 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2021-02-27 00:57:07Z\">3 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>Can someone explain me what <code>random_state</code> means in below example?</p>\n<pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nX, y = np.arange(10).reshape((5, 2)), range(5)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42) \n</code></pre>\n<p>Why is it hard coded to 42?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Isn't that obvious? 42 is the <a href=\"https://en.wikipedia.org/wiki/42_(number)#Popular_culture\" rel=\"noreferrer\">Answer to the Ultimate Question of Life, the Universe, and Everything</a>.</p>\n<p>On a serious note, <code>random_state</code> simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time.</p>\n<p><a href=\"http://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\" rel=\"noreferrer\">Relevant documentation</a>:</p>\n<blockquote>\n<p><strong><em><code>random_state</code></em></strong> : <strong><em><code>int</code>, <code>RandomState</code> instance or <code>None</code>, optional\n  (default=<code>None</code>)</em></strong><br/>\n  If <code>int</code>, <code>random_state</code> is the seed used by the random\n  number generator; If <code>RandomState</code> instance, <code>random_state</code> is the random\n  number generator; If <code>None</code>, the random number generator is the\n  <code>RandomState</code> instance used by <code>np.random</code>.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you don't specify the random_state in the code, then every time you run(execute) your code a new random value is generated and the train and test datasets would have different values each time.</p>\n<p>However, if a fixed value is assigned like random_state = 0 or 1 or 42 or any other integer then no matter how many times you execute your code the result would be the same .i.e, same values in train and test datasets.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits. The random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I cannot seem to get the value of learning rate. What I get is below. </p>\n<p>I've tried the model for 200 epochs and want to see/change the learning rate. Is this not the correct way?</p>\n<pre><code>&gt;&gt;&gt; print(ig_cnn_model.optimizer.lr)\n&lt;tf.Variable 'lr_6:0' shape=() dtype=float32_ref&gt;\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <code>eval()</code> from <code>keras.backend</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(1, input_shape=(1,)))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\n\nprint(K.eval(model.optimizer.lr))\n</code></pre>\n<p>Output:</p>\n<pre><code>0.001\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The best way to get all information related to the optimizer would be with <code>.get_config()</code>.</p>\n<p>Example:</p>\n<pre><code>model.compile(optimizer=optimizerF,\n                  loss=lossF,\n                  metrics=['accuracy'])\n\nmodel.optimizer.get_config()\n\n&gt;&gt;&gt; {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n</code></pre>\n<p>It returns a dict with all information.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can change your learning rate by  </p>\n<pre><code>from keras.optimizers import Adam\n\nmodel.compile(optimizer=Adam(lr=0.001), \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In sklearn.metrics.f1_score, the f1 score has a parameter called \"average\". What does macro, micro, weighted, and samples mean? Please elaborate, because in the documentation, it was not explained properly. Or simply answer the following:</p>\n<ol>\n<li>Why is \"samples\" best parameter for multilabel classification? </li>\n<li>Why is micro best for an imbalanced dataset? </li>\n<li>what's the difference between weighted and macro?</li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The question is about the meaning of the <code>average</code> parameter in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" rel=\"noreferrer\"><code>sklearn.metrics.f1_score</code></a>.</p>\n<p>As you can see from the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/metrics/classification.py#L620\" rel=\"noreferrer\">code</a>:</p>\n<ul>\n<li><code>average=micro</code> says the function to compute f1 by considering total true positives, false negatives and false positives (no matter of the prediction for each label in the dataset)</li>\n<li><code>average=macro</code> says the function to compute f1 for each label, and returns the average without considering the proportion for each label in the dataset.</li>\n<li><code>average=weighted</code> says the function to compute f1 for each label, and returns the average considering the proportion for each label in the dataset.</li>\n<li><code>average=samples</code> says the function to compute f1 for each instance, and returns the average. Use it for multilabel classification.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I found a really helpful article explaining the differences more thoroughly and with examples: <a href=\"https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\" rel=\"noreferrer\">https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1</a></p>\n<p>Unfortunately, it doesn't tackle the 'samples' parameter and I did not experiment with multi-label classification yet, so I'm not able to answer question number 1. As for the others:</p>\n<ol start=\"2\">\n<li><p>Where does this information come from? If I understood the differences correctly, micro is not the best indicator for an imbalanced dataset, but one of the worst since it does not include the proportions. As described in the article, micro-f1 equals accuracy which is a flawed indicator for imbalanced data.\nFor example: The classifier is supposed to identify cat pictures among thousands of random pictures, only 1% of the data set consists of cat pictures (imbalanced data set). Even if it does not identify a single cat picture, it has an accuracy / micro-f1-score of 99%, since 99% of the data was correctly identified as not cat pictures.</p>\n</li>\n<li><p>Trying to put it in a nutshell: Macro is simply the arithmetic mean of the individual scores, while weighted includes the individual sample sizes. I recommend the article for details, I can provide more examples if needed.</p>\n</li>\n</ol>\n<p>I know that the question is quite old, but I hope this helps someone.\nPlease correct me if I'm wrong. I've done some research, but am not an expert.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Here is an example calculation (without the samples option)</strong></p>\n<p>Macro average prefers the \"under-represented\" classes. Therefore encourages the model / algorithm / evaluation to pay attention more to the \"small\" classes, than if they were treated according to how many data points they have, which is what micro-f1 is doing.</p>\n<p>Concrete example:</p>\n<ul>\n<li>Assume: there are two classes of emails (important, junk)</li>\n<li>Also assume your data has 1000 emails</li>\n<li>Also assume the data has only 10 important emails</li>\n</ul>\n<p>Now:</p>\n<p>Let's assume we build an email-classifier that predicts the label \"junk\" all the time.</p>\n<p>I.e.:</p>\n<ul>\n<li>Pred_junk=1000</li>\n<li>Pred_important = 0</li>\n</ul>\n<p>The results would be:</p>\n<ul>\n<li>TP_junk = 990</li>\n<li>TP_important = 0</li>\n<li>FP_junk = 10</li>\n<li>FP_important = 0</li>\n<li>FN_junk = 0</li>\n<li>FN_important = 10</li>\n<li>Recall_junk = TP/ (TP + FN) = 990/990 = 1</li>\n<li>Recall_important = 0/10 = 0</li>\n<li>Precision_junk = TP / (TP + FP) = 990/1000 = 0.99</li>\n<li>Precision_important = UNDEFINDED ( ~&gt; 0/0) can be set to 1</li>\n</ul>\n<p>So now:</p>\n<ul>\n<li><p>F1_junk = 2 * Precision_junk * Recall_junk / (Precision_junk + Recall_junk) =  1.98/1.99 = 0.995</p>\n</li>\n<li><p>F1_important = 0</p>\n</li>\n</ul>\n<p>Now, Macro F1 is:</p>\n<ul>\n<li><strong>F1_Macro</strong> = 1/2 F1_junk + 1/2 F1_important = 0.4975</li>\n</ul>\n<p>Weighted-F1 would be:</p>\n<ul>\n<li><strong>F1_weighted</strong> = 999/1000 F1_junk + 10/1000 F1_important = 0.994</li>\n</ul>\n<p>While Micro F1 is calculated by putting all TP, and FP together</p>\n<ul>\n<li>TP_total = TP_junk + TP_important = 990</li>\n<li>FP_total = FP_junk + FP_important = 10</li>\n<li>FN_total = FP_junk + FP_important = 10</li>\n</ul>\n<p>Micro-F1 is therefore:</p>\n<ul>\n<li>Precision_micro = 990 / 1000 = 0.99</li>\n<li>Recall_micro = 990 /1000 = 0.99</li>\n<li><strong>F1_micro_total</strong> = 0.99</li>\n</ul>\n<p>So, what you can see is that the F1 score gets penalised very heavily in the Macro setting, as it \"weighs\" every class the same regardless of how often it appears in the dataset. In our case this leads to a massive difference in F1 scores, as you can see. Therefore the Macro F1 score is much better to tackle class imbalance, as it penalises the model / algorithm for performing poorly on the under-represented dataset.</p>\n<p>Hope this helps.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2016-01-28 20:24:56Z\">8 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/4743996/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I'm new to machine learning, and for my first project I'd like to write a naive Bayes spam filter. I was wondering if there are any publicly available training sets of labeled spam/not spam emails, preferably in plain text and not a dump of a relational database (unless they pretty-print those?). </p>\n<p>I know such a publicly available database exists for other kinds of text classification, specifically news article text. I just haven't been able to find the same sort of thing for emails.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is what I was looking for: <a href=\"http://untroubled.org/spam/\">http://untroubled.org/spam/</a></p>\n<p>This archive has around a gigabyte of compressed accumulated spam messages dating 1998 - 2011. Now I just need to get non-spam email. So I'll just query my own Gmail for that using the getmail program and the tutorial at <a href=\"http://www.mattcutts.com/blog/backup-gmail-in-linux-with-getmail/\">mattcutts.com</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sure, there's <a href=\"http://www-stat.stanford.edu/~tibs/ElemStatLearn/\" rel=\"noreferrer\">Spambase</a>, which is as far as i'm aware, is the most widely cited spam data set in the machine learning literature.</p>\n<p>I have used this data set many times; each time i am impressed how much effort has been put into the formatting and documentation of this data set.</p>\n<p>A few characteristics of the Spambase set:</p>\n<ul>\n<li><p>4601 data points--all complete</p></li>\n<li><p>each comprised of 58 features\n(attributes)</p></li>\n<li><p>each data point is labelled 'spam' or\n'no spam'</p></li>\n<li><p>approx. 40% are labeled spam</p></li>\n<li><p>of the features, all are continuous\n(vs. discrete)</p></li>\n<li><p>a representative feature: <em>average\ncontinuous sequence of capital\nletters</em></p></li>\n</ul>\n<p><br/>\nSpambase is archived in the <a href=\"http://archive.ics.uci.edu/ml/datasets/Spambase\" rel=\"noreferrer\">UCI Machine Learning Repository</a>; in addition, it's also available on the <a href=\"http://www-stat.stanford.edu/~tibs/ElemStatLearn/\" rel=\"noreferrer\">Website</a> for the excellent ML/Statistical Computation Treatise, <strong><em>Elements of Statistical Learning</em></strong> by Hastie et al.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>SpamAssassin has a <a href=\"https://spamassassin.apache.org/publiccorpus/\">public corpus</a> of both spam and non-spam messages, although it hasn't been updated in a few years. Read the readme.html file to learn what's there.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm following <a href=\"http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\" rel=\"nofollow noreferrer\">this tutorial</a> (section 6: Tying it All Together), with my own dataset. I can get the example in the tutorial working, no problem, with the sample dataset provided.</p>\n<p>I'm getting a binary cross-entropy error that is negative, and no improvements as epochs progress. I'm pretty sure binary cross-entropy should always be positive, and I should see some improvement in the loss. I've truncated the sample output (and code call) below to 5 epochs. Others seem to run into similar problems sometimes when training CNNs, but I didn't see a clear solution in my case. Does anyone know why this is happening?</p>\n<p>Sample output:</p>\n<pre class=\"lang-none prettyprint-override\"><code>Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: GeForce GTX TITAN Black, pci bus id: 0000:84:00.0)\n10240/10240 [==============================] - 2s - loss: -5.5378 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000\nEpoch 2/5\n10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000\nEpoch 3/5\n10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000\nEpoch 4/5\n10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000\nEpoch 5/5\n10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000\n</code></pre>\n<p>My code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndataset = np.loadtxt('train_rows.csv', delimiter=\",\")\ntestset = np.loadtxt('test_rows.csv', delimiter=\",\")\n\n# split into input (X) and output (Y) variables\nX = dataset[:, :62]\nY = dataset[:, 62]\n\nX_test = testset[:, :62]\nY_test = testset[:, 62]\n\n### create model\nmodel = Sequential()\nmodel.add(Dense(100, input_dim=(62,), activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n## Fit the model\nmodel.fit(X, Y, validation_data=(X_test, Y_test), epochs=5, batch_size=128)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I should have printed out my response variable. The categories were labelled as 1 and 2 instead of 0 and 1, which confused the classifier.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're getting negative losses, then the solution is to coerce the target to be 0 or 1. In the OP's case, it was subtracting 1 from the target/label.</p>\n<pre class=\"lang-py prettyprint-override\"><code>Y = np.array([1, 2, 1, 2])\nY -= 1                       # array([0, 1, 0, 1])\n</code></pre>\n<p>Depending on the use case, there are other ways to \"coerce\" the target to be either 1 or 0. One way is a boolean conditional check. For example, the following converts every value greater than 0 to 1 and all others to 0.</p>\n<pre class=\"lang-py prettyprint-override\"><code>Y = np.array([-1, 1, -1, 1])\nY = (Y &gt; 0).astype(int)      # array([0, 1, 0, 1])\n</code></pre>\n<p>Another is <code>clip()</code>, which limits every value to be between 0 and 1. If the labels were 0/255, then it would do the job.</p>\n<pre class=\"lang-py prettyprint-override\"><code>Y = [0, 255, 0, 1]\nY = np.clip(Y, 0, 1)         # array([0, 1, 0, 1])\n</code></pre>\n<hr/>\n<h5>How can the loss be negative?</h5>\n<p>The way <code>binary_crossentropy</code> is implemented in Keras (<a href=\"https://github.com/keras-team/keras/blob/b3ffea6602dbbb481e82312baa24fe657de83e11/keras/losses.py#L2431-L2434\" rel=\"nofollow noreferrer\">1</a>, <a href=\"https://github.com/keras-team/keras/blob/b3ffea6602dbbb481e82312baa24fe657de83e11/keras/backend.py#L5813-L5819\" rel=\"nofollow noreferrer\">2</a>) is to coerce the predicted probabilities (<code>output</code> below) to be in the (0, 1) interval and return the result of the following computation.</p>\n<pre class=\"lang-none prettyprint-override\"><code>bce = target * log(output) + (1 - target) * log(1 - output)\nreturn mean(-bce)\n</code></pre>\n<p>Now, since <code>output</code> is between 0 and 1, <code>log(output)</code> and <code>log(1-output)</code> are both negative. If <code>target</code> is either 0 or 1, <code>bce</code> is negative, so <code>mean(-bce)</code> is a positive number which is the binary cross entropy loss. However, if target is not 0 or 1, this logic breaks down. In particular, if <code>target</code> is greater than 1 and <code>output</code> is sufficiently large, then <code>bce</code> would be positive and <code>mean(-bce)</code> would be negative.</p>\n<hr/>\n<p>A summarized implementation of <code>binary_crossentropy()</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nepsilon_ = tf.keras.backend.epsilon()\ntarget = tf.convert_to_tensor(y_true)\noutput = tf.clip_by_value(output, epsilon_, 1 - epsilon_)\n\nbce = target * tf.math.log(output + epsilon_)\nbce += (1 - target) * tf.math.log(1 - output + epsilon_)\nloss = tf.reduce_mean(-bce)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What are the differences between all these cross-entropy losses?</p>\n<p>Keras is talking about</p>\n<ul>\n<li><strong>Binary cross-entropy</strong></li>\n<li><strong>Categorical cross-entropy</strong></li>\n<li><strong>Sparse categorical cross-entropy</strong></li>\n</ul>\n<p>While TensorFlow has</p>\n<ul>\n<li><strong>Softmax cross-entropy with logits</strong></li>\n<li><strong>Sparse softmax cross-entropy with logits</strong></li>\n<li><strong>Sigmoid cross-entropy with logits</strong></li>\n</ul>\n<p>What are the differences and relationships between them? What are the typical applications for them? What's the mathematical background? Are there other cross-entropy types that one should know? Are there any cross-entropy types without logits?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is just one cross (Shannon) entropy defined as:</p>\n<pre><code>H(P||Q) = - SUM_i P(X=i) log Q(X=i)\n</code></pre>\n<p>In machine learning usage, <code>P</code> is the actual (ground truth) distribution, and <code>Q</code> is the predicted distribution. All the functions you listed are just <strong>helper functions</strong> which accepts different ways to represent <code>P</code> and <code>Q</code>.</p>\n<p>There are basically 3 main things to consider:</p>\n<ul>\n<li><p>there are either 2 possibles outcomes (binary classification) or more. If there are just two outcomes, then <code>Q(X=1) = 1 - Q(X=0)</code> so a single float in (0,1) identifies the whole distribution, this is why neural network in binary classification has a single output (and so does logistic regresssion). If there are K&gt;2 possible outcomes one has to define K outputs (one per each <code>Q(X=...)</code>)</p></li>\n<li><p>one either produces proper probabilities (meaning that <code>Q(X=i)&gt;=0</code> and <code>SUM_i Q(X=i) =1</code> or one just produces a \"score\" and has some fixed method of transforming score to probability. For example a single real number can be \"transformed to probability\" by taking sigmoid, and a set of real numbers can be transformed by taking their softmax and so on.</p></li>\n<li><p>there is <code>j</code> such that <code>P(X=j)=1</code> (there is one \"true class\", targets are \"hard\", like \"this image represent a cat\") or there are \"soft targets\" (like \"we are 60% sure this is a cat, but for 40% it is actually a dog\").</p></li>\n</ul>\n<p>Depending on these three aspects, different helper function should be used:</p>\n<pre><code>                                  outcomes     what is in Q    targets in P   \n-------------------------------------------------------------------------------\nbinary CE                                2      probability         any\ncategorical CE                          &gt;2      probability         soft\nsparse categorical CE                   &gt;2      probability         hard\nsigmoid CE with logits                   2      score               any\nsoftmax CE with logits                  &gt;2      score               soft\nsparse softmax CE with logits           &gt;2      score               hard\n</code></pre>\n<p>In the end one could just use \"categorical cross entropy\", as this is how it is mathematically defined, however since things like hard targets or binary classification are very popular - modern ML libraries do provide these additional helper functions to make things simpler. In particular \"stacking\" sigmoid and cross entropy might be numerically unstable, but if one knows these two operations are applied together - there is a numerically stable version of them combined (which is implemented in TF).</p>\n<p>It is important to notice that if you apply wrong helper function the code will usually still execute, but results will be wrong. For example if you apply softmax_* helper for binary classification with one output your network will be considered to always produce \"True\" at the output.</p>\n<p>As a final note - this answer considers <strong>classification</strong>, it is slightly different when you consider <strong>multi label</strong> case (when a single point can have multiple labels), as then Ps do not sum to 1, and one should use sigmoid_cross_entropy_with_logits despite having multiple output units.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>Logits</h1>\n<p>For this purpose, \"logits\" can be seen as the <strong>non-activated</strong> outputs of the model.</p>\n<ul>\n<li>While <strong>Keras</strong> losses always take an <strong>\"activated\"</strong> output (you must apply \"sigmoid\" or \"softmax\" before the loss)</li>\n<li><strong>Tensorflow</strong> takes them with \"logits\" or <strong>\"non-activated\"</strong> (you should not  apply \"sigmoid\" or \"softmax\" before the loss)</li>\n</ul>\n<p>Losses \"with logits\" will apply the activation internally.\nSome functions allow you to choose <code>logits=True</code> or <code>logits=False</code>, which will tell the function whether to \"apply\" or \"not apply\" the activations.</p>\n<hr/>\n<h1>Sparse</h1>\n<ul>\n<li><strong>Sparse</strong> functions use the target data (ground truth) as \"integer labels\": 0, 1, 2, 3, 4.....</li>\n<li><strong>Non-sparse</strong> functions use the target data as \"one-hot labels\": [1,0,0], [0,1,0], [0,0,1]</li>\n</ul>\n<hr/>\n<h2>Binary crossentropy = Sigmoid crossentropy</h2>\n<ul>\n<li>Problem type:\n<ul>\n<li>single class (false/true); or</li>\n<li><strong>non-exclusive</strong> multiclass (many classes may be correct)</li>\n</ul>\n</li>\n<li>Model output shape: <code>(batch, ..., &gt;=1)</code></li>\n<li>Activation: <code>\"sigmoid\"</code></li>\n</ul>\n<h2>Categorical crossentropy = Softmax crossentropy</h2>\n<ul>\n<li>Problem type: <strong>exclusive</strong> classes (only one class may be correct)</li>\n<li>Model output shape: <code>(batch, ..., &gt;=2)</code></li>\n<li>Activation: <code>\"softmax\"</code></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm new in machine learning. I'm preparing my data for classification using Scikit Learn SVM. In order to select the best features I have used the following method:</p>\n<pre class=\"lang-py prettyprint-override\"><code>SelectKBest(chi2, k=10).fit_transform(A1, A2)\n</code></pre>\n<p>Since my dataset consist of negative values, I get the following error:</p>\n<pre class=\"lang-none prettyprint-override\"><code>ValueError                                Traceback (most recent call last)\n\n/media/5804B87404B856AA/TFM_UC3M/test2_v.py in &lt;module&gt;()\n----&gt; 1 \n      2 \n      3 \n      4 \n      5 \n\n/usr/local/lib/python2.6/dist-packages/sklearn/base.pyc in fit_transform(self, X, y,     **fit_params)\n    427         else:\n    428             # fit method of arity 2 (supervised transformation)\n\n--&gt; 429             return self.fit(X, y, **fit_params).transform(X)\n    430 \n    431 \n\n/usr/local/lib/python2.6/dist-packages/sklearn/feature_selection/univariate_selection.pyc in fit(self, X, y)\n    300         self._check_params(X, y)\n    301 \n--&gt; 302         self.scores_, self.pvalues_ = self.score_func(X, y)\n    303         self.scores_ = np.asarray(self.scores_)\n    304         self.pvalues_ = np.asarray(self.pvalues_)\n\n/usr/local/lib/python2.6/dist-  packages/sklearn/feature_selection/univariate_selection.pyc in chi2(X, y)\n    190     X = atleast2d_or_csr(X)\n    191     if np.any((X.data if issparse(X) else X) &lt; 0):\n--&gt; 192         raise ValueError(\"Input X must be non-negative.\")\n    193 \n    194     Y = LabelBinarizer().fit_transform(y)\n\nValueError: Input X must be non-negative.\n</code></pre>\n<p>Can someone tell me how can I transform my data ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The error message <code>Input X must be non-negative</code> says it all: <a href=\"https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\" rel=\"noreferrer\">Pearson's chi square test (goodness of fit)</a> does not apply to negative values. It's logical because the chi square test assumes frequencies distribution and a frequency can't be a negative number. Consequently, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\" rel=\"noreferrer\"><code>sklearn.feature_selection.chi2</code></a> asserts the input is non-negative.</p>\n<p>You are saying that your features are \"min, max, mean, median and FFT of accelerometer signal\". In many cases, it may be quite safe to simply shift each feature to make it all positive, or even normalize to <code>[0, 1]</code> interval as suggested by EdChum.</p>\n<p>If data transformation is for some reason not possible (e.g. a negative value is an important factor), you should pick another statistic to score your features:</p>\n<ul>\n<li><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif\" rel=\"noreferrer\"><code>sklearn.feature_selection.f_classif</code></a> computes ANOVA f-value</li>\n<li><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif\" rel=\"noreferrer\"><code>sklearn.feature_selection.mutual_info_classif</code></a> computes the mutual information</li>\n</ul>\n<p>Since the whole point of this procedure is to prepare the features for another method, it's not a big deal to pick anyone, the end result usually the same or very close.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I understand F1-measure is a harmonic mean of precision and recall. But what values define how good/bad a F1-measure is? I can't seem to find any references (google or academic) answering my question.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Consider <code>sklearn.dummy.DummyClassifier(strategy='uniform')</code> which is a classifier that make random guesses (a.k.a bad classifier). We can view DummyClassifier as a benchmark to beat, now let's see its f1-score.</p>\n<p>In a binary classification problem, with balanced dataset: 6198 total samples, 3099 samples labelled as <code>0</code> and 3099 samples labelled as <code>1</code>, f1-score is <code>0.5</code> for both classes, and weighted average is <code>0.5</code>:</p>\n<p><a href=\"https://i.sstatic.net/EFdv5m.png\" rel=\"nofollow noreferrer\"><img alt=\"strategy_uniform\" src=\"https://i.sstatic.net/EFdv5m.png\"/></a></p>\n<p>Second example, using <code>DummyClassifier(strategy='constant')</code>, i.e. guessing the same label every time, guessing label <code>1</code> every time in this case, average of f1-scores is <code>0.33</code>, while f1 for label <code>0</code> is <code>0.00</code>:</p>\n<p><a href=\"https://i.sstatic.net/BV5kAm.png\" rel=\"nofollow noreferrer\"><img alt=\"strategy_constant\" src=\"https://i.sstatic.net/BV5kAm.png\"/></a></p>\n<p>I consider these to be bad f1-scores, <strong>given the balanced dataset</strong>.</p>\n<p>PS. summary generated using <code>sklearn.metrics.classification_report</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You did not find any reference for f1 measure range because there is not any range. The F1 measure is a combined matrix of precision and recall. </p>\n<p>Let's say you have two algorithms, one has higher precision and lower recall. By this observation , you can not tell that which algorithm is better, unless until your goal is to maximize precision. </p>\n<p>So, given this ambiguity about how to select superior algorithm among two (one with higher recall and other with higher precision), we use f1-measure to select superior among them. </p>\n<p>f1-measure is a relative term that's why there is no absolute range to define how better your algorithm is. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While constructing each tree in the random forest using bootstrapped samples, for each terminal node, we select m variables at random from p variables to find the best split (p is the total number of features in your data). My questions (for RandomForestRegressor) are:</p>\n<p>1) What does max_features correspond to (m or p or something else)?</p>\n<p>2) Are m variables selected at random from max_features variables (what is the value of m)?</p>\n<p>3) If max_features corresponds to m, then why would I want to set it equal to p for regression (the default)? Where is the randomness with this setting (i.e., how is it different from bagging)?</p>\n<p>Thanks.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Straight from the <a href=\"http://scikit-learn.org/stable/modules/ensemble.html#parameters\" rel=\"noreferrer\">documentation</a>: </p>\n<blockquote>\n<p>[<code>max_features</code>] is the size of the random subsets of features to consider when splitting a node.</p>\n</blockquote>\n<p>So <code>max_features</code> is what you call <em>m</em>. When <code>max_features=\"auto\"</code>, <em>m</em> = <em>p</em> and no feature subset selection is performed in the trees, so the \"random forest\" is actually a bagged ensemble of ordinary regression trees. The docs go on to say that</p>\n<blockquote>\n<p>Empirical good default values are <code>max_features=n_features</code> for regression problems, and <code>max_features=sqrt(n_features)</code> for classification tasks</p>\n</blockquote>\n<p>By setting <code>max_features</code> differently, you'll get a \"true\" random forest.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>@lynnyi, max_features is the number of features that are considered on a per-split level, rather than on the entire decision tree construction. More clear, during the construction of each decision tree, RF will still use all the features (n_features), but it only consider number of \"max_features\" features for node splitting. And the \"max_features\" features are randomly selected from the entire features. You could confirm this by plotting one decision tree from a RF with max_features=1, and check all the nodes of that tree to count the number of features involved.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>max_features is basically the number of features selected at random and without replacement at split. Suppose you have 10 independent columns or features, then max_features=5 will select at random and without replacement 5 features at every split.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'd like to ask everyone a question about how correlated features (variables) affect the classification accuracy of machine learning algorithms. With correlated features I mean a correlation between them and not with the target class (i.e the perimeter and the area of a geometric figure or the level of education and the average income). In my opinion correlated features negatively affect eh accuracy of a classification algorithm, I'd say because the correlation makes one of them useless. Is it truly like this?  Does the problem change with the respect of the classification algorithm type? Any suggestion on papers and lectures are really welcome! Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Correlated features do not affect classification accuracy per se.  The problem in realistic situations is that we have a finite number of training examples with which to train a classifier.  For a fixed number of training examples, increasing the number of features typically increases classification accuracy to a point but as the number of features continue to increase, classification accuracy will eventually decrease because we are then <em>undersampled</em> relative to the large number of features.  To learn more about the implications of this, look at the <a href=\"http://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a>.</p>\n<p>If two numerical features are perfectly correlated, then one doesn't add any additional information (it is determined by the other).  So if the number of features is too high (relative to the training sample size), then it is beneficial to reduce the number of features through a <a href=\"http://en.wikipedia.org/wiki/Feature_extraction\">feature extraction</a> technique (e.g., via <a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\">principal components</a>)</p>\n<p>The effect of correlation does depend on the type of classifier.  Some nonparametric classifiers are less sensitive to correlation of variables (although training time will likely increase with an increase in the number of features).  For statistical methods such as Gaussian maximum likelihood, having too many correlated features relative to the training sample size will render the classifier unusable in the original feature space (the covariance matrix of the sample data becomes singular). </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In general, I'd say the more uncorrelated the features are, the better the classifier performance is going to be. Given a set of highly correlated features, it may be possible to use PCA techniques to make them as orthogonal as possible to improve classifier performance. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n                                As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, <a href=\"/help/reopen-questions\">visit the help center</a> for guidance.\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2012-06-05 18:15:29Z\">12 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>Where can I find some real world typo statistics?  </p>\n<p>I'm trying to match people's input text to internal objects, and people tend to make spelling mistakes.<br/>\nThere are 2 kinds of mistakes:  </p>\n<ol>\n<li><code>typos</code> - \"Helllo\" instead of \"Hello\" / \"Satudray\" instead of \"Saturday\" etc.  </li>\n<li><code>Spelling</code> - \"Shikago\" instead of \"Chicago\" </li>\n</ol>\n<p>I use  <a href=\"http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\" rel=\"noreferrer\">Damerau-Levenshtein distance</a> for the typos and <a href=\"http://en.wikipedia.org/wiki/Double_Metaphone\" rel=\"noreferrer\">Double Metaphone</a> for spelling (Python implementations <a href=\"http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\" rel=\"noreferrer\">here</a> and <a href=\"http://atomboy.isa-geek.com/plone/Members/acoil/programing/double-metaphone/metaphone.py/view\" rel=\"noreferrer\">here</a>).</p>\n<p>I want to focus on the Damerau-Levenshtein (or simply <code>edit-distance</code>). The textbook implementations always use '1' for the weight of deletions, insertions substitutions and transpositions. While this is simple and allows for nice algorithms it doesn't match \"reality\" / \"real-world probabilities\".  </p>\n<p>Examples:   </p>\n<ul>\n<li>I'm sure the likelihood of \"Helllo\" (\"Hello\") is greater than \"Helzlo\", yet they are both 1 edit distance away.</li>\n<li>\"Gello\" is closer than \"Qello\" to \"Hello\" on a QWERTY keyboard.</li>\n<li>Unicode transliterations: What is the \"real\" distance between \"München\" and \"Munchen\"?</li>\n</ul>\n<p>What should the \"real world\" weights be for deletions, insertions, substitutions, and transpositions?  </p>\n<p>Even <a href=\"http://norvig.com/spell-correct.html\" rel=\"noreferrer\">Norvig's very cool spell corrector</a> uses non-weighted edit distance.</p>\n<p>BTW- I'm sure the weights need to be functions and not simple floats (per the above \nexamples)...</p>\n<p>I can adjust the algorithm, but where can I \"learn\" these weights? I don't have access to <a href=\"http://www.google.com/jobs/britney.html\" rel=\"noreferrer\">Google-scale data</a>...  </p>\n<p>Should I just guess them?</p>\n<p><strong>EDIT - trying to answer user questions:</strong></p>\n<ul>\n<li>My current non-weighted algorithm fails often when faced with typos for the above reasons. \"Return on Tursday\": every \"real person\" can easily tell Thursday is more likely than Tuesday, yet they are both 1-edit-distance away! (Yes, I do log and measure my performance).</li>\n<li>I'm developing an NLP Travel Search engine, so my dictionary contains ~25K destinations (expected to grow to 100K), Time Expressions ~200 (expected 1K), People expressions ~100 (expected 300), Money Expressions ~100 (expected 500), \"glue logic words\" (\"from\", \"beautiful\", \"apartment\") ~2K (expected 10K) and so on...</li>\n<li>Usage of the edit distance is different for each of the above word-groups. I try to \"auto-correct when obvious\", e.g. 1 edit distance away from only 1 other word in the dictionary. I have <strong>many</strong> other hand-tuned rules, e.g. Double Metaphone fix which is not more than 2 edit distance away from a dictionary word with a length &gt; 4... The list of rules continues to grow as I learn from real world input.</li>\n<li>\"How many pairs of dictionary entries are within your threshold?\": well, that depends on the \"fancy weighting system\" and on real world (future) input, doesn't it? Anyway, I have extensive unit tests so that every change I make to the system only makes it better (based on past inputs, of course). Most sub-6 letter words are within 1 edit distance from a word that is 1 edit distance away from another dictionary entry.</li>\n<li>Today when there are 2 dictionary entries at the same distance from the input I try to apply various statistics to better guess which the user meant (e.g. Paris, France is more likely to show up in my search than Pārīz, Iran).</li>\n<li>The cost of choosing a wrong word is returning semi-random (often ridiculous) results  to the end-user and potentially losing a customer. The cost of not understanding is slightly less expensive: the user will be asked to rephrase.</li>\n<li>Is the cost of complexity worth it? Yes, I'm sure it is. You would not believe the amount of typos people throw at the system and expect it to understand, and I could sure use the boost in <a href=\"http://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">Precision and Recall</a>.</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Possible source for real world typo statistics would be in the <strong>Wikipedia's complete edit history</strong>. </p>\n<p><a href=\"http://download.wikimedia.org/\" rel=\"noreferrer\">http://download.wikimedia.org/</a></p>\n<p>Also, you might be interested in the AWB's RegExTypoFix</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Wikipedia:AWB/T\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Wikipedia:AWB/T</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would advise you to check the <a href=\"http://www.postgresql.org/docs/8.3/interactive/pgtrgm.html\" rel=\"nofollow noreferrer\">trigram alogrithm</a>. In my opinion it works better for finding typos then edit distance algorithm. It should work faster as well and if you keep dictionary in postgres database you can make use of index.</p>\n<p>You may find useful stackoverflow <a href=\"https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work\">topic</a> about google \"Did you mean\"</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"http://publication.wilsonwong.me/paper/233281639.pdf\" rel=\"noreferrer\">Probability Scoring for Spelling Correction</a> by Church and Gale might help.  In that paper, the authors model typos as a noisy channel between the author and the computer.  The appendix has tables for typos seen in a corpus of Associated Press publications.  There is a table for each of the following kinds of typos:</p>\n<ul>\n<li>deletion</li>\n<li>insertion</li>\n<li>substitution</li>\n<li>transposition</li>\n</ul>\n<p>For example, examining the insertion table, we can see that <em>l</em> was incorrectly inserted after <em>l</em>  128 times (the highest number in that column).  Using these tables, you can generate the probabilities you're looking for.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there a way to plot a decision tree in a Jupyter Notebook, such that I can interactively explore its nodes? I am thinking about something like this <a href=\"https://i.sstatic.net/MQCT6.png\" rel=\"noreferrer\"><img alt=\"dt\" src=\"https://i.sstatic.net/MQCT6.png\"/></a>. This is an example from KNIME.</p>\n<p>I have found <a href=\"https://planspace.org/20151129-see_sklearn_trees_with_d3/\" rel=\"noreferrer\">https://planspace.org/20151129-see_sklearn_trees_with_d3/</a> and <a href=\"https://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f\" rel=\"noreferrer\">https://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f</a> and I know you can run d3 in Jupyter, but I have not found any packages, that do that.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Updated Answer with collapsible graph using d3js in Jupyter Notebook</strong> </p>\n<p><strong>Start of 1st cell in notebook</strong> </p>\n<pre><code>%%html\n&lt;div id=\"d3-example\"&gt;&lt;/div&gt;\n&lt;style&gt;\n\n.node circle {\n  cursor: pointer;\n  stroke: #3182bd;\n  stroke-width: 1.5px;\n}\n\n.node text {\n  font: 10px sans-serif;\n  pointer-events: none;\n  text-anchor: middle;\n}\n\nline.link {\n  fill: none;\n  stroke: #9ecae1;\n  stroke-width: 1.5px;\n}\n&lt;/style&gt;\n</code></pre>\n<p><strong>End of 1st cell in notebook</strong></p>\n<p><strong>Start of 2nd cell in notebook</strong></p>\n<pre><code>%%javascript\n// We load the d3.js library from the Web.\nrequire.config({paths:\n    {d3: \"http://d3js.org/d3.v3.min\"}});\nrequire([\"d3\"], function(d3) {\n  // The code in this block is executed when the\n  // d3.js library has been loaded.\n\n  // First, we specify the size of the canvas\n  // containing the visualization (size of the\n  // &lt;div&gt; element).\n  var width = 960,\n    height = 500,\n    root;\n\n  // We create a color scale.\n  var color = d3.scale.category10();\n\n  // We create a force-directed dynamic graph layout.\n//   var force = d3.layout.force()\n//     .charge(-120)\n//     .linkDistance(30)\n//     .size([width, height]);\n    var force = d3.layout.force()\n    .linkDistance(80)\n    .charge(-120)\n    .gravity(.05)\n    .size([width, height])\n    .on(\"tick\", tick);\nvar svg = d3.select(\"body\").append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height);\n\nvar link = svg.selectAll(\".link\"),\n    node = svg.selectAll(\".node\");\n\n  // In the &lt;div&gt; element, we create a &lt;svg&gt; graphic\n  // that will contain our interactive visualization.\n var svg = d3.select(\"#d3-example\").select(\"svg\")\n  if (svg.empty()) {\n    svg = d3.select(\"#d3-example\").append(\"svg\")\n          .attr(\"width\", width)\n          .attr(\"height\", height);\n  }\nvar link = svg.selectAll(\".link\"),\n    node = svg.selectAll(\".node\");\n  // We load the JSON file.\n  d3.json(\"graph2.json\", function(error, json) {\n    // In this block, the file has been loaded\n    // and the 'graph' object contains our graph.\n if (error) throw error;\nelse\n    test(1);\nroot = json;\n      test(2);\n      console.log(root);\n  update();\n\n\n\n  });\n    function test(rr){console.log('yolo'+String(rr));}\n\nfunction update() {\n    test(3);\n  var nodes = flatten(root),\n      links = d3.layout.tree().links(nodes);\n\n  // Restart the force layout.\n  force\n      .nodes(nodes)\n      .links(links)\n      .start();\n\n  // Update links.\n  link = link.data(links, function(d) { return d.target.id; });\n\n  link.exit().remove();\n\n  link.enter().insert(\"line\", \".node\")\n      .attr(\"class\", \"link\");\n\n  // Update nodes.\n  node = node.data(nodes, function(d) { return d.id; });\n\n  node.exit().remove();\n\n  var nodeEnter = node.enter().append(\"g\")\n      .attr(\"class\", \"node\")\n      .on(\"click\", click)\n      .call(force.drag);\n\n  nodeEnter.append(\"circle\")\n      .attr(\"r\", function(d) { return Math.sqrt(d.size) / 10 || 4.5; });\n\n  nodeEnter.append(\"text\")\n      .attr(\"dy\", \".35em\")\n      .text(function(d) { return d.name; });\n\n  node.select(\"circle\")\n      .style(\"fill\", color);\n}\n    function tick() {\n  link.attr(\"x1\", function(d) { return d.source.x; })\n      .attr(\"y1\", function(d) { return d.source.y; })\n      .attr(\"x2\", function(d) { return d.target.x; })\n      .attr(\"y2\", function(d) { return d.target.y; });\n\n  node.attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n}\n          function color(d) {\n  return d._children ? \"#3182bd\" // collapsed package\n      : d.children ? \"#c6dbef\" // expanded package\n      : \"#fd8d3c\"; // leaf node\n}\n      // Toggle children on click.\nfunction click(d) {\n  if (d3.event.defaultPrevented) return; // ignore drag\n  if (d.children) {\n    d._children = d.children;\n    d.children = null;\n  } else {\n    d.children = d._children;\n    d._children = null;\n  }\n  update();\n}\n    function flatten(root) {\n  var nodes = [], i = 0;\n\n  function recurse(node) {\n    if (node.children) node.children.forEach(recurse);\n    if (!node.id) node.id = ++i;\n    nodes.push(node);\n  }\n\n  recurse(root);\n  return nodes;\n}\n\n});\n</code></pre>\n<p><strong>End of 2nd cell in notebook</strong> </p>\n<p><strong>Contents of graph2.json</strong></p>\n<pre><code>   {\n \"name\": \"flare\",\n \"children\": [\n  {\n   \"name\": \"analytics\"\n    },\n    {\n   \"name\": \"graph\"\n    }\n   ]\n}\n</code></pre>\n<p><strong>The graph</strong>\n<a href=\"https://i.sstatic.net/V0umO.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/V0umO.png\"/></a></p>\n<p>Click on flare, which is the root node, the other nodes will collapse</p>\n<p><a href=\"https://i.sstatic.net/mwkwG.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/mwkwG.png\"/></a></p>\n<p><strong>Github repository for notebook used here</strong>: <a href=\"https://github.com/mohdkashif93/Collapsible-tree-in-Ipython-notebook\" rel=\"noreferrer\">Collapsible tree in ipython notebook</a></p>\n<p><strong>References</strong></p>\n<ul>\n<li><a href=\"https://bl.ocks.org/mbostock/1093130\" rel=\"noreferrer\">Collapsible graph in d3.js</a></li>\n<li><a href=\"https://ipython-books.github.io/64-visualizing-a-networkx-graph-in-the-notebook-with-d3js/\" rel=\"noreferrer\">Networkx graph in notebook using d3.js</a></li>\n</ul>\n<p><strong>Old Answer</strong></p>\n<p>I found <a href=\"https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084\" rel=\"noreferrer\">this tutorial here</a> for interactive visualization of Decision Tree in Jupyter Notebook.  </p>\n<p><strong>Install graphviz</strong></p>\n<p>There are 2 steps for this :\nStep 1: Install graphviz for python using pip</p>\n<pre><code>pip install graphviz\n</code></pre>\n<p>Step 2: Then you have to install graphviz seperately. Check this <a href=\"https://graphviz.gitlab.io/download/\" rel=\"noreferrer\">link</a>.\nThen based on your system OS you need to set the path accordingly:</p>\n<p>For windows and Mac OS <a href=\"https://enterprise-architecture.org/downloads?id=208\" rel=\"noreferrer\">check this link</a>. \nFor Linux/Ubuntu <a href=\"https://serverfault.com/questions/575239/error-unable-to-find-dot-command-of-the-graphviz-package\">check this link</a> </p>\n<p><strong>Install ipywidgets</strong></p>\n<p>Using pip</p>\n<pre><code>pip install ipywidgets\njupyter nbextension enable --py widgetsnbextension\n</code></pre>\n<p>Using conda </p>\n<pre><code>conda install -c conda-forge ipywidgets\n</code></pre>\n<p>Now for the code</p>\n<pre><code>from IPython.display import SVG\nfrom graphviz import Source\nfrom sklearn.datasets load_iris\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import tree\nfrom ipywidgets import interactive\nfrom IPython.display import display                               \n</code></pre>\n<p><strong>Load the dataset, say for instance iris dataset in this case</strong></p>\n<pre><code>data = load_iris()\n\n#Get the feature matrix\nfeatures = data.data\n\n#Get the labels for the sampels\ntarget_label = data.target\n\n#Get feature names\nfeature_names = data.feature_names\n</code></pre>\n<p>**Function to plot the decision tree **</p>\n<pre><code>def plot_tree(crit, split, depth, min_split, min_leaf=0.17):\n    classifier = DecisionTreeClassifier(random_state = 123, criterion = crit, splitter = split, max_depth = depth, min_samples_split=min_split, min_samples_leaf=min_leaf)\n    classifier.fit(features, target_label)\n\n    graph = Source(tree.export_graphviz(classifier, out_file=None, feature_names=feature_names, class_names=['0', '1', '2'], filled = True))\n\n    display(SVG(graph.pipe(format='svg')))\nreturn classifier\n</code></pre>\n<p><strong>Call the function</strong></p>\n<pre><code>decision_plot = interactive(plot_tree, crit = [\"gini\", \"entropy\"], split = [\"best\", \"random\"]  , depth=[1, 2, 3, 4, 5, 6, 7], min_split=(0.1,1), min_leaf=(0.1,0.2,0.3,0.5))\n\ndisplay(decision_plot)\n</code></pre>\n<p>You will get the following the graph\n<a href=\"https://i.sstatic.net/yZHu2.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/yZHu2.png\"/></a></p>\n<p><strong>You can change the parameters interactively in the output cell by the chnaging the following values</strong></p>\n<p><a href=\"https://i.sstatic.net/jGAlE.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/jGAlE.png\"/></a></p>\n<p><strong>Another decision tree on the same data but different parameters</strong>\n<a href=\"https://i.sstatic.net/jn9MG.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/jn9MG.png\"/></a></p>\n<p>References :</p>\n<ul>\n<li><a href=\"https://medium.com/@daphne.sive/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084\" rel=\"noreferrer\">Using ipywidgets to plot interactive decision trees</a></li>\n<li><a href=\"https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\" rel=\"noreferrer\">Plotting decision trees in python</a></li>\n<li><a href=\"https://ipywidgets.readthedocs.io/en/latest/\" rel=\"noreferrer\">ipywidgets</a></li>\n<li><a href=\"https://github.com/ContinuumIO/anaconda-issues/issues/1666\" rel=\"noreferrer\">In case you get issues with Graphviz</a></li>\n<li><a href=\"https://github.com/scikit-learn/scikit-learn/issues/6261\" rel=\"noreferrer\">scikit-learn issue :Improve decision tree plotting in Jupyter environment  </a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>1.</strong> In case you simply want to use D3 in Jupyter, here is a tutorial: <a href=\"https://medium.com/@stallonejacob/d3-in-juypter-notebook-685d6dca75c8\" rel=\"noreferrer\">https://medium.com/@stallonejacob/d3-in-juypter-notebook-685d6dca75c8</a></p>\n<p><a href=\"https://i.sstatic.net/B8Az8.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/B8Az8.png\"/></a></p>\n<p><a href=\"https://i.sstatic.net/P6D8G.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/P6D8G.png\"/></a></p>\n<p><strong>2.</strong> For building an interactive decision tree, here is another interesting GUI toolkit called the TMVAGui.</p>\n<p>In this the code is just one-liner:\n<code>factory.DrawDecisionTree(dataset, \"BDT\")</code></p>\n<p><a href=\"https://indico.cern.ch/event/572131/contributions/2315243/attachments/1343269/2023816/gsoc16_4thpresentation.pdf\" rel=\"noreferrer\">https://indico.cern.ch/event/572131/contributions/2315243/attachments/1343269/2023816/gsoc16_4thpresentation.pdf</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is a module called pydot. You can create graphs and add edges to make a decision tree. </p>\n<pre><code>import pydot # \n\ngraph = pydot.Dot(graph_type='graph')\nedge1 = pydot.Edge('1', '2', label = 'edge1')\nedge2 = pydot.Edge('1', '3', label = 'edge2')\ngraph.add_edge(edge1)\ngraph.add_edge(edge2)\n\ngraph.write_png('my_graph.png')\n</code></pre>\n<p>This is an example that would output a png file of your decision tree. Hope this helps!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using XGBoost with Python and have successfully trained a model using the XGBoost <code>train()</code> function called on <code>DMatrix</code> data. The matrix was created from a Pandas dataframe, which has feature names for the columns.</p>\n<pre><code>Xtrain, Xval, ytrain, yval = train_test_split(df[feature_names], y, \\\n                                    test_size=0.2, random_state=42)\ndtrain = xgb.DMatrix(Xtrain, label=ytrain)\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=60, \\\n                  early_stopping_rounds=50, maximize=False, verbose_eval=10)\n\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nxgb.plot_importance(model, max_num_features=5, ax=ax)\n</code></pre>\n<p>I want to now see the feature importance using the <code>xgboost.plot_importance()</code> function, but the resulting plot doesn't show the feature names. Instead, the features are listed as <code>f1</code>, <code>f2</code>, <code>f3</code>, etc. as shown below.</p>\n<p><a href=\"https://i.sstatic.net/zhehV.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zhehV.png\"/></a></p>\n<p>I think the problem is that I converted my original Pandas data frame into a DMatrix. How can I associate feature names properly so that the feature importance plot shows them?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you're using the scikit-learn wrapper you'll need to access the underlying XGBoost Booster and set the feature names on it, instead of the scikit model, like so:</p>\n<pre><code>model = joblib.load(\"your_saved.model\")\nmodel.get_booster().feature_names = [\"your\", \"feature\", \"name\", \"list\"]\nxgboost.plot_importance(model.get_booster())\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You want to use the <code>feature_names</code> parameter when creating your <code>xgb.DMatrix</code></p>\n<pre><code>dtrain = xgb.DMatrix(Xtrain, label=ytrain, feature_names=feature_names)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>train_test_split</code> will convert the dataframe to numpy array which dont have columns information anymore.</p>\n<p>Either you can do what @piRSquared suggested and pass the features as a parameter to DMatrix constructor. Or else, you can convert the numpy array returned from the <code>train_test_split</code> to a Dataframe and then use your code.</p>\n<pre><code>Xtrain, Xval, ytrain, yval = train_test_split(df[feature_names], y, \\\n                                    test_size=0.2, random_state=42)\n\n# See below two lines\nX_train = pd.DataFrame(data=Xtrain, columns=feature_names)\nXval = pd.DataFrame(data=Xval, columns=feature_names)\n\ndtrain = xgb.DMatrix(Xtrain, label=ytrain)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am new to TensorFlow. While I am reading the existing documentation, I found the term <code>tensor</code> really confusing. Because of it, I need to clarify the following questions:</p>\n<ol>\n<li>What is the relationship between <code>tensor</code> and <code>Variable</code>, <code>tensor</code><br/>\nvs. <code>tf.constant</code>, 'tensor' vs. <code>tf.placeholder</code>?</li>\n<li>Are they all types of tensors?</li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>TensorFlow doesn't have first-class Tensor objects, meaning that there are no notion of <code>Tensor</code> in the underlying graph that's executed by the runtime. Instead the graph consists of op nodes connected to each other, representing operations. An operation allocates memory for its outputs, which are available on endpoints <code>:0</code>, <code>:1</code>, etc, and you can think of each of these endpoints as a <code>Tensor</code>. If you have <code>tensor</code> corresponding to <code>nodename:0</code> you can fetch its value as <code>sess.run(tensor)</code> or <code>sess.run('nodename:0')</code>. Execution granularity happens at operation level, so the <code>run</code> method will execute op which will compute all of the endpoints, not just the <code>:0</code> endpoint. It's possible to have an Op node with no outputs (like <code>tf.group</code>) in which case there are no tensors associated with it. It is not possible to have tensors without an underlying Op node.</p>\n<p>You can examine what happens in underlying graph by doing something like this</p>\n<pre><code>tf.reset_default_graph()\nvalue = tf.constant(1)\nprint(tf.get_default_graph().as_graph_def())\n</code></pre>\n<p>So with <code>tf.constant</code> you get a single operation node, and you can fetch it using <code>sess.run(\"Const:0\")</code> or <code>sess.run(value)</code></p>\n<p>Similarly, <code>value=tf.placeholder(tf.int32)</code> creates a regular node with name <code>Placeholder</code>, and you could feed it as <code>feed_dict={\"Placeholder:0\":2}</code> or <code>feed_dict={value:2}</code>. You can not feed and fetch a placeholder in the same <code>session.run</code> call, but you can see the result by attaching a <code>tf.identity</code> node on top and fetching that.</p>\n<p>For variable</p>\n<pre><code>tf.reset_default_graph()\nvalue = tf.Variable(tf.ones_initializer()(()))\nvalue2 = value+3\nprint(tf.get_default_graph().as_graph_def())\n</code></pre>\n<p>You'll see that it creates two nodes <code>Variable</code> and <code>Variable/read</code>, the <code>:0</code> endpoint is a valid value to fetch on both of these nodes. However <code>Variable:0</code> has a special <code>ref</code> type meaning it can be used as an input to mutating operations. The result of Python call <code>tf.Variable</code> is a Python <code>Variable</code> object and there's some Python magic to substitute <code>Variable/read:0</code> or <code>Variable:0</code> depending on whether mutation is necessary. Since most ops have only 1 endpoint, <code>:0</code> is dropped. Another example is <code>Queue</code> -- <code>close()</code> method will create a new <code>Close</code> op node which connects to <code>Queue</code> op. To summarize -- operations on python objects like <code>Variable</code> and <code>Queue</code> map to different underlying TensorFlow op nodes depending on usage. </p>\n<p>For ops like <code>tf.split</code> or <code>tf.nn.top_k</code> which create nodes with multiple endpoints, Python's <code>session.run</code> call automatically wraps output in <code>tuple</code> or <code>collections.namedtuple</code> of <code>Tensor</code> objects which can be fetched individually.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the <a href=\"https://www.tensorflow.org/versions/r0.9/resources/glossary.html#glossary\" rel=\"noreferrer\">glossary</a>:</p>\n<blockquote>\n<p>A Tensor is a typed multi-dimensional array. For example, a 4-D array of floating point numbers representing a mini-batch of images with dimensions [batch, height, width, channel].</p>\n</blockquote>\n<p>Basically, every <strong>data</strong> is a Tensor in TensorFlow (hence the name):</p>\n<ul>\n<li>placeholders are Tensors to which you can feed a value (with the <code>feed_dict</code> argument in <code>sess.run()</code>)</li>\n<li>Variables are Tensors which you can update (with <code>var.assign()</code>). Technically speaking, <code>tf.Variable</code> is not a subclass of <code>tf.Tensor</code> though</li>\n<li><code>tf.constant</code> is just the most basic Tensor, which contains a fixed value given when you create it</li>\n</ul>\n<hr/>\n<p>However, in the graph, every node is an operation, which can have Tensors as inputs or outputs.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As already mentioned by others, yes they are all tensors.</p>\n<p>The way I understood those is to first visualize and understand 1D, 2D, 3D, 4D, 5D, and 6D tensors as in the picture below. (source: <em>knoldus</em>)</p>\n<p><a href=\"https://i.sstatic.net/Lv1qU.jpg\" rel=\"noreferrer\"><img alt=\"tensor-definition\" src=\"https://i.sstatic.net/Lv1qU.jpg\"/></a></p>\n<p>Now, in the context of TensorFlow, you can imagine a computation graph like the one below,</p>\n<p><a href=\"https://i.sstatic.net/NvQN8.png\" rel=\"noreferrer\"><img alt=\"computation-graph\" src=\"https://i.sstatic.net/NvQN8.png\"/></a></p>\n<p>Here, the <code>Op</code>s take two tensors <code>a</code> and <code>b</code> as <em>input</em>; <em>multiplies</em> the tensors with itself and then <em>adds</em> the result of these multiplications to produce the result tensor <code>t3</code>. And these <em>multiplications</em> and <em>addition</em> <code>Op</code>s happen at the nodes in the computation graph.</p>\n<p>And these tensors <code>a</code> and <code>b</code> can be constant tensors, Variable tensors, or placeholders. It doesn't matter, as long as they are of the same <em>data type</em> and compatible shapes(or <code>broadcast</code>able to it) to achieve the operations.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have trained an XGBoostRegressor model. When I have to use this trained model for predicting for a new input, the predict() function throws a feature_names mismatch error, although the input feature vector has the same structure as the training data.</p>\n<p>Also, in order to build the feature vector in the same structure as the training data, I am doing a lot inefficient processing such as adding new empty columns (if data does not exist) and then rearranging the data columns so that it matches with the training structure. Is there a better and cleaner way of formatting the input so that it matches the training structure?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is the case where the order of column-names while model building is different from order of column-names while model scoring.</p>\n<p>I have used the following steps to overcome this error</p>\n<p>First load the pickle file</p>\n<pre><code>model = pickle.load(open(\"saved_model_file\", \"rb\"))\n</code></pre>\n<p>extraxt all the columns with order in which they were used </p>\n<pre><code>cols_when_model_builds = model.get_booster().feature_names\n</code></pre>\n<p>reorder the pandas dataframe</p>\n<pre><code>pd_dataframe = pd_dataframe[cols_when_model_builds]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try converting data into ndarray before passing it to fit/predict.\nFor eg:\nif your train data is train_df and test data is test_df. Use below code:</p>\n<pre><code>train_x = train_df.values\ntest_x = test_df.values\n</code></pre>\n<p>Now fit the model:</p>\n<pre><code>xgb.fit(train_x,train_y)\n</code></pre>\n<p>Finally, predict:</p>\n<pre><code>pred = xgb.predict(test_x)\n</code></pre>\n<p>Hope this helps!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From what I could find, the predict function does not take the DataFrame (or a sparse matrix) as input. It is one of the bugs which can be found here <a href=\"https://github.com/dmlc/xgboost/issues/1238\" rel=\"noreferrer\">https://github.com/dmlc/xgboost/issues/1238</a></p>\n<p>In order to get around this issue, use as_matrix() function in case of a DataFrame or toarray() in case of a sparse matrix.</p>\n<p>This is the only workaround till the bug is fixed or the feature is implemented in a different manner.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a tensor of pictures, and would like to randomly select from it. I'm looking for the equivalent of <code>np.random.choice()</code>. </p>\n<pre><code>import torch\n\npictures = torch.randint(0, 256, (1000, 28, 28, 3))\n</code></pre>\n<p>Let's say I want 10 of these pictures.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>torch</code> has no equivalent implementation of <code>np.random.choice()</code>, see the discussion <a href=\"https://github.com/pytorch/pytorch/issues/16897\" rel=\"noreferrer\">here</a>. The alternative is indexing with a shuffled index or random integers.</p>\n<h2>To do it <em>with</em> replacement:</h2>\n<ol>\n<li>Generate <em>n</em> random indices</li>\n<li>Index your original tensor with these indices </li>\n</ol>\n<pre><code>pictures[torch.randint(len(pictures), (10,))]  \n</code></pre>\n<h2>To do it <em>without</em> replacement:</h2>\n<ol>\n<li>Shuffle the index</li>\n<li>Take the <em>n</em> first elements</li>\n</ol>\n<pre><code>indices = torch.randperm(len(pictures))[:10]\n\npictures[indices]\n</code></pre>\n<p>Read more about <a href=\"https://pytorch.org/docs/stable/torch.html#torch.randint\" rel=\"noreferrer\"><code>torch.randint</code></a> and <a href=\"https://pytorch.org/docs/stable/torch.html#torch.randperm\" rel=\"noreferrer\"><code>torch.randperm</code></a>. Second code snippet is inspired by this <a href=\"https://discuss.pytorch.org/t/torch-equivalent-of-numpy-random-choice/16146/2\" rel=\"noreferrer\">post</a> in PyTorch Forums.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://pytorch.org/docs/master/generated/torch.multinomial.html#torch.multinomial\" rel=\"noreferrer\"><code>torch.multinomial</code></a> provides equivalent behaviour to numpy's <a href=\"https://discuss.pytorch.org/t/torch-equivalent-of-numpy-random-choice/16146/13\" rel=\"noreferrer\"><code>random.choice</code></a> (including sampling with/without replacement):</p>\n<pre class=\"lang-py prettyprint-override\"><code># Uniform weights for random draw\nunif = torch.ones(pictures.shape[0])\n\nidx = unif.multinomial(10, replacement=True)\nsamples = pictures[idx]\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>samples.shape\n&gt;&gt;&gt; torch.Size([10, 28, 28, 3])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For this size of tensor:</p>\n<pre class=\"lang-py prettyprint-override\"><code>N, D = 386363948, 2\nk = 190973\nvalues = torch.randn(N, D)\n</code></pre>\n<p>The following code works fairly fast. It takes around 0.2s:</p>\n<pre class=\"lang-py prettyprint-override\"><code>indices = torch.tensor(random.sample(range(N), k))\nindices = torch.tensor(indices)\nsampled_values = values[indices]\n</code></pre>\n<p>Using <code>torch.randperm</code>, however, would take more than 20s:</p>\n<pre class=\"lang-py prettyprint-override\"><code>sampled_values = values[torch.randperm(N)[:k]]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have performed GaussianNB classification using sklearn. I tried to calculate the metrics using the following code:</p>\n<pre><code>print accuracy_score(y_test, y_pred)\nprint precision_score(y_test, y_pred)\n</code></pre>\n<p>Accuracy score is working correctly but precision score calculation is showing error as:</p>\n<blockquote>\n<p>ValueError: Target is multiclass but average='binary'. Please choose another average setting.</p>\n</blockquote>\n<p>As target is multiclass, can i have the metric scores of precision, recall etc.?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The function call <code>precision_score(y_test, y_pred)</code> is equivalent to <code>precision_score(y_test, y_pred, pos_label=1, average='binary')</code>.\nThe documentation (<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a>) tells us: </p>\n<blockquote>\n<p>'binary':</p>\n<p>Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.</p>\n</blockquote>\n<p>So the problem is that your labels are not binary, but probably one-hot encoded. Fortunately, there are other options which should work with your data: </p>\n<p><code>precision_score(y_test, y_pred, average=None)</code> will return the precision scores for each class, while </p>\n<p><code>precision_score(y_test, y_pred, average='micro')</code> will return the total ratio \nof tp/(tp + fp)</p>\n<p>The <code>pos_label</code> argument will be ignored if you choose another <code>average</code> option than <code>binary</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I tried to implement a weighted binary crossentropy with Keras, but I am not sure if the code is correct. The training output seems to be a bit confusing. After a few epochs I just get an accuracy of ~0.15. I think thats much too less (even for a random guess).</p>\n<p>There are in general about 11% ones in the output and 89% zeros, therefore the weights are w_zero=0.89 and w_one=0.11.</p>\n<p>My code:</p>\n<pre class=\"lang-python prettyprint-override\"><code>def create_weighted_binary_crossentropy(zero_weight, one_weight):\n\n    def weighted_binary_crossentropy(y_true, y_pred):\n\n        # Original binary crossentropy (see losses.py):\n        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n\n        # Calculate the binary crossentropy\n        b_ce = K.binary_crossentropy(y_true, y_pred)\n\n        # Apply the weights\n        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n        weighted_b_ce = weight_vector * b_ce\n\n        # Return the mean error\n        return K.mean(weighted_b_ce)\n\n    return weighted_binary_crossentropy\n</code></pre>\n<p>Maybe someone sees whats wrong?</p>\n<p>Thank you</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\" rel=\"noreferrer\">sklearn module</a> to automatically calculate the weights for each class like this:</p>\n<pre><code># Import\nimport numpy as np\nfrom sklearn.utils import class_weight\n\n# Example model\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=100))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Use binary crossentropy loss\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Calculate the weights for each class so that we can balance the data\nweights = class_weight.compute_class_weight('balanced',\n                                            np.unique(y_train),\n                                            y_train)\n\n# Add the class weights to the training                                         \nmodel.fit(x_train, y_train, epochs=10, batch_size=32, class_weight=weights)\n</code></pre>\n<p>Note that the output of the <code>class_weight.compute_class_weight()</code> is an numpy array like this: <code>[2.57569845 0.68250928]</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Normally, the minority class will have a higher class weight. It'll be better to use <code>one_weight=0.89, zero_weight=0.11</code> (btw, you can use <code>class_weight={0: 0.11, 1: 0.89}</code>, as suggested in the comment).</p>\n<p>Under class imbalance, your model is seeing much more zeros than ones. It will also learn to predict more zeros than ones because the training loss can be minimized by doing so. That's also why you're seeing an accuracy close to the proportion 0.11. If you take an average over model predictions, it should be very close to zero.</p>\n<p>The purpose of using class weights is to change the loss function so that the training loss cannot be minimized by the \"easy solution\" (i.e., predicting zeros), and that's why it'll be better to use a higher weight for ones.</p>\n<p>Note that the best weights are not necessarily 0.89 and 0.11. Sometimes you might have to try something like taking logarithms or square roots (or any weights satisfying <code>one_weight &gt; zero_weight</code>) to make it work.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using <code>class_weights</code> in <code>model.fit</code> is slightly different: it actually updates samples rather than calculating weighted loss.</p>\n<p>I also found that <code>class_weights</code>, as well as <code>sample_weights</code>, are ignored in TF 2.0.0 when <code>x</code> is sent into model.fit as TFDataset, or generator. It's fixed though in TF 2.1.0+ I believe.</p>\n<p>Here is my weighted binary cross entropy function for multi-hot encoded labels.</p>\n<pre><code>import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport numpy as np\n# weighted loss functions\n\n\ndef weighted_binary_cross_entropy(weights: dict, from_logits: bool = False):\n    '''\n    Return a function for calculating weighted binary cross entropy\n    It should be used for multi-hot encoded labels\n\n    # Example\n    y_true = tf.convert_to_tensor([1, 0, 0, 0, 0, 0], dtype=tf.int64)\n    y_pred = tf.convert_to_tensor([0.6, 0.1, 0.1, 0.9, 0.1, 0.], dtype=tf.float32)\n    weights = {\n        0: 1.,\n        1: 2.\n    }\n    # with weights\n    loss_fn = get_loss_for_multilabels(weights=weights, from_logits=False)\n    loss = loss_fn(y_true, y_pred)\n    print(loss)\n    # tf.Tensor(0.6067193, shape=(), dtype=float32)\n\n    # without weights\n    loss_fn = get_loss_for_multilabels()\n    loss = loss_fn(y_true, y_pred)\n    print(loss)\n    # tf.Tensor(0.52158177, shape=(), dtype=float32)\n\n    # Another example\n    y_true = tf.convert_to_tensor([[0., 1.], [0., 0.]], dtype=tf.float32)\n    y_pred = tf.convert_to_tensor([[0.6, 0.4], [0.4, 0.6]], dtype=tf.float32)\n    weights = {\n        0: 1.,\n        1: 2.\n    }\n    # with weights\n    loss_fn = get_loss_for_multilabels(weights=weights, from_logits=False)\n    loss = loss_fn(y_true, y_pred)\n    print(loss)\n    # tf.Tensor(1.0439969, shape=(), dtype=float32)\n\n    # without weights\n    loss_fn = get_loss_for_multilabels()\n    loss = loss_fn(y_true, y_pred)\n    print(loss)\n    # tf.Tensor(0.81492424, shape=(), dtype=float32)\n\n    @param weights A dict setting weights for 0 and 1 label. e.g.\n        {\n            0: 1.\n            1: 8.\n        }\n        For this case, we want to emphasise those true (1) label, \n        because we have many false (0) label. e.g. \n            [\n                [0 1 0 0 0 0 0 0 0 1]\n                [0 0 0 0 1 0 0 0 0 0]\n                [0 0 0 0 1 0 0 0 0 0]\n            ]\n\n        \n\n    @param from_logits If False, we apply sigmoid to each logit\n    @return A function to calcualte (weighted) binary cross entropy\n    '''\n    assert 0 in weights\n    assert 1 in weights\n\n    def weighted_cross_entropy_fn(y_true, y_pred):\n        tf_y_true = tf.cast(y_true, dtype=y_pred.dtype)\n        tf_y_pred = tf.cast(y_pred, dtype=y_pred.dtype)\n\n        weights_v = tf.where(tf.equal(tf_y_true, 1), weights[1], weights[0])\n        weights_v = tf.cast(weights_v, dtype=y_pred.dtype)\n        ce = K.binary_crossentropy(tf_y_true, tf_y_pred, from_logits=from_logits)\n        loss = K.mean(tf.multiply(ce, weights_v))\n        return loss\n\n    return weighted_cross_entropy_fn\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am working on a seq2seq keras/tensorflow 2.0 model. Every time the user inputs something, my model prints the response perfectly fine. However on the last line of each response I get this:</p>\n<blockquote>\n<p>You: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least <code>steps_per_epoch * epochs</code> batches (in this case, 2 batches). You may need to use the repeat() function when building your dataset.</p>\n</blockquote>\n<p>The \"You:\" is my last output, before the user is supposed to type something new in. The model works totally fine, but I guess no error is ever good, but I don't quite get this error. It says \"interrupting training\", however I am not training anything, this program loads an already trained model. I guess this is why the error is not stopping the program?</p>\n<p>In case it helps, my model looks like this:</p>\n<pre><code>intent_model = keras.Sequential([\n    keras.layers.Dense(8, input_shape=[len(train_x[0])]),  # input layer\n    keras.layers.Dense(8),  # hidden layer\n    keras.layers.Dense(len(train_y[0]), activation=\"softmax\"),  # output layer\n])\n\nintent_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nintent_model.fit(train_x, train_y, epochs=epochs)\n\ntest_loss, test_acc = intent_model.evaluate(train_x, train_y)\nprint(\"Tested Acc:\", test_acc)\n\nintent_model.save(\"models/intent_model.h5\")\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To make sure that you have \"<em>at least <code>steps_per_epoch * epochs</code> batches</em>\", set the <code>steps_per_epoch</code> to</p>\n<pre><code>steps_per_epoch = len(X_train)//batch_size\n\nvalidation_steps = len(X_test)//batch_size # if you have validation data \n</code></pre>\n<p>You can see the maximum number of batches that <code>model.fit()</code> can take by the progress bar when the training interrupts:</p>\n<pre><code>5230/10000 [==============&gt;...............] - ETA: 2:05:22 - loss: 0.0570\n</code></pre>\n<p>Here, the maximum would be 5230 - 1</p>\n<p>Importantly, keep in mind that by default, <code>batch_size</code> is 32 in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\" rel=\"noreferrer\"><code>model.fit()</code></a>.</p>\n<p>If you're using a <code>tf.data.Dataset</code>, you can also add the <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat\" rel=\"noreferrer\"><code>repeat()</code></a> method, but be careful: it will loop indefinitely (unless you specify a number).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have also had a number of models crash with the same warnings while trying to train them.  The training dataset if created using the tf.keras.preprocessing.image_dataset_from_directory() and split 80/20.  I have created a variable to try and not run out of image.  Using ResNet50 with my own images.....</p>\n<pre><code>TRAIN_STEPS_PER_EPOCH = np.ceil((image_count*0.8/BATCH_SIZE)-1)\n# to ensure that there are enough images for training bahch\nVAL_STEPS_PER_EPOCH = np.ceil((image_count*0.2/BATCH_SIZE)-1)\n</code></pre>\n<p>but it still does. BATCH_SIZE is set to 32 so i am taking 80% of the number of images and dividing by 32 then taking away 1 to have surplus.....or so i thought.</p>\n<pre><code>history = model.fit(\n        train_ds,\n        steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n        epochs=EPOCHS,\n        verbose = 1,\n        validation_data=val_ds,\n        validation_steps=VAL_STEPS_PER_EPOCH,\n        callbacks=tensorboard_callback)\n</code></pre>\n<p>Error after 3 hours processing a a single successful Epoch is:</p>\n<blockquote>\n<p>Epoch 1/25 374/374 [==============================] - 8133s 22s/step -\n  loss: 7.0126 - accuracy: 0.0028 - val_loss: 6.8585 - val_accuracy:\n  0.0000e+00 Epoch 2/25   1/374 [..............................] - ETA: 0s - loss: 6.0445 - accuracy: 0.0000e+00WARNING:tensorflow:Your input\n  ran out of data; interrupting training. Make sure that your dataset or\n  generator can generate at least <code>steps_per_epoch * epochs</code> batches (in\n  this case, 9350.0 batches). You may need to use the repeat() function\n  when building your dataset.</p>\n</blockquote>\n<p>this might help....</p>\n<pre><code>&gt; &gt; print(train_ds) &lt;BatchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int32)&gt;\n&gt; \n&gt; print(val_ds) BatchDataset shapes: ((None, 224, 224, 3), (None,)),types: (tf.float32, tf.int32)&gt;\n&gt; \n&gt; print(TRAIN_STEPS_PER_EPOCH)\n&gt; 374.0\n&gt; \n&gt; print(VAL_STEPS_PER_EPOCH)\n&gt; 93.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Solution which worked for me was to set <code>drop_remainder=True</code> while generating the dataset. This automatically handles any extra data that is left over.</p>\n<p>For example:</p>\n<pre><code>dataset = tf.data.Dataset.from_tensor_slices((images, targets)) \\\n        .batch(12, drop_remainder=True)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think it would be immensely helpful to the Tensorflow community if there was a well-documented solution to the crucial task of testing a single new image against the model created by the <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html\" rel=\"noreferrer\">convnet in the CIFAR-10 tutorial</a>. </p>\n<p>I may be wrong, but this critical step that makes the trained model usable in practice seems to be lacking. There is a \"missing link\" in that tutorial—a script that would directly load a single image (as array or binary), compare it against the trained model, and return a classification.</p>\n<p>Prior answers give partial solutions that explain the overall approach, but none of which I've been able to implement successfully. Other bits and pieces can be found here and there, but unfortunately haven't added up to a working solution. Kindly consider the research I've done, before tagging this as duplicate or already answered.</p>\n<p><a href=\"https://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python\">Tensorflow: how to save/restore a model?</a></p>\n<p><a href=\"https://stackoverflow.com/questions/34982492/restoring-tensorflow-model\">Restoring TensorFlow model</a></p>\n<p><a href=\"https://stackoverflow.com/questions/37187597/unable-to-restore-models-in-tensorflow-v0-8\">Unable to restore models in tensorflow v0.8</a></p>\n<p><a href=\"https://gist.github.com/nikitakit/6ef3b72be67b86cb7868\" rel=\"noreferrer\">https://gist.github.com/nikitakit/6ef3b72be67b86cb7868</a></p>\n<p>The most popular answer is the first, in which @RyanSepassi and @YaroslavBulatov describe the problem and an approach: one needs to \"manually construct a graph with identical node names, and use Saver to load the weights into it\". Although both answers are helpful, it is not apparent how one would go about plugging this into the CIFAR-10 project.</p>\n<p>A fully functional solution would be highly desirable so we could port it to other single image classification problems. There are several questions on SO in this regard that ask for this, but still no full answer (for example <a href=\"https://stackoverflow.com/questions/37058236/load-checkpoint-and-evaluate-single-image-with-tensorflow-dnn\">Load checkpoint and evaluate single image with tensorflow DNN</a>).</p>\n<p>I hope we can converge on a working script that everyone could use.</p>\n<p>The below script is not yet functional, and I'd be happy to hear from you on how this can be improved to provide a solution for single-image classification using the CIFAR-10 TF tutorial trained model.</p>\n<p>Assume all variables, file names etc. are untouched from the original tutorial.</p>\n<p>New file: <strong>cifar10_eval_single.py</strong></p>\n<pre><code>import cv2\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('eval_dir', './input/eval',\n                           \"\"\"Directory where to write event logs.\"\"\")\ntf.app.flags.DEFINE_string('checkpoint_dir', './input/train',\n                           \"\"\"Directory where to read model checkpoints.\"\"\")\n\ndef get_single_img():\n    file_path = './input/data/single/test_image.tif'\n    pixels = cv2.imread(file_path, 0)\n    return pixels\n\ndef eval_single_img():\n\n    # below code adapted from @RyanSepassi, however not functional\n    # among other errors, saver throws an error that there are no\n    # variables to save\n    with tf.Graph().as_default():\n\n        # Get image.\n        image = get_single_img()\n\n        # Build a Graph.\n        # TODO\n\n        # Create dummy variables.\n        x = tf.placeholder(tf.float32)\n        w = tf.Variable(tf.zeros([1, 1], dtype=tf.float32))\n        b = tf.Variable(tf.ones([1, 1], dtype=tf.float32))\n        y_hat = tf.add(b, tf.matmul(x, w))\n\n        saver = tf.train.Saver()\n\n        with tf.Session() as sess:\n            sess.run(tf.initialize_all_variables())\n            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                print('Checkpoint found')\n            else:\n                print('No checkpoint found')\n\n            # Run the model to get predictions\n            predictions = sess.run(y_hat, feed_dict={x: image})\n            print(predictions)\n\ndef main(argv=None):\n    if tf.gfile.Exists(FLAGS.eval_dir):\n        tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n    tf.gfile.MakeDirs(FLAGS.eval_dir)\n    eval_single_img()\n\nif __name__ == '__main__':\n    tf.app.run()\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are two methods to feed a single new image to the <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py\" rel=\"noreferrer\">cifar10</a> model. The first method is a cleaner approach but requires modification in the main file, hence will require retraining. The second method is applicable when a user does not want to modify the model files and instead wants to use the existing check-point/meta-graph files.</p>\n<p>The code for the first approach is as follows:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport cv2\n\nsess = tf.Session('', tf.Graph())\nwith sess.graph.as_default():\n    # Read meta graph and checkpoint to restore tf session\n    saver = tf.train.import_meta_graph(\"/tmp/cifar10_train/model.ckpt-200.meta\")\n    saver.restore(sess, \"/tmp/cifar10_train/model.ckpt-200\")\n\n    # Read a single image from a file.\n    img = cv2.imread('tmp.png')\n    img = np.expand_dims(img, axis=0)\n\n    # Start the queue runners. If they are not started the program will hang\n    # see e.g. https://www.tensorflow.org/programmers_guide/reading_data\n    coord = tf.train.Coordinator()\n    threads = []\n    for qr in sess.graph.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                         start=True))\n\n    # In the graph created above, feed \"is_training\" and \"imgs\" placeholders.\n    # Feeding them will disconnect the path from queue runners to the graph \n    # and enable a path from the placeholder instead. The \"img\" placeholder will be \n    # fed with the image that was read above.\n    logits = sess.run('softmax_linear/softmax_linear:0', \n                     feed_dict={'is_training:0': False, 'imgs:0': img})\n\n    #Print classifiction results.\n    print(logits) \n</code></pre>\n<p>The script requires that a user creates two placeholders and a conditional execution statement for it to work.</p>\n<p>The placeholders and conditional execution statement are added in cifar10_train.py as shown below:</p>\n<pre><code>def train():   \n\"\"\"Train CIFAR-10 for a number of steps.\"\"\"   \n    with tf.Graph().as_default():\n        global_step = tf.contrib.framework.get_or_create_global_step()\n\n    with tf.device('/cpu:0'):\n        images, labels = cifar10.distorted_inputs()\n\n    is_training = tf.placeholder(dtype=bool,shape=(),name='is_training')\n    imgs = tf.placeholder(tf.float32, (1, 32, 32, 3), name='imgs')\n    images = tf.cond(is_training, lambda:images, lambda:imgs)\n    logits = cifar10.inference(images)\n</code></pre>\n<p>The inputs in cifar10 model are connected to queue runner object which is a multistage queue that can prefetch data from files in parallel. See a nice animation of queue runner <a href=\"https://www.tensorflow.org/programmers_guide/reading_data\" rel=\"noreferrer\">here</a></p>\n<p>While queue runners are efficient in prefetching large dataset for training, they are an overkill for inference/testing where only a single file is needed to be classified, also they are a bit more involved to modify/maintain.\nFor that reason, I have added a placeholder \"is_training\", which is set to False while training as shown below:</p>\n<pre><code> import numpy as np\n tmp_img = np.ndarray(shape=(1,32,32,3), dtype=float)\n with tf.train.MonitoredTrainingSession(\n     checkpoint_dir=FLAGS.train_dir,\n     hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n            tf.train.NanTensorHook(loss),\n            _LoggerHook()],\n     config=tf.ConfigProto(\n         log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n   while not mon_sess.should_stop():\n     mon_sess.run(train_op, feed_dict={is_training: True, imgs: tmp_img})\n</code></pre>\n<p>Another placeholder \"imgs\" holds a tensor of shape (1,32,32,3) for the image that will be fed during inference -- the first dimension is the batch size which is one in this case. I have modified cifar model to accept 32x32 images instead of 24x24 as the original cifar10 images are 32x32.</p>\n<p>Finally, the conditional statement feeds the placeholder or queue runner output to the graph. The \"is_training\" placeholder is set to False during inference and \"img\" placeholder is fed a numpy array -- the numpy array is reshaped from 3 to 4 dimensional vector to conform to the input tensor to inference function in the model.</p>\n<p>That is all there is to it. Any model can be inferred with a single/user defined test data like shown in the script above. Essentially read the graph, feed data to the graph nodes and run the graph to get the final output.</p>\n<p>Now the second method. The other approach is to hack cifar10.py and cifar10_eval.py to change batch size to one and replace the data coming from the queue runner with the one read from a file.</p>\n<p>Set batch size to 1:</p>\n<pre><code>tf.app.flags.DEFINE_integer('batch_size', 1,\n                             \"\"\"Number of images to process in a batch.\"\"\")\n</code></pre>\n<p>Call inference with an image file read.</p>\n<pre><code>def evaluate():   with tf.Graph().as_default() as g:\n    # Get images and labels for CIFAR-10.\n    eval_data = FLAGS.eval_data == 'test'\n    images, labels = cifar10.inputs(eval_data=eval_data)\n    import cv2\n    img = cv2.imread('tmp.png')\n    img = np.expand_dims(img, axis=0)\n    img = tf.cast(img, tf.float32)\n\n    logits = cifar10.inference(img)\n</code></pre>\n<p>Then pass logits to eval_once and modify eval once to evaluate logits:</p>\n<pre><code>def eval_once(saver, summary_writer, top_k_op, logits, summary_op): \n    ...\n    while step &lt; num_iter and not coord.should_stop():\n        predictions = sess.run([top_k_op])\n        print(sess.run(logits))\n</code></pre>\n<p>There is no separate script to run this method of inference, just run cifar10_eval.py which will now read a file from the user defined location with a batch size of one. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's how I ran a single image at a time.  I'll admit it seems a bit hacky with the reuse of getting the scope.  </p>\n<p>This is a helper function</p>\n<pre><code>def restore_vars(saver, sess, chkpt_dir):\n    \"\"\" Restore saved net, global score and step, and epsilons OR\n    create checkpoint directory for later storage. \"\"\"\n    sess.run(tf.initialize_all_variables())\n\n    checkpoint_dir = chkpt_dir\n\n    if not os.path.exists(checkpoint_dir):\n        try:\n            os.makedirs(checkpoint_dir)\n        except OSError:\n            pass\n\n    path = tf.train.get_checkpoint_state(checkpoint_dir)\n    #print(\"path1 = \",path)\n    #path = tf.train.latest_checkpoint(checkpoint_dir)\n    print(checkpoint_dir,\"path = \",path)\n    if path is None:\n        return False\n    else:\n        saver.restore(sess, path.model_checkpoint_path)\n        return True\n</code></pre>\n<p>Here is the main part of the code that runs a single image at a time within the for loop.  </p>\n<pre><code>to_restore = True\nwith tf.Session() as sess:\n\n    for i in test_img_idx_set:\n\n            # Gets the image\n            images = get_image(i)\n            images = np.asarray(images,dtype=np.float32)\n            images = tf.convert_to_tensor(images/255.0)\n            # resize image to whatever you're model takes in\n            images = tf.image.resize_images(images,256,256)\n            images = tf.reshape(images,(1,256,256,3))\n            images = tf.cast(images, tf.float32)\n\n            saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=1)\n\n            #print(\"infer\")\n            with tf.variable_scope(tf.get_variable_scope()) as scope:\n                if to_restore:\n                    logits = inference(images)\n                else:\n                    scope.reuse_variables()\n                    logits = inference(images)\n\n\n            if to_restore:\n                restored = restore_vars(saver, sess,FLAGS.train_dir)\n                print(\"restored \",restored)\n                to_restore = False\n\n            logit_val = sess.run(logits)\n            print(logit_val)\n</code></pre>\n<p>Here is an alternative implementation to the above using place holders it's a bit cleaner in my opinion. but I'll leave the above example for historical reasons.</p>\n<pre><code>imgs_place = tf.placeholder(tf.float32, shape=[my_img_shape_put_here])\nimages = tf.reshape(imgs_place,(1,256,256,3))\n\nsaver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=1)\n\n#print(\"infer\")\nlogits = inference(images)\n\nrestored = restore_vars(saver, sess,FLAGS.train_dir)\nprint(\"restored \",restored)\n\nwith tf.Session() as sess:\n    for i in test_img_idx_set:\n        logit_val = sess.run(logits,feed_dict={imgs_place=i})\n        print(logit_val)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>got it working with this</p>\n<pre><code>softmax = gn.inference(image)\nsaver = tf.train.Saver()\nckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\nwith tf.Session() as sess:\n  saver.restore(sess, ckpt.model_checkpoint_path)\n  softmaxval = sess.run(softmax)\n  print(softmaxval)\n</code></pre>\n<p>output</p>\n<pre><code>[[  6.73550041e-03   4.44930716e-04   9.92570221e-01   1.00681427e-06\n    3.05406687e-08   2.38927707e-04   1.89839399e-12   9.36238484e-06\n    1.51646684e-09   3.38977535e-09]]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm a beginner to python and machine learning . I get below error when i try to fit data into statsmodels.formula.api OLS.fit()</p>\n<p>Traceback (most recent call last):</p>\n<blockquote>\n<p>File \"\", line 47, in \n      regressor_OLS = sm.OLS(y , X_opt).fit()</p>\n<p>File\n  \"E:\\Anaconda\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\",\n  line 190, in fit\n      self.pinv_wexog, singular_values = pinv_extended(self.wexog)</p>\n<p>File \"E:\\Anaconda\\lib\\site-packages\\statsmodels\\tools\\tools.py\",\n  line 342, in pinv_extended\n      u, s, vt = np.linalg.svd(X, 0)</p>\n<p>File \"E:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line\n  1404, in svd\n      u, s, vt = gufunc(a, signature=signature, extobj=extobj)</p>\n<p>TypeError: No loop matching the specified signature and casting was\n  found for ufunc svd_n_s</p>\n</blockquote>\n<p>code</p>\n<pre><code>#Importing Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #Visualization\n\n\n#Importing the dataset\ndataset = pd.read_csv('Video_Games_Sales_as_at_22_Dec_2016.csv')\n#dataset.head(10) \n\n#Encoding categorical data using panda get_dummies function . Easier and straight forward than OneHotEncoder in sklearn\n#dataset = pd.get_dummies(data = dataset , columns=['Platform' , 'Genre' , 'Rating' ] , drop_first = True ) #drop_first use to fix dummy varible trap \n\n\ndataset=dataset.replace('tbd',np.nan)\n\n#Separating Independent &amp; Dependant Varibles\n#X = pd.concat([dataset.iloc[:,[11,13]], dataset.iloc[:,13: ]] , axis=1).values  #Getting important  variables\nX = dataset.iloc[:,[10,12]].values\ny = dataset.iloc[:,9].values #Dependant Varible (Global sales)\n\n\n#Taking care of missing data\nfrom sklearn.preprocessing import Imputer\nimputer =  Imputer(missing_values = 'NaN' , strategy = 'mean' , axis = 0)\nimputer = imputer.fit(X[:,0:2])\nX[:,0:2] = imputer.transform(X[:,0:2])\n\n\n#Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2 , random_state = 0)\n\n#Fitting Mutiple Linear Regression to the Training Set\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\n\n#Predicting the Test set Result\ny_pred = regressor.predict(X_test)\n\n\n#Building the optimal model using Backward Elimination (p=0.050)\nimport statsmodels.formula.api as sm\nX = np.append(arr = np.ones((16719,1)).astype(float) , values = X , axis = 1)\n\nX_opt = X[:, [0,1,2]]\nregressor_OLS = sm.OLS(y , X_opt).fit()\nregressor_OLS.summary() \n</code></pre>\n<p>Dataset</p>\n<p><a href=\"https://www.dropbox.com/s/w2hq4t0utbvk7bu/Video_Games_Sales_as_at_22_Dec_2016.csv?dl=0\" rel=\"noreferrer\">dataset link</a></p>\n<p>Couldn't find anything helpful to solve this issue on stack-overflow or google . </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>try specifiying the </p>\n<blockquote>\n<p>dtype = 'float'</p>\n</blockquote>\n<p>When the matrix is created.\nExample:</p>\n<pre><code>a=np.matrix([[1,2],[3,4]], dtype='float')\n</code></pre>\n<p>Hope this works!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Faced the similar problem. Solved the problem my mentioning dtype  and flatten the array. </p>\n<p>numpy version: 1.17.3</p>\n<pre><code>a = np.array(a, dtype=np.float)\na = a.flatten()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As suggested previously, you need to ensure X_opt is a float type.\nFor example in your code, it would look like this:</p>\n<pre><code>X_opt = X[:, [0,1,2]]\nX_opt = X_opt.astype(float)\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://stackoverflow.com/questions/33659424/tensorflow-mnist-example-not-running\">TensorFlow MNIST example not running with fully_connected_feed.py</a></p>\n<p>I checked this out and realized that <code>input_data</code> was not built-in.  So I downloaded the whole folder from <a href=\"https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py\" rel=\"noreferrer\">here</a>. How can I start the tutorial:</p>\n<pre><code>import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-6-a5af65173c89&gt; in &lt;module&gt;()\n----&gt; 1 import input_data\n      2 mnist = tf.input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nImportError: No module named input_data\n</code></pre>\n<p>I'm using iPython (Jupyter) so do I need to change my working directory to this folder I downloaded? or can I add this to my <code>tensorflow</code> directory? If so, where do I add the files? I installed <code>tensorflow</code> with <code>pip</code> (on my OSX) and the current location is <code>~/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py</code></p>\n<p>Are these files meant to be accessed directly through <code>tensorflow</code> like <code>sklearn</code> datasets? or am I just supposed to cd into the directory and work from there? The example is not clear. </p>\n<p>EDIT:</p>\n<p>This post is very out-dated</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So let's assume that you are in the directory: <code>/somePath/tensorflow/tutorial</code> (and this is your working directory).</p>\n<p>All you need to do is to download the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py\" rel=\"nofollow noreferrer\">input_data.py</a> file and place it like this. Let's assume that the file name you invoke:</p>\n<pre><code>import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n...\n</code></pre>\n<p>is <code>main.py</code> and it is also in the same directory.</p>\n<p>Once this is done, you can just start running <code>main.py</code> which will start downloading the files and will put them in the MNIST_data folder (once they are there the script will not be downloading them next time).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The old tutorial said, to import the MNIST data, use:</p>\n<pre><code>import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n</code></pre>\n<p>This will cause the error.\nThe new tutorial uses the following code to do so:</p>\n<pre><code>from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n</code></pre>\n<p>And this works well.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using different version - following Install on Windows with Docker <a href=\"http://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/\" rel=\"nofollow\">here</a> - and had similar problem.</p>\n<p>An easy workaround I've found was:</p>\n<p>1.Into the Linux command line, figure out where is the input_data.py on my Docker image (in your case you mentionned that you had to download it manually. In my case, it was already here). I used the follwing linux command:</p>\n<pre><code>$ sudo find . -print | grep -i '.*[.]py'\n</code></pre>\n<p>I've got the files &amp; path</p>\n<pre><code>./tensorflow/g3doc/tutorials/mnist/mnist.py\n./tensorflow/g3doc/tutorials/mnist/input_data.py\n</code></pre>\n<p>2.launch Python and type the following command using SYS:</p>\n<pre><code>&gt;&gt; import sys\n&gt;&gt; print(sys.path)\n</code></pre>\n<p>you will get the existing paths.</p>\n<pre><code>['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat']\n</code></pre>\n<p>4.add the path of inputa_data.py:</p>\n<pre><code>&gt;&gt; sys.path.insert(1,'/tensorflow/tensorflow/g3doc/tutorials/mnist')\n</code></pre>\n<p>Hope that it can help. If you found better option, let me know. :)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The keras <a href=\"https://keras.io/layers/normalization/\" rel=\"noreferrer\"><code>BatchNormalization</code> layer</a> uses <code>axis=-1</code> as a default value and states that the feature axis is typically normalized. Why is this the case?</p>\n<p>I suppose this is surprising because I'm more familiar with using something like <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" rel=\"noreferrer\"><code>StandardScaler</code></a>, which would be equivalent to using <code>axis=0</code>. This would normalize the features individually.</p>\n<p>Is there a reason why samples are individually normalized by default (i.e. <code>axis=-1</code>) in keras as opposed to features?</p>\n<p><strong>Edit: example for concreteness</strong></p>\n<p>It's common to transform data such that each feature has zero mean and unit variance. Let's just consider the \"zero mean\" part with this mock dataset, where each row is a sample:</p>\n<pre><code>&gt;&gt;&gt; data = np.array([[   1,   10,  100, 1000],\n                     [   2,   20,  200, 2000],\n                     [   3,   30,  300, 3000]])\n\n&gt;&gt;&gt; data.mean(axis=0)\narray([    2.,    20.,   200.,  2000.])\n\n&gt;&gt;&gt; data.mean(axis=1)\narray([ 277.75,  555.5 ,  833.25])\n</code></pre>\n<p>Wouldn't it make more sense to subtract the <code>axis=0</code> mean, as opposed to the <code>axis=1</code> mean? Using <code>axis=1</code>, the units and scales can be completely different.</p>\n<p>Edit 2:</p>\n<p>The first equation of section 3 in <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" rel=\"noreferrer\">this paper</a> seems to imply that <code>axis=0</code> should be used for calculating expectations and variances for each feature individually, assuming you have an (m, n) shaped dataset where m is the number of samples and n is the number of features.</p>\n<p>Edit 3: another example</p>\n<p>I wanted to see the dimensions of the means and variances <code>BatchNormalization</code> was calculating on a toy dataset:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import BatchNormalization, Dense, Input\n\n\niris = load_iris()\nX = iris.data\ny = pd.get_dummies(iris.target).values\n\ninput_ = Input(shape=(4, ))\nnorm = BatchNormalization()(input_)\nl1 = Dense(4, activation='relu')(norm)\noutput = Dense(3, activation='sigmoid')(l1)\n\nmodel = Model(input_, output)\nmodel.compile(Adam(0.01), 'categorical_crossentropy')\nmodel.fit(X, y, epochs=100, batch_size=32)\n\nbn = model.layers[1]\nbn.moving_mean  # &lt;tf.Variable 'batch_normalization_1/moving_mean:0' shape=(4,) dtype=float32_ref&gt;\n</code></pre>\n<p>The input X has shape (150, 4), and the <code>BatchNormalization</code> layer calculated 4 means, which means it operated over <code>axis=0</code>.</p>\n<p>If <code>BatchNormalization</code> has a default of <code>axis=-1</code> then shouldn't there be 150 means?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The confusion is due to the meaning of <code>axis</code> in <code>np.mean</code> versus in <code>BatchNormalization</code>.</p>\n<p>When we take the mean along an axis, we collapse that dimension and preserve all other dimensions. In your example <code>data.mean(axis=0)</code> collapses the <code>0-axis</code>, which is the vertical dimension of <code>data</code>.</p>\n<p>When we compute a <code>BatchNormalization</code> along an axis, we preserve the dimensions of the array, and we normalize with respect to the mean and standard deviation over <em>every other axis</em>. So in your <code>2D</code> example <code>BatchNormalization</code> with <code>axis=1</code> <em>is</em> subtracting the mean for <code>axis=0</code>, just as you expect. This is why <code>bn.moving_mean</code> has shape <code>(4,)</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I know this post is old, but am still answering it because the confusion still lingers on in Keras documentation. I had to go through the code to figure this out:</p>\n<ol>\n<li>The axis variable which is documented as being an integer can actually be a list of integers denoting multiple axes. So for e.g. if my input had an image in the NHWC or NCHW formats, provide axis=[1,2,3] if I wanted to perform BatchNormalization in the way that the OP wants (i.e. normalize across the batch dimension only).</li>\n<li>The axis list (or integer) should contain the axes that you <em>do not want</em> to reduce while calculating the mean and variance. In other words it is the complement of the axes along which you want to normalize - quite opposite of what the documentation appears to say if you go by the conventional definition of 'axes'. So for e.g. if your input I was of shape (N,H,W,C) or (N,C,H,W) - i.e. the first dimension was the batch dimension and you only wanted the mean and variance to be calculated across the batch dimension you should supply axis=[1,2,3]. This will cause Keras to calculate mean M and variance V tensors of shape (1,H,W,C) or (1,C,H,W) respectively - i.e. batch dimension would get marginalized/reduced owing to the aggregation (i.e. mean or variance is calculated across the first dimension). In later operations like (I-M) and (I-M)/V, the first dimension of M and V would get broadcast to all of the N samples of the batch.</li>\n<li>The BatchNorm layer ends up calling tf.nn.moments with axes=(1,) in this example! That's so because the definition of axes in tf.nn.moments is the correct one.</li>\n<li>Similarly tf.nn.moments calls tf.nn.reduce_mean, where again the definition of axes is the correct one (i.e. opposite of tf.keras.layers.BatchNormalization).</li>\n<li>That said, the BatchNormalization paper suggests normalizing across the HxW spatial map in additon to the batch dimension (N). Hence if one were to follow that advice, then axis would only include the channel dimension (C) because that's the only remaining dimension that you didn't want to reduce. The Keras documentation is probably alluding to this, although it is quite cryptic.</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>if your mini-batch is a matrix <strong><em>A</em></strong> <em>mxn</em>, i.e. <strong><em>m</em></strong> samples and <strong><em>n</em></strong> features, the normalization axis should be <strong><em>axis=0</em></strong>. As your said, what we want is to normalize every feature individually, the default <strong><em>axis = -1</em></strong> in keras because when it is used in the convolution-layer, the dimensions of figures dataset are usually <strong><em>(samples, width, height, channal)</em></strong>, and the batch samples are normalized long the <strong><em>channal axis(the last axis)</em></strong>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm building a <code>ResNet-18</code> classification model for the <strong>Stanford Cars</strong> dataset using transfer learning. I would like to implement <a href=\"https://arxiv.org/pdf/1701.06548.pdf\" rel=\"noreferrer\">label smoothing</a> to penalize overconfident predictions and improve generalization.</p>\n<p><code>TensorFlow</code> has a simple keyword argument in <a href=\"https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy\" rel=\"noreferrer\"><code>CrossEntropyLoss</code></a>. Has anyone built a similar function for <code>PyTorch</code> that I could plug-and-play with?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The generalization and learning speed of a multi-class neural network can often be significantly improved by using <strong>soft targets</strong> that are a <strong>weighted average</strong> of the <strong>hard targets</strong> and the <strong>uniform distribution</strong> over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation, and speech recognition.</p>\n<hr/>\n<p><strong>Label Smoothing</strong> is already implemented in <code>Tensorflow</code> within the cross-entropy loss functions. <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy\" rel=\"noreferrer\">BinaryCrossentropy</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\" rel=\"noreferrer\">CategoricalCrossentropy</a>. But currently, there is no official implementation of <strong>Label Smoothing</strong> in <code>PyTorch</code>. However, there is going an active discussion on it and hopefully, it will be provided with an official package. Here is that discussion thread: <a href=\"https://github.com/pytorch/pytorch/issues/7455\" rel=\"noreferrer\">Issue #7455</a>.</p>\n<p>Here We will bring some available best implementation of <strong>Label Smoothing (LS)</strong> from <code>PyTorch</code> practitioner. Basically, there are many ways to implement the <strong>LS</strong>. Please refer to this specific discussion on this, one is <a href=\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-746999397\" rel=\"noreferrer\">here</a>, and <a href=\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-747769787\" rel=\"noreferrer\">another here</a>. Here we will bring implementation in <strong>2</strong> unique ways with two versions of each; so total <strong>4</strong>.</p>\n<h2>Option 1: CrossEntropyLossWithProbs</h2>\n<p>In this way, it accepts the <code>one-hot</code> target vector. The user must manually smooth their target vector. And it can be done within <code>with torch.no_grad()</code> scope, as it temporarily sets all of the <code>requires_grad</code> flags to false.</p>\n<ol>\n<li><a href=\"https://github.com/PistonY\" rel=\"noreferrer\">Devin Yang</a>: <a href=\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\" rel=\"noreferrer\">Source</a></li>\n</ol>\n<pre><code>import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n        \"\"\"if smoothing == 0, it's one-hot method\n           if 0 &lt; smoothing &lt; 1, it's smooth method\n        \"\"\"\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.weight = weight\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        assert 0 &lt;= self.smoothing &lt; 1\n        pred = pred.log_softmax(dim=self.dim)\n\n        if self.weight is not None:\n            pred = pred * self.weight.unsqueeze(0)   \n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n</code></pre>\n<p>Additionally, we've added an assertion checkmark on <code>self. smoothing</code> and added loss weighting support on this implementation.</p>\n<ol start=\"2\">\n<li><a href=\"https://stackoverflow.com/users/207661/shital-shah\">Shital Shah</a>: <a href=\"https://stackoverflow.com/a/59264908/9215780\">Source</a></li>\n</ol>\n<p>Shital already posted the answer here. Here we're pointing out that this implementation is similar to<a href=\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631\" rel=\"noreferrer\"> Devin Yang</a>'s above implementation. However, here we're mentioning his code with minimizing a bit of <code>code syntax</code>.</p>\n<pre><code>class SmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n        with torch.no_grad():\n            targets = torch.empty(size=(targets.size(0), n_classes),\n                                  device=targets.device) \\\n                                  .fill_(smoothing /(n_classes-1)) \\\n                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n        return targets\n\n    def reduce_loss(self, loss):\n        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n        if self.reduction == 'sum' else loss\n\n    def forward(self, inputs, targets):\n        assert 0 &lt;= self.smoothing &lt; 1\n\n        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n        log_preds = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            log_preds = log_preds * self.weight.unsqueeze(0)\n\n        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))\n</code></pre>\n<p>Check</p>\n<pre><code>import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nif __name__==\"__main__\":\n    # 1. Devin Yang\n    crit = LabelSmoothingLoss(classes=5, smoothing=0.5)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\n    # 2. Shital Shah\n    crit = SmoothCrossEntropyLoss(smoothing=0.5)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\ntensor(1.4178)\ntensor(1.4178)\n</code></pre>\n<hr/>\n<h2>Option 2: LabelSmoothingCrossEntropyLoss</h2>\n<p>By this, it accepts the target vector and uses doesn't manually smooth the target vector, rather the built-in module takes care of the label smoothing. It allows us to implement label smoothing in terms of <a href=\"https://github.com/pytorch/pytorch/blob/1442a92741d8f39e3d6228af2ed8800cc29ed16f/torch/nn/functional.py#L2429\" rel=\"noreferrer\"><code>F.nll_loss</code></a>.</p>\n<p>(a). <a href=\"https://github.com/wangleiofficial\" rel=\"noreferrer\">Wangleiofficial</a>: <a href=\"https://github.com/pytorch/pytorch/issues/7455#issuecomment-720100742\" rel=\"noreferrer\">Source</a> - (AFAIK), Original Poster</p>\n<p>(b). <a href=\"https://www.kaggle.com/anjum48\" rel=\"noreferrer\">Datasaurus</a>: <a href=\"https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/166833#930136\" rel=\"noreferrer\">Source</a>  - Added Weighting Support</p>\n<p>Further, we slightly minimize the coding write-up to make it more concise.</p>\n<pre><code>class LabelSmoothingLoss(torch.nn.Module):\n    def __init__(self, smoothing: float = 0.1, \n                 reduction=\"mean\", weight=None):\n        super(LabelSmoothingLoss, self).__init__()\n        self.smoothing   = smoothing\n        self.reduction = reduction\n        self.weight    = weight\n\n    def reduce_loss(self, loss):\n        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n         if self.reduction == 'sum' else loss\n\n    def linear_combination(self, x, y):\n        return self.smoothing * x + (1 - self.smoothing) * y\n\n    def forward(self, preds, target):\n        assert 0 &lt;= self.smoothing &lt; 1\n\n        if self.weight is not None:\n            self.weight = self.weight.to(preds.device)\n\n        n = preds.size(-1)\n        log_preds = F.log_softmax(preds, dim=-1)\n        loss = self.reduce_loss(-log_preds.sum(dim=-1))\n        nll = F.nll_loss(\n            log_preds, target, reduction=self.reduction, weight=self.weight\n        )\n        return self.linear_combination(loss / n, nll)\n</code></pre>\n<ol start=\"2\">\n<li><a href=\"https://github.com/NVIDIA/DeepLearningExamples\" rel=\"noreferrer\">NVIDIA/DeepLearningExamples</a>: <a href=\"https://github.com/NVIDIA/DeepLearningExamples/blob/8d8b21a933fff3defb692e0527fca15532da5dc6/PyTorch/Classification/ConvNets/image_classification/smoothing.py#L18\" rel=\"noreferrer\">Source</a></li>\n</ol>\n<pre><code>class LabelSmoothing(nn.Module):\n    \"\"\"NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n</code></pre>\n<p>Check</p>\n<pre><code>if __name__==\"__main__\":\n    # Wangleiofficial\n    crit = LabelSmoothingLoss(smoothing=0.3, reduction=\"mean\")\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\n    # NVIDIA\n    crit = LabelSmoothing(smoothing=0.3)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\ntensor(1.3883)\ntensor(1.3883)\n</code></pre>\n<hr/>\n<h2>Update: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\" rel=\"noreferrer\">Officially Added</a></h2>\n<pre><code>torch.nn.CrossEntropyLoss(weight=None, size_average=None, \n                          ignore_index=- 100, reduce=None, \n                          reduction='mean', label_smoothing=0.0)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been looking for options that derives from <code>_Loss</code> like other loss classes in PyTorch and respects basic parameters such as <code>reduction</code>. Unfortunately I can't find straight forward replacement so ended up writing my own. I haven't fully tested this yet, however:</p>\n<pre><code>import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth_one_hot(targets:torch.Tensor, n_classes:int, smoothing=0.0):\n        assert 0 &lt;= smoothing &lt; 1\n        with torch.no_grad():\n            targets = torch.empty(size=(targets.size(0), n_classes),\n                    device=targets.device) \\\n                .fill_(smoothing /(n_classes-1)) \\\n                .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n            self.smoothing)\n        lsm = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            lsm = lsm * self.weight.unsqueeze(0)\n\n        loss = -(targets * lsm).sum(-1)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n</code></pre>\n<p>Other options:</p>\n<ul>\n<li><a href=\"https://github.com/eladhoffer/utils.pytorch/blob/master/cross_entropy.py#L70\" rel=\"noreferrer\">utils.pytorch</a> Implementation</li>\n<li><a href=\"http://pages.cs.wisc.edu/~sidharth/deepmatcher/_modules/deepmatcher/optim.html\" rel=\"noreferrer\">DeepMatch</a> implementation</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>None that I know of.</p>\n<p>Here are two examples of PyTorch implementation:</p>\n<ul>\n<li><p><a href=\"https://github.com/OpenNMT/OpenNMT-py/blob/e8622eb5c6117269bb3accd8eb6f66282b5e67d9/onmt/utils/loss.py#L186\" rel=\"noreferrer\"><code>LabelSmoothingLoss</code> module</a> in OpenNMT framework for machine translation</p></li>\n<li><p><a href=\"https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/train.py#L38\" rel=\"noreferrer\"><code>attention-is-all-you-need-pytorch</code></a>, re-implementation of Google's <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noreferrer\">Attention is all you need paper</a></p></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I can't figure out if I've setup my binary classification problem correctly. I labeled the positive class 1 and the negative 0. However It is my understanding that by default scikit-learn uses class 0 as the positive class in its confusion matrix (so the inverse of how I set it up). This is confusing to me. Is the top row, in scikit-learn's default setting, the positive or negative class?\nLets assume the confusion matrix output:</p>\n<pre><code>confusion_matrix(y_test, preds)\n [ [30  5]\n    [2 42] ]\n</code></pre>\n<p>How would it look like in a confusion matrix? Are the actual instances the rows or the columns in scikit-learn? </p>\n<pre><code>          prediction                        prediction\n           0       1                          1       0\n         -----   -----                      -----   -----\n      0 | TN   |  FP        (OR)         1 |  TP  |  FP\nactual   -----   -----             actual   -----   -----\n      1 | FN   |  TP                     0 |  FN  |  TN\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>scikit learn sorts labels in ascending order, thus 0's are first column/row and 1's are the second one</p>\n<pre><code>&gt;&gt;&gt; from sklearn.metrics import confusion_matrix as cm\n&gt;&gt;&gt; y_test = [1, 0, 0]\n&gt;&gt;&gt; y_pred = [1, 0, 0]\n&gt;&gt;&gt; cm(y_test, y_pred)\narray([[2, 0],\n       [0, 1]])\n&gt;&gt;&gt; y_pred = [4, 0, 0]\n&gt;&gt;&gt; y_test = [4, 0, 0]\n&gt;&gt;&gt; cm(y_test, y_pred)\narray([[2, 0],\n       [0, 1]])\n&gt;&gt;&gt; y_test = [-2, 0, 0]\n&gt;&gt;&gt; y_pred = [-2, 0, 0]\n&gt;&gt;&gt; cm(y_test, y_pred)\narray([[1, 0],\n       [0, 2]])\n&gt;&gt;&gt; \n</code></pre>\n<p>This is written in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\" rel=\"noreferrer\">docs</a>:</p>\n<blockquote>\n<p>labels : array, shape = [n_classes], optional\nList of labels to index the matrix. This may be used to reorder or select a subset of labels. <strong>If none is given</strong>, those that appear at least once in y_true or y_pred are <strong>used in sorted order</strong>.</p>\n</blockquote>\n<p>Thus you can alter this behavior by providing labels to confusion_matrix call</p>\n<pre><code>&gt;&gt;&gt; y_test = [1, 0, 0]\n&gt;&gt;&gt; y_pred = [1, 0, 0]\n&gt;&gt;&gt; cm(y_test, y_pred)\narray([[2, 0],\n       [0, 1]])\n&gt;&gt;&gt; cm(y_test, y_pred, labels=[1, 0])\narray([[1, 0],\n       [0, 2]])\n</code></pre>\n<p>And actual/predicted are oredered just like in your images - predictions are in columns and actual values in rows</p>\n<pre><code>&gt;&gt;&gt; y_test = [5, 5, 5, 0, 0, 0]\n&gt;&gt;&gt; y_pred = [5, 0, 0, 0, 0, 0]\n&gt;&gt;&gt; cm(y_test, y_pred)\narray([[3, 0],\n       [2, 1]])\n</code></pre>\n<ul>\n<li>true: 0, predicted: 0 (value: 3, position [0, 0])</li>\n<li>true: 5, predicted: 0 (value: 2, position [1, 0])</li>\n<li>true: 0, predicted: 5 (value: 0, position [0, 1])</li>\n<li>true: 5, predicted: 5 (value: 1, position [1, 1])</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Supporting Answer:</strong></p>\n<p>When drawing the confusion matrix values using <strong>sklearn.metrics</strong>, be aware that the order of the values are </p>\n<p><strong>[ True Negative    False positive]\n  [ False Negative   True Positive ]</strong></p>\n<p>If you interpret the values wrong, say TP for TN, your accuracies and AUC_ROC will more or less match, but your <strong>precision, recall, sensitivity, and f1-score will take a hit</strong> and you will end up with completely different metrics. This will result in you making a false judgement of your model's performance.</p>\n<p>Do make sure to clearly identify what the 1 and 0 in your model represent. This heavily dictates the results of the confusion matrix.</p>\n<p><strong>Experience:</strong></p>\n<p>I was working on predicting fraud (binary supervised classification), where fraud was denoted by 1 and non-fraud by 0. My model was trained on a <strong>scaled up, perfectly balanced data set</strong>, hence during in-time testing, values of confusion matrix did not seem suspicious when my results were of the order\n<strong>[TP FP]\n  [FN TN]</strong></p>\n<p>Later, when I had to perform an <strong>out-of-time test on a new imbalanced test set</strong>, I realized that the above order of confusion matrix was <strong>wrong</strong> and different from the one mentioned on sklearn's documentation page which refers to the order as <strong>tn,fp,fn,tp</strong>. Plugging in the new order made me realize the blunder and what a difference it had caused in my judgement of the model's performance.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Following the example of <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\" rel=\"nofollow noreferrer\">wikipedia</a>. If a classification system has been trained to distinguish between cats and non cats, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 27 animals — 8 cats, and 19 non cats, the resulting confusion matrix could look like the table below:</p>\n<p><a href=\"https://i.sstatic.net/cSmi9.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/cSmi9.png\"/></a></p>\n<p><strong>With sklearn</strong></p>\n<p>If you want to maintain the structure of the wikipedia confusion matrix, first go the predicted values and then the actual class.</p>\n<pre><code>from sklearn.metrics import confusion_matrix\ny_true = [0,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,1,1,0,1,0,0,0,0]\ny_pred = [0,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0]\nconfusion_matrix(y_pred, y_true, labels=[1,0])\n\nOut[1]: \narray([[ 5,  2],\n       [ 3, 17]], dtype=int64)\n</code></pre>\n<p><strong>Another way with crosstab pandas</strong></p>\n<pre><code>true = pd.Categorical(list(np.where(np.array(y_true) == 1, 'cat','non-cat')), categories = ['cat','non-cat'])\npred = pd.Categorical(list(np.where(np.array(y_pred) == 1, 'cat','non-cat')), categories = ['cat','non-cat'])\n\npd.crosstab(pred, true, \n            rownames=['pred'], \n            colnames=['Actual'], margins=False, margins_name=\"Total\")\n\nOut[2]: \nActual   cat  non-cat\npred                 \ncat        5        2\nnon-cat    3       17\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have seen in many kaggle notebooks people talk about oof approach when they do machine learning with K-Fold validation. What is oof and is it related to k-fold validation ? Also can you suggest some useful resources for it to get the concept in detail</p>\n<p>Thanks for helping!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>OOF simply stands for \"Out-of-fold\" and refers to a step in the learning process when using k-fold validation in which the predictions from each set of folds are grouped together into one group of 1000 predictions. These predictions are now \"out-of-the-folds\" and thus error can be calculated on these to get a good measure of how good your model is. </p>\n<p>In terms of learning more about it, there's really not a ton more to it than that, and it certainly isn't its own technique to learning or anything. If you have a follow up question that is small, please leave a comment and I will try and update my answer to include this.</p>\n<p><strong>EDIT:</strong> While ambling around the inter-webs I stumbled upon <a href=\"https://stats.stackexchange.com/questions/161491/how-to-evaluate-the-final-model-after-k-fold-cross-validation\">this</a> relatively similar question from Cross-Validated (with a slightly more detailed answer), perhaps it will add some intuition if you are still confused.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I found <a href=\"https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/#:%7E:text=An%20out%2Dof%2Dfold%20prediction,example%20in%20the%20training%20dataset.\" rel=\"nofollow noreferrer\">this article</a> from machine learning mastery explaining out of the fold predictions quite in depth.\nBelow an extract from the article explaining what out of fold (OOF) prediction is:</p>\n<p><em>\"An out-of-fold prediction is a prediction by the model during the k-fold cross-validation procedure.\nThat is, out-of-fold predictions are those predictions made on the holdout datasets during the resampling procedure. If performed correctly, there will be one prediction for each example in the training dataset.\"</em></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am copying the pyspark.ml example from the official document website:\n<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer\" rel=\"noreferrer\">http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer</a></p>\n<pre><code>data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\ndf = spark.createDataFrame(data, [\"features\"])\nkmeans = KMeans(k=2, seed=1)\nmodel = kmeans.fit(df)\n</code></pre>\n<p>However, the example above wouldn't run and gave me the following errors:</p>\n<pre><code>---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-28-aaffcd1239c9&gt; in &lt;module&gt;()\n      1 from pyspark import *\n      2 data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n----&gt; 3 df = spark.createDataFrame(data, [\"features\"])\n      4 kmeans = KMeans(k=2, seed=1)\n      5 model = kmeans.fit(df)\n\nNameError: name 'spark' is not defined\n</code></pre>\n<p>What additional configuration/variable needs to be set to get the example running?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can add</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nsc = SparkContext('local')\nspark = SparkSession(sc)\n</code></pre>\n<p>to the begining of your code to define a SparkSession, then the <code>spark.createDataFrame()</code> should work.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://stackoverflow.com/a/43231461/6188720\">Answer by 率怀一</a> is good and will work for the first time.\nBut the second time you try it, it will throw the following exception :</p>\n<pre class=\"lang-py prettyprint-override\"><code>ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at &lt;ipython-input-3-786525f7559f&gt;:10 \n</code></pre>\n<p>There are two ways to avoid it.</p>\n<p>1) Using <code>SparkContext.getOrCreate()</code> instead of <code>SparkContext()</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nsc = SparkContext.getOrCreate()\nspark = SparkSession(sc)\n</code></pre>\n<p>2) Using <code>sc.stop()</code> in the end, or before you start another SparkContext.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since you are calling <a href=\"https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext.createDataFrame\" rel=\"noreferrer\">createDataFrame()</a>, you need to do this:</p>\n<pre><code>df = sqlContext.createDataFrame(data, [\"features\"])\n</code></pre>\n<p>instead of this:</p>\n<pre><code>df = spark.createDataFrame(data, [\"features\"])\n</code></pre>\n<p><code>spark</code> stands there as the <code>sqlContext</code>. </p>\n<hr/>\n<p>In general, some people have that as <code>sc</code>, so if that didn't work, you could try:</p>\n<pre><code>df = sc.createDataFrame(data, [\"features\"])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between <code>MinMaxScaler()</code> and <code>StandardScaler()</code>.</p>\n<p><code>mms = MinMaxScaler(feature_range = (0, 1))</code> (Used in a machine learning model)</p>\n<p><code>sc = StandardScaler()</code> (In another machine learning model they used standard-scaler and not min-max-scaler)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>MinMaxScaler(feature_range = (0, 1))</code> will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).</p>\n<p><code>StandardScaler()</code> will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.</p>\n<p>If there are outliers, use <code>RobustScaler()</code>. Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)</p>\n<p>Additional Note: If scaler is used before train_test_split, data leakage will happen. Do use scaler after train_test_split</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From <a href=\"http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\" rel=\"noreferrer\">ScikitLearn site</a>: </p>\n<blockquote>\n<p><code>StandardScaler</code> removes the mean and scales the data to unit variance.\n  However, the outliers have an influence when computing the empirical\n  mean and standard deviation which shrink the range of the feature\n  values as shown in the left figure below. Note in particular that\n  because the outliers on each feature have different magnitudes, the\n  spread of the transformed data on each feature is very different: most\n  of the data lie in the [-2, 4] range for the transformed median income\n  feature while the same data is squeezed in the smaller [-0.2, 0.2]\n  range for the transformed number of households.</p>\n<p>StandardScaler therefore cannot guarantee balanced feature scales in\n  the presence of outliers.</p>\n<p><code>MinMaxScaler</code> rescales the data set such that all feature values are in\n  the range [0, 1] as shown in the right panel below. However, this\n  scaling compress all inliers in the narrow range [0, 0.005] for the\n  transformed number of households.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\nScaling the data means it helps to  Normalize the data within a particular range.</p>\n<p>When MinMaxScaler is used the it is also known as Normalization and it transform all the values in range between (0 to 1)\nformula is x = [(value - min)/(Max- Min)]</p>\n<p>StandardScaler comes under Standardization and its value ranges between (-3 to +3)\nformula is z = [(x - x.mean)/Std_deviation]</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/1877505/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2017-02-09 06:59:39Z\">7 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/1877505/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is the best programming language for artificial intelligence purposes? </p>\n<p>Mind that using suggested language I must be able to employ any AI technique (or at least most of them).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>All the cool bearded gurus in what's left of AI research use Lisp :)</p>\n<p>There are two big camps: Common Lisp and Scheme. They have different syntax, etc. Lots of good stuff written for both.</p>\n<p>Java is a very popular all-purpose language but a lot of the interesting stuff in AI / Functional Programming, such as passing closures as first-order objects, is clumsy to do in Java.</p>\n<p>My personal preference would be to stay away from Windowsy languages like C# and F#. Cool people develop under Unix. Or Linux if they're cool but poor.</p>\n<p>Some cool but weird people program in Haskell. A reasonably modern FP language with good performance. I tried it once, it made my brain hurt; but you might be smarter than I am.</p>\n<hr/>\n<p><strong>UPDATE:</strong> Answers to Steve's questions.</p>\n<ol>\n<li><p>I wouldn't be the one paying for a Unix variant; that's what corporations and research institutes do. The idea is, you want to be doing AI research for an outfit that sinks millions into their hardware and doesn't balk at paying a few thousand for an operating system. That's the kind of outfit likely to have good food in the cafeteria and/or pay well for doing fun work. But I'm certainly not knocking Linux.</p></li>\n<li><p>F# may be cool but I see a whole raft of issues getting it to run on Linux or any other Unix (that's what I meant by \"windowsy\"), and I don't want to work under Windows (that's what I meant by \"personal preference\"). </p></li>\n<li><p>To elaborate on the \"windowsy\" theme: You mention that F# is an OCaml variant. From my own admittedly brief research, it seems that F# is missing functors, OCaml-style objects, polymorphic variants and the camlp4 preprocessor. A functional language without functors? Really? If one were disposed to not like Microsoft, as I admittedly am, one could conclude that they had gone ahead and crowbarred a perfectly good functional language, OCaml, into something they could get to run in their CLR so they could claim to \"have\" a functional language. Finally, because I don't suspect, I <em>know</em> that Microsoft always prioritizes market dominance over product quality, I don't plan to touch F#. But this is my personal preference, and clearly identified as such, while we're really more concerned with making a good recommendation for mary.ja45 .</p></li>\n</ol>\n<p>I have better reasons to recommend Lisp over F# and even OCaml and Haskell. These are mostly based on the historic preponderance of Lisp over any other language in the AI field. </p>\n<ul>\n<li><p>The bulk of AI literature is based on programs written in Lisp or Prolog. If nothing else, good knowledge of Lisp would allow a student to understand the sample programs. My personal favorite AI megaproject, Cyc, has runtimes in your choice of Common Lisp or C.</p></li>\n<li><p>In the TIOBE index of programming language (as seen and used in industry), Lisp takes 15th place while Haskell takes 43rd and F# and OCaml place below 50th. Presence on the market correlates with employment opportunities, naturally.</p></li>\n</ul>\n<p>That said, it's quite possible that a number of the younger \"AI interesting\" languages are poised to skyrocket. If some major research institute published some groundbreaking, defining-the-field research in, say, Scala, you'd see Scala's popularity advance sharply in the research community and, with some lag, in industry.</p>\n<p>I (obviously) can't comment on F#'s other qualities but you're as welcome to make recommendations as I was.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Python seems to be used a lot in the general scientific community. It has a lot of libraries available and it's easy to learn.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'll throw Scala into the pot.</p>\n<ul>\n<li>it's usable for functional programming</li>\n<li>it can be made as fast as Java</li>\n<li>it's a modern language with lot's of nice aspects</li>\n<li>Java seems to be a bit popular in AI, too and so you can use all those Java libraries from Scala</li>\n</ul>\n<p>I've solved all exercises from a basic AI course in Scala. It worked really well.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between </p>\n<pre><code>   tf.add(x, y)\n</code></pre>\n<p>and </p>\n<pre><code>   x + y\n</code></pre>\n<p>in TensorFlow? What would be different in your computation graph when you construct your graph with <code>+</code> instead of <code>tf.add()</code>? </p>\n<p>More generally, are  <code>+</code> or other operations overloaded for tensors?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If at least one of <code>x</code> or <code>y</code> is a <a href=\"https://www.tensorflow.org/versions/0.6.0/api_docs/python/framework.html#Tensor\" rel=\"noreferrer\"><code>tf.Tensor</code></a> object, the expressions <code>tf.add(x, y)</code> and <code>x + y</code> are equivalent. The main reason you might use <code>tf.add()</code> is to specify an explicit <a href=\"https://www.tensorflow.org/versions/0.6.0/api_docs/python/math_ops.html#add\" rel=\"noreferrer\"><code>name</code></a> keyword argument for the created op, which is not possible with the overloaded operator version.</p>\n<p>Note that if neither <code>x</code> nor <code>y</code> is a <code>tf.Tensor</code>—for example if they are NumPy arrays—then <code>x + y</code> will not create a TensorFlow op. <code>tf.add()</code> always creates a TensorFlow op and converts its arguments to <code>tf.Tensor</code> objects. Therefore, if you are writing a library function that might accept both tensors and NumPy arrays, you might prefer to use <code>tf.add()</code>.</p>\n<p>The following operators are overloaded in the TensorFlow Python API:</p>\n<ul>\n<li><code>__neg__</code> (unary <code>-</code>)</li>\n<li><code>__abs__</code> (<code>abs()</code>)</li>\n<li><code>__invert__</code> (unary <code>~</code>)</li>\n<li><code>__add__</code> (binary <code>+</code>)</li>\n<li><code>__sub__</code> (binary <code>-</code>)</li>\n<li><code>__mul__</code> (binary elementwise <code>*</code>)</li>\n<li><code>__div__</code> (binary <code>/</code> in Python 2)</li>\n<li><code>__floordiv__</code> (binary <code>//</code> in Python 3)</li>\n<li><code>__truediv__</code> (binary <code>/</code> in Python 3)</li>\n<li><code>__mod__</code> (binary <code>%</code>)</li>\n<li><code>__pow__</code> (binary <code>**</code>)</li>\n<li><code>__and__</code> (binary <code>&amp;</code>)</li>\n<li><code>__or__</code> (binary <code>|</code>)</li>\n<li><code>__xor__</code> (binary <code>^</code>)</li>\n<li><code>__lt__</code> (binary <code>&lt;</code>)</li>\n<li><code>__le__</code> (binary <code>&lt;=</code>)</li>\n<li><code>__gt__</code> (binary <code>&gt;</code>)</li>\n<li><code>__ge__</code> (binary <code>&gt;=</code>)</li>\n</ul>\n<p>Please note, <code>__eq__</code> ( binary <code>==</code> ) is <strong>not</strong> overloaded. <code>x == y</code> will simply return a Python boolean whether <code>x</code> and <code>y</code> refer to the same tensor. You need to use <a href=\"https://www.tensorflow.org/api_docs/python/tf/equal\" rel=\"noreferrer\"><code>tf.equal()</code></a> explicitly to check for element-wise equality. Same goes for not equal, <code>__ne__</code> ( binary <code>!=</code> ).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Mrry nicely explained that there is no real difference. I will just add when using <code>tf.add</code> is beneficial.</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/math/add\" rel=\"nofollow noreferrer\">tf.add</a> has one important parameter which is <code>name</code>. It allows you to name the operation in a graph which will be visible in tensorboard. So my rule of thumb, if it will be beneficial to name an operation in tensorboard, I use <code>tf.</code> equivalent, otherwise I go for brevity and use overloaded version.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have the following data:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   Group_ID Item_id Target\n0         1       1      0\n1         1       2      0\n2         1       3      1\n3         2       4      0\n4         2       5      1\n5         2       6      1\n6         3       7      0\n7         4       8      0\n8         5       9      0\n9         5      10      1\n</code></pre>\n<p>I need to split the dataset into a training and testing set based on the \"Group_ID\" so that 80% of the data goes into a training set and 20% into a test set.</p>\n<p>That is, I need my training set to look something like:</p>\n<pre class=\"lang-none prettyprint-override\"><code>    Group_ID Item_id Target\n0          1       1      0\n1          1       2      0\n2          1       3      1\n3          2       4      0\n4          2       5      1\n5          2       6      1\n6          3       7      0\n7          4       8      0\n</code></pre>\n<p>And test set:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   Group_ID Item_id Target\n8         5       9      0\n9         5      10      1\n</code></pre>\n<p>What would be the simplest way to do this? As far as I know, the standard <code>test_train_split</code> function in sklearn does not support splitting by groups in a way where I can also indicate the size of the split (e.g. 80/20).</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I figured out the answer. This seems to work:</p>\n<pre><code>from sklearn.model_selection import GroupShuffleSplit \n\nsplitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7)\nsplit = splitter.split(df, groups=df['Group_Id'])\ntrain_inds, test_inds = next(split)\n\ntrain = df.iloc[train_inds]\ntest = df.iloc[test_inds]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am having trouble with the Keras backend functions for setting values.  I am trying to convert a model from PyTorch to Keras and am trying to set the weights of the Keras model, but the weights do not appear to be getting set.  Note: I am not actually setting with np.ones just using that for an example.</p>\n<p>I have tried...</p>\n<p>Loading an existing model</p>\n<pre><code>import keras\nfrom keras.models import load_model, Model\nmodel = load_model(model_dir+file_name)\nkeras_layer = [layer for layer in model.layers if layer.name=='conv2d_1'][0]\n</code></pre>\n<p>Creating a simple model</p>\n<pre><code>img_input = keras.layers.Input(shape=(3,3,3))\nx = keras.layers.Conv2D(1, kernel_size=1, strides=1, padding=\"valid\", \nuse_bias=False, name='conv1')(img_input)\nmodel = Model(img_input, x)\nkeras_layer = [layer for layer in model.layers if layer.name=='conv1'][0]\n</code></pre>\n<p>Then using set_weights or set_value</p>\n<pre><code>keras_layer.set_weights([np.ones((1, 1, 3, 1))])\n</code></pre>\n<p>or...</p>\n<pre><code>K.batch_set_value([(weight,np.ones((1, 1, 3, 1))) for weight in keras_layer.weights])\n</code></pre>\n<p>afterwards I call either one of the following:</p>\n<pre><code>K.batch_get_value([weight for weight in keras_layer.weights])\nkeras_layer.get_weights()\n</code></pre>\n<p>And None of the weights appear to have been set.  The same values as before are returned.</p>\n<pre><code>[array([[[[  1.61547325e-06],\n      [  2.97779252e-06],\n      [  1.50160542e-06]]]], dtype=float32)]\n</code></pre>\n<p>How do I set the weights of a layer in Keras with a numpy array of values?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is <code>keras_layer</code> in your code?</p>\n<p>You can set weights these ways:</p>\n<pre><code>model.layers[i].set_weights(listOfNumpyArrays)    \nmodel.get_layer(layerName).set_weights(...)\nmodel.set_weights(listOfNumpyArrays)\n</code></pre>\n<p>Where <code>model</code> is an instance of an existing model. \nYou can see the expected length of the list and its array shapes using the method <code>get_weights()</code> from the same instances above.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The set_weights() method of keras accepts a list of numpy arrays, what you have passed to the method seems like a single array.\nThe shape of this should be the same as the shape of the output of get_weights() on the same layer.\nHere's the code:</p>\n<pre><code>l=[]\nx=np.array() #weights\ny=np.array() #array of biases\nl.append(x)\nl.append(y)\nloaded_model.layers[0].set_weights(l) #loaded_model.layer[0] being the layer\n</code></pre>\n<p>This worked for me and it returns the updated weights on calling get_weights().</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you are trying to convert Pytorch model to Keras model, you can also try a <a href=\"https://github.com/nerox8664/pytorch2keras\" rel=\"nofollow noreferrer\">Pytorch2Keras</a> converter.</p>\n<p>It supports base layers like Conv2d, Linear, Activations, some element-wise operations etc. You can follow <code>pytorch2keras/layers.py</code> for layer convertion functions. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>All this time (specially in Netflix contest), I always come across this blog (or leaderboard forum) where they mention how by applying a simple SVD step on data helped them in reducing sparsity in data or in general improved the performance of their algorithm in hand.\nI am trying to think (since long time) but I am not able to guess why is it so.\nIn general, the data in hand I get is very noisy (which is also the fun part of bigdata) and then I do know some basic feature scaling stuff like log-transformation stuff , mean normalization.\nBut how does something like SVD helps.\nSo lets say i have a huge matrix of user rating movies..and then in this matrix, I implement some version of recommendation system (say collaborative filtering):</p>\n<pre><code>1) Without SVD\n2) With SVD\n</code></pre>\n<p>how does it helps</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>SVD is not used to normalize the data, but to get rid of redundant data, that is, for dimensionality reduction. For example, if you have two variables, one is humidity index and another one is probability of rain, then their correlation is so high, that the second one does not contribute with any additional information useful for a classification or regression task. The eigenvalues in SVD help you determine what variables are most informative, and which ones you can do without. </p>\n<p>The way it works is simple. You perform SVD over your training data (call it matrix A), to obtain U, S and V*. Then set to zero all values of S less than a certain arbitrary threshold (e.g. 0.1), call this new matrix S'. Then obtain A' = US'V* and use A' as your new training data. Some of your features are now set to zero and can be removed, sometimes without any performance penalty (depending on your data and the threshold chosen). This is called <strong><em>k</em></strong>-truncated SVD.</p>\n<p>SVD doesn't help you with sparsity though, only helps you when features are redundant. Two features can be both sparse and informative (relevant) for a prediction task, so you can't remove either one.</p>\n<p>Using SVD, you go from <strong><em>n</em></strong> features to <strong><em>k</em></strong> features, where each one will be a linear combination of the original <code>n</code>. It's a dimensionality reduction step, just like feature selection is. When redundant features are present, though, a feature selection algorithm may lead to better classification performance than SVD depending on your data set (for example, maximum entropy feature selection). <a href=\"http://www.cs.waikato.ac.nz/ml/weka/\" rel=\"noreferrer\">Weka</a> comes with a bunch of them.</p>\n<p>See: <a href=\"http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition\" rel=\"noreferrer\">http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition</a></p>\n<p><a href=\"https://stats.stackexchange.com/questions/33142/what-happens-when-you-apply-svd-to-a-collaborative-filtering-problem-what-is-th\">https://stats.stackexchange.com/questions/33142/what-happens-when-you-apply-svd-to-a-collaborative-filtering-problem-what-is-th</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The Singular Value Decomposition is often used to approximate a matrix <code>X</code> by a low rank matrix <code>X_lr</code>:</p>\n<ol>\n<li>Compute the SVD <code>X = U D V^T</code>.</li>\n<li>Form the matrix <code>D'</code> by keeping the <code>k</code> largest singular values and setting the others to zero.</li>\n<li>Form the matrix <code>X_lr</code> by <code>X_lr = U D' V^T</code>.</li>\n</ol>\n<p>The matrix <code>X_lr</code> is then the best approximation of rank <code>k</code> of the matrix <code>X</code>, for the <a href=\"http://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm\" rel=\"noreferrer\">Frobenius norm</a> (the equivalent of the <code>l2</code>-norm for matrices). It is computationally efficient to use this representation, because if your matrix <code>X</code> is <code>n</code> by <code>n</code> and <code>k &lt;&lt; n</code>, you can store its low rank approximation with only <code>(2n + 1)k</code> coefficients (by storing <code>U</code>, <code>D'</code> and <code>V</code>).</p>\n<p>This was often used in matrix completion problems (such as collaborative filtering) because the true matrix of user ratings is assumed to be low rank (or well approximated by a low rank matrix). So, you wish to recover the true matrix by computing the best low rank approximation of your data matrix. However, there are now better ways to recover low rank matrices from noisy and missing observations, namely nuclear norm minimization. See for example the paper <a href=\"http://www-stat.stanford.edu/~candes/papers/OptimalCompletion.pdf\" rel=\"noreferrer\">The power of convex relaxation: Near-optimal matrix completion</a> by E. Candes and T. Tao. </p>\n<p>(Note: the algorithms derived from this technique also store the SVD of the estimated matrix, but it is computed differently).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>PCA or SVD, when used for dimensionality reduction, reduce the number of inputs. This, besides saving computational cost of learning and/or predicting, can <em>sometimes</em> produce more robust models that are not optimal in statistical sense, but have better performance in noisy conditions.</p>\n<p>Mathematically, simpler models have less variance, i.e. they are less prone to overfitting. Underfitting, of-course, can be a problem too. This is known as bias-variance dilemma. Or, as said in plain words by Einstein: Things should be made as simple as possible, but not simpler.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to build a simple Autoencoder using the KMNIST dataset from Tensorflow and some sample code from a textbook I'm using, but I keep getting an error when I try to fit the model.</p>\n<p>The error says <code>ValueError: Layer sequential_20 expects 1 inputs, but it received 2 input tensors.</code></p>\n<p>I'm really new to TensorFlow, and all my research on this error has baffled me since it seems to involve things not in my code.\n<a href=\"https://stackoverflow.com/questions/61006764/valueerror-layer-model-2-expects-2-inputs-but-it-received-1-input-tensors\">This thread</a> wasn't helpful since I'm only using sequential layers.</p>\n<p>Code in full:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#data = tfds.load(name = 'kmnist')\n\n(img_train, label_train), (img_test, label_test) = tfds.as_numpy(tfds.load(\n    name = 'kmnist',\n    split=['train', 'test'],\n    batch_size=-1,\n    as_supervised=True,\n))\n\nimg_train = img_train.squeeze()\nimg_test = img_test.squeeze()\n\n## From Hands on Machine Learning Textbook, chapter 17\n\nstacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(30, activation=\"selu\"),\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\nstacked_ae.compile(loss=\"binary_crossentropy\",\n                   optimizer=keras.optimizers.SGD(lr=1.5))\n\nhistory = stacked_ae.fit(img_train, img_train, epochs=10,\n                         validation_data=[img_test, img_test])\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>it helped me when I changed:<br/>\n<code>validation_data=[X_val, y_val]</code>  into <code>validation_data=(X_val, y_val)</code><br/>\nActually still wonder why?</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As stated in the Keras API reference (<a href=\"https://keras.io/api/models/model_training_apis/\" rel=\"noreferrer\">link</a>),</p>\n<blockquote>\n<p><strong>validation_data</strong>: ... validation_data could be: - <code>tuple</code> (x_val, y_val) of Numpy arrays or tensors - <code>tuple</code> (x_val, y_val, val_sample_weights) of Numpy arrays - dataset ...</p>\n</blockquote>\n<p>So, <strong>validation_data</strong> has to be a tuple rather than a list (of Numpy arrays or tensors). We should use parentheses (round brackets) <code>(...)</code>, not square brackets <code>[...]</code>.</p>\n<p>According to my limited experience, however, <strong>TensorFlow 2.0.0 would be indifferent to the use of square brackets, but TensorFlow 2.3.0 would complain about it</strong>. Your script would be fine if it is run under TF 2.0 intead of TF 2.3.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <code>validation_data=(img_test, img_test)</code> instead of <code>validation_data=[img_test, img_test]</code></p>\n<p>Here the example with encoder and decoder combined together:</p>\n<pre><code>stacked_ae = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(30, activation=\"selu\"),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\nstacked_ae.compile(loss=\"binary_crossentropy\",\n                   optimizer=keras.optimizers.SGD(lr=1.5))\n\nhistory = stacked_ae.fit(img_train, img_train, epochs=10,\n                         validation_data=(img_test, img_test))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm working on a multivariate (100+ variables) multi-step (t1 to t30) forecasting problem where the time series frequency is every 1 minute. The problem requires to forecast one of the 100+ variables as target.\nI'm interested to know if it's possible to do it using FB Prophet's Python API. I was able to do it in a univariate fashion using only the target variable and the datetime variable. Any help and direction is appreciated. Please let me know if any further input or clarity is needed on the question.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can add additional variables in Prophet using the <a href=\"https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#additional-regressors\" rel=\"noreferrer\">add_regressor method</a>.</p>\n<p>For example if we want to predict variable <code>y</code> using also the values of the additional variables <code>add1</code> and <code>add2</code>.</p>\n<p>Let's first create a sample df:</p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame(pd.date_range(start=\"2019-09-01\", end=\"2019-09-30\", freq='D', name='ds'))\ndf[\"y\"] = range(1,31)\ndf[\"add1\"] = range(101,131)\ndf[\"add2\"] = range(201,231)\ndf.head()\n            ds  y   add1 add2\n0   2019-09-01  1   101 201\n1   2019-09-02  2   102 202\n2   2019-09-03  3   103 203\n3   2019-09-04  4   104 204\n4   2019-09-05  5   105 205\n</code></pre>\n<p>and split train and test:</p>\n<pre><code>df_train = df.loc[df[\"ds\"]&lt;\"2019-09-21\"]\ndf_test  = df.loc[df[\"ds\"]&gt;=\"2019-09-21\"]\n</code></pre>\n<p>Before training the forecaster, we can add regressors that use the additional variables. Here the argument of <code>add_regressor</code> is the column name of the additional variable in the training df.</p>\n<pre><code>from fbprophet import Prophet\nm = Prophet()\nm.add_regressor('add1')\nm.add_regressor('add2')\nm.fit(df_train)\n</code></pre>\n<p>The predict method will then use the additional variables to forecast:</p>\n<pre><code>forecast = m.predict(df_test.drop(columns=\"y\"))\n</code></pre>\n<p>Note that <strong>the additional variables should have values for your future (test) data</strong>. If you don't have them, you could start by predicting <code>add1</code> and <code>add2</code> with univariate timeseries, and then predict <code>y</code> with <code>add_regressor</code> and the predicted <code>add1</code> and <code>add2</code> as future values of the additional variables.</p>\n<p>From the documentation I understand that the forecast of <code>y</code> for t+1 will only use the values of <code>add1</code> and <code>add2</code> at t+1, and not their values at t, t-1, ..., t-n as it does with <code>y</code>. If that is important for you, you could create new additional variables with the lags.</p>\n<p>See also <a href=\"https://nbviewer.jupyter.org/github/nicolasfauchereau/Auckland_Cycling/blob/master/notebooks/Auckland_cycling_and_weather.ipynb\" rel=\"noreferrer\">this notebook</a>, with an example of using weather factors as extra regressors in a forecast of bicycle usage.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To do forecasting for more than one dependent variable you need to implement that time series using Vector Auto Regression. </p>\n<p>In  VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables.</p>\n<p>for more information on VAR go to <a href=\"https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/\" rel=\"nofollow noreferrer\">https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am confused, it seems like there is no agreement if Prophet works in multivariate way, see the github issues <a href=\"https://github.com/facebook/prophet/issues/665\" rel=\"nofollow noreferrer\">here</a> and <a href=\"https://github.com/facebook/prophet/issues/101\" rel=\"nofollow noreferrer\">here</a>. Judging by some comments, queise's answer and a nice youtube tutorial you can somehow make a work around to multivariate functionality, see the video here: <a href=\"https://www.youtube.com/watch?v=XZhPO043lqU\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=XZhPO043lqU</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I thought <code>mask_zero=True</code> will output 0's when the input value is 0, so the following layers could skip computation or something.</p>\n<p>How does <code>mask_zero</code> works? </p>\n<p>Example: </p>\n<pre><code>data_in = np.array([\n  [1, 2, 0, 0]\n])\ndata_in.shape\n&gt;&gt;&gt; (1, 4)\n\n# model\nx = Input(shape=(4,))\ne = Embedding(5, 5, mask_zero=True)(x)\n\nm = Model(inputs=x, outputs=e)\np = m.predict(data_in)\nprint(p.shape)\nprint(p)\n</code></pre>\n<p>The actual output is: (the numbers are random)</p>\n<pre><code>(1, 4, 5)\n[[[ 0.02499047  0.04617121  0.01586803  0.0338897   0.009652  ]\n  [ 0.04782704 -0.04035913 -0.0341589   0.03020919 -0.01157228]\n  [ 0.00451764 -0.01433611  0.02606953  0.00328832  0.02650392]\n  [ 0.00451764 -0.01433611  0.02606953  0.00328832  0.02650392]]]\n</code></pre>\n<p>However, I thought the output will be:</p>\n<pre><code>[[[ 0.02499047  0.04617121  0.01586803  0.0338897   0.009652  ]\n  [ 0.04782704 -0.04035913 -0.0341589   0.03020919 -0.01157228]\n  [ 0 0 0 0 0]\n  [ 0 0 0 0 0]]]\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Actually, setting <code>mask_zero=True</code> for the Embedding layer does not result in returning a zero vector. Rather, the behavior of the Embedding layer would not change and it would return the embedding vector with index zero. You can confirm this by checking the Embedding layer weights (i.e. in the example you mentioned it would be <code>m.layers[0].get_weights()</code>). Instead, it would affect the behavior of the following layers such as RNN layers. </p>\n<p>If you inspect the source code of Embedding layer you would see a method called <a href=\"https://github.com/keras-team/keras/blob/920e8af34a43ad2cd11190a21200a2acbfd83e11/keras/layers/embeddings.py#L112\" rel=\"noreferrer\"><code>compute_mask</code></a>:</p>\n<pre><code>def compute_mask(self, inputs, mask=None):\n    if not self.mask_zero:\n        return None\n    output_mask = K.not_equal(inputs, 0)\n    return output_mask\n</code></pre>\n<p>This output mask will be passed, as the <code>mask</code> argument, to the following layers which support masking. This has been implemented in the <a href=\"https://github.com/keras-team/keras/blob/d2ebf181b603e7d03e65df941dd754df5de32913/keras/engine/base_layer.py#L442\" rel=\"noreferrer\"><code>__call__</code></a> method of base layer, <code>Layer</code>:</p>\n<pre><code># Handle mask propagation.\nprevious_mask = _collect_previous_mask(inputs)\nuser_kwargs = copy.copy(kwargs)\nif not is_all_none(previous_mask):\n    # The previous layer generated a mask.\n    if has_arg(self.call, 'mask'):\n        if 'mask' not in kwargs:\n            # If mask is explicitly passed to __call__,\n            # we should override the default mask.\n            kwargs['mask'] = previous_mask\n</code></pre>\n<p>And this makes the following layers to ignore (i.e. does not consider in their computations) this inputs steps. Here is a minimal example:</p>\n<pre><code>data_in = np.array([\n  [1, 0, 2, 0]\n])\n\nx = Input(shape=(4,))\ne = Embedding(5, 5, mask_zero=True)(x)\nrnn = LSTM(3, return_sequences=True)(e)\n\nm = Model(inputs=x, outputs=rnn)\nm.predict(data_in)\n\narray([[[-0.00084503, -0.00413611,  0.00049972],\n        [-0.00084503, -0.00413611,  0.00049972],\n        [-0.00144554, -0.00115775, -0.00293898],\n        [-0.00144554, -0.00115775, -0.00293898]]], dtype=float32)\n</code></pre>\n<p>As you can see the outputs of the LSTM layer for the second and forth timesteps are the same as the output of first and third timesteps, respectively. This means that  those timesteps have been masked.</p>\n<p><strong>Update:</strong> The mask will also be considered when computing the loss since the loss functions are internally augmented to support masking using <a href=\"https://github.com/keras-team/keras/blob/920e8af34a43ad2cd11190a21200a2acbfd83e11/keras/engine/training_utils.py#L375\" rel=\"noreferrer\"><code>weighted_masked_objective</code></a>:</p>\n<pre><code>def weighted_masked_objective(fn):\n    \"\"\"Adds support for masking and sample-weighting to an objective function.\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    \"\"\"\n</code></pre>\n<p><a href=\"https://github.com/keras-team/keras/blob/920e8af34a43ad2cd11190a21200a2acbfd83e11/keras/engine/training.py#L142\" rel=\"noreferrer\">when compiling the model</a>:</p>\n<pre><code>weighted_losses = [weighted_masked_objective(fn) for fn in loss_functions]\n</code></pre>\n<p>You can verify this using the following example:</p>\n<pre><code>data_in = np.array([[1, 2, 0, 0]])\ndata_out = np.arange(12).reshape(1,4,3)\n\nx = Input(shape=(4,))\ne = Embedding(5, 5, mask_zero=True)(x)\nd = Dense(3)(e)\n\nm = Model(inputs=x, outputs=d)\nm.compile(loss='mse', optimizer='adam')\npreds = m.predict(data_in)\nloss = m.evaluate(data_in, data_out, verbose=0)\nprint(preds)\nprint('Computed Loss:', loss)\n\n[[[ 0.009682    0.02505393 -0.00632722]\n  [ 0.01756451  0.05928303  0.0153951 ]\n  [-0.00146054 -0.02064196 -0.04356086]\n  [-0.00146054 -0.02064196 -0.04356086]]]\nComputed Loss: 9.041069030761719\n\n# verify that only the first two outputs \n# have been considered in the computation of loss\nprint(np.square(preds[0,0:2] - data_out[0,0:2]).mean())\n\n9.041070036475277\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The process of informing the Model that some part of the Data is actually Padding and should be ignored is called <strong>Masking</strong>.</p>\n<p>There are three ways to introduce <strong><code>input masks</code></strong> in Keras models:</p>\n<ol>\n<li>Add a <strong><code>keras.layers.Masking</code></strong> layer.</li>\n<li>Configure a <strong><code>keras.layers.Embedding</code></strong> layer with <strong><code>mask_zero=True</code></strong>.</li>\n<li>Pass a mask argument manually when calling layers that support this argument (e.g. RNN layers).</li>\n</ol>\n<p>Given below is the code to introduce <strong><code>Input Masks</code></strong> using <strong><code>keras.layers.Embedding</code></strong></p>\n<pre><code>import numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\n\nraw_inputs = [[83, 91, 1, 645, 1253, 927],[73, 8, 3215, 55, 927],[711, 632, 71]]\npadded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n                                                              padding='post')\n\nprint(padded_inputs)\n\nembedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\nmasked_output = embedding(padded_inputs)\n\nprint(masked_output._keras_mask)\n</code></pre>\n<p>Output of the above code is shown below:</p>\n<pre><code>[[  83   91    1  645 1253  927]\n [  73    8 3215   55  927    0]\n [ 711  632   71    0    0    0]]\n\ntf.Tensor(\n[[ True  True  True  True  True  True]\n [ True  True  True  True  True False]\n [ True  True  True False False False]], shape=(3, 6), dtype=bool)\n</code></pre>\n<p>For more information, refer this <a href=\"https://www.tensorflow.org/guide/keras/masking_and_padding\" rel=\"noreferrer\">Tensorflow Tutorial</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to evaluate a regression model build with scikitlearn using cross-validation and getting confused, which of the two functions <code>cross_val_score</code> and <code>cross_val_predict</code> I should use.\nOne option would be :</p>\n<pre><code>cvs = DecisionTreeRegressor(max_depth = depth)\nscores = cross_val_score(cvs, predictors, target, cv=cvfolds, scoring='r2')\nprint(\"R2-Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n</code></pre>\n<p>An other one, to use the cv-predictions with the standard <code>r2_score</code>:</p>\n<pre><code>cvp = DecisionTreeRegressor(max_depth = depth)\npredictions = cross_val_predict(cvp, predictors, target, cv=cvfolds)\nprint (\"CV R^2-Score: {}\".format(r2_score(df[target], predictions_cv)))\n</code></pre>\n<p>I would assume that both methods are valid and give similar results. But that is only the case with small k-folds. While the r^2 is roughly the same for 10-fold-cv, it gets increasingly lower for higher k-values in the case of the first version using \"cross_vall_score\". The second version is mostly unaffected by changing numbers of folds.</p>\n<p>Is this behavior to be expected and do I lack some understanding regarding CV in SKLearn? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>cross_val_score</code> returns score of test fold where <code>cross_val_predict</code> returns predicted y values for the test fold.</p>\n<p>For the <code>cross_val_score()</code>, you are using the average of the output, which will be affected by the number of folds because then it may have some folds which may have high error (not fit correctly).</p>\n<p>Whereas, <code>cross_val_predict()</code> returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. [Note that only cross-validation strategies that assign all elements to a test set exactly once can be used]. So the increasing the number of folds, only increases the training data for the test element, and hence its result may not be affected much.</p>\n<p><strong>Edit</strong> (after comment)</p>\n<p>Please have a look the following answer on how <code>cross_val_predict</code> works:</p>\n<p><a href=\"https://stackoverflow.com/questions/41458834/how-is-scikit-learn-cross-val-predict-accuracy-score-calculated\">How is scikit-learn cross_val_predict accuracy score calculated?</a></p>\n<p>I think that <code>cross_val_predict</code> will be overfit because as the folds increase, more data will be for train and less will for test. So the resultant label is more dependent on training data. Also as already told above, the prediction for one sample is done only once, so it may be susceptible to the splitting of data more. \nThats why most of the places or tutorials recommend using the <code>cross_val_score</code> for analysis.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So this question also bugged me and while the other's made good points, they didn't answer all aspects of OP's question.</p>\n<p>The true answer is: The divergence in scores for increasing k is due to the chosen metric R2 (coefficient of determination). For e.g. MSE, MSLE or MAE there won't be any difference in using <code>cross_val_score</code> or <code>cross_val_predict</code>.</p>\n<p>See the <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" rel=\"noreferrer\">definition of R2</a>:</p>\n<p><em>R^2 = 1 - (MSE(ground truth, prediction)/ MSE(ground truth, <strong>mean(ground truth)</strong>))</em></p>\n<p>The bold part explains why the score starts to differ for increasing k: the more splits we have, the fewer samples in the test fold and the higher the variance in the mean of the test fold.\nConversely, for small k, the mean of the test fold won't differ much of the full ground truth mean, as sample size is still large enough to have small variance.</p>\n<p>Proof:</p>\n<pre><code>import numpy as np\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import mean_squared_log_error as msle, r2_score\n\npredictions = np.random.rand(1000)*100\ngroundtruth = np.random.rand(1000)*20\n\ndef scores_for_increasing_k(score_func):\n    skewed_score = score_func(groundtruth, predictions)\n    print(f'skewed score (from cross_val_predict): {skewed_score}')\n    for k in (2,4,5,10,20,50,100,200,250):\n        fold_preds = np.split(predictions, k)\n        fold_gtruth = np.split(groundtruth, k)\n        correct_score = np.mean([score_func(g, p) for g,p in zip(fold_gtruth, fold_preds)])\n\n        print(f'correct CV for k={k}: {correct_score}')\n\nfor name, score in [('MAE', mae), ('MSLE', msle), ('R2', r2_score)]:\n    print(name)\n    scores_for_increasing_k(score)\n    print()\n</code></pre>\n<p>Output will be:</p>\n<pre><code>MAE\nskewed score (from cross_val_predict): 42.25333901481263\ncorrect CV for k=2: 42.25333901481264\ncorrect CV for k=4: 42.25333901481264\ncorrect CV for k=5: 42.25333901481264\ncorrect CV for k=10: 42.25333901481264\ncorrect CV for k=20: 42.25333901481264\ncorrect CV for k=50: 42.25333901481264\ncorrect CV for k=100: 42.25333901481264\ncorrect CV for k=200: 42.25333901481264\ncorrect CV for k=250: 42.25333901481264\n\nMSLE\nskewed score (from cross_val_predict): 3.5252449697327175\ncorrect CV for k=2: 3.525244969732718\ncorrect CV for k=4: 3.525244969732718\ncorrect CV for k=5: 3.525244969732718\ncorrect CV for k=10: 3.525244969732718\ncorrect CV for k=20: 3.525244969732718\ncorrect CV for k=50: 3.5252449697327175\ncorrect CV for k=100: 3.5252449697327175\ncorrect CV for k=200: 3.5252449697327175\ncorrect CV for k=250: 3.5252449697327175\n\nR2\nskewed score (from cross_val_predict): -74.5910282783694\ncorrect CV for k=2: -74.63582817089443\ncorrect CV for k=4: -74.73848598638291\ncorrect CV for k=5: -75.06145142821893\ncorrect CV for k=10: -75.38967601572112\ncorrect CV for k=20: -77.20560102267272\ncorrect CV for k=50: -81.28604960074824\ncorrect CV for k=100: -95.1061197684949\ncorrect CV for k=200: -144.90258384605787\ncorrect CV for k=250: -210.13375041871123\n</code></pre>\n<p>Of course, there is another effect not shown here, which was mentioned by others.\nWith increasing k, there are more models trained on more samples and validated on fewer samples, which will effect the final scores, but this is not induced by the choice between <code>cross_val_score</code> and <code>cross_val_predict</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think the difference can be made clear by inspecting their outputs. Consider this snippet:</p>\n<pre><code># Last column is the label\nprint(X.shape)  # (7040, 133)\n\nclf = MLPClassifier()\n\nscores = cross_val_score(clf, X[:,:-1], X[:,-1], cv=5)\nprint(scores.shape)  # (5,)\n\ny_pred = cross_val_predict(clf, X[:,:-1], X[:,-1], cv=5)\nprint(y_pred.shape)  # (7040,)\n</code></pre>\n<p>Notice the shapes: why are these so?\n<code>scores.shape</code> has length 5 because it is a score computed with cross-validation over 5 folds (see argument <code>cv=5</code>). Therefore, a single real value is computed for each fold. That value is the score of the classifier:</p>\n<blockquote>\n<p>given true labels and predicted labels, how many answers the predictor were right in a particular fold?</p>\n</blockquote>\n<p>In this case, the y labels given in input are used twice: to learn from data and to evaluate the performances of the classifier.</p>\n<p>On the other hand, <code>y_pred.shape</code> has length 7040, which is the shape of the dataset. That is the length of the input dataset. This means that each value is not a score computed on multiple values, but a single value: the prediction of the classifier:</p>\n<blockquote>\n<p>given the input data and their labels, what is the prediction of the classifier on a specific example that was in a test set of a particular fold?</p>\n</blockquote>\n<p>Note that you do not know what fold was used: each output was computed on the test data of a certain fold, but you can't tell which (from this output, at least).</p>\n<p>In this case, the labels are used just once: to train the classifier. It's your job to compare these outputs to the true outputs to compute the score. If you just average them, as you did, the output is not a score, it's just the average prediction.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Simple machine learning question. Probably numerous ways to solve this:</p>\n<p>There is an <strong>infinite</strong> stream of  4 possible events:</p>\n<p><code>'event_1', 'event_2', 'event_4', 'event_4'</code></p>\n<p>The events do not come in in completely random order. We will assume that there are some complex patterns to the order that most events come in, and the rest of the events are just random. We do not know the patterns ahead of time though.</p>\n<p>After each event is received, I want to predict what the next event will be based on the order that events have come in in the past. So my question is: <strong>What machine learning algorithm should I use for this predictor?</strong></p>\n<p>The predictor will then be told what the next event actually was:</p>\n<pre><code>Predictor=new_predictor()\n\nprev_event=False\nwhile True:\n    event=get_event()\n    if prev_event is not False:\n        Predictor.last_event_was(prev_event)\n    predicted_event=Predictor.predict_next_event(event)\n</code></pre>\n<p>The question arises of how long of a history that the predictor should maintain, since maintaining infinite history will not be possible. I'll leave this up to you to answer. The answer can't be infinte though for practicality.</p>\n<p>So I believe that the predictions will have to be done with some kind of rolling history. Adding a new event and expiring an old event should therefore be rather efficient, and not require rebuilding the entire predictor model, for example.</p>\n<p>Specific code, instead of research papers, would add for me <strong>immense value</strong> to your responses. Python or C libraries are nice, but anything will do.</p>\n<p><strong>Update:</strong> And what if more than one event can happen simultaneously on each round. Does that change the solution?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is essentially a sequence prediction problem, so you want Recurrent neural networks or hidden Markov models.</p>\n<p>If you only have a fixed time to look back, time window approaches might suffice. You take the sequence data and split it into overlapping windows of length n. (eg. you split a sequence ABCDEFG into ABC, BCD, CDE, DEF, EFG). Then you train a function approximator (e.g. neural network or linear regression) to map the first n-1 parts of that window onto the nth part.</p>\n<p>Your predictor will not be able to look back in time longer than the size of your window. RNNs and HMMs can do so in theory, but are hard to tune or sometimes just don't work.</p>\n<p>(State of the art RNN implementations can be found in PyBrain <a href=\"http://pybrain.org\" rel=\"noreferrer\">http://pybrain.org</a>)</p>\n<p>Update: Here is the pybrain code for your problem. (I haven't tested it, there might be some typos and stuff, but the overall structure should work.)</p>\n<pre><code>from pybrain.datasets import SequentialDataSet\nfrom pybrain.supervised.trainers import BackpropTrainer\nfrom pybrain.tools.shortcuts import buildNetwork\nfrom pybrain.structure import SigmoidLayer\n\nINPUTS = 4\nHIDDEN = 10\nOUTPUTS = 4\n\nnet = buildNetwork(INPUTS, HIDDEN, OUTPUTS, hiddenclass=LSTMLayer, outclass=SigmoidLayer, recurrent=True)\n\nds = SequentialDataSet(INPUTS, OUTPUTS)\n\n# your_sequences is a list of lists of tuples which each are a bitmask\n# indicating the event (so 1.0 at position i if event i happens, 0.0 otherwise)\n\nfor sequence in your_sequences:\n    for (inpt, target) in zip(sequence, sequence[1:]):\n        ds.newSequence()\n        ds.appendLinked(inpt, target)\n\nnet.randomize()\n\ntrainer = BackpropTrainer(net, ds, learningrate=0.05, momentum=0.99)\nfor _ in range(1000):\n    print trainer.train()\n</code></pre>\n<p>This will train the recurrent network for 1000 epochs and print out the error after every epochs. Afterwards you can check for correct predictions like this:</p>\n<pre><code>net.reset()\nfor i in sequence:\n  next_item = net.activate(i) &gt; 0.5\n  print next_item\n</code></pre>\n<p>This will print an array of booleans for every event.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Rather than keeping a full history, one can keep <strong><em>aggregated information</em></strong> about the past (along with a relatively short sliding history, to be used as input to the Predictor logic).</p>\n<p>A tentative implementation could go like this:<br/>\nIn a nutshell: <strong>Managing a set of Markov chains <em>of increasing order</em>, and <em>grading</em> and <em>averaging</em> their predictions</strong></p>\n<ul>\n<li>keep a table of individual event counts, the purpose is to calculate the probability of any of the 4 different events, without regards to any sequence.</li>\n<li>keep a table of bigram <em>counts</em>, i.e. a cumulative count the events observed [so far]<br/>\nTable starts empty, upon the second event observe, we can store the first bigram, with a count of 1.  upond the third event, the bigram made of the 2nd and 3rd events is \"added\" to the  table: either incrementing the count of an existing bigram or added with original count 1, as a new (never-seen-so-far) bigram.  etc.<br/>\nIn parallel, keep a total count of bigrams in the table.<br/>\nThis table and the total tally allow calculating the probability of a given event, based on the one preceding event. </li>\n<li>In a similar fashion keep a table of trigram counts, and a running tally of total trigram seen (note that this would be equal to the number of bigrams, minus one, since the first trigram is added one event after the first bigram, and after that one of each is added with each new event).  This trigram table allows calculating the probability of a given event based on the two preceding events.</li>\n<li>likewise, keep tables for N-Grams, up to, say, 10-grams (the algorithm will tell if we need to increase or decrease this).</li>\n<li>keep an sliding windows into the last 10 events.</li>\n<li>The above tables provide the basis for prediction; the general idea are to:\n\n<ul>\n<li>use a formula which expresses the probabilities of the next event as a weighed average of the individual probabilities based on the different N-grams. </li>\n<li>reward the better individual N-gram length by increasing the corresponding weight in the formula; punish the worse lengths in the reverse fashion. (Beware the marginal probability of individual events needs to be taken into account lest we favor N-grams which happen to predict the most frequent events, regardless of the relative poor predicting value associated with them them)</li>\n<li>Once the system has \"seen\" enough events, see the current values for the weights associated with the long N-Grams, and if these are relatively high, consider adding tables to keep aggregate info about bigger N-Grams.  (This unfortunately hurts the algorightm both in terms of space and time)</li>\n</ul></li>\n</ul>\n<p>There can be <strong>several variations on the general logic described above</strong>. In particular in the choice of the particular metric used to \"grade\" the quality of prediction of the individual N-Gram lengths. <br/>\nOther considerations should be put with regards to <strong>detecting and adapting to possible shifts in the events distribution</strong> (the above assumes a generally ergodic event source).  One possible approach is to use two sets of tables (combining the probabilities accordingly), and periodically dropping the contents of all tables of one of the sets.  Choosing the right period for these resets is a tricky business, essentially balancing the need for statistically significant volumes of history and the need for short enough period lest me miss on the shorter modulations...</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/13760967/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-05-26 12:48:10Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/13760967/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>In the beginning, I would like to describe my current position and the goal that I would like to achieve.</p>\n<p>I am a researcher dealing with machine learning. So far have gone through several theoretical courses covering machine learning algorithms and social network analysis and therefore have gained some theoretical concepts useful for implementing machine learning algorithms and feed in the real data.</p>\n<p>On simple examples, the algorithms work well and the running time is acceptable whereas the big data represent a problem if trying to run algorithms on my PC. Regarding the software I have enough experience to implement whatever algorithm from articles or design my own using whatever language or IDE (so far have used Matlab, Java with Eclipse, .NET...) but so far haven't got much experience with setting-up infrastructure. I have started to learn about Hadoop, NoSQL databases, etc, but I am not sure what strategy would be the best taking into consideration the learning time constraints.</p>\n<p><strong>The final goal is to be able to set-up a working platform for analyzing big data with focusing on implementing my own machine learning algorithms and put all together into production, ready for solving useful question by processing big data.</strong></p>\n<p>As the main focus is on implementing machine learning algorithms I would like to ask whether there is any existing running platform, offering enough CPU resources to feed in large data, upload own algorithms and simply process the data without thinking about distributed processing.</p>\n<p>Nevertheless, such a platform exists or not, I would like to gain a picture Big enough to be able to work in a team that could put into production the whole system tailored upon the specific customer demands. For example, a retailer would like to analyze daily purchases so all the daily records have to be uploaded to some infrastructure, capable enough to process the data by using custom machine learning algorithms.</p>\n<p>To put all the above into simple question: <strong>How to design a custom data mining solution for real-life problems with main focus on machine learning algorithms and put it into production, if possible, by using the existing infrastructure and if not, design distributed system (by using Hadoop or whatever framework).</strong></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First of all, your question needs to define more clearly what you intend by Big Data.</p>\n<p>Indeed, Big Data is a buzzword that may refer to various size of problems. I tend to define Big Data as the category of problems where the Data size or the Computation time is big enough for \"the hardware abstractions to become broken\", which means that a single commodity machine cannot perform the computations without intensive care of computations and memory.</p>\n<p>The scale threshold beyond which data become Big Data is therefore unclear and is sensitive to your implementation. Is your algorithm bounded by Hard-Drive bandwidth ? Does it have to feet into memory ? Did you try to avoid unnecessary quadratic costs ? Did you make any effort to improve cache efficiency, etc.</p>\n<p>From several years of experience in running medium large-scale machine learning challenge (on up to 250 hundreds commodity machine), I strongly believe that many problems that seem to require distributed infrastructure can actually be run on a single commodity machine if the problem is expressed correctly. For example, you are mentioning large scale data for retailers. I have been working on this exact subject for several years, and I often managed to make all the computations run on a single machine, provided a bit of optimisation. My company has been working on simple custom data format that allows one year of all the data from a very large retailer to be stored within 50GB, which means a single commodity hard-drive could hold 20 years of history. You can have a look for example at : <a href=\"https://github.com/Lokad/lokad-receiptstream\" rel=\"noreferrer\">https://github.com/Lokad/lokad-receiptstream</a></p>\n<p>From my experience, it is worth spending time in trying to optimize algorithm and memory so that you could avoid to resort to distributed architecture. Indeed, distributed architectures come with a triple cost. First of all, the strong knowledge requirements. Secondly, it comes with a large complexity overhead in the code. Finally, distributed architectures come with a significant latency overhead (with the exception of local multi-threaded distribution). </p>\n<p>From a practitioner point of view, being able to perform a given data mining or machine learning algorithm in 30 seconds is one the key factor to efficiency. I have noticed than when some computations, whether sequential or distributed, take 10 minutes, my focus and efficiency tend to drop quickly as it becomes much more complicated to iterate quickly and quickly test new ideas. The latency overhead introduced by many of the distributed frameworks is such that you will inevitably be in this low-efficiency scenario.</p>\n<p>If the scale of the problem is such that even with strong effort you cannot perform it on a single machine, then I strongly suggest to resort to on-shelf distributed frameworks instead of building your own. One of the most well known framework is the MapReduce abstraction, available through Apache Hadoop. Hadoop can be run on 10 thousands nodes cluster, probably much more than you will ever need. If you do not own the hardware, you can \"rent\" the use of a Hadoop cluster, for example through Amazon MapReduce.</p>\n<p>Unfortunately, the MapReduce abstraction is not suited to all Machine Learning computations.\nAs far as Machine Learning is concerned, MapReduce is a rigid framework and numerous cases have proved to be difficult or inefficient to adapt to this framework:</p>\n<p>– The MapReduce framework is in itself related to functional programming. The\nMap procedure is applied to each data chunk independently. Therefore, the\nMapReduce framework is not suited to algorithms where the application of the\nMap procedure to some data chunks need the results of the same procedure to\nother data chunks as a prerequisite. In other words, the MapReduce framework\nis not suited when the computations between the different pieces of data are\nnot independent and impose a specific chronology.</p>\n<p>– MapReduce is designed to provide a single execution of the map and of the\nreduce steps and does not directly provide iterative calls. It is therefore not\ndirectly suited for the numerous machine-learning problems implying iterative\nprocessing (Expectation-Maximisation (EM), Belief Propagation, etc.). The\nimplementation of these algorithms in a MapReduce framework means the\nuser has to engineer a solution that organizes results retrieval and scheduling\nof the multiple iterations so that each map iteration is launched after the reduce\nphase of the previous iteration is completed and so each map iteration is fed\nwith results provided by the reduce phase of the previous iteration.</p>\n<p>– Most MapReduce implementations have been designed to address production needs and\nrobustness. As a result, the primary concern of the framework is to handle\nhardware failures and to guarantee the computation results. The MapReduce efficiency\nis therefore partly lowered by these reliability constraints. For example, the\nserialization on hard-disks of computation results turns out to be rather costly\nin some cases.</p>\n<p>– MapReduce is not suited to asynchronous algorithms.</p>\n<p>The questioning of the MapReduce framework has led to richer distributed frameworks where more control and freedom are left to the framework user, at the price of more complexity for this user. Among these frameworks, GraphLab and Dryad (both based on Direct Acyclic Graphs of computations) are well-known.</p>\n<p>As a consequence, there is no \"One size fits all\" framework, such as there is no \"One size fits all\" data storage solution.</p>\n<p>To start with Hadoop, you can have a look at the book <a href=\"https://rads.stackoverflow.com/amzn/click/com/1449311520\" rel=\"nofollow noreferrer\">Hadoop: The Definitive Guide by Tom White</a></p>\n<p>If you are interested in how large-scale frameworks fit into Machine Learning requirements, you may be interested by the second chapter (in English) of my PhD, available here: <a href=\"http://tel.archives-ouvertes.fr/docs/00/74/47/68/ANNEX/texfiles/PhD%20Main/PhD.pdf\" rel=\"noreferrer\">http://tel.archives-ouvertes.fr/docs/00/74/47/68/ANNEX/texfiles/PhD%20Main/PhD.pdf</a> </p>\n<p>If you provide more insight about the specific challenge you want to deal with (type of algorithm, size of the data, time and money constraints, etc.), we probably could provide you a more specific answer.</p>\n<p>edit : another reference that could prove to be of interest : <a href=\"https://rads.stackoverflow.com/amzn/click/com/0521192242\" rel=\"nofollow noreferrer\">Scaling-up Machine Learning</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had to implement a couple of Data Mining algorithms to work with BigData too, and I ended up using Hadoop. \nI don't know if you are familiar to Mahout (<a href=\"http://mahout.apache.org/\" rel=\"noreferrer\">http://mahout.apache.org/</a>), which already has several algorithms ready to use with Hadoop.</p>\n<p>Nevertheless, if you want to implement your own Algorithm, you can still adapt it to Hadoop's MapReduce paradigm and get good results. This is an excellent book on how to adapt Artificial Intelligence algorithms to MapReduce:</p>\n<p>Mining of Massive Datasets - <a href=\"http://infolab.stanford.edu/~ullman/mmds.html\" rel=\"noreferrer\">http://infolab.stanford.edu/~ullman/mmds.html</a> </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This seems to be an old question. However given your usecase, the main frameworks focusing on Machine Learning in Big Data domain are Mahout, Spark (MLlib), H2O etc. However to run Machine Learning algorithms on Big Data you have to convert them to parallel programs based on Map Reduce paradigm. This is a nice article giving a brief introduction to major (not all) big Data frameworks: </p>\n<p><a href=\"http://www.codophile.com/big-data-frameworks-every-programmer-should-know/\" rel=\"nofollow\">http://www.codophile.com/big-data-frameworks-every-programmer-should-know/</a> </p>\n<p>I hope this will help.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I tried to play with Resnet. I tried to overfit over small data (3 <em>different</em> images) and see if I can get almost 0 loss and 1.0 accuracy - and I did.</p>\n<p>The problem is that predictions on the <strong>training</strong> images (i.e. the same 3 images used for training) are not correct..</p>\n<p><strong>Training Images</strong></p>\n<p><a href=\"https://i.sstatic.net/e7rjH.jpg\" rel=\"nofollow noreferrer\"><img alt=\"image 1\" src=\"https://i.sstatic.net/e7rjH.jpg\"/></a> <a href=\"https://i.sstatic.net/P1uJd.jpg\" rel=\"nofollow noreferrer\"><img alt=\"image 2\" src=\"https://i.sstatic.net/P1uJd.jpg\"/></a>\n<a href=\"https://i.sstatic.net/JGtlL.jpg\" rel=\"nofollow noreferrer\"><img alt=\"image 3\" src=\"https://i.sstatic.net/JGtlL.jpg\"/></a></p>\n<p><strong>Image labels</strong></p>\n<p><code>[1,0,0]</code>, <code>[0,1,0]</code>, <code>[0,0,1]</code></p>\n<p><strong>My python code</strong></p>\n<pre class=\"lang-python prettyprint-override\"><code>#loading 3 images and resizing them\nimgs = np.array([np.array(Image.open(\"./Images/train/\" + fname)\n                          .resize((197, 197), Image.ANTIALIAS)) for fname in\n                 os.listdir(\"./Images/train/\")]).reshape(-1,197,197,1)\n# creating labels\ny = np.array([[1,0,0],[0,1,0],[0,0,1]])\n# create resnet model\nmodel = ResNet50(input_shape=(197, 197,1),classes=3,weights=None)\n\n# compile &amp; fit model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n\nmodel.fit(imgs,y,epochs=5,shuffle=True)\n\n# predict on training data\nprint(model.predict(imgs))\n</code></pre>\n<p>The model does overfit the data:</p>\n<pre class=\"lang-python prettyprint-override\"><code>3/3 [==============================] - 22s - loss: 1.3229 - acc: 0.0000e+00\nEpoch 2/5\n3/3 [==============================] - 0s - loss: 0.1474 - acc: 1.0000\nEpoch 3/5\n3/3 [==============================] - 0s - loss: 0.0057 - acc: 1.0000\nEpoch 4/5\n3/3 [==============================] - 0s - loss: 0.0107 - acc: 1.0000\nEpoch 5/5\n3/3 [==============================] - 0s - loss: 1.3815e-04 - acc: 1.0000\n</code></pre>\n<p>but predictions are:</p>\n<pre class=\"lang-python prettyprint-override\"><code> [[  1.05677405e-08   9.99999642e-01   3.95520459e-07]\n [  1.11955103e-08   9.99999642e-01   4.14905685e-07]\n [  1.02637095e-07   9.99997497e-01   2.43751242e-06]]\n</code></pre>\n<p>which means that all images got <code>label=[0,1,0]</code></p>\n<p>why? and how can that happen?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It's because of the batch normalization layers.</p>\n<p>In training phase, the batch is normalized w.r.t. its mean and variance. However, in testing phase, the batch is normalized w.r.t. the <strong>moving average</strong> of previously observed mean and variance.</p>\n<p>Now this is a problem when the number of observed batches is small (e.g., 5 in your example) because in the <code>BatchNormalization</code> layer, by default <code>moving_mean</code> is initialized to be 0 and <code>moving_variance</code> is initialized to be 1.</p>\n<p>Given also that the default <code>momentum</code> is 0.99, you'll need to update the moving averages <em>quite a lot of times</em> before they converge to the \"real\" mean and variance.</p>\n<p>That's why the prediction is wrong in the early stage, but is correct after 1000 epochs.</p>\n<hr/>\n<p>You can verify it by forcing the <code>BatchNormalization</code> layers to operate in \"training mode\".</p>\n<p>During training, the accuracy is 1 and the loss is close to zero:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.fit(imgs,y,epochs=5,shuffle=True)\nEpoch 1/5\n3/3 [==============================] - 19s 6s/step - loss: 1.4624 - acc: 0.3333\nEpoch 2/5\n3/3 [==============================] - 0s 63ms/step - loss: 0.6051 - acc: 0.6667\nEpoch 3/5\n3/3 [==============================] - 0s 57ms/step - loss: 0.2168 - acc: 1.0000\nEpoch 4/5\n3/3 [==============================] - 0s 56ms/step - loss: 1.1921e-07 - acc: 1.0000\nEpoch 5/5\n3/3 [==============================] - 0s 53ms/step - loss: 1.1921e-07 - acc: 1.0000\n</code></pre>\n<p>Now if we evaluate the model, we'll observe high loss and low accuracy because after 5 updates, the moving averages are still pretty close to the initial values:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.evaluate(imgs,y)\n3/3 [==============================] - 3s 890ms/step\n[10.745396614074707, 0.3333333432674408]\n</code></pre>\n<p>However, if we manually specify the \"learning phase\" variable and let the <code>BatchNormalization</code> layers use the \"real\" batch mean and variance, the result becomes the same as what's observed in <code>fit()</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>sample_weights = np.ones(3)\nlearning_phase = 1  # 1 means \"training\"\nins = [imgs, y, sample_weights, learning_phase]\nmodel.test_function(ins)\n[1.192093e-07, 1.0]\n</code></pre>\n<hr/>\n<p>It's also possible to verify it by changing the momentum to a smaller value.</p>\n<p>For example, by adding <code>momentum=0.01</code> to all the batch norm layers in <code>ResNet50</code>, the prediction after 20 epochs is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model.predict(imgs)\narray([[  1.00000000e+00,   1.34882026e-08,   3.92139575e-22],\n       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n       [  8.70998792e-06,   5.31159838e-10,   9.99991298e-01]], dtype=float32)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm building a model that converts a string to another string using recurrent layers (GRUs). I have tried both a Dense and a TimeDistributed(Dense) layer as the last-but-one layer, but I don't understand the difference between the two when using return_sequences=True, especially as they seem to have the same number of parameters.</p>\n<p>My simplified model is the following:</p>\n<pre><code>InputSize = 15\nMaxLen = 64\nHiddenSize = 16\n\ninputs = keras.layers.Input(shape=(MaxLen, InputSize))\nx = keras.layers.recurrent.GRU(HiddenSize, return_sequences=True)(inputs)\nx = keras.layers.TimeDistributed(keras.layers.Dense(InputSize))(x)\npredictions = keras.layers.Activation('softmax')(x)\n</code></pre>\n<p>The summary of the network is:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 64, 15)            0         \n_________________________________________________________________\ngru_1 (GRU)                  (None, 64, 16)            1536      \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 64, 15)            255       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 64, 15)            0         \n=================================================================\n</code></pre>\n<p>This makes sense to me as my understanding of TimeDistributed is that it applies the same layer at all timepoints, and so the Dense layer has 16*15+15=255 parameters (weights+biases).</p>\n<p>However, if I switch to a simple Dense layer:</p>\n<pre><code>inputs = keras.layers.Input(shape=(MaxLen, InputSize))\nx = keras.layers.recurrent.GRU(HiddenSize, return_sequences=True)(inputs)\nx = keras.layers.Dense(InputSize)(x)\npredictions = keras.layers.Activation('softmax')(x)\n</code></pre>\n<p>I still only have 255 parameters:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 64, 15)            0         \n_________________________________________________________________\ngru_1 (GRU)                  (None, 64, 16)            1536      \n_________________________________________________________________\ndense_1 (Dense)              (None, 64, 15)            255       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 64, 15)            0         \n=================================================================\n</code></pre>\n<p>I wonder if this is because Dense() will only use the last dimension in the shape, and effectively treat everything else as a batch-like dimension. But then I'm no longer sure what the difference is between Dense and TimeDistributed(Dense).</p>\n<p><strong>Update</strong> Looking at <a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/core.py\" rel=\"noreferrer\">https://github.com/fchollet/keras/blob/master/keras/layers/core.py</a> it does seem that Dense uses the last dimension only to size itself:</p>\n<pre class=\"lang-python prettyprint-override\"><code>def build(self, input_shape):\n    assert len(input_shape) &gt;= 2\n    input_dim = input_shape[-1]\n\n    self.kernel = self.add_weight(shape=(input_dim, self.units),\n</code></pre>\n<p>It also uses keras.dot to apply the weights:</p>\n<pre class=\"lang-python prettyprint-override\"><code>def call(self, inputs):\n    output = K.dot(inputs, self.kernel)\n</code></pre>\n<p>The docs of keras.dot imply that it works fine on n-dimensional tensors. I wonder if its exact behavior means that Dense() will in effect be called at every time step. If so, the question still remains what TimeDistributed() achieves in this case.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>TimeDistributedDense</code> applies a same dense to every time step during GRU/LSTM Cell unrolling. So the error function will be between predicted label sequence and the actual label sequence. (Which is normally the requirement for sequence to sequence labeling problems).</p>\n<p>However, with <code>return_sequences=False</code>, <code>Dense</code> layer is applied only once at the last cell. This is normally the case when RNNs are used for classification problem. If <code>return_sequences=True</code> then <code>Dense</code> layer is applied to every timestep just like <code>TimeDistributedDense</code>.</p>\n<p>So for as per your models both are same, but if you change your second model to <code>return_sequences=False</code>, then <code>Dense</code> will be applied only at the last cell. Try changing it and the model will throw as error because then the <code>Y</code> will be of size <code>[Batch_size, InputSize]</code>, it is no more a sequence to sequence but a full sequence to label problem.</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Activation, TimeDistributed\nfrom keras.layers.recurrent import GRU\nimport numpy as np\n\nInputSize = 15\nMaxLen = 64\nHiddenSize = 16\n\nOutputSize = 8\nn_samples = 1000\n\nmodel1 = Sequential()\nmodel1.add(GRU(HiddenSize, return_sequences=True, input_shape=(MaxLen, InputSize)))\nmodel1.add(TimeDistributed(Dense(OutputSize)))\nmodel1.add(Activation('softmax'))\nmodel1.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n\nmodel2 = Sequential()\nmodel2.add(GRU(HiddenSize, return_sequences=True, input_shape=(MaxLen, InputSize)))\nmodel2.add(Dense(OutputSize))\nmodel2.add(Activation('softmax'))\nmodel2.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nmodel3 = Sequential()\nmodel3.add(GRU(HiddenSize, return_sequences=False, input_shape=(MaxLen, InputSize)))\nmodel3.add(Dense(OutputSize))\nmodel3.add(Activation('softmax'))\nmodel3.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nX = np.random.random([n_samples,MaxLen,InputSize])\nY1 = np.random.random([n_samples,MaxLen,OutputSize])\nY2 = np.random.random([n_samples, OutputSize])\n\nmodel1.fit(X, Y1, batch_size=128, nb_epoch=1)\nmodel2.fit(X, Y1, batch_size=128, nb_epoch=1)\nmodel3.fit(X, Y2, batch_size=128, nb_epoch=1)\n\nprint(model1.summary())\nprint(model2.summary())\nprint(model3.summary())\n</code></pre>\n<p>In the above example architecture of <code>model1</code> and <code>model2</code> are sample (sequence to sequence models) and <code>model3</code> is a full sequence to label model.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I trained my CNN (VGG) through google colab and generated .h5 file. Now problem is, I can predict my output successfully through google colab but when i download that .h5 trained model file and try to predict output on my laptop, I am getting error when loading the model.</p>\n<p>Here is the code:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nimport h5py\n\n# Initialization\n\nloaded_model = keras.models.load_model('./train_personCount_model.h5')\n</code></pre>\n<p>And the error:</p>\n<pre><code>ValueError: Unknown initializer: GlorotUniform\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I ran into the same issue. After changing:</p>\n<p><code>from tensorflow import keras</code></p>\n<p>to:</p>\n<p><code>import keras</code></p>\n<p>life is once again worth living.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I fixed the problem:</p>\n<p>Before:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from keras.models import load_model\nclassifierLoad = load_model('model/modeltest.h5')\n</code></pre>\n<p>Works for me</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf \nclassifierLoad = tf.keras.models.load_model('model/modeltest.h5')\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Wow I, just spent 6 Hours of my life trying to figure this out.. Dmitri posted a solution to this here: <a href=\"https://stackoverflow.com/questions/53051274/i-trained-a-keras-model-on-google-colab-now-not-able-to-load-it-locally-on-my-s\">I trained a keras model on google colab. Now not able to load it locally on my system.</a></p>\n<p>I'm just basically reposting it here because it worked for me.</p>\n<p>This looks like some kind of a serialization bug in keras. \nIf you wrap your load_model with the below CustomObjectScope thingy... all should work..</p>\n<pre><code>import keras\nfrom keras.models import load_model\nfrom keras.utils import CustomObjectScope\nfrom keras.initializers import glorot_uniform\n\nwith CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n        model = load_model('imdb_mlp_model.h5')\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently reading the Machine Learning book by Tom Mitchell. When talking about neural networks, Mitchell states:</p>\n<blockquote>\n<p>\"Although the perceptron rule finds a successful weight vector when\n  the training examples are linearly separable, it can fail to converge\n  if the examples are not linearly separable. \"</p>\n</blockquote>\n<p>I am having problems understanding what he means with \"linearly separable\"? Wikipedia tells me that \"two sets of points in a two-dimensional space are linearly separable if they can be completely separated by a single line.\"</p>\n<p>But how does this apply to the training set for neural networks? How can inputs (or action units) be linearly separable or not? </p>\n<p>I'm not the best at geometry and maths - could anybody explain it to me as though I were 5? ;) Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose you want to write an algorithm that decides, based on two parameters, size and price, if an house will sell in the same year it was put on sale or not. So you have 2 inputs, size and price, and one output, will sell or will not sell. Now, when you receive your training sets, it could happen that the output is not accumulated to make our prediction easy (Can you tell me, based on the first graph if <code>X</code> will be an N or S? How about the second graph):</p>\n<pre><code>        ^\n        |  N S   N\n       s|  S X    N\n       i|  N     N S\n       z|  S  N  S  N\n       e|  N S  S N\n        +-----------&gt;\n          price\n\n\n        ^\n        |  S S   N\n       s|  X S    N\n       i|  S     N N\n       z|  S  N  N  N\n       e|    N N N\n        +-----------&gt;\n          price\n</code></pre>\n<p>Where:</p>\n<pre><code>S-sold,\nN-not sold\n</code></pre>\n<p>As you can see in the first graph, you can't really separate the two possible outputs (sold/not sold) by a straight line, no matter how you try there will always be both <code>S</code> and <code>N</code> on the both sides of the line, which means that your algorithm will have a lot of <code>possible</code> lines but no ultimate, correct line to split the 2 outputs (and of course to predict new ones, which is the goal from the very beginning). That's why <code>linearly separable</code> (the second graph) data sets are much easier to predict.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This means that there is a hyperplane (which splits your input space into two half-spaces) such that all points of the first class are in one half-space and those of the second class are in the other half-space.</p>\n<p>In two dimensions, that means that there is a line which separates points of one class from points of the other class.</p>\n<p>EDIT: for example, in this image, if blue circles represent points from one class and red circles represent points from the other class, then these points are linearly separable.</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/fFaAp.jpg\"/></p>\n<p>In three dimensions, it means that there is a plane which separates points of one class from points of the other class.</p>\n<p>In higher dimensions, it's similar: there must exist a hyperplane which separates the two sets of points.</p>\n<p>You mention that you're not good at math, so I'm not writing the formal definition, but let me know (in the comments) if that would help.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Look at the following two data sets:</p>\n<pre><code>^                         ^\n|   X    O                |  AA    /\n|                         |  A    /\n|                         |      /   B\n|   O    X                |  A  /   BB\n|                         |    /   B\n+-----------&gt;             +-----------&gt;\n</code></pre>\n<p>The left data set is <em>not</em> linearly separable (without using a kernel). The right one is separable into two parts for <code>A' and</code>B` by the indicated line.</p>\n<p>I.e. You cannot <strong>draw a <em>straight</em> line</strong> into the left image, so that all the <code>X</code> are on one side, and all the <code>O</code> are on the other. That is why it is called \"not linearly separable\" == there exist no linear manifold separating the two classes.</p>\n<p>Now the famous <a href=\"https://en.wikipedia.org/wiki/Kernel_trick\" rel=\"noreferrer\">kernel trick</a> (which will certainly be discussed in the book next) actually allows many linear methods to be used for non-linear problems by virtually adding additional dimensions to make a non-linear problem linearly separable.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Module's <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\" rel=\"noreferrer\">parameters</a> get changed during training, that is, they are what is learnt during training of a neural network, but what is a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter\" rel=\"noreferrer\">buffer</a>?</p>\n<p>and is it learnt during neural network training?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Pytorch <a href=\"https://pytorch.org/docs/1.1.0/nn.html#torch.nn.Module.register_buffer\" rel=\"noreferrer\">doc</a> for <code>register_buffer()</code> method reads</p>\n<blockquote>\n<p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm’s <code>running_mean</code> is not a parameter, but is part of the persistent state.</p>\n</blockquote>\n<p>As you already observed, model <em>parameters</em> are learned and updated using SGD during the training process.<br/>\nHowever, sometimes there are other quantities that are part of a model's \"state\" and should be<br/>\n - saved as part of <code>state_dict</code>.<br/>\n - moved to <code>cuda()</code> or <code>cpu()</code> with the rest of the model's parameters.<br/>\n - cast to <code>float</code>/<code>half</code>/<code>double</code> with the rest of the model's parameters.<br/>\nRegistering these \"arguments\" as the model's <code>buffer</code> allows pytorch to track them and save them like regular parameters, but prevents pytorch from updating them using SGD mechanism.</p>\n<p>An example for a buffer can be found in <a href=\"https://pytorch.org/docs/1.1.0/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\" rel=\"noreferrer\"><code>_BatchNorm</code></a> module where the <code>running_mean</code> , <code>running_var</code> and <code>num_batches_tracked</code> are registered as buffers and updated by accumulating statistics of data forwarded through the layer. This is in contrast to <code>weight</code> and <code>bias</code> parameters that learns an affine transformation of the data using regular SGD optimization.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Both parameters and buffers you create for a module (<code>nn.Module</code>).</p>\n<p>Say you have a linear layer <code>nn.Linear</code>. You already have <code>weight</code> and <code>bias</code> parameters. But if you need a new parameter you use <a href=\"https://pytorch.org/docs/master/nn.html#torch.nn.Module.register_parameter\" rel=\"noreferrer\"><code>register_parameter()</code></a> to register a new named parameter that is a tensor.</p>\n<p>When you register a new parameter it will appear inside the <code>module.parameters()</code> iterator, but when you register a buffer it will not.</p>\n<blockquote>\n<p>The difference:</p>\n</blockquote>\n<p><a href=\"https://pytorch.org/docs/master/nn.html#torch.nn.Module.register_buffer\" rel=\"noreferrer\">Buffers</a> are named tensors that do not update gradients at every step, like parameters.\nFor buffers, you create your custom logic (fully up to you).</p>\n<p>The good thing is when you save the model, all params and buffers are saved, and when you move the model to or off the CUDA params and buffers will go as well.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I just try to find out how I can use <a href=\"http://caffe.berkeleyvision.org/\">Caffe</a>. To do so, I just took a look at the different <code>.prototxt</code> files in the examples folder. There is one option I don't understand:</p>\n<pre><code># The learning rate policy\nlr_policy: \"inv\"\n</code></pre>\n<p>Possible values seem to be:</p>\n<ul>\n<li><code>\"fixed\"</code></li>\n<li><code>\"inv\"</code></li>\n<li><code>\"step\"</code></li>\n<li><code>\"multistep\"</code></li>\n<li><code>\"stepearly\"</code></li>\n<li><code>\"poly\"</code> </li>\n</ul>\n<p>Could somebody please explain those options?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is a common practice to decrease the learning rate (lr) as the optimization/learning process progresses. However, it is not clear how exactly the learning rate should be decreased as a function of the iteration number.</p>\n<p>If you use <a href=\"https://github.com/NVIDIA/DIGITS\" rel=\"noreferrer\">DIGITS</a> as an interface to Caffe, you will be able to visually see how the different choices affect the learning rate.</p>\n<p><strong>fixed:</strong> the learning rate is kept fixed throughout the learning process.</p>\n<hr/>\n<p><strong>inv:</strong> the learning rate is decaying as ~<code>1/T</code><br/>\n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/LScLY.png\"/></p>\n<hr/>\n<p><strong>step:</strong> the learning rate is piecewise constant, dropping every X iterations<br/>\n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/W5h6j.png\"/> </p>\n<hr/>\n<p><strong>multistep:</strong> piecewise constant at arbitrary intervals<br/>\n<img alt=\"enter image description here\" src=\"https://i.sstatic.net/DW0qa.png\"/></p>\n<hr/>\n<p>You can see exactly how the learning rate is computed in the function <a href=\"https://github.com/BVLC/caffe/blob/master/src/caffe/solvers/sgd_solver.cpp#L27\" rel=\"noreferrer\"><code>SGDSolver&lt;Dtype&gt;::GetLearningRate</code></a> (<em>solvers/sgd_solver.cpp</em> line ~30).</p>\n<hr/>\n<p>Recently, I came across an interesting and unconventional approach to learning-rate tuning: <a href=\"http://arxiv.org/abs/1506.01186\" rel=\"noreferrer\">Leslie N. Smith's work \"No More Pesky Learning Rate Guessing Games\"</a>. In his report, Leslie suggests to use <code>lr_policy</code> that alternates between decreasing and <em>increasing</em> the learning rate. His work also suggests how to implement this policy in Caffe.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you look inside the <code>/caffe-master/src/caffe/proto/caffe.proto</code> file (you can find it online <a href=\"https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto#L157-L172\">here</a>) you will see the following descriptions:</p>\n<pre><code>// The learning rate decay policy. The currently implemented learning rate\n// policies are as follows:\n//    - fixed: always return base_lr.\n//    - step: return base_lr * gamma ^ (floor(iter / step))\n//    - exp: return base_lr * gamma ^ iter\n//    - inv: return base_lr * (1 + gamma * iter) ^ (- power)\n//    - multistep: similar to step but it allows non uniform steps defined by\n//      stepvalue\n//    - poly: the effective learning rate follows a polynomial decay, to be\n//      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)\n//    - sigmoid: the effective learning rate follows a sigmod decay\n//      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))\n//\n// where base_lr, max_iter, gamma, step, stepvalue and power are defined\n// in the solver parameter protocol buffer, and iter is the current iteration.\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is difference between SVM and Neural Network?\nIs it true that linear svm is same NN, and for non-linear separable problems, NN uses adding hidden layers and SVM uses changing space dimensions?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are two parts to this question.  The first part is \"what is the form of function learned by these methods?\"  For NN and SVM this is typically the same.  For example, a single hidden layer neural network uses exactly the same form of model as an SVM.  That is:</p>\n<p>Given an input vector x, the output is:\noutput(x) = sum_over_all_i weight_i * nonlinear_function_i(x)</p>\n<p>Generally the nonlinear functions will also have some parameters.  So these methods need to learn how many nonlinear functions should be used, what their parameters are, and what the value of all the weight_i weights should be.</p>\n<p>Therefore, the difference between a SVM and a NN is in how they decide what these parameters should be set to.  Usually when someone says they are using a neural network they mean they are trying to find the parameters which minimize the mean squared prediction error with respect to a set of training examples.  They will also almost always be using the <a href=\"http://en.wikipedia.org/wiki/Stochastic_gradient_descent\" rel=\"noreferrer\">stochastic gradient descent</a> optimization algorithm to do this.  SVM's on the other hand try to minimize both training error and some measure of \"hypothesis complexity\".  So they will find a set of parameters that fits the data but also is \"simple\" in some sense.  You can think of it like Occam's razor for machine learning.  The most common optimization algorithm used with SVMs is <a href=\"http://en.wikipedia.org/wiki/Sequential_minimal_optimization\" rel=\"noreferrer\">sequential minimal optimization</a>.</p>\n<p>Another big difference between the two methods is that stochastic gradient descent isn't guaranteed to find the optimal set of parameters when used the way NN implementations employ it.  However, any decent SVM implementation is going to find the optimal set of parameters.  People like to say that neural networks get stuck in a local minima while SVMs don't.  </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>NNs are heuristic, while SVMs are theoretically founded. A SVM is guaranteed to converge towards the best solution in the PAC (probably approximately correct) sense. For example, for two linearly separable classes SVM will draw the separating hyperplane directly halfway between the nearest points of the two classes (these become <em>support vectors</em>). A neural network would draw any line which separates the samples, which is correct for the training set, but might not have the best generalization properties.</p>\n<p>So no, even for linearly separable problems NNs and SVMs are not same.</p>\n<p>In case of linearly non-separable classes, both SVMs and NNs apply non-linear projection into higher-dimensional space. In the case of NNs this is achieved by introducing additional neurons in the hidden layer(s). For SVMs, a <em>kernel function</em> is used to the same effect. A neat property of the kernel function is that the computational complexity doesn't rise with the number of dimensions, while for NNs it obviously rises with the number of neurons.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Running a simple <a href=\"http://indiji.com/svm-vs-nn.html\" rel=\"nofollow noreferrer\">out-of-the-box comparison between support vector machines and neural networks</a> (WITHOUT any parameter-selection) on several popular regression and classification datasets demonstrates the practical differences: an SVM becomes a very slow predictor if many support vectors are being created while a neural network's prediction speed is much higher and model-size much smaller. On the other hand, the training time is much shorter for SVMs. Concerning the accuracy/loss - despite the aforementioned theoretical drawbacks of neural networks - both methods are on par - especially for regression problems, neural networks often outperform support vector machines. Depending on your specific problem, this might help to choose the right model. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between <em>deep</em> reinforcement learning and reinforcement learning? I basically know what reinforcement learning is about, but what does the concrete term <strong>deep</strong> stand for in this context?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h3>Reinforcement Learning</h3>\n<p>In reinforcement learning, an agent tries to come up with the best action given a state.</p>\n<p>For example, in the video game Pac-Man, the state space would be the 2D game world you are in, the surrounding items (pac-dots, enemies, walls, etc), and actions would be moving through that 2D space (going up/down/left/right).</p>\n<p>So, given the state of the game world, the agent needs to pick the best action to maximise rewards. Through reinforcement learning's trial and error, it accumulates \"knowledge\" through these <code>(state, action)</code> pairs, as in, it can tell if there would be positive or negative reward given a <code>(state, action)</code> pair. Let's call this value <code>Q(state, action)</code>.</p>\n<p>A rudimentary way to store this knowledge would be a table like below</p>\n<pre><code>state | action | Q(state, action)\n---------------------------------\n  ... |   ...  |   ...\n</code></pre>\n<p><strong>The <code>(state, action)</code> space can be very big</strong></p>\n<p>However, when the game gets complicated, the knowledge space can become huge and it no longer becomes feasible to store all <code>(state, action)</code> pairs. If you think about it in raw terms, even a slightly different state is still a distinct state (e.g. different position of the enemy coming through the same corridor). You could use something that can generalize the knowledge instead of <em>storing</em> and <em>looking up</em> every little distinct state.</p>\n<p>So, what you can do is create a neural network, that e.g. predicts the reward for an input <code>(state, action)</code> (or pick the best action given a state, however you like to look at it)</p>\n<p><strong>Approximating the <code>Q</code> value with a Neural Network</strong></p>\n<p>So, what you effectively have is <em>a NN that <strong>predicts</strong> the <code>Q</code> value</em>, based on the input <code>(state, action)</code>. This is way more tractable than storing every possible value like we did in the table above.</p>\n<pre><code>Q = neural_network.predict(state, action)\n</code></pre>\n<h3>Deep Reinforcement Learning</h3>\n<p><strong>Deep Neural Networks</strong></p>\n<p>To be able to do that for complicated games, the NN may need to be \"deep\", meaning a few hidden layers may not suffice to capture all the intricate details of that knowledge, hence the use of deep NNs (lots of hidden layers).</p>\n<p>The extra hidden layers allows the network to internally come up with features that can help it learn and generalize complex problems that may have been impossible on a shallow network.</p>\n<h3>Closing words</h3>\n<p>In short, the deep neural network allows reinforcement learning to be applied to larger problems. You can use any function approximator instead of an NN to approximate <code>Q</code>, and if you do choose NNs, it doesn't <em>absolutely</em> have to be a deep one. It's just researchers have had great success using them recently.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Summary:</strong> Deep RL uses a Deep Neural Network to approximate Q(s,a). \nNon-Deep RL defines Q(s,a) using a tabular function.</p>\n<hr/>\n<p>Popular Reinforcement Learning algorithms use functions Q(s,a) or V(s) to estimate the Return (sum of discounted rewards). The function can be defined by a tabular mapping of discrete inputs and outputs. However, this is limiting for continuous states or an infinite/large number of states. A more generalized approach is necessary for large number of states.</p>\n<p>Function approximation is used for a large state space. A popular function approximation method is Neural Networks. You can make a Deep Neural Network by adding many hidden layers. </p>\n<p>Thus, Deep Reinforcement Learning uses Function Approximation, as opposed to tabular functions. Specifically DRL uses Deep Neural Networks to approximate Q or V (or even A).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to replace a column within a Pandas DataFrame containing strings into a one-hot encoded equivalent using Scikit-Learn's OneHotEncoder. My code below doesn't work:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.preprocessing import OneHotEncoder\n# data is a Pandas DataFrame\n\njobs_encoder = OneHotEncoder()\njobs_encoder.fit(data['Profession'].unique().reshape(1, -1))\ndata['Profession'] = jobs_encoder.transform(data['Profession'].to_numpy().reshape(-1, 1))\n</code></pre>\n<p>It produces the following error (strings in the list are omitted):</p>\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-91-3a1f568322f5&gt; in &lt;module&gt;()\n      3 jobs_encoder = OneHotEncoder()\n      4 jobs_encoder.fit(data['Profession'].unique().reshape(1, -1))\n----&gt; 5 data['Profession'] = jobs_encoder.transform(data['Profession'].to_numpy().reshape(-1, 1))\n\n/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py in transform(self, X)\n    730                                        copy=True)\n    731         else:\n--&gt; 732             return self._transform_new(X)\n    733 \n    734     def inverse_transform(self, X):\n\n/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py in _transform_new(self, X)\n    678         \"\"\"New implementation assuming categorical input\"\"\"\n    679         # validation of X happens in _check_X called by _transform\n--&gt; 680         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n    681 \n    682         n_samples, n_features = X_int.shape\n\n/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py in _transform(self, X, handle_unknown)\n    120                     msg = (\"Found unknown categories {0} in column {1}\"\n    121                            \" during transform\".format(diff, i))\n--&gt; 122                     raise ValueError(msg)\n    123                 else:\n    124                     # Set the problematic rows to an acceptable value and\n\nValueError: Found unknown categories ['...', ..., '...'] in column 0 during transform\n</code></pre>\n<p>Here's some sample data:</p>\n<pre class=\"lang-py prettyprint-override\"><code>data['Profession'] =\n\n0         unkn\n1         safe\n2         rece\n3         unkn\n4         lead\n          ... \n111988    indu\n111989    seni\n111990    mess\n111991    seni\n111992    proj\nName: Profession, Length: 111993, dtype: object\n</code></pre>\n<p>What exactly am I doing wrong?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\" rel=\"nofollow noreferrer\"><strong>OneHotEncoder</strong></a> Encodes categorical integer features as a one-hot numeric array. Its <strong>Transform</strong> method returns a sparse matrix if <code>sparse=True</code>, otherwise it returns a 2-d array.</p>\n<p>You can't cast a <strong>2-d array</strong> (or sparse matrix) into a <strong>Pandas Series</strong>. You must create a Pandas Serie (a column in a Pandas dataFrame) for each <strong>category</strong>.</p>\n<p>I would recommend <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\" rel=\"nofollow noreferrer\">pandas.get_dummies</a> instead:</p>\n<pre><code>data = pd.get_dummies(data,prefix=['Profession'], columns = ['Profession'], drop_first=True)\n</code></pre>\n<p><strong>EDIT:</strong></p>\n<p>Using Sklearn OneHotEncoder:</p>\n<pre><code>transformed = jobs_encoder.transform(data['Profession'].to_numpy().reshape(-1, 1))\n#Create a Pandas DataFrame of the hot encoded column\nohe_df = pd.DataFrame(transformed, columns=jobs_encoder.get_feature_names())\n#concat with original data\ndata = pd.concat([data, ohe_df], axis=1).drop(['Profession'], axis=1)\n</code></pre>\n<p><strong>Other Options:</strong> If you are doing hyperparameter tuning with <a href=\"https://scikit-learn.org/stable/modules/grid_search.html\" rel=\"nofollow noreferrer\">GridSearch</a> it's recommanded to use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\" rel=\"nofollow noreferrer\">ColumnTransformer</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html\" rel=\"nofollow noreferrer\">FeatureUnion</a> with <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\" rel=\"nofollow noreferrer\">Pipeline</a> or directly <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html\" rel=\"nofollow noreferrer\">make_column_transformer</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So turned out that Scikit-Learns <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\" rel=\"noreferrer\">LabelBinarizer</a> gave me better luck in converting the data to one-hot encoded format, with help from <a href=\"https://stackoverflow.com/a/58101528/9035662\">Amnie's solution</a>, my final code is as follows</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.preprocessing import LabelBinarizer\n\njobs_encoder = LabelBinarizer()\njobs_encoder.fit(data['Profession'])\ntransformed = jobs_encoder.transform(data['Profession'])\nohe_df = pd.DataFrame(transformed)\ndata = pd.concat([data, ohe_df], axis=1).drop(['Profession'], axis=1)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This below is an approach suggested by Kaggle Learn. Do not think there is a simpler way to do so at the moment to go from an original pandas <code>DataFrame</code> to a one-hot encoded <code>DataFrame</code>.</p>\n<pre><code># Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnumeric_X_train = X_train.drop(low_cardinality_cols, axis=1)\nnumeric_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nnew_X_train = pd.concat([numeric_X_train, OH_cols_train], axis=1)\nnew_X_valid = pd.concat([numeric_X_valid, OH_cols_valid], axis=1)\nprint(new_X_train)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using Keras from Tensorflow 1.4.1, how does one copy weights from one model to another?</p>\n<p>As some background, I'm trying to implement a deep-q network (DQN) for Atari games following the DQN publication by DeepMind.  My understanding is that the implementation uses two networks, Q and Q'.  The weights of Q are trained using gradient descent, and then the weights are copied periodically to Q'.</p>\n<p>Here's how I build Q and Q':</p>\n<pre><code>ACT_SIZE   = 4\nLEARN_RATE = 0.0025\nOBS_SIZE   = 128\n\ndef buildModel():\n  model = tf.keras.models.Sequential()\n\n  model.add(tf.keras.layers.Lambda(lambda x: x / 255.0, input_shape=OBS_SIZE))\n  model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n  model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n  model.add(tf.keras.layers.Dense(ACT_SIZE, activation=\"linear\"))\n  opt = tf.keras.optimizers.RMSprop(lr=LEARN_RATE)\n\n  model.compile(loss=\"mean_squared_error\", optimizer=opt)\n\n  return model\n</code></pre>\n<p>I call that twice to get Q and Q'.</p>\n<p>I have an <code>updateTargetModel</code> method below that is my attempt at copying weights.  The code runs fine, but my overall DQN implementation is failing.  I'm really just trying to verify if this is a valid way of copying weights from one network to another.</p>\n<pre><code>def updateTargetModel(model, targetModel):\n  modelWeights       = model.trainable_weights\n  targetModelWeights = targetModel.trainable_weights\n\n  for i in range(len(targetModelWeights)):\n    targetModelWeights[i].assign(modelWeights[i])\n</code></pre>\n<p>There's another question here that discusses saving and loading weights to and from disk (<a href=\"https://stackoverflow.com/questions/38065448/tensorflow-copy-weights-issue\">Tensorflow Copy Weights Issue</a>), but there's no accepted answer.  There is also a question about loading weights from individual layers (<a href=\"https://stackoverflow.com/questions/47667541/copying-weights-from-one-conv2d-layer-to-another\">Copying weights from one Conv2D layer to another</a>), but I'm wanting to copy the entire model's weights.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Actually what you've done is much more than simply copying weights. You made these two models identical <strong>all the time</strong>. Every time you update one model - the second one is also updated - as both models have the same <code>weights</code> variables. </p>\n<p>If you want to just copy weights - the simplest way is by this command:</p>\n<pre><code>target_model.set_weights(model.get_weights()) \n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using dropout in neural network model in keras. Little bit code is like</p>\n<pre><code>model.add(Dropout(0.5))\nmodel.add(Dense(classes))\n</code></pre>\n<p>For testing, I am using <code>preds = model_1.predict_proba(image)</code>.</p>\n<p>But while testing <strong>Dropout</strong> is also participating to predict the score which should not be happen. I search a lot to disable the dropout but didn't get any hint yet.</p>\n<p>Do anyone have solution to disable the <strong>Dropout</strong> while testing in keras??</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Keras does this by default. In Keras dropout is disabled in test mode. You can look at the code <a href=\"https://github.com/keras-team/keras/blob/dc95ceca57cbfada596a10a72f0cb30e1f2ed53b/keras/layers/core.py#L109\" rel=\"noreferrer\">here</a> and see that they use the dropped input in training and the actual input while testing.</p>\n<p>As far as I know you have to build your own training function from the layers and specify the training flag to predict with dropout (e.g. its not possible to specify a training flag for the predict functions). This is a problem in case you want to do GANs, which use the intermediate output for training and also train the network as a whole, due to a divergence between generated training images and generated test images.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As previously stated, dropout in Keras happens only at train time (with proportionate weight adjustment during training such that learned weights are appropriate for prediction when dropout is disabled).</p>\n<p>This is not ideal for cases in which we wish to use a dropout NNET as a probabilistic predictor (such that it produces a distribution when asked to predict the same inputs repeatedly).  In other words, Keras' Dropout layer is designed to give you regularization at train time, but the \"mean function\" of the learned distribution when predicting.</p>\n<p>If you want to retain dropout for prediction, you can easily implement a permanent dropout (\"PermaDropout\") layer (this was based on suggestions made by F. Chollet on the <a href=\"https://github.com/keras-team/keras/issues/1606\" rel=\"noreferrer\">GitHub discussion area for Keras</a>): </p>\n<pre><code>from keras.layers.core import Lambda\nfrom keras import backend as K\n\ndef PermaDropout(rate):\n    return Lambda(lambda x: K.dropout(x, level=rate))\n</code></pre>\n<p>By replacing any dropout layer in a Keras model with \"PermaDropout\", you'll get the probabilistic behavior in prediction as well.</p>\n<pre><code># define the LSTM model\nn_vocab = text_to_train.n_vocab\n\nmodel = Sequential()\nmodel.add(LSTM(n_vocab*4, \n          input_shape=input_shape, \n          return_sequences=True))\n# Replace Dropout with PermaDropout\n# model.add(Dropout(0.3)\nmodel.add(PermaDropout(0.3))\nmodel.add(LSTM(n_vocab*2))\n# Replace Dropout with PermaDropout\n# model.add(Dropout(0.3)\nmodel.add(PermaDropout(0.3))\n#model.add(Dense(n_vocab*2))\nmodel.add(Dense(n_vocab, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To activate dropout for inference time u simply have to specify <code>training=True</code> in the layer of interest (<code>Dropout</code> in our case):</p>\n<p>with <code>training=False</code></p>\n<pre><code>inp = Input(shape=(10,))\nx = Dropout(0.3)(inp, training=False)\nx = Dense(1)(x)\nm = Model(inp,x)\n# m.compile(...)\n# m.fit(...)\n\nX = np.random.uniform(0,1, (1,10))\n\noutput = []\nfor i in range(0,100):\n    output.append(m.predict(X)) # always the same\n</code></pre>\n<p>with <code>training=True</code></p>\n<pre><code>inp = Input(shape=(10,))\nx = Dropout(0.3)(inp, training=True)\nx = Dense(1)(x)\nm = Model(inp,x)\n# m.compile(...)\n# m.fit(...)\n\nX = np.random.uniform(0,1, (1,10))\n\noutput = []\nfor i in range(0,100):\n    output.append(m.predict(X)) # always different\n</code></pre>\n<p><strong>by default, training is set to False</strong></p>\n<p><strong><a href=\"https://towardsdatascience.com/extreme-event-forecasting-with-lstm-autoencoders-297492485037\" rel=\"nofollow noreferrer\">HERE</a></strong> a full example of the usage of droput at inference time</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My machine has the following spec: </p>\n<p>CPU: Xeon E5-1620 v4</p>\n<p>GPU: Titan X (Pascal) </p>\n<p>Ubuntu 16.04</p>\n<p>Nvidia driver 375.26</p>\n<p>CUDA tookit 8.0</p>\n<p>cuDNN 5.1</p>\n<p>I've benchmarked on the following Keras examples with Tensorflow as the backed <a href=\"https://github.com/fchollet/keras/tree/master/examples\" rel=\"noreferrer\">reference</a>: </p>\n<pre><code>SCRIPT NAME                  GPU       CPU\nstated_lstm.py               5sec      5sec \nbabi_rnn.py                  10sec     12sec\nimdb_bidirectional_lstm.py   240sec    116sec\nimbd_lstm.py                 113sec    106sec\n</code></pre>\n<p>My gpu is clearly out performing my cpu in non-lstm models. </p>\n<pre><code>SCRIPT NAME                  GPU       CPU\ncifar10_cnn.py               12sec     123sec\nimdb_cnn.py                  5sec      119sec\nmnist_cnn.py                 3sec      47sec \n</code></pre>\n<p>Has anyone else experienced this? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you use Keras, use <a href=\"https://keras.io/layers/recurrent/#cudnnlstm\" rel=\"noreferrer\">CuDNNLSTM</a> in place of <a href=\"https://keras.io/layers/recurrent/#lstm\" rel=\"noreferrer\">LSTM</a> or <a href=\"https://keras.io/layers/recurrent/#cudnngru\" rel=\"noreferrer\">CuDNNGRU</a> in place of <a href=\"https://keras.io/layers/recurrent/#gru\" rel=\"noreferrer\">GRU</a>. In my case (2 Tesla M60), I am seeing 10x boost of performance. By the way I am using batch size 128 as suggested by @Alexey Golyshev.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Too small batch size. Try to increase.</p>\n<p>Results for my GTX1050Ti:</p>\n<pre>\nimdb_bidirectional_lstm.py\nbatch_size      time\n32 (default)    252\n64              131\n96              87\n128             66\n\nimdb_lstm.py\nbatch_size      time\n32 (default)    108\n64              50\n96              34\n128             25\n</pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It's just a tip.</p>\n<p><b>Using GPU is powerful when</b></p>\n<p><i>1. your neural network model is big.</i> <br/>\n<i>2. batch size is big.</i></p>\n<p>It's what I found from googling.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is it possible to have missing values in scikit-learn ? How should they be represented? I couldn't find any documentation about that.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><del>Missing values are simply not supported in scikit-learn. There has been discussion on the mailing list about this before, but no attempt to actually write code to handle them.</del></p>\n<p><del>Whatever you do, <em>don't</em> use NaN to encode missing values, since many of the algorithms refuse to handle samples containing NaNs.</del></p>\n<p>The above answer is outdated; the latest release of scikit-learn has a class <a href=\"https://scikit-learn.org/stable/modules/impute.html#impute\" rel=\"nofollow noreferrer\"><code>Imputer</code></a> that does simple, per-feature missing value imputation. You can feed it arrays containing NaNs to have those replaced by the mean, median or mode of the corresponding feature.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I wish I could provide a simple example, but I have found that RandomForestRegressor does <em>not</em> handle NaN's gracefully. Performance gets steadily worse when adding features with increasing percentages of NaN's. Features that have \"too many\" NaN's are completely ignored, even when the nan's indicate very useful information.</p>\n<p>This is because the algorithm will <em>never</em> create a split on the decision \"isnan\" or \"ismissing\". The algorithm will <em>ignore a feature</em> at a particular level of the tree if that feature has a single NaN in that subset of samples. But, at lower levels of the tree, when sample sizes are smaller, it becomes more likely that a subset of samples won't have a NaN in a particular feature's values, and a split can occur on that feature.</p>\n<p>I have tried various imputation techniques to deal with the problem (replace with mean/median, predict missing values using a different model, etc.), but the results were mixed.</p>\n<p>Instead, this is my solution: replace NaN's with a single, obviously out-of-range value (like -1.0). This enables the tree to split on the criteria \"unknown-value vs known-value\". However, there is a strange side-effect of using such out-of-range values: known values near the out-of-range value could get lumped together with the out-of-range value when the algorithm tries to find a good place to split. For example, known 0's could get lumped with the -1's used to replace the NaN's. So your model could change depending on if your out-of-range value is less than the minimum or if it's greater than the maximum (it could get lumped in with the minimum value or maximum value, respectively). This may or may not help the generalization of the technique, the outcome will depend on how similar in behavior minimum- or maximum-value samples are to NaN-value samples.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have come across very similar issue, when running the <em>RandomForestRegressor</em> on data. The presence of NA values were throwing out \"nan\" for predictions. From scrolling around several discussions, the Documentation by Breiman recommends two solutions for continuous and categorical data respectively.</p>\n<ol>\n<li>Calculate the Median of the data from the column(Feature) and use\nthis (Continuous Data) </li>\n<li>Determine the most frequently occurring Category and use this\n(Categorical Data)</li>\n</ol>\n<p>According to Breiman the random nature of the algorithm and the number of trees will allow for the correction without too much effect on the accuracy of the prediction. This I feel would be the case if the presence of NA values is sparse, a feature containing many NA values I think will most likely have an affect.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I already know \"<code>xgboost.XGBRegressor</code> is a Scikit-Learn Wrapper interface for XGBoost.\"</p>\n<p>But do they have any other difference?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\" rel=\"noreferrer\"><code>xgboost.train</code></a> is the low-level API to train the model via gradient boosting method. </p>\n<p><code>xgboost.XGBRegressor</code> and <code>xgboost.XGBClassifier</code> are the wrappers (<em>Scikit-Learn-like wrappers</em>, as they call it) that prepare the <code>DMatrix</code> and pass in the corresponding objective function and parameters. In the end, the <code>fit</code> call simply boils down to:</p>\n<pre class=\"lang-py prettyprint-override\"><code>self._Booster = train(params, dmatrix,\n                      self.n_estimators, evals=evals,\n                      early_stopping_rounds=early_stopping_rounds,\n                      evals_result=evals_result, obj=obj, feval=feval,\n                      verbose_eval=verbose)\n</code></pre>\n<p>This means that <strong>everything</strong> that can be done with <code>XGBRegressor</code> and <code>XGBClassifier</code> is doable via underlying <code>xgboost.train</code> function. The other way around it's obviously not true, for instance, some useful parameters of <code>xgboost.train</code> are not supported in <code>XGBModel</code> API. The list of notable differences includes:</p>\n<ul>\n<li><code>xgboost.train</code> allows to set the <code>callbacks</code> applied at end of each iteration.</li>\n<li><code>xgboost.train</code> allows training continuation via <code>xgb_model</code> parameter.</li>\n<li><code>xgboost.train</code> allows not only minization of the eval function, but maximization as well.</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>@Maxim, as of xgboost 0.90 (or much before), these differences don't exist anymore in that <a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit\" rel=\"noreferrer\">xgboost.XGBClassifier.fit</a>:</p>\n<ul>\n<li>has <code>callbacks</code></li>\n<li>allows contiunation with the <code>xgb_model</code> parameter</li>\n<li>and supports the same builtin eval metrics or custom eval functions</li>\n</ul>\n<p>What I find is different is <code>evals_result</code>, in that it has to be retrieved separately after fit (<code>clf.evals_result()</code>) and the resulting <code>dict</code> is different because it can't take advantage of the name of the evals in the watchlist ( <code>watchlist = [(d_train, 'train'), (d_valid, 'valid')]</code> ) .</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From my opinion the main difference is the training/prediction speed.</p>\n<p>For further reference I will call the <code>xgboost.train</code> - 'native_implementation' and <code>XGBClassifier.fit</code> - 'sklearn_wrapper'</p>\n<p>I have made some benchmarks on a dataset shape (240000, 348)</p>\n<p>Fit/train time:\n<code>sklearn_wrapper</code> time = 89 seconds\n<code>native_implementation</code> time = 7 seconds</p>\n<p>Prediction time:\n<code>sklearn_wrapper</code> = 6 seconds\n<code>native_implementation</code> = 3.5 milliseconds</p>\n<p>I believe this is reasoned by the fact that <code>sklearn_wrapper</code> is designed to use the pandas/numpy objects as input where the <code>native_implementation</code> needs the input data to be converted into a xgboost.DMatrix object.</p>\n<p>In addition one can optimise n_estimators using a <code>native_implementation</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am fine-tuning a MobileNet with 14 new classes. When I add new layers by:</p>\n<pre><code>x=mobile.layers[-6].output\nx=Flatten(x)\npredictions = Dense(14, activation='softmax')(x)\nmodel = Model(inputs=mobile.input, outputs=predictions)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>'Tensor' object has no attribute 'lower'\n</code></pre>\n<p>Also using:</p>\n<pre><code>model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit_generator(train_batches, steps_per_epoch=18,\n                validation_data=valid_batches, validation_steps=3, epochs=60, verbose=2)\n</code></pre>\n<p>I get the error:</p>\n<pre><code>Error when checking target: expected dense_1 to have 4 dimensions, but got array with shape (10, 14)\n</code></pre>\n<p>What does <code>lower</code> mean? I saw other fine-tuning scripts and there were no other arguments other than the name of the model which is <code>x</code> in this case.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The tensor must be passed to the layer when you are calling it, and not as an argument. Therefore it must be like this:</p>\n<pre><code>x = Flatten()(x)  # first the layer is constructed and then it is called on x\n</code></pre>\n<p>To make it more clear, it is equivalent to this:</p>\n<pre><code>flatten_layer = Flatten()  # instantiate the layer\nx = flatten_layer(x)       # call it on the given tensor\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am developing e-shop where I will sell food. I want to have a suggestion box where I would suggest what else my user could buy based on what he's already have in cart. If he has beer, I want him to suggest chips and other things by descending precentage of probability that he'll buy it too. But I want that my algorithm would learn to suggest groceries based on the all users' previous purchases. Where should I start? I have groceries table <code>user_id</code>, <code>item_id</code>, <code>date</code> and similar. How can I make a suggestion box without brute-forcing which is impossible.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The thing you're describing is a recommendation engine; more specifically collaborative filtering. It's the heart of Amazon's \"people who bought x also bought y\" feature, and Netflix's recommendation engine. </p>\n<p>It's a non-trivial undertaking. As in, to get anything that's even remotely useful could easily take more than building the ecommerce site in the first place. </p>\n<p>For instance:</p>\n<ul>\n<li>you don't want to recommend items that are already in the basket.</li>\n<li>you don't want to recommend cheaper versions of the things that are already in the basket.</li>\n<li>you don't want to recommend items that are out of stock.</li>\n<li>you don't want to recommend items that are statistically valid, but make no sense (\"hey, you bought nappies, why not buy beer?\" - there is a story that in supermarkets, there <em>is</em> a statistical correlation because dads go out at night to buy nappies and pick up a six pack at the same time). </li>\n<li>you <em>do</em> want to recommend items that are in a promotion right now</li>\n<li>you <em>don't</em> want to recommend items that are similar to items in a promotion right now</li>\n</ul>\n<p>When I tried a similar project, it was very hard to explain to non-technical people that the computer simply didn't understand that recommending beer alongside nappies wasn't appropriate. Once we got the basic solution working, building the exclusion and edge case logic took at least as long. </p>\n<p>Realistically, I think these are your options:</p>\n<ul>\n<li>manually maintain the related products. Time consuming, but unlikely to lead to weirdness.</li>\n<li>use an off-the-shelf solution - either SaaS or include a library like R which supports this.</li>\n<li>recommend (semi)random products. Have a set of products you want to recommend, and pick one at random - for instance, products on promotion, products which are in the \"best seller\" list, products which cost less than x. Exclude categories that could be problematic.</li>\n</ul>\n<p>All those options are achievable in reasonable time; the problem with building a proper solution from scratch is that everyone will measure it against Amazon, and they've got a bit of a head start on you... </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a common problem solved by <a href=\"http://nikhilvithlani.blogspot.sg/2012/03/apriori-algorithm-for-data-mining-made.html#!/2012/03/apriori-algorithm-for-data-mining-made.html\">Apriori</a> Algorithm in Data Mining. You may need to create another table which maintains this statistics and then suggest based on the preferred combination</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think the best approach is to categorize your items and use that information to make the choice.</p>\n<p>I did this on a grocery website and the results worked quite well. The idea is to cross group items into a number of categories.</p>\n<p>For example, lets take a banana. It's a fruit, but it is also commonly used with cornflakes or cereal for breakfast. Cereals are also a breakfast food but certain ones might be considered health foods while others are sugary treats.</p>\n<p>With this sort of approach, you can quickly start making a table like this:</p>\n<pre><code>Item         | Category\n-------------+------------\nBanana       | Breakfast\nBanana       | Quick\nBanana       | Fruit\nBanana       | Healthy\nMuseli       | Breakfast\nMuseli       | Healthy\nSugar Puffs  | Breakfast\nSugar Puffs  | Treat\nKiwi Fruit   | Fruit\nKiwi Fruit   | Healtyh\nKiwi Fruit   | Dessert\nMilk         | Breakfast\n</code></pre>\n<p>With a simple lookup like this, you can easily find good items to suggest based on these groupings.</p>\n<p>Lets say someone's basket contains a Banana, Museli and Sugar Puffs.</p>\n<p>That's three breakfast items, two healthy, one not so much.</p>\n<p>Suggest Milk as it matches all three. No impulse buy? Try again, throw in a Kiwi Fruit. and so on and so on.</p>\n<p>The idea here is to match items across many different categories (especially ones that may not be directly apparent) and use these counts to suggest the best items for your customer.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> This question does not appear to be about programming within the scope defined in the <a href=\"https://stackoverflow.com/help/on-topic\">help center</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-03-25 01:39:58Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/45493384/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I know in regular neural nets people use batch norm before activation and it will reduce the reliance on good weight initialization. I wonder if it would do the same to RNN/lstm RNN when i use it. Does anyone have any experience with it?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>No, you cannot use Batch Normalization on a recurrent neural network, as the statistics are computed per batch, this does not consider the recurrent part of the network. Weights are shared in an RNN, and the activation response for each \"recurrent loop\" might have completely different statistical properties.</p>\n<p>Other techniques similar to Batch Normalization that take these limitations into account have been developed, for example <a href=\"https://arxiv.org/abs/1607.06450\" rel=\"noreferrer\">Layer Normalization</a>. There are also reparametrizations of the LSTM layer that allow Batch Normalization to be used, for example as described in <a href=\"https://arxiv.org/abs/1603.09025\" rel=\"noreferrer\">Recurrent Batch Normalization by Coijmaans et al. 2016.</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Batch normalization applied to RNNs is similar to batch normalization applied to CNNs: you compute the statistics in such a way that the recurrent/convolutional properties of the layer still hold after BN is applied.</p>\n<p>For CNNs, this means computing the relevant statistics not just over the mini-batch, but also over the two spatial dimensions; in other words, the normalization is applied over the channels dimension. </p>\n<p>For RNNs, this means computing the relevant statistics over the mini-batch and the time/step dimension, so the normalization is applied only over the vector depths. This also means that you only batch normalize the transformed input (so in the vertical directions, e.g. <code>BN(W_x * x)</code>) since the horizontal (across time) connections are time-dependent and shouldn't just be plainly averaged.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In any non-recurrent network (convnet or not) when you do BN each layer gets to adjust the incoming scale and mean so the incoming distribution for each layer doesn't keep changing (which is what the authors of the BN paper claim is the advantage of BN).</p>\n<p>The problem with doing this for the recurrent outputs of an RNN is that the parameters for the incoming distribution are now shared between all timesteps (which are effectively layers in backpropagation-through-time, or BPTT). So the distribution ends up being fixed across the temporal layers of BPTT. This may not make sense as there may be structure in the data that varies (in a non-random way) across the time series. For example, if the time series is a sentence certain words are much more likely to appear in the beginning or end. So having the distribution fixed might reduce the effectiveness of BN.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is an example that creates two data sets:</p>\n<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\n# data set 1\nX1, y1 = make_classification(n_classes=2, n_features=5, random_state=1)\n# data set 2\nX2, y2 = make_classification(n_classes=2, n_features=5, random_state=2)\n</code></pre>\n<p>I want to use the <code>LogisticRegression</code> estimator with the same parameter values to fit a classifier on each data set:</p>\n<pre><code>lr = LogisticRegression()\n\nclf1 = lr.fit(X1, y1)\nclf2 = lr.fit(X2, y2)\n\nprint \"Classifier for data set 1: \"\nprint \"  - intercept: \", clf1.intercept_\nprint \"  - coef_: \", clf1.coef_\n\nprint \"Classifier for data set 2: \"\nprint \"  - intercept: \", clf2.intercept_\nprint \"  - coef_: \", clf2.coef_\n</code></pre>\n<p>The problem is that both classifiers are the same:</p>\n<pre><code>Classifier for data set 1: \n  - intercept:  [ 0.05191729]\n  - coef_:  [[ 0.06704494  0.00137751 -0.12453698 -0.05999127  0.05798146]]\nClassifier for data set 2: \n  - intercept:  [ 0.05191729]\n  - coef_:  [[ 0.06704494  0.00137751 -0.12453698 -0.05999127  0.05798146]]\n</code></pre>\n<p>For this simple example, I could use something like:</p>\n<pre><code>lr1 = LogisticRegression()\nlr2 = LogisticRegression()\n\nclf1 = lr1.fit(X1, y1)\nclf2 = lr2.fit(X2, y2)\n</code></pre>\n<p>to avoid the problem. However, the question remains: How to duplicate / copy an estimator with its particular parameter values in general?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>from sklearn.base import clone\n\nlr1 = LogisticRegression()\nlr2 = clone(lr1)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use scikit linear regression and if I change the order of the features, the coef are still printed in the same order, hence I would like to know the mapping of the feature with the coeff.</p>\n<pre><code>#training the model\nmodel_1_features = ['sqft_living', 'bathrooms', 'bedrooms', 'lat', 'long']\nmodel_2_features = model_1_features + ['bed_bath_rooms']\nmodel_3_features = model_2_features + ['bedrooms_squared', 'log_sqft_living', 'lat_plus_long']\n\nmodel_1 = linear_model.LinearRegression()\nmodel_1.fit(train_data[model_1_features], train_data['price'])\n\nmodel_2 = linear_model.LinearRegression()\nmodel_2.fit(train_data[model_2_features], train_data['price'])\n\nmodel_3 = linear_model.LinearRegression()\nmodel_3.fit(train_data[model_3_features], train_data['price'])\n\n# extracting the coef\nprint model_1.coef_\nprint model_2.coef_\nprint model_3.coef_\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The trick is that right after you have trained your model, you know the order of the coefficients:</p>\n<pre><code>model_1 = linear_model.LinearRegression()\nmodel_1.fit(train_data[model_1_features], train_data['price'])\nprint(list(zip(model_1.coef_, model_1_features)))\n</code></pre>\n<p>This will print the coefficients and the correct feature. (Tested with pandas DataFrame)</p>\n<p>If you want to reuse the coefficients later you can also put them in a dictionary:</p>\n<pre><code>coef_dict = {}\nfor coef, feat in zip(model_1.coef_,model_1_features):\n    coef_dict[feat] = coef\n</code></pre>\n<p>(You can test it for yourself by training two models with the same features but, as you said, shuffled order of features.)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>import pandas as pd\n\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\ncoef_table = pd.DataFrame(list(X_train.columns)).copy()\ncoef_table.insert(len(coef_table.columns),\"Coefs\",regressor.coef_.transpose())\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>@Robin posted a great answer, but for me I had to make one tweak on it to work the way I wanted, and it was to refer to the dimension of the 'coef_' np.array that I wanted, namely modifying to this: model_1.coef_[0,:], as below:</p>\n<pre><code>coef_dict = {}\nfor coef, feat in zip(model_1.coef_[0,:],model_1_features):\n    coef_dict[feat] = coef\n</code></pre>\n<p>Then the dict was created as I pictured it, with {'feature_name' : coefficient_value} pairs.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Confused about <code>random_state</code> parameter, not sure why decision tree training needs some randomness. My thoughts</p>\n<ol>\n<li>is it related to random forest?</li>\n<li>is it related to split training testing data set? If so, why not use training testing split method directly (<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\" rel=\"nofollow noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html</a>)?</li>\n</ol>\n<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" rel=\"nofollow noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a></p>\n<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\niris = load_iris()\ncross_val_score(clf, iris.data, iris.target, cv=10)\n...                             \n...\narray([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is explained in <a href=\"http://scikit-learn.org/stable/modules/tree.html#tree\" rel=\"noreferrer\">the documentation</a></p>\n<blockquote>\n<p>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</p>\n</blockquote>\n<p>So, basically, a sub-optimal greedy algorithm is repeated a number of times using random selections of features and samples (a similar technique used in random forests). The <code>random_state</code> parameter allows controlling these random choices.</p>\n<p>The <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" rel=\"noreferrer\">interface documentation</a> specifically states:</p>\n<blockquote>\n<p>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.</p>\n</blockquote>\n<p>So, the random algorithm will be used in any case. Passing any value (whether a specific int, e.g., 0, or a <code>RandomState</code> instance), will not change that. The only rationale for passing in an int value (0 or otherwise) is to make the outcome consistent across calls: if you call this with <code>random_state=0</code> (or any other value), then each and every time, you'll get the same result.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>random_state</code> parameter present for decision trees in scikit-learn determines which feature to select for a split if (and only if) there are two splits that are equally good (i.e. two features yield the exact same improvement in the selected splitting criteria (e.g. gini)). If this is not the case, the <code>random_state</code> parameter has no effect.</p>\n<p>The <a href=\"https://github.com/scikit-learn/scikit-learn/issues/8443\" rel=\"noreferrer\">issue linked in teatrader's answer</a> discusses this in more detail and as a result of that discussion the following section was added to the docs (emphasis added):</p>\n<blockquote>\n<p><strong>random_state</strong> int, RandomState instance or None, default=None</p>\n<p>Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to \"best\". When max_features &lt; n_features, the algorithm will select max_features at random at each split before finding the best split among them. <strong>But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random.</strong> To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details.</p>\n</blockquote>\n<p>To illustrate, let's consider the following example with the iris sample data set and a shallow decision tree containing just a single split:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\niris = load_iris(as_frame=True)\nclf = DecisionTreeClassifier(max_depth=1)\nclf = clf.fit(iris.data, iris.target)\nplot_tree(clf, feature_names=iris['feature_names'], class_names=iris['target_names']);\n</code></pre>\n<p>The output of this code will alternate between the two following trees based on which <code>random_state</code> is used.</p>\n<p><a href=\"https://i.sstatic.net/VGhL3.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/VGhL3.png\"/></a></p>\n<p><a href=\"https://i.sstatic.net/olx3C.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/olx3C.png\"/></a></p>\n<p>The reason for this is that splitting on either <code>petal length &lt;= 2.45</code> or <code>petal width &lt;= 0.8</code> will both perfectly separate out the setosa class from the other two classes (we can see that the leftmost setosa node contains all 50 of the setosa observations).</p>\n<p>If we change just one observation of the data so that one of the previous two  splitting criteria no longer produces a perfect separation, the <code>random_state</code> will have no effect and we will always end up with the same result, for example:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Change the petal width for first observation of the \"Setosa\" class\n# so that it overlaps with the values of the other two classes\niris['data'].loc[0, 'petal width (cm)'] = 5\n\nclf = DecisionTreeClassifier(max_depth=1)\nclf = clf.fit(iris.data, iris.target)\nplot_tree(clf, feature_names=iris['feature_names'], class_names=iris['target_names']);\n</code></pre>\n<p><a href=\"https://i.sstatic.net/E0mXC.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/E0mXC.png\"/></a></p>\n<p>The first split will now always be <code>petal length &lt;= 2.45</code> since the split <code>petal width &lt;= 0.8</code> can only separate out 49 of the 50 setosa classes (in other words a lesser decreases in the gini score).</p>\n<p>For a random forest (which consists of many decision trees), we would create each individual tree with a random selections of features and samples (see <a href=\"https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters\" rel=\"noreferrer\">https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters</a> for details), so there is a bigger role for the <code>random_state</code> parameter, but this is not the case when training just a single decision tree (this is true with the default parameters, but it is worth noting that some parameters could be affected by randomness if they are changed from the default value, most notably setting <code>splitter=\"random\"</code>).</p>\n<p>A couple of related issues:</p>\n<ul>\n<li><a href=\"https://ai.stackexchange.com/questions/11576/are-decision-tree-learning-algorithms-deterministic\">https://ai.stackexchange.com/questions/11576/are-decision-tree-learning-algorithms-deterministic</a></li>\n<li><a href=\"https://stackoverflow.com/questions/42402752/is-the-cart-algorithm-used-by-scikit-learn-deterministic\">Is the CART algorithm used by scikit-learn deterministic?</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The above cited part of the documentation is misleading, the underlying problem is not greediness of the algorithm. The CART algorithm is deterministic (see e.g. <a href=\"https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0098450.s002&amp;type=supplementary\" rel=\"nofollow noreferrer\">here</a>) and finds a global minimum of the weighted Gini indices.</p>\n<p>Repeated runs of the decision tree can give different results because it is sometimes possible to split the data using different features and still achieve the same Gini index. This is described here:\n<a href=\"https://github.com/scikit-learn/scikit-learn/issues/8443\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/issues/8443</a>.</p>\n<p>Setting the random state simply assures that the CART implementation works through the same randomized list of features when looking for the minimum.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to get a confidence score of each of the predictions that it makes, showing on how sure the classifier is on its prediction that it is correct.  </p>\n<p>I want something like this:</p>\n<p>How sure is the classifier on its prediction?</p>\n<p>Class 1: 81% that this is class 1<br/>\nClass 2: 10%<br/>\nClass 3: 6%<br/>\nClass 4: 3%  </p>\n<p>Samples of my code:</p>\n<pre><code>features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(main, target, test_size = 0.4)\n\n# Determine amount of time to train\nt0 = time()\nmodel = SVC()\n#model = SVC(kernel='poly')\n#model = GaussianNB()\n\nmodel.fit(features_train, labels_train)\n\nprint 'training time: ', round(time()-t0, 3), 's'\n\n# Determine amount of time to predict\nt1 = time()\npred = model.predict(features_test)\n\nprint 'predicting time: ', round(time()-t1, 3), 's'\n\naccuracy = accuracy_score(labels_test, pred)\n\nprint 'Confusion Matrix: '\nprint confusion_matrix(labels_test, pred)\n\n# Accuracy in the 0.9333, 9.6667, 1.0 range\nprint accuracy\n\n\n\nmodel.predict(sub_main)\n\n# Determine amount of time to predict\nt1 = time()\npred = model.predict(sub_main)\n\nprint 'predicting time: ', round(time()-t1, 3), 's'\n\nprint ''\nprint 'Prediction: '\nprint pred\n</code></pre>\n<p>I suspect that I would use the score() function, but I seem to keep implementing it correctly. I don't know if that's the right function or not, but how would one get the confidence percentage of a classifier's prediction?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Per the <a href=\"http://scikit-learn.sourceforge.net/stable/modules/generated/sklearn.svm.SVC.html\" rel=\"noreferrer\">SVC documentation</a>, it looks like you need to change how you construct the SVC:</p>\n<pre><code>model = SVC(probability=True)\n</code></pre>\n<p>and then use the predict_proba method:</p>\n<pre><code>class_probabilities = model.predict_proba(sub_main)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For those estimators implementing <code>predict_proba()</code> method, like Justin Peel suggested, You can just use <code>predict_proba()</code> to produce probability on your prediction.</p>\n<p>For those estimators which do not implement <code>predict_proba()</code> method, you can construct confidence interval by yourself using bootstrap concept (repeatedly calculate your point estimates in many sub-samples).</p>\n<p>Let me know if you need any detailed examples to demonstrate either of these two cases.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the <a href=\"http://scikit-learn.org/stable/modules/svm.html\">documentation</a> scikit-learn implements SVC, NuSVC and LinearSVC which are classes capable of performing multi-class classification on a dataset. By the other hand I also read about that scikit learn also uses libsvm for support vector machine algorithm. I'm a bit confused about what's the difference between SVC and libsvm versions, by now I guess the difference is that SVC is the support vector machine algorithm fot the multiclass problem and libsvm is for the binary class problem. Could anybody help me to understad the difference between this?.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>They are just different implementations of the same algorithm. The SVM module (SVC, NuSVC, etc) is a wrapper around the <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\" rel=\"noreferrer\">libsvm</a> library and supports different kernels while <code>LinearSVC</code> is based on <a href=\"http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" rel=\"noreferrer\">liblinear</a> and only supports a linear kernel. So:</p>\n<pre><code>SVC(kernel = 'linear')\n</code></pre>\n<p>is in theory \"equivalent\" to:</p>\n<pre><code>LinearSVC()\n</code></pre>\n<p>Because the implementations are different in practice you will get different results, the most important ones being that LinearSVC only supports a linear kernel, is faster and can scale a lot better.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a snapshot from the book <strong>Hands-on Machine Learning</strong></p>\n<p><img alt=\"\" src=\"https://i.sstatic.net/BtPP2.png\"/></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been reading up on Decision Trees and Cross Validation, and I understand both concepts. However, I'm having trouble understanding Cross Validation as it pertains to Decision Trees. Essentially Cross Validation allows you to alternate between training and testing when your dataset is relatively small to maximize your error estimation. A very simple algorithm goes something like this: </p>\n<ol>\n<li>Decide on the number of folds you want (k) </li>\n<li>Subdivide your dataset into k folds </li>\n<li>Use k-1 folds for a training set to build a tree. </li>\n<li>Use the testing set to estimate statistics about the error in your tree. </li>\n<li>Save your results for later </li>\n<li>Repeat steps 3-6 for k times leaving out a different fold for your test set. </li>\n<li>Average the errors across your iterations to predict the overall error </li>\n</ol>\n<p>The problem I can't figure out is at the end you'll have k Decision trees that could all be slightly different because they might not split the same way, etc. Which tree do you pick? One idea I had was pick the one with minimal errors (although that doesn't make it optimal just that it performed best on the fold it was given - maybe using stratification will help but everything I've read say it only helps a little bit). </p>\n<p>As I understand cross validation the point is to compute in node statistics that can later be used for pruning. So really each node in the tree will have statistics calculated for it based on the test set given to it. What's important are these in node stats, but if your averaging your error.  How do you merge these stats within each node across k trees when each tree could vary in what they choose to split on, etc.</p>\n<p>What's the point of calculating the overall error across each iteration?  That's not something that could be used during pruning.</p>\n<p>Any help with this little wrinkle would be much appreciated. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><em>The problem I can't figure out is at the end you'll have k Decision trees that could all be slightly different because they might not split the same way, etc. Which tree do you pick?</em></p>\n<p><strong>The purpose of cross validation</strong> is not to help select a particular <em>instance</em> of the classifier (or decision tree, or whatever automatic learning application) but rather to qualify the <em>model</em>, i.e. to provide metrics such as the average error ratio, the deviation relative to this average etc. which can be useful in asserting the level of precision one can expect from the application.  One of the things cross validation can help assert is whether the training data is big enough.</p>\n<p><strong>With regards to selecting a particular tree</strong>, you should instead run yet another training on 100% of the training data available, as this typically will produce a better tree. (The downside of the Cross Validation approach is that we need to divide the [typically little] amount of training data into \"folds\" and as you hint in the question this can lead to trees which are either overfit or underfit for particular data instances).</p>\n<p>In the case of decision tree, I'm not sure what your reference to statistics gathered in the node and used to prune the tree pertains to.  Maybe a particular use of cross-validation related techniques?...</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For the first part, and like the others have pointed out, we usually use the entire dataset for building the final model, but we use <strong>cross-validation</strong> (CV) to get a better estimate of the generalization error on new unseen data.</p>\n<p>For the second part, I think you are confusing CV with the <strong>validation set</strong>, used to avoid <strong>overfitting</strong> the tree by pruning a node when some function value computed on the validation set does not increase before/after the split.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Cross validation isn't used for buliding/pruning the decision tree. It's used to estimate how good the tree (built on all of the data) will perform by simulating arrival of new data (by building the tree without some elements just as you wrote). I doesn't really make sense to pick one of the trees generated by it because the model is constrained by the data you have (and not using it all might actually be worse when you use the tree for new data).<br/>\n    The tree is built over the data that you choose (usualy all of it). Pruning is usually done by using some heuristic (i.e. 90% of the elements in the node belongs to class A so we don't go any further or the information gain is too small).<br/></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose I want to have the general neural network architecture:</p>\n<pre><code>Input1 --&gt; CNNLayer \n                    \\\n                     ---&gt; FCLayer ---&gt; Output\n                    /\nInput2 --&gt; FCLayer\n</code></pre>\n<p>Input1 is image data, input2 is non-image data. I have implemented this architecture in Tensorflow.</p>\n<p>All pytorch examples I have found are one input go through each layer. How can I define forward func to process 2 inputs separately then combine them in a middle layer? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>By \"combine them\" I assume you mean to <a href=\"https://pytorch.org/docs/master/torch.html?highlight=norm#torch.cat\" rel=\"noreferrer\">concatenate</a> the two inputs.<br/>\nAssuming you concat along the second dimension:</p>\n<pre><code>import torch\nfrom torch import nn\n\nclass TwoInputsNet(nn.Module):\n  def __init__(self):\n    super(TwoInputsNet, self).__init__()\n    self.conv = nn.Conv2d( ... )  # set up your layer here\n    self.fc1 = nn.Linear( ... )  # set up first FC layer\n    self.fc2 = nn.Linear( ... )  # set up the other FC layer\n\n  def forward(self, input1, input2):\n    c = self.conv(input1)\n    f = self.fc1(input2)\n    # now we can reshape `c` and `f` to 2D and concat them\n    combined = torch.cat((c.view(c.size(0), -1),\n                          f.view(f.size(0), -1)), dim=1)\n    out = self.fc2(combined)\n    return out\n</code></pre>\n<p>Note that when you define the number of inputs to <code>self.fc2</code> you need to take into account both <code>out_channels</code> of <code>self.conv</code> as well as the output spatial dimensions of <code>c</code>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Having this:</p>\n<pre><code>text = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n</code></pre>\n<p>And running:</p>\n<pre><code>nltk.pos_tag(text)\n</code></pre>\n<p>I get:</p>\n<pre><code>[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n</code></pre>\n<p>This is incorrect. The tags for <code>quick brown lazy</code> in the sentence should be:</p>\n<pre><code>('quick', 'JJ'), ('brown', 'JJ') , ('lazy', 'JJ')\n</code></pre>\n<p>Testing this through their <a href=\"http://nlp.stanford.edu:8080/corenlp/process\">online tool</a> gives the same result; <code>quick</code>, <code>brown</code> and <code>fox</code> should be adjectives not nouns.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>In short</strong>:</p>\n<blockquote>\n<p>NLTK is not perfect. In fact, no model is perfect.</p>\n</blockquote>\n<p><strong>Note:</strong></p>\n<p>As of NLTK version 3.1, default <code>pos_tag</code> function is no longer the <a href=\"https://stackoverflow.com/questions/31386224/what-created-maxent-treebank-pos-tagger-english-pickle\">old MaxEnt English pickle</a>. </p>\n<p>It is now the <strong>perceptron tagger</strong> from <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tag/perceptron.py\" rel=\"noreferrer\">@Honnibal's implementation</a>, see <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py#L87\" rel=\"noreferrer\"><code>nltk.tag.pos_tag</code></a></p>\n<pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; print inspect.getsource(pos_tag)\ndef pos_tag(tokens, tagset=None):\n    tagger = PerceptronTagger()\n    return _pos_tag(tokens, tagset, tagger) \n</code></pre>\n<p>Still it's better but not perfect:</p>\n<pre><code>&gt;&gt;&gt; from nltk import pos_tag\n&gt;&gt;&gt; pos_tag(\"The quick brown fox jumps over the lazy dog\".split())\n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n</code></pre>\n<p>At some point, if someone wants <code>TL;DR</code> solutions, see <a href=\"https://github.com/alvations/nltk_cli\" rel=\"noreferrer\">https://github.com/alvations/nltk_cli</a></p>\n<hr/>\n<p><strong>In long</strong>:</p>\n<p><strong>Try using other tagger (see <a href=\"https://github.com/nltk/nltk/tree/develop/nltk/tag\" rel=\"noreferrer\">https://github.com/nltk/nltk/tree/develop/nltk/tag</a>) , e.g.</strong>:</p>\n<ul>\n<li>HunPos</li>\n<li>Stanford POS</li>\n<li>Senna</li>\n</ul>\n<p><strong>Using default MaxEnt POS tagger from NLTK, i.e. <code>nltk.pos_tag</code></strong>:</p>\n<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; pos_tag(word_tokenize(text))\n[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n</code></pre>\n<p><strong>Using Stanford POS tagger</strong>:</p>\n<pre><code>$ cd ~\n$ wget http://nlp.stanford.edu/software/stanford-postagger-2015-04-20.zip\n$ unzip stanford-postagger-2015-04-20.zip\n$ mv stanford-postagger-2015-04-20 stanford-postagger\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser(\"~\")\n&gt;&gt;&gt; from nltk.tag.stanford import POSTagger\n&gt;&gt;&gt; _path_to_model = home + '/stanford-postagger/models/english-bidirectional-distsim.tagger'\n&gt;&gt;&gt; _path_to_jar = home + '/stanford-postagger/stanford-postagger.jar'\n&gt;&gt;&gt; st = POSTagger(path_to_model=_path_to_model, path_to_jar=_path_to_jar)\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; st.tag(text.split())\n[(u'The', u'DT'), (u'quick', u'JJ'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'jumps', u'VBZ'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n</code></pre>\n<p><strong>Using HunPOS</strong> (NOTE: the default encoding is ISO-8859-1 not UTF8):</p>\n<pre><code>$ cd ~\n$ wget https://hunpos.googlecode.com/files/hunpos-1.0-linux.tgz\n$ tar zxvf hunpos-1.0-linux.tgz\n$ wget https://hunpos.googlecode.com/files/en_wsj.model.gz\n$ gzip -d en_wsj.model.gz \n$ mv en_wsj.model hunpos-1.0-linux/\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser(\"~\")\n&gt;&gt;&gt; from nltk.tag.hunpos import HunposTagger\n&gt;&gt;&gt; _path_to_bin = home + '/hunpos-1.0-linux/hunpos-tag'\n&gt;&gt;&gt; _path_to_model = home + '/hunpos-1.0-linux/en_wsj.model'\n&gt;&gt;&gt; ht = HunposTagger(path_to_model=_path_to_model, path_to_bin=_path_to_bin)\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; ht.tag(text.split())\n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n</code></pre>\n<p><strong>Using Senna</strong> (Make sure you've the latest version of NLTK, there were some changes made to the API):</p>\n<pre><code>$ cd ~\n$ wget http://ronan.collobert.com/senna/senna-v3.0.tgz\n$ tar zxvf senna-v3.0.tgz\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser(\"~\")\n&gt;&gt;&gt; from nltk.tag.senna import SennaTagger\n&gt;&gt;&gt; st = SennaTagger(home+'/senna')\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; st.tag(text.split())\n[('The', u'DT'), ('quick', u'JJ'), ('brown', u'JJ'), ('fox', u'NN'), ('jumps', u'VBZ'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'JJ'), ('dog', u'NN')]\n</code></pre>\n<hr/>\n<p><strong>Or try building a better POS tagger</strong>:</p>\n<ul>\n<li>Ngram Tagger: <a href=\"http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/\" rel=\"noreferrer\">http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/</a></li>\n<li>Affix/Regex Tagger: <a href=\"http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/\" rel=\"noreferrer\">http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/</a> </li>\n<li>Build Your Own Brill (Read the code it's a pretty fun tagger, <a href=\"http://www.nltk.org/_modules/nltk/tag/brill.html\" rel=\"noreferrer\">http://www.nltk.org/_modules/nltk/tag/brill.html</a>), see <a href=\"http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/\" rel=\"noreferrer\">http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/</a></li>\n<li>Perceptron Tagger: <a href=\"https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\" rel=\"noreferrer\">https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/</a></li>\n<li>LDA Tagger: <a href=\"http://scm.io/blog/hack/2015/02/lda-intentions/\" rel=\"noreferrer\">http://scm.io/blog/hack/2015/02/lda-intentions/</a></li>\n</ul>\n<hr/>\n<p><strong>Complains about <code>pos_tag</code> accuracy on stackoverflow include</strong>:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/13529945/pos-tagging-nltk-thinks-noun-is-adjective\">POS tagging - NLTK thinks noun is adjective</a></li>\n<li><a href=\"https://stackoverflow.com/questions/21786257/python-nltk-pos-tagger-not-behaving-as-expected\">python NLTK POS tagger not behaving as expected</a></li>\n<li><a href=\"https://stackoverflow.com/questions/8146748/how-to-obtain-better-results-using-nltk-pos-tag\">How to obtain better results using NLTK pos tag</a></li>\n<li><a href=\"https://stackoverflow.com/questions/8365557/pos-tag-in-nltk-does-not-tag-sentences-correctly\">pos_tag in NLTK does not tag sentences correctly</a></li>\n</ul>\n<p><strong>Issues about NLTK HunPos include</strong>:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/5088448/how-to-i-tag-textfiles-with-hunpos-in-nltk\">How do I tag textfiles with hunpos in nltk?</a> </li>\n<li><a href=\"https://stackoverflow.com/questions/5091389/does-anyone-know-how-to-configure-the-hunpos-wrapper-class-on-nltk\">Does anyone know how to configure the hunpos wrapper class on nltk?</a></li>\n</ul>\n<p><strong>Issues with NLTK and Stanford POS tagger include</strong>:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk\">trouble importing stanford pos tagger into nltk</a></li>\n<li><a href=\"https://stackoverflow.com/questions/27116495/java-command-fails-in-nltk-stanford-pos-tagger\">Java Command Fails in NLTK Stanford POS Tagger</a></li>\n<li><a href=\"https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python\">Error using Stanford POS Tagger in NLTK Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/23322674/how-to-improve-speed-with-stanford-nlp-tagger-and-nltk\">How to improve speed with Stanford NLP Tagger and NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/27171298/nltk-stanford-pos-tagger-error-java-command-failed\">Nltk stanford pos tagger error : Java command failed</a></li>\n<li><a href=\"https://stackoverflow.com/questions/8555312/instantiating-and-using-stanfordtagger-within-nltk\">Instantiating and using StanfordTagger within NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/26647253/running-stanford-pos-tagger-in-nltk-leads-to-not-a-valid-win32-application-on\">Running Stanford POS tagger in NLTK leads to \"not a valid Win32 application\" on Windows</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Solutions such as changing to the Stanford or Senna or HunPOS tagger will definitely yield results, but here is a much simpler way to experiment with different taggers that are also included within NLTK.</p>\n<p>The default POS tagger in NTLK right now is the averaged perceptron tagger.  Here's a function that will opt to use the Maxent Treebank Tagger instead:</p>\n<pre><code>def treebankTag(text)\n    words = nltk.word_tokenize(text)\n    treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n    return treebankTagger.tag(words)\n</code></pre>\n<p>I have found that the averaged perceptron pre-trained tagger in NLTK is biased to treating some adjectives as nouns, as in your example.  The treebank tagger has gotten more adjectives correct for me.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>def tagPOS(textcontent, taggedtextcontent, defined_tags):\n    # Write your code here\n    token = nltk.word_tokenize(textcontent)\n    nltk_pos_tags = nltk.pos_tag(token)\n    \n    unigram_pos_tag = nltk.UnigramTagger(model=defined_tags).tag(token)\n    \n    tagged_pos_tag = [ nltk.tag.str2tuple(word) for word in taggedtextcontent.split() ]\n    \n    return (nltk_pos_tags,tagged_pos_tag,unigram_pos_tag)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a sentiment analysis task, for this Im using this <a href=\"http://pastebin.com/ikbKQcsc\" rel=\"noreferrer\">corpus</a> the opinions have 5 classes (<code>very neg</code>, <code>neg</code>, <code>neu</code>, <code>pos</code>, <code>very pos</code>), from 1 to 5. So I do the classification as follows:</p>\n<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ntfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,\n                            sublinear_tf=False, ngram_range=(2,2))\nfrom sklearn.cross_validation import train_test_split, cross_val_score\n\nimport pandas as pd\n\ndf = pd.read_csv('/corpus.csv',\n                     header=0, sep=',', names=['id', 'content', 'label'])\n\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\n\n\nfrom sklearn.svm import SVC\nsvm_1 = SVC(kernel='linear')\nsvm_1.fit(X, y)\nsvm_1_prediction = svm_1.predict(X_test)\n</code></pre>\n<p>Then with the metrics I obtained the following confusion matrix and classification report, as follows:</p>\n<pre><code>print '\\nClasification report:\\n', classification_report(y_test, svm_1_prediction)\nprint '\\nConfussion matrix:\\n',confusion_matrix(y_test, svm_1_prediction)\n</code></pre>\n<p>Then, this is the result:</p>\n<pre><code>Clasification report:\n             precision    recall  f1-score   support\n\n          1       1.00      0.76      0.86        71\n          2       1.00      0.84      0.91        43\n          3       1.00      0.74      0.85        89\n          4       0.98      0.95      0.96       288\n          5       0.87      1.00      0.93       367\n\navg / total       0.94      0.93      0.93       858\n\n\nConfussion matrix:\n[[ 54   0   0   0  17]\n [  0  36   0   1   6]\n [  0   0  66   5  18]\n [  0   0   0 273  15]\n [  0   0   0   0 367]]\n</code></pre>\n<p>How can I interpret the above confusion matrix and classification report. I tried reading the <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\" rel=\"noreferrer\">documentation</a> and this <a href=\"https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix\">question</a>. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow \"diagonal\"?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Classification report must be straightforward - a report of P/R/F-Measure for each element in your test data. In Multiclass problems, it is not a good idea to read Precision/Recall and F-Measure over the whole data any imbalance would make you feel you've reached better results. That's where such reports help.</p>\n<p>Coming to confusion matrix, it is much detailed representation of what's going on with your labels. So there were 71 points in the first class (label 0). Out of these, your model was successful in identifying 54 of those correctly in label 0, but 17 were marked as label 4. Similarly look at second row. There were 43 points in class 1, but 36 of them were marked correctly. Your classifier predicted 1 in class 3 and 6 in class 4.</p>\n<p><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0yLu8.png\"/></p>\n<p>Now you can see the pattern this follows. An ideal classifiers with 100% accuracy would produce a pure diagonal matrix which would have all the points predicted in their correct class. </p>\n<p>Coming to Recall/Precision. They are some of the mostly used measures in evaluating how good your system works. Now you had 71 points in first class (call it 0 class). Out of them your classifier was able to get 54 elements correctly. That's your recall. 54/71 = 0.76. Now look only at first column in the table. There is one cell with entry 54, rest all are zeros. This means your classifier marked 54 points in class 0, and all 54 of them were actually in class 0. This is precision. 54/54 = 1. Look at column marked 4. In this column, there are elements scattered in all the five rows. 367 of them were marked correctly. Rest all are incorrect. So that reduces your precision.</p>\n<p>F Measure is harmonic mean of Precision and Recall. \nBe sure you read details about these. <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Precision_and_recall</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's the documentation for scikit-learn's sklearn.metrics.precision_recall_fscore_support method: <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support\" rel=\"nofollow\">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support</a></p>\n<p>It seems to indicate that the support is the number of occurrences of each particular class in the true responses (responses in your test set). You can calculate it by summing the rows of the confusion matrix. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am a newbie in Machine learning and Natural language processing.</p>\n<p>I am always confused between what are those three terms?</p>\n<p>From my understanding:</p>\n<p>class: The various categories our model output. Given a name of person identify whether he/she is male or female?</p>\n<p>Lets say I am using Naive Bayes classifier.</p>\n<p>What would be my features and parameters?</p>\n<p>Also, what are some of the aliases of the above words which are used interchangeably?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let's use the example of classifying the gender of a person. Your understanding about class is correct! Given an input observation, our Naive Bayes Classifier should output a category. The class is that category.</p>\n<p><strong>Features</strong>: Features in a Naive Bayes Classifier, or any general ML Classification Algorithm, are the data points we choose to define our input. For the example of a person, we can't possibly input all data points about a person; instead, we pick a few features to define a person (say \"Height\", \"Weight\", and \"Foot Size\"). Specifically, in a <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\" rel=\"nofollow noreferrer\">Naive Bayes Classifier</a>, the key assumption we make is that these features are independent (they don't affect each other): a person's height doesn't affect weight doesn't affect foot size. This assumption may or not be true, but for a Naive Bayes, we assume that it is true. In the particular case of your example where the input is just the name, features might be frequency of letters, number of vowels, length of name, or suffix/prefixes.</p>\n<p><strong>Parameters</strong>: Parameters in Naive Bayes are the estimates of the true distribution of whatever we're trying to classify. For example, we could say that roughly 50% of people are male, and the distribution of male height is a Gaussian distribution with mean 5' 7\" and standard deviation 3\".  The parameters would be the 50% estimate, the 5' 7\" mean estimate, and the 3\" standard deviation estimate.</p>\n<p><strong>Aliases</strong>: Features are also referred to as attributes. I'm not aware of any common replacements for 'parameters'.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>@txizzle explained the case of Naive Bayes well. In a more general sense:</p>\n<p><strong>Class:</strong> The output category of your data. You can call these categories as well. The labels on your data will point to one of the classes (if it's a classification problem, of course.)</p>\n<p><strong>Features:</strong> The characteristics that define your problem. These are also called attributes.</p>\n<p><strong>Parameters:</strong> The variables your algorithm is trying to tune to build an accurate model.</p>\n<p>As an example, let us say you are trying to decide to whether admit a student to grad school or not based on various factors like his/her undergrad GPA, test scores, scores on recommendations, projects etc. In this case, the factors mentioned above are your features/attributes, whether the student is given an admission or not become your 2 classes, and the numbers which decide how these features combine together to get your output become your parameters. What the parameters actually represent depends on your algorithm. For a Neural Net, it's the weights on the synaptic links. Similarly, for a regression problem, the parameters are the coefficients of your features when they are combined.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>take a simple linear classification problem-</p>\n<p>y={0 if 5x-3&gt;=0 else 1}</p>\n<p>here y is class, x is feature, 5,3 are parameters.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In R, after running \"random forest\" model, I can use <code>save.image(\"***.RData\")</code> to store the model. Afterwards, I can just load the model to do predictions directly.</p>\n<p>Can you do a similar thing in python? I separate the Model and Prediction into two files. And in Model file:</p>\n<pre><code>rf= RandomForestRegressor(n_estimators=250, max_features=9,compute_importances=True)\nfit= rf.fit(Predx, Predy)\n</code></pre>\n<p>I tried to return <code>rf</code> or <code>fit</code>, but still can't load the model in the prediction file.</p>\n<p>Can you separate the model and prediction using the sklearn random forest package?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>...\nimport cPickle\n\nrf = RandomForestRegresor()\nrf.fit(X, y)\n\nwith open('path/to/file', 'wb') as f:\n    cPickle.dump(rf, f)\n\n\n# in your prediction file                                                                                                                                                                                                           \n\nwith open('path/to/file', 'rb') as f:\n    rf = cPickle.load(f)\n\n\npreds = rf.predict(new_X)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://joblib.readthedocs.io/en/latest/index.html\" rel=\"noreferrer\"><code>joblib</code></a> to save and load the Random Forest from scikit-learn (in fact, any model from scikit-learn)</p>\n<p>The example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import joblib\nfrom sklearn.ensemble import RandomForestClassifier\n# create RF\nrf = RandomForestClassifier()\n# fit on some data\nrf.fit(X, y)\n\n# save\njoblib.dump(rf, \"my_random_forest.joblib\")\n\n# load\nloaded_rf = joblib.load(\"my_random_forest.joblib\")\n\n</code></pre>\n<p>What is more, the <code>joblib.dump</code> <code>has compress</code> argument, so the model can be compressed. I made very simple <a href=\"https://mljar.com/blog/save-load-random-forest/\" rel=\"noreferrer\">test</a> on iris dataset and <code>compress=3</code> reduces the size of the file about 5.6 times.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use dill, it stores all the data and I think possibly module information? Maybe not. I remember trying to use <code>pickle</code> for storing these really complicated objects and it didn't work for me.  <code>cPickle</code> probably does the same job as <code>dill</code> but i've never tried <code>cpickle</code>. it looks like it works in literally the exact same way.  I use \"obj\" extension but that's by no means conventional...It just made sense for me since I was storing an object.</p>\n<pre><code>import dill\nwd = \"/whatever/you/want/your/working/directory/to/be/\"\nrf= RandomForestRegressor(n_estimators=250, max_features=9,compute_importances=True)\nrf.fit(Predx, Predy)\ndill.dump(rf, open(wd + \"filename.obj\",\"wb\"))\n</code></pre>\n<p>btw, not sure if you use iPython, but sometimes writing a file that way doesn't so you  have to do the:</p>\n<pre><code>with open(wd + \"filename.obj\",\"wb\") as f:\n    dill.dump(rf,f)\n</code></pre>\n<p>call the objects again:</p>\n<pre><code>model = dill.load(open(wd + \"filename.obj\",\"rb\"))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Consider the following training data sets.. </p>\n<pre><code>+-------+-------+----------+-------------+\n| Size  | Color | Shape    | Class/Label |\n+=======+=======+==========+=============+\n| big   | red   | circle   | No          |\n| small | red   | triangle | No          |\n| small | red   | circle   | Yes         |\n| big   | blue  | circle   | No          |\n| small | blue  | circle   | Yes         |\n+-------+-------+----------+-------------+\n</code></pre>\n<p>I would like to understand how the algorithm proceeds when it starts with a negative example and when two negative examples come together.</p>\n<p>This is not an assignment question by the way.</p>\n<p>Examples with other data sets are also welcome! This is to understand the negative part of the algorithm.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For your hypothesis space (H), you start with your sets of maximally general (G) and maximally specific (S) hypotheses:</p>\n<pre><code>G0 = {&lt;?, ?, ?&gt;}\nS0 = {&lt;0, 0, 0&gt;}\n</code></pre>\n<p>When you are presented with a negative example, you need to remove from S any hypothesis inconsistent with the current observation and replace any inconsistent hypothesis in G with its minimal specializations that are consistent with the observation but still more general than some member of S.</p>\n<p>So for your first (negative) example, <code>(big, red, circle)</code>, the minimal specializations would make the new hypothesis space</p>\n<pre><code>G1 = {&lt;small, ? , ?&gt;, &lt;?, blue, ?&gt;, &lt;?, ?, triangle&gt;}\nS1 = S0 = {&lt;0, 0, 0&gt;}\n</code></pre>\n<p>Note that S did not change. For your next example, <code>(small, red, triangle)</code>, which is also negative, you will need to further specialize G. Note that the second hypothesis in G1 does not match the new observation so only the first and third hypotheses in G1 need to be specialized. That would yield</p>\n<pre><code>G2 = {&lt;small, blue, ?&gt;, &lt;small, ?, circle&gt;, &lt;?, blue, ?&gt;, &lt;big, ?, triangle&gt;, &lt;?, blue, triangle&gt;}\n</code></pre>\n<p>However, since the first and last hypotheses in G2 above are specializations of the middle hypothesis (<code>&lt;?, blue, ?&gt;</code>), we drop those two, giving</p>\n<pre><code>G2 = {&lt;small, ?, circle&gt;, &lt;?, blue, ?&gt;, &lt;big, ?, triangle&gt;}\nS2 = S1 = S0 = {&lt;0, 0, 0&gt;}\n</code></pre>\n<p>For the positive <code>(small, red, circle)</code> observation, you must generalize S and remove anything in G that is inconsistent, which gives</p>\n<pre><code>G3 = {&lt;small, ?, circle&gt;}\nS3 = {&lt;small, red, circle&gt;}\n</code></pre>\n<p><code>(big, blue, circle)</code> is the next negative example. But since it in not consistent with G, there is nothing to do so</p>\n<pre><code>G4 = G3 = {&lt;small, ?, circle&gt;}\nS4 = S3 = {&lt;small, red, circle&gt;}\n</code></pre>\n<p>Lastly, you have the positive example of <code>(small, blue, circle)</code>, which requires you to generalize S to make it consistent with the example, giving</p>\n<pre><code>G5 = {&lt;small, ?, circle&gt;}\nS5 = {&lt;small, ?, circle&gt;}\n</code></pre>\n<p>Since G and S are equal, you have learned the concept of \"small circles\".</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm dealing with an imbalanced dataset and want to do a grid search to tune my model's parameters using scikit's gridsearchcv. To oversample the data, I want to use SMOTE, and I know I can include that as a stage of a pipeline and pass it to gridsearchcv.\nMy concern is that I think smote will be applied to both train and validation folds, which is not what you are supposed to do. The validation set should not be oversampled.\nAm I right that the whole pipeline will be applied to both dataset splits? And if yes, how can I turn around this?\nThanks a lot in advance</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, it can be done, but with <a href=\"https://imbalanced-learn.org/stable/auto_examples/pipeline/plot_pipeline_classification.html\" rel=\"noreferrer\">imblearn Pipeline</a>.</p>\n<p>You see, imblearn has its own Pipeline to handle the samplers correctly. I described this in <a href=\"https://stackoverflow.com/a/49771602/3374996\">a similar question here</a>.</p>\n<p>When called <code>predict()</code> on a <code>imblearn.Pipeline</code> object, it will skip the sampling method and leave the data as it is to be passed to next transformer.\nYou can confirm that by looking at the <a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn/blob/d9c8c64/imblearn/pipeline.py#L372\" rel=\"noreferrer\">source code here</a>:</p>\n<pre><code>        if hasattr(transform, \"fit_sample\"):\n            pass\n        else:\n            Xt = transform.transform(Xt)\n</code></pre>\n<p>So for this to work correctly, you need the following:</p>\n<pre><code>from imblearn.pipeline import Pipeline\nmodel = Pipeline([\n        ('sampling', SMOTE()),\n        ('classification', LogisticRegression())\n    ])\n\ngrid = GridSearchCV(model, params, ...)\ngrid.fit(X, y)\n</code></pre>\n<p>Fill the details as necessary, and the pipeline will take care of the rest.</p>\n</div>"
        ]
    }
]