[
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How to load a model from an HDF5 file in Keras?</p>\n<p>What I tried:</p>\n<pre><code>model = Sequential()\n\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, init='uniform'))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2, init='uniform'))\nmodel.add(Activation('softmax'))\n\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\ncheckpointer = ModelCheckpoint(filepath=\"/weights.hdf5\", verbose=1, save_best_only=True)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2, callbacks=[checkpointer])\n</code></pre>\n<p>The above code successfully saves the best model to a file named weights.hdf5. What I want to do is then load that model. The below code shows how I tried to do so:</p>\n<pre><code>model2 = Sequential()\nmodel2.load_weights(\"/Users/Desktop/SquareSpace/weights.hdf5\")\n</code></pre>\n<p>This is the error I get:</p>\n<pre><code>IndexError                                Traceback (most recent call last)\n&lt;ipython-input-101-ec968f9e95c5&gt; in &lt;module&gt;()\n      1 model2 = Sequential()\n----&gt; 2 model2.load_weights(\"/Users/Desktop/SquareSpace/weights.hdf5\")\n\n/Applications/anaconda/lib/python2.7/site-packages/keras/models.pyc in load_weights(self, filepath)\n    582             g = f['layer_{}'.format(k)]\n    583             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n--&gt; 584             self.layers[k].set_weights(weights)\n    585         f.close()\n    586 \n\nIndexError: list index out of range\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you stored the complete model, not only the weights, in the HDF5 file, then it is as simple as</p>\n<pre><code>from keras.models import load_model\nmodel = load_model('model.h5')\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>load_weights</code> only sets the weights of your network. You still need to define its architecture before calling <code>load_weights</code>:</p>\n<pre><code>def create_model():\n   model = Sequential()\n   model.add(Dense(64, input_dim=14, init='uniform'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5)) \n   model.add(Dense(64, init='uniform'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5))\n   model.add(Dense(2, init='uniform'))\n   model.add(Activation('softmax'))\n   return model\n\ndef train():\n   model = create_model()\n   sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n   model.compile(loss='binary_crossentropy', optimizer=sgd)\n\n   checkpointer = ModelCheckpoint(filepath=\"/tmp/weights.hdf5\", verbose=1, save_best_only=True)\n   model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose=2, callbacks=[checkpointer])\n\ndef load_trained_model(weights_path):\n   model = create_model()\n   model.load_weights(weights_path)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>See the following sample code on how to Build a basic Keras Neural Net Model, save Model (JSON) &amp; Weights (HDF5) and load them:</p>\n<pre><code># create model\nmodel = Sequential()\nmodel.add(Dense(X.shape[1], input_dim=X.shape[1], activation='relu')) #Input Layer\nmodel.add(Dense(X.shape[1], activation='relu')) #Hidden Layer\nmodel.add(Dense(output_dim, activation='softmax')) #Output Layer\n\n# Compile &amp; Fit model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X,Y,nb_epoch=5,batch_size=100,verbose=1)    \n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"Data/model.json\", \"w\") as json_file:\n    json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n\n# serialize weights to HDF5\nmodel.save_weights(\"Data/model.h5\")\nprint(\"Saved model to disk\")\n\n# load json and create model\njson_file = open('Data/model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"Data/model.h5\")\nprint(\"Loaded model from disk\")\n\n# evaluate loaded model on test data \n# Define X_test &amp; Y_test data first\nloaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, Y_test, verbose=0)\nprint (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am totally new to Machine Learning and I have been working with unsupervised learning technique.</p>\n<p>Image shows my sample Data(After all Cleaning) Screenshot :\n<a href=\"https://i.sstatic.net/sxXbT.png\" rel=\"noreferrer\">Sample Data</a></p>\n<p>I have this two Pipline built to Clean the Data:</p>\n<pre><code>num_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nprint(type(num_attribs))\n\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_attribs)),\n    ('imputer', Imputer(strategy=\"median\")),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attribs)),\n    ('label_binarizer', LabelBinarizer())\n])\n</code></pre>\n<p>Then I did the union of this two pipelines and the code for the same is shown below :</p>\n<pre><code>from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])\n</code></pre>\n<p>Now I am trying to do fit_transform on the <a href=\"https://i.sstatic.net/HJ30l.png\" rel=\"noreferrer\">Data</a> But Its showing Me the Error.</p>\n<p>Code for Transformation:</p>\n<pre><code>housing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared\n</code></pre>\n<p>Error message:</p>\n<blockquote>\n<p>fit_transform() takes 2 positional arguments but 3 were given</p>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>The Problem:</strong></p>\n<p>The pipeline is assuming LabelBinarizer's <code>fit_transform</code> method is defined to take three positional arguments: </p>\n<pre><code>def fit_transform(self, x, y)\n    ...rest of the code\n</code></pre>\n<p>while it is defined to take only two: </p>\n<pre><code>def fit_transform(self, x):\n    ...rest of the code\n</code></pre>\n<p><strong>Possible Solution:</strong></p>\n<p>This can be solved by making a custom transformer that can handle 3 positional arguments:</p>\n<ol>\n<li><p>Import and make a new class:</p>\n<pre><code>from sklearn.base import TransformerMixin #gives fit_transform method for free\nclass MyLabelBinarizer(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelBinarizer(*args, **kwargs)\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)\n</code></pre></li>\n<li><p>Keep your code the same only instead of using LabelBinarizer(), use the class we created : MyLabelBinarizer().</p></li>\n</ol>\n<p><hr/>\nNote: If you want access to LabelBinarizer Attributes (e.g. classes_), add the following line to the <code>fit</code> method:</p>\n<pre><code>    self.classes_, self.y_type_, self.sparse_input_ = self.encoder.classes_, self.encoder.y_type_, self.encoder.sparse_input_\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I believe your example is from the book <em>Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow</em>. Unfortunately, I ran into this problem, as well. A recent change in <code>scikit-learn</code> (<code>0.19.0</code>) changed <code>LabelBinarizer</code>'s <code>fit_transform</code> method. Unfortunately, <code>LabelBinarizer</code> was never intended to work how that example uses it. You can see information about the change <a href=\"https://github.com/scikit-learn/scikit-learn/issues/7238\" rel=\"noreferrer\">here</a> and <a href=\"https://github.com/scikit-learn/scikit-learn/pull/7670\" rel=\"noreferrer\">here</a>.</p>\n<p>Until they come up with a solution for this, you can install the previous version (<code>0.18.0</code>) as follows:</p>\n<pre><code>$ pip install scikit-learn==0.18.0\n</code></pre>\n<p>After running that, your code should run without issue.</p>\n<p>In the future, it looks like the correct solution may be to use a <code>CategoricalEncoder</code> class or something similar to that. They have been trying to solve this problem for years apparently. You can see the new class <a href=\"https://github.com/scikit-learn/scikit-learn/pull/9151\" rel=\"noreferrer\">here</a> and further discussion of the problem <a href=\"https://github.com/scikit-learn/scikit-learn/pull/7375/files#diff-1e175ddb0d84aad0a578d34553f6f9c6\" rel=\"noreferrer\">here</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you are going through the examples from the book: <a href=\"https://www.amazon.com.au/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291\" rel=\"noreferrer\">Hands on Machine Learning with Scikit Learn and Tensorflow</a>. I ran into the same problem when going through the example in Chapter 2. </p>\n<p>As mentioned by other people, the problem is to do with sklearn's LabelBinarizer. It takes less args in its fit_transform method compared to other transformers in the pipeline. (only y when other transformers normally take both X and y, see <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer\" rel=\"noreferrer\">here</a> for details). That's why when we run pipeline.fit_transform, we fed more args into this transformer than required.</p>\n<p>An easy fix I used is to just use OneHotEncoder and set the \"sparse\" to False to ensure the output is a numpy array same as the num_pipeline output. (this way you don't need to code up your own custom encoder)</p>\n<p>your original cat_pipeline:</p>\n<pre><code>cat_pipeline = Pipeline([\n('selector', DataFrameSelector(cat_attribs)),\n('label_binarizer', LabelBinarizer())\n])\n</code></pre>\n<p>you can simply change this part to:</p>\n<pre><code>cat_pipeline = Pipeline([\n('selector', DataFrameSelector(cat_attribs)),\n('one_hot_encoder', OneHotEncoder(sparse=False))\n])\n</code></pre>\n<p>You can go from here and everything should work.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am receiving the error:\n<code>ValueError: Wrong number of items passed 3, placement implies 1</code>, and I am struggling to figure out where, and how I may begin addressing the problem.</p>\n<p>I don't really understand the meaning of the error; which is making it difficult for me to troubleshoot.  I have also included the block of code that is triggering the error in my Jupyter Notebook.</p>\n<p>The data is tough to attach; so I am not looking for anyone to try and re-create this error for me.  I am just looking for some feedback on how I could address this error.</p>\n<pre><code>KeyError                                  Traceback (most recent call last)\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   1944             try:\n-&gt; 1945                 return self._engine.get_loc(key)\n   1946             except KeyError:\n\npandas\\index.pyx in pandas.index.IndexEngine.get_loc (pandas\\index.c:4154)()\n\npandas\\index.pyx in pandas.index.IndexEngine.get_loc (pandas\\index.c:4018)()\n\npandas\\hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12368)()\n\npandas\\hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12322)()\n\nKeyError: 'predictedY'\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in set(self, item, value, check)\n   3414         try:\n-&gt; 3415             loc = self.items.get_loc(item)\n   3416         except KeyError:\n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   1946             except KeyError:\n-&gt; 1947                 return self._engine.get_loc(self._maybe_cast_indexer(key))\n   1948 \n\npandas\\index.pyx in pandas.index.IndexEngine.get_loc (pandas\\index.c:4154)()\n\npandas\\index.pyx in pandas.index.IndexEngine.get_loc (pandas\\index.c:4018)()\n\npandas\\hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12368)()\n\npandas\\hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12322)()\n\nKeyError: 'predictedY'\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-95-476dc59cd7fa&gt; in &lt;module&gt;()\n     26     return gp, results\n     27 \n---&gt; 28 gp_dailyElectricity, results_dailyElectricity = predictAll(3, 0.04, trainX_dailyElectricity, trainY_dailyElectricity, testX_dailyElectricity, testY_dailyElectricity, testSet_dailyElectricity, 'Daily Electricity')\n\n&lt;ipython-input-95-476dc59cd7fa&gt; in predictAll(theta, nugget, trainX, trainY, testX, testY, testSet, title)\n      8 \n      9     results = testSet.copy()\n---&gt; 10     results['predictedY'] = predictedY\n     11     results['sigma'] = sigma\n     12 \n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __setitem__(self, key, value)\n   2355         else:\n   2356             # set column\n-&gt; 2357             self._set_item(key, value)\n   2358 \n   2359     def _setitem_slice(self, key, value):\n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in _set_item(self, key, value)\n   2422         self._ensure_valid_index(value)\n   2423         value = self._sanitize_column(key, value)\n-&gt; 2424         NDFrame._set_item(self, key, value)\n   2425 \n   2426         # check if we are modifying a copy\n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in _set_item(self, key, value)\n   1462 \n   1463     def _set_item(self, key, value):\n-&gt; 1464         self._data.set(key, value)\n   1465         self._clear_item_cache()\n   1466 \n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in set(self, item, value, check)\n   3416         except KeyError:\n   3417             # This item wasn't present, just insert at end\n-&gt; 3418             self.insert(len(self.items), item, value)\n   3419             return\n   3420 \n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in insert(self, loc, item, value, allow_duplicates)\n   3517 \n   3518         block = make_block(values=value, ndim=self.ndim,\n-&gt; 3519                            placement=slice(loc, loc + 1))\n   3520 \n   3521         for blkno, count in _fast_count_smallints(self._blknos[loc:]):\n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in make_block(values, placement, klass, ndim, dtype, fastpath)\n   2516                      placement=placement, dtype=dtype)\n   2517 \n-&gt; 2518     return klass(values, ndim=ndim, fastpath=fastpath, placement=placement)\n   2519 \n   2520 # TODO: flexible with index=None and/or items=None\n\nC:\\Users\\brennn1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in __init__(self, values, placement, ndim, fastpath)\n     88             raise ValueError('Wrong number of items passed %d, placement '\n     89                              'implies %d' % (len(self.values),\n---&gt; 90                                              len(self.mgr_locs)))\n     91 \n     92     @property\n\nValueError: Wrong number of items passed 3, placement implies 1\n</code></pre>\n<p>My code is as follows:</p>\n<pre><code>def predictAll(theta, nugget, trainX, trainY, testX, testY, testSet, title):\n\n    gp = gaussian_process.GaussianProcess(theta0=theta, nugget =nugget)\n    gp.fit(trainX, trainY)\n\n    predictedY, MSE = gp.predict(testX, eval_MSE = True)\n    sigma = np.sqrt(MSE)\n\n    results = testSet.copy()\n    results['predictedY'] = predictedY\n    results['sigma'] = sigma\n\n    print (\"Train score R2:\", gp.score(trainX, trainY))\n    print (\"Test score R2:\", sklearn.metrics.r2_score(testY, predictedY))\n\n    plt.figure(figsize = (9,8))\n    plt.scatter(testY, predictedY)\n    plt.plot([min(testY), max(testY)], [min(testY), max(testY)], 'r')\n    plt.xlim([min(testY), max(testY)])\n    plt.ylim([min(testY), max(testY)])\n    plt.title('Predicted vs. observed: ' + title)\n    plt.xlabel('Observed')\n    plt.ylabel('Predicted')\n    plt.show()\n\n    return gp, results\n\ngp_dailyElectricity, results_dailyElectricity = predictAll(3, 0.04, trainX_dailyElectricity, trainY_dailyElectricity, testX_dailyElectricity, testY_dailyElectricity, testSet_dailyElectricity, 'Daily Electricity')\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In general, the error <code>ValueError: Wrong number of items passed 3, placement implies 1</code> suggests that you are attempting to put too many pigeons in too few pigeonholes. In this case, the value on the right of the equation</p>\n<p><code>results['predictedY'] = predictedY</code> </p>\n<p>is trying to put 3 \"things\" into a container that allows only one. Because the left side is a dataframe column, and can accept multiple items on that (column) dimension, you should see that there are too many items on another dimension. </p>\n<p>Here, it appears you are using sklearn for modeling, which is where <code>gaussian_process.GaussianProcess()</code> is coming from (I'm guessing, but correct me and revise the question if this is wrong). </p>\n<p>Now, you generate predicted values for <strong>y</strong> here: </p>\n<p><code>predictedY, MSE = gp.predict(testX, eval_MSE = True)</code></p>\n<p>However, as we can see from <a href=\"http://scikit-learn.org/0.17/modules/generated/sklearn.gaussian_process.GaussianProcess.html#sklearn.gaussian_process.GaussianProcess.predict\" rel=\"noreferrer\">the documentation for GaussianProcess</a>, <code>predict()</code> returns two items. The first is <strong>y</strong>, which is <em>array-like</em> (emphasis mine). That means that it can have more than one dimension, or, to be concrete for thick headed people like me, it can have more than one column -- see that it can return <code>(n_samples, n_targets)</code> which, depending on <code>testX</code>, could be <code>(1000, 3)</code> (just to pick numbers). Thus, your <code>predictedY</code> might have 3 columns. </p>\n<p>If so, when you try to put something with three \"columns\" into a single dataframe column, you are passing 3 items where only 1 would fit. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Not sure if this is relevant to your question but it might be relevant to someone else in the future: I had a similar error. Turned out that the df was empty (had zero rows) and that is what was causing the error in my command.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Another cause of this error is when you apply a function on a DataFrame where there are two columns with the same name.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As from the title I am wondering what is the difference between</p>\n<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\" rel=\"nofollow noreferrer\">StratifiedKFold</a> with the parameter <code>shuffle=True</code></p>\n<pre><code>StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n</code></pre>\n<p>and</p>\n<p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\" rel=\"nofollow noreferrer\">StratifiedShuffleSplit</a></p>\n<pre><code>StratifiedShuffleSplit(n_splits=10, test_size=’default’, train_size=None, random_state=0)\n</code></pre>\n<p>and what is the advantage of using StratifiedShuffleSplit</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In <code>stratKFolds</code>, each test set should not overlap, even when shuffle is included. With <code>stratKFolds</code> and <code>shuffle=True</code>, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest.</p>\n<p>In <code>ShuffleSplit</code>, the data is shuffled every time, and then split. This means the test sets may overlap between the splits.</p>\n<p>See this block for an example of the difference. Note the overlap of the elements in the test sets for <code>ShuffleSplit</code>.</p>\n<pre><code>splits = 5\n\ntx = range(10)\nty = [0] * 5 + [1] * 5\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn import datasets\n\nstratKfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\nshufflesplit = StratifiedShuffleSplit(n_splits=splits, random_state=42, test_size=2)\n\nprint(\"stratKFold\")\nfor train_index, test_index in stratKfold.split(tx, ty):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"Shuffle Split\")\nfor train_index, test_index in shufflesplit.split(tx, ty):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n</code></pre>\n<p>Output:</p>\n<pre><code>stratKFold\nTRAIN: [0 2 3 4 5 6 7 9] TEST: [1 8]\nTRAIN: [0 1 2 3 5 7 8 9] TEST: [4 6]\nTRAIN: [0 1 3 4 5 6 8 9] TEST: [2 7]\nTRAIN: [1 2 3 4 6 7 8 9] TEST: [0 5]\nTRAIN: [0 1 2 4 5 6 7 8] TEST: [3 9]\nShuffle Split\nTRAIN: [8 4 1 0 6 5 7 2] TEST: [3 9]\nTRAIN: [7 0 3 9 4 5 1 6] TEST: [8 2]\nTRAIN: [1 2 5 6 4 8 9 0] TEST: [3 7]\nTRAIN: [4 6 7 8 3 5 1 2] TEST: [9 0]\nTRAIN: [7 2 6 5 4 3 0 9] TEST: [1 8]\n</code></pre>\n<p>As for when to use them, I tend to use <code>stratKFolds</code> for any cross validation, and I use <code>ShuffleSplit</code> with a split of 2 for my train/test set splits. But I'm sure there are other use cases for both.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>@Ken Syme already has a very good answer. I just want to add something.</p>\n<ul>\n<li><strong><code>StratifiedKFold</code></strong> is a variation of <code>KFold</code>. First, <code>StratifiedKFold</code> shuffles your data, after that splits the data into <code>n_splits</code> parts and Done. \nNow, it will use each part as a test set. Note that <strong>it only and always shuffles data one time</strong> before splitting.</li>\n</ul>\n<p>With  <code>shuffle = True</code>, the data is shuffled by your <code>random_state</code>. Otherwise, \nthe data is shuffled by <code>np.random</code> (as default).\nFor example, with <code>n_splits = 4</code>, and your data has 3 classes (label) for <code>y</code> (dependent variable). 4 test sets cover all the data without any overlap.</p>\n<p><a href=\"https://i.sstatic.net/XJZve.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/XJZve.png\"/></a></p>\n<ul>\n<li>On the other hand, <strong><code>StratifiedShuffleSplit</code></strong> is a variation of <code>ShuffleSplit</code>.\nFirst, <code>StratifiedShuffleSplit</code> shuffles your data, and then it also splits the data into <code>n_splits</code> parts. However, it's not done yet. After this step, <code>StratifiedShuffleSplit</code> picks one part to use as a test set.\nThen it repeats the same process <code>n_splits - 1</code> other times, to get <code>n_splits - 1</code> other test sets. Look at the picture below, with the same data, but this time, the 4 test sets do not cover all the data, i.e there are overlaps among test sets.</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/AGv9B.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/AGv9B.png\"/></a></p>\n<p>So, the difference here is that <code>StratifiedKFold</code> <strong>just shuffles and splits once, therefore the test sets do not overlap</strong>, while <code>StratifiedShuffleSplit</code> <strong>shuffles each time before splitting, and it splits <code>n_splits</code> times, the test sets can overlap</strong>. </p>\n<ul>\n<li><strong>Note</strong>: the two methods uses \"stratified fold\" (that why \"stratified\" appears in both names). It means each part preserves the same percentage of samples of each class (label) as the original data. You can read more at <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\" rel=\"noreferrer\">cross_validation documents</a></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Output examples of KFold, StratifiedKFold, StratifiedShuffleSplit:\n<a href=\"https://i.sstatic.net/PUBCh.jpg\" rel=\"noreferrer\"><img alt=\"Output examples of KFold, StratifiedKFold, StratifiedShuffleSplit\" src=\"https://i.sstatic.net/PUBCh.jpg\"/></a></p>\n<p>The above pictorial output is an extension of <code>@Ken Syme</code>'s code:</p>\n<pre><code>from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\nSEED = 43\nSPLIT = 3\n\nX_train = [0,1,2,3,4,5,6,7,8]\ny_train = [0,0,0,0,0,0,1,1,1]   # note 6,7,8 are labelled class '1'\n\nprint(\"KFold, shuffle=False (default)\")\nkf = KFold(n_splits=SPLIT, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"KFold, shuffle=True\")\nkf = KFold(n_splits=SPLIT, shuffle=True, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"\\nStratifiedKFold, shuffle=False (default)\")\nskf = StratifiedKFold(n_splits=SPLIT, random_state=SEED)\nfor train_index, test_index in skf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \nprint(\"StratifiedKFold, shuffle=True\")\nskf = StratifiedKFold(n_splits=SPLIT, shuffle=True, random_state=SEED)\nfor train_index, test_index in skf.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \nprint(\"\\nStratifiedShuffleSplit\")\nsss = StratifiedShuffleSplit(n_splits=SPLIT, random_state=SEED, test_size=3)\nfor train_index, test_index in sss.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nprint(\"\\nStratifiedShuffleSplit (can customise test_size)\")\nsss = StratifiedShuffleSplit(n_splits=SPLIT, random_state=SEED, test_size=2)\nfor train_index, test_index in sss.split(X_train, y_train):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between the two? It seems that both create new columns, which their number is equal to the number of unique categories in the feature. Then they assign 0 and 1 to data points depending on what category they are in. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A simple example which encodes an array using LabelEncoder, OneHotEncoder, LabelBinarizer is shown below.</p>\n<p>I see that OneHotEncoder needs data in integer encoded form first to convert into its respective encoding which is not required in the case of LabelBinarizer.</p>\n<pre><code>from numpy import array\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\n\n# define example\ndata = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', \n'warm', 'hot']\nvalues = array(data)\nprint \"Data: \", values\n# integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(values)\nprint \"Label Encoder:\" ,integer_encoded\n\n# onehot encode\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nprint \"OneHot Encoder:\", onehot_encoded\n\n#Binary encode\nlb = LabelBinarizer()\nprint \"Label Binarizer:\", lb.fit_transform(values)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/gx6cj.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/gx6cj.png\"/></a></p>\n<p>Another good link which explains the OneHotEncoder is: <a href=\"https://stackoverflow.com/questions/42728772/explain-onehotencoder-using-python\">Explain onehotencoder using python</a></p>\n<p>There may be other valid differences between the two which experts can probably explain.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A difference is that you can use <code>OneHotEncoder</code> for multi column data, while not for <code>LabelBinarizer</code> and <code>LabelEncoder</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OneHotEncoder\n\nX = [[\"US\", \"M\"], [\"UK\", \"M\"], [\"FR\", \"F\"]]\nOneHotEncoder().fit_transform(X).toarray()\n\n# array([[0., 0., 1., 0., 1.],\n#        [0., 1., 0., 0., 1.],\n#        [1., 0., 0., 1., 0.]])\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>LabelBinarizer().fit_transform(X)\n# ValueError: Multioutput target data is not supported with label binarization\n\nLabelEncoder().fit_transform(X)\n# ValueError: bad input shape (3, 2)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Scikitlearn suggests using OneHotEncoder for X matrix i.e. the features you feed in a model, and to use a LabelBinarizer for the y labels. </p>\n<p>They are quite similar, except that OneHotEncoder could return a sparse matrix that saves a lot of memory and you won't really need that in y labels.</p>\n<p>Even if you have a multi-label multi-class problem, you can use MultiLabelBinarizer for your y labels rather than switching to OneHotEncoder for multi hot encoding.</p>\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\" rel=\"noreferrer\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have just built my first model using Keras and this is the output. It looks like the standard output you get after building any Keras artificial neural network. Even after looking in the documentation, I do not fully understand what the epoch is and what the loss is which is printed in the output.</p>\n<p><strong>What is epoch and loss in Keras?</strong> </p>\n<p>(I know it's probably an extremely basic question, but I couldn't seem to locate the answer online, and if the answer is really that hard to glean from the documentation I thought others would have the same question and thus decided to post it here.)</p>\n<pre><code>Epoch 1/20\n1213/1213 [==============================] - 0s - loss: 0.1760     \nEpoch 2/20\n1213/1213 [==============================] - 0s - loss: 0.1840     \nEpoch 3/20\n1213/1213 [==============================] - 0s - loss: 0.1816     \nEpoch 4/20\n1213/1213 [==============================] - 0s - loss: 0.1915     \nEpoch 5/20\n1213/1213 [==============================] - 0s - loss: 0.1928     \nEpoch 6/20\n1213/1213 [==============================] - 0s - loss: 0.1964     \nEpoch 7/20\n1213/1213 [==============================] - 0s - loss: 0.1948     \nEpoch 8/20\n1213/1213 [==============================] - 0s - loss: 0.1971     \nEpoch 9/20\n1213/1213 [==============================] - 0s - loss: 0.1899     \nEpoch 10/20\n1213/1213 [==============================] - 0s - loss: 0.1957     \nEpoch 11/20\n1213/1213 [==============================] - 0s - loss: 0.1923     \nEpoch 12/20\n1213/1213 [==============================] - 0s - loss: 0.1910     \nEpoch 13/20\n1213/1213 [==============================] - 0s - loss: 0.2104     \nEpoch 14/20\n1213/1213 [==============================] - 0s - loss: 0.1976     \nEpoch 15/20\n1213/1213 [==============================] - 0s - loss: 0.1979     \nEpoch 16/20\n1213/1213 [==============================] - 0s - loss: 0.2036     \nEpoch 17/20\n1213/1213 [==============================] - 0s - loss: 0.2019     \nEpoch 18/20\n1213/1213 [==============================] - 0s - loss: 0.1978     \nEpoch 19/20\n1213/1213 [==============================] - 0s - loss: 0.1954     \nEpoch 20/20\n1213/1213 [==============================] - 0s - loss: 0.1949\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just to answer the questions more specifically, here's a definition of epoch and loss:</p>\n<p><strong>Epoch</strong>: A full pass over all of your <em>training</em> data. </p>\n<p>For example, in your view above, you have 1213 observations. So an epoch concludes when it has finished a training pass over all 1213 of your observations. </p>\n<p><strong>Loss</strong>: A scalar value that we attempt to minimize during our training of the model. The lower the loss, the closer our predictions are to the true labels. </p>\n<p>This is usually Mean Squared Error (MSE) as David Maust said above, or often in Keras, <a href=\"http://keras.io/backend/#categorical_crossentropy\" rel=\"noreferrer\" title=\"Categorical Cross-Entropy\">Categorical Cross Entropy</a></p>\n<hr/>\n<p>What you'd expect to see from running fit on your Keras model, is a decrease in loss over n number of epochs. Your training run is rather abnormal, as your loss is actually increasing. This <em>could</em> be due to a learning rate that is too large, which is causing you to overshoot optima. </p>\n<p>As jaycode mentioned, you will want to look at your model's performance on unseen data, as this is the general use case of Machine Learning. </p>\n<p>As such, you should include a list of metrics in your compile method, which could look like:</p>\n<pre><code>model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n</code></pre>\n<p>As well as run your model on validation during the fit method, such as: </p>\n<pre><code>model.fit(data, labels, validation_split=0.2)\n</code></pre>\n<hr/>\n<p>There's a lot more to explain, but hopefully this gets you started.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One epoch ends when your model had run the data through all nodes in your network and ready to update the weights to reach optimal loss value. That is, smaller is better. In your case, as there are higher loss scores on higher epoch, it \"seems\" the model is better on first epoch.</p>\n<p>I said \"seems\" since we can't actually tell for sure yet as the model has not been tested using proper cross validation method i.e. it is evaluated only against its training data.</p>\n<p>Ways to improve your model:</p>\n<ul>\n<li>Use cross validation in your Keras model in order to find out how the model actually perform, does it generalize well when predicting new data it has never seen before?</li>\n<li>Adjust your learning rate, structure of neural network model, number of hidden units / layers, init, optimizer, and activator parameters used in your model among myriad other things.</li>\n</ul>\n<p>Combining sklearn's GridSearchCV with Keras can automate this process.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to to use numbers to indicate references in footnotes, so I was wondering inside of Jupyter Notebook how can I use superscripts and subscripts?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can do this inside of a <a href=\"http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html\" rel=\"noreferrer\">markdown cell</a>. A markdown cell can be created by selecting a cell then pressing the <code>esc</code> key followed by the <code>M</code> key. You can tell when you have converted a cell to markdown when <code>In [ ]:</code> seen to the right of the default code cell is gone. Then you can input the following code that uses <a href=\"http://www.latex-project.org/\" rel=\"noreferrer\">latex</a> with <a href=\"https://daringfireball.net/projects/markdown/\" rel=\"noreferrer\">markdown</a> to represent sub/super-scripts:</p>\n<pre><code>Latex subscript:\n\n$x_{2}$\n\nLatex superscript:\n\n$x^{2}$\n</code></pre>\n<p>You can find more detailed examples <a href=\"https://www.sharelatex.com/learn/Subscripts_and_superscripts\" rel=\"noreferrer\">here</a>.</p>\n<p>Please comment below if you are still having difficulty.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>&lt;sup&gt;superscript text &lt;/sup&gt;</code>  also works, and might be better because latex formatting changes the whole line etc. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What a meta question! One needs to use Markdown to answer Markdown syntax questions. Markdown does have the <code>&lt;sup&gt;&lt;/sup&gt;</code> and <code>&lt;sub&gt;&lt;/sub&gt;</code> tags that will adjust text to super- or sub- script, respectively in the typeface of the current block. If you are using the scripts for mathematical statements \n<a href=\"https://i.sstatic.net/khHoo.png\" rel=\"noreferrer\">like this</a>\nthe LaTeX transformation makes sense. If you are using the scripts for footnotes or perhaps for something like chemical formulas (e.g. H<sub>2</sub>O) it might be preferable to use the first method rather than LaTeX. Mixing fonts is generally not considered a good graphics/typography practice!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn\">pandas dataframe columns scaling with sklearn</a>\n<span class=\"question-originals-answer-count\">\n                                (9 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2022-03-03 08:42:13Z\">2 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>I want to use <code>sklearn</code>'s <code>StandardScaler</code>. Is it possible to apply it to some feature columns but not others?</p>\n<p>For instance, say my <code>data</code> is:</p>\n<pre><code>data = pd.DataFrame({'Name' : [3, 4,6], 'Age' : [18, 92,98], 'Weight' : [68, 59,49]})\n\n   Age  Name  Weight\n0   18     3      68\n1   92     4      59\n2   98     6      49\n\n\ncol_names = ['Name', 'Age', 'Weight']\nfeatures = data[col_names]\n</code></pre>\n<p>I fit and transform the <code>data</code></p>\n<pre><code>scaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\nscaled_features = pd.DataFrame(features, columns = col_names)\n\n       Name       Age    Weight\n0 -1.069045 -1.411004  1.202703\n1 -0.267261  0.623041  0.042954\n2  1.336306  0.787964 -1.245657\n</code></pre>\n<p>But of course the names are not really integers but strings and I don't want to standardize them. How can I apply the <code>fit</code> and <code>transform</code> methods only on the columns <code>Age</code> and <code>Weight</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Introduced in v0.20 is <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\" rel=\"noreferrer\">ColumnTransformer</a> which applies transformers to a specified set of columns of an array or pandas DataFrame.</p>\n<pre><code>import pandas as pd\ndata = pd.DataFrame({'Name' : [3, 4,6], 'Age' : [18, 92,98], 'Weight' : [68, 59,49]})\n\ncol_names = ['Name', 'Age', 'Weight']\nfeatures = data[col_names]\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\nct = ColumnTransformer([\n        ('somename', StandardScaler(), ['Age', 'Weight'])\n    ], remainder='passthrough')\n\nct.fit_transform(features)\n</code></pre>\n<p>NB: Like Pipeline it also has a shorthand version <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html\" rel=\"noreferrer\">make_column_transformer</a> which doesn't require naming the transformers</p>\n<h2>Output</h2>\n<pre><code>-1.41100443,  1.20270298,  3.       \n 0.62304092,  0.04295368,  4.       \n 0.78796352, -1.24565666,  6.       \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Update:</h2>\n<p>Currently the best way to handle this is to use ColumnTransformer as explained <a href=\"https://stackoverflow.com/a/54322771/2285236\">here</a>.</p>\n<hr/>\n<p>First create a copy of your dataframe:</p>\n<pre><code>scaled_features = data.copy()\n</code></pre>\n<p>Don't include the Name column in the transformation:</p>\n<pre><code>col_names = ['Age', 'Weight']\nfeatures = scaled_features[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n</code></pre>\n<p>Now, don't create a new dataframe but assign the result to those two columns:</p>\n<pre><code>scaled_features[col_names] = features\nprint(scaled_features)\n\n\n        Age  Name    Weight\n0 -1.411004     3  1.202703\n1  0.623041     4  0.042954\n2  0.787964     6 -1.245657\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Late to the party, but here's my preferred solution:</p>\n<pre><code>#load data\ndata = pd.DataFrame({'Name' : [3, 4,6], 'Age' : [18, 92,98], 'Weight' : [68, 59,49]})\n\n#list for cols to scale\ncols_to_scale = ['Age','Weight']\n\n#create and fit scaler\nscaler = StandardScaler()\nscaler.fit(data[cols_to_scale])\n\n#scale selected data\ndata[cols_to_scale] = scaler.transform(data[cols_to_scale])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't understand which accuracy in the output to use to compare my 2 Keras models to see which one is better. </p>\n<p>Do I use the \"acc\" (from the training data?) one or the \"val acc\" (from the validation data?) one?</p>\n<p>There are different accs and val accs for each epoch. How do I know the acc or val acc for my model as a whole? Do I average all of the epochs accs or val accs to find the acc or val acc of the model as a whole?</p>\n<p><strong>Model 1 Output</strong></p>\n<pre><code>Train on 970 samples, validate on 243 samples\nEpoch 1/20\n0s - loss: 0.1708 - acc: 0.7990 - val_loss: 0.2143 - val_acc: 0.7325\nEpoch 2/20\n0s - loss: 0.1633 - acc: 0.8021 - val_loss: 0.2295 - val_acc: 0.7325\nEpoch 3/20\n0s - loss: 0.1657 - acc: 0.7938 - val_loss: 0.2243 - val_acc: 0.7737\nEpoch 4/20\n0s - loss: 0.1847 - acc: 0.7969 - val_loss: 0.2253 - val_acc: 0.7490\nEpoch 5/20\n0s - loss: 0.1771 - acc: 0.8062 - val_loss: 0.2402 - val_acc: 0.7407\nEpoch 6/20\n0s - loss: 0.1789 - acc: 0.8021 - val_loss: 0.2431 - val_acc: 0.7407\nEpoch 7/20\n0s - loss: 0.1789 - acc: 0.8031 - val_loss: 0.2227 - val_acc: 0.7778\nEpoch 8/20\n0s - loss: 0.1810 - acc: 0.8010 - val_loss: 0.2438 - val_acc: 0.7449\nEpoch 9/20\n0s - loss: 0.1711 - acc: 0.8134 - val_loss: 0.2365 - val_acc: 0.7490\nEpoch 10/20\n0s - loss: 0.1852 - acc: 0.7959 - val_loss: 0.2423 - val_acc: 0.7449\nEpoch 11/20\n0s - loss: 0.1889 - acc: 0.7866 - val_loss: 0.2523 - val_acc: 0.7366\nEpoch 12/20\n0s - loss: 0.1838 - acc: 0.8021 - val_loss: 0.2563 - val_acc: 0.7407\nEpoch 13/20\n0s - loss: 0.1835 - acc: 0.8041 - val_loss: 0.2560 - val_acc: 0.7325\nEpoch 14/20\n0s - loss: 0.1868 - acc: 0.8031 - val_loss: 0.2573 - val_acc: 0.7407\nEpoch 15/20\n0s - loss: 0.1829 - acc: 0.8072 - val_loss: 0.2581 - val_acc: 0.7407\nEpoch 16/20\n0s - loss: 0.1878 - acc: 0.8062 - val_loss: 0.2589 - val_acc: 0.7407\nEpoch 17/20\n0s - loss: 0.1833 - acc: 0.8072 - val_loss: 0.2613 - val_acc: 0.7366\nEpoch 18/20\n0s - loss: 0.1837 - acc: 0.8113 - val_loss: 0.2605 - val_acc: 0.7325\nEpoch 19/20\n0s - loss: 0.1906 - acc: 0.8010 - val_loss: 0.2555 - val_acc: 0.7407\nEpoch 20/20\n0s - loss: 0.1884 - acc: 0.8062 - val_loss: 0.2542 - val_acc: 0.7449\n</code></pre>\n<p><strong>Model 2 Output</strong></p>\n<pre><code>Train on 970 samples, validate on 243 samples\nEpoch 1/20\n0s - loss: 0.1735 - acc: 0.7876 - val_loss: 0.2386 - val_acc: 0.6667\nEpoch 2/20\n0s - loss: 0.1733 - acc: 0.7825 - val_loss: 0.1894 - val_acc: 0.7449\nEpoch 3/20\n0s - loss: 0.1781 - acc: 0.7856 - val_loss: 0.2028 - val_acc: 0.7407\nEpoch 4/20\n0s - loss: 0.1717 - acc: 0.8021 - val_loss: 0.2545 - val_acc: 0.7119\nEpoch 5/20\n0s - loss: 0.1757 - acc: 0.8052 - val_loss: 0.2252 - val_acc: 0.7202\nEpoch 6/20\n0s - loss: 0.1776 - acc: 0.8093 - val_loss: 0.2449 - val_acc: 0.7490\nEpoch 7/20\n0s - loss: 0.1833 - acc: 0.7897 - val_loss: 0.2272 - val_acc: 0.7572\nEpoch 8/20\n0s - loss: 0.1827 - acc: 0.7928 - val_loss: 0.2376 - val_acc: 0.7531\nEpoch 9/20\n0s - loss: 0.1795 - acc: 0.8062 - val_loss: 0.2445 - val_acc: 0.7490\nEpoch 10/20\n0s - loss: 0.1746 - acc: 0.8103 - val_loss: 0.2491 - val_acc: 0.7449\nEpoch 11/20\n0s - loss: 0.1831 - acc: 0.8082 - val_loss: 0.2477 - val_acc: 0.7449\nEpoch 12/20\n0s - loss: 0.1831 - acc: 0.8113 - val_loss: 0.2496 - val_acc: 0.7490\nEpoch 13/20\n0s - loss: 0.1920 - acc: 0.8000 - val_loss: 0.2459 - val_acc: 0.7449\nEpoch 14/20\n0s - loss: 0.1945 - acc: 0.7928 - val_loss: 0.2446 - val_acc: 0.7490\nEpoch 15/20\n0s - loss: 0.1852 - acc: 0.7990 - val_loss: 0.2459 - val_acc: 0.7449\nEpoch 16/20\n0s - loss: 0.1800 - acc: 0.8062 - val_loss: 0.2495 - val_acc: 0.7449\nEpoch 17/20\n0s - loss: 0.1891 - acc: 0.8000 - val_loss: 0.2469 - val_acc: 0.7449\nEpoch 18/20\n0s - loss: 0.1891 - acc: 0.8041 - val_loss: 0.2467 - val_acc: 0.7531\nEpoch 19/20\n0s - loss: 0.1853 - acc: 0.8072 - val_loss: 0.2511 - val_acc: 0.7449\nEpoch 20/20\n0s - loss: 0.1905 - acc: 0.8062 - val_loss: 0.2460 - val_acc: 0.7531\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>Do I use the \"acc\" (from the training data?) one or the \"val acc\" (from the validation data?) one?</p>\n</blockquote>\n<p>If you want to estimate the ability of your model to generalize to new data (which is probably what you want to do), then you look at the validation accuracy, because the validation split contains only data that the model never sees during the training and therefor cannot just memorize.</p>\n<p>If your training data accuracy (\"acc\") keeps improving while your validation data accuracy (\"val_acc\") gets worse, you are likely in an <a href=\"https://en.wikipedia.org/wiki/Overfitting\">overfitting</a> situation, i.e. your model starts to basically just memorize the data.</p>\n<blockquote>\n<p>There are different accs and val accs for each epoch. How do I know the acc or val acc for my model as a whole? Do I average all of the epochs accs or val accs to find the acc or val acc of the model as a whole?</p>\n</blockquote>\n<p>Each epoch is a training run over all of your data. During that run the parameters of your model are adjusted according to your loss function. The result is a set of parameters which have a certain ability to generalize to new data. That ability is reflected by the validation accuracy. So think of every epoch as its own model, which can get better or worse if it is trained for another epoch. Whether it got better or worse is judged by the change in validation accuracy (better = validation accuracy increased). Therefore pick the model of the epoch with the highest validation accuracy. Don't average the accuracies over different epochs, that wouldn't make much sense. You can use the Keras callback <code>ModelCheckpoint</code> to automatically save the model with the highest validation accuracy (see <a href=\"http://keras.io/callbacks/\">callbacks documentation</a>).</p>\n<p>The highest accuracy in model 1 is <code>0.7737</code> and the highest one in model 2 is <code>0.7572</code>. Therefore you should view model 1 (at epoch 3) as better. Though it is possible that the <code>0.7737</code> was just a random outlier.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to key on decreasing val_loss or increasing val_acc, ultimately it doesn't matter much.   The differences are well within random/rounding errors.</p>\n<p>In practice, the training loss can drop significantly due to over-fitting, which is why you want to look at validation loss.</p>\n<p>In your case, you can see that your training loss is not dropping - which means you are learning nothing after each epoch.   It look like there's nothing to learn in this model, aside from some trivial linear-like fit or cutoff value.</p>\n<p>Also, when learning nothing, or a trivial linear thing, you should a similar performance on training and validation (trivial learning is always generalizable).  You should probably shuffle your data before using the validation_split feature.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As part of a machine-learning deployment project, I built a proof-of-concept where I created two simple logistic regression models for a binary classification task using R's <code>glm</code> function and python's <code>scikit-learn</code>. Afterwards, I converted those trained simple models to <code>PMML</code>s using the <code>pmml</code> function in R, and the <code>from sklearn2pmml.pipeline import PMMLPipeline</code> function in Python.</p>\n<p>Next, I opened a very simple workflow in KNIME to see if I can put those two <code>PMML</code>s into action. Basically the goal of this proof-of-concept is to test if IT can score new data using the <code>PMML</code>s that I simply hand over to them. This exercise must produce probabilities, just like the original logistic regressions would.</p>\n<p>In KNIME, I read a test data of only 4 rows using <code>CSV Reader</code> node, read the <code>PMML</code> using the <code>PMML Reader</code> node, and finally get that model to score that test data using <code>PMML Predictor</code> node. The problem is that the predictions are not final probabilities that I want, but one step before that (sum of coefficients times independent variable values, called XBETA I guess?). Please see the workflow and predictions in the picture below: </p>\n<p><a href=\"https://i.sstatic.net/UMpdd.png\" rel=\"noreferrer\"><img alt=\"KNIME output\" src=\"https://i.sstatic.net/UMpdd.png\"/></a></p>\n<p>To get to the final probabilities, one needs to run these numbers through the sigmoid function. So basically for the first record, instead of <code>2.654</code>, I need <code>1/(1+exp(-2.654)) = 0.93</code>. I am sure the <code>PMML</code> file contains the required information to enable KNIME (<strong>or any other similar platform</strong>) to perform this sigmoid operation for me, but I failed to find it. That is where I desperately need help.</p>\n<p>I looked into <a href=\"http://dmg.org/pmml/v4-2-1/Regression.html\" rel=\"noreferrer\">regression</a> and <a href=\"http://dmg.org/pmml/v4-2-1/GeneralRegression.html#xsdType_LINK-FUNCTION\" rel=\"noreferrer\">general regression</a> <code>PMML</code> documentations, and my PMMLs look just fine, but I can't figure out why I am unable to get those probabilities.</p>\n<p>Any help is highly appreciated!</p>\n<h3>Attachment 1 - Here is my test data:</h3>\n<pre><code>age credit  payfreq gmi\n25  550 4   1500\n27  650 4   3400\n35  600 2   3200\n40  680 2   4000\n</code></pre>\n<h3>Attachment 2 - Here is my R-generated PMML:</h3>\n<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;PMML version=\"4.2\" xmlns=\"http://www.dmg.org/PMML-4_2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.dmg.org/PMML-4_2 http://www.dmg.org/v4-2/pmml-4-2.xsd\"&gt;\n &lt;Header copyright=\"Copyright (c) 2018 fakici\" description=\"Generalized Linear Regression Model\"&gt;\n  &lt;Extension name=\"user\" value=\"fakici\" extender=\"Rattle/PMML\"/&gt;\n  &lt;Application name=\"Rattle/PMML\" version=\"1.4\"/&gt;\n  &lt;Timestamp&gt;2018-10-30 17:36:39&lt;/Timestamp&gt;\n &lt;/Header&gt;\n &lt;DataDictionary numberOfFields=\"5\"&gt;\n  &lt;DataField name=\"bad\" optype=\"categorical\" dataType=\"double\"/&gt;\n  &lt;DataField name=\"age\" optype=\"continuous\" dataType=\"double\"/&gt;\n  &lt;DataField name=\"credit\" optype=\"continuous\" dataType=\"double\"/&gt;\n  &lt;DataField name=\"payfreq\" optype=\"continuous\" dataType=\"double\"/&gt;\n  &lt;DataField name=\"gmi\" optype=\"continuous\" dataType=\"double\"/&gt;\n &lt;/DataDictionary&gt;\n &lt;GeneralRegressionModel modelName=\"General_Regression_Model\" modelType=\"generalLinear\" functionName=\"regression\" algorithmName=\"glm\" distribution=\"binomial\" linkFunction=\"logit\" targetReferenceCategory=\"1\"&gt;\n  &lt;MiningSchema&gt;\n   &lt;MiningField name=\"bad\" usageType=\"predicted\" invalidValueTreatment=\"returnInvalid\"/&gt;\n   &lt;MiningField name=\"age\" usageType=\"active\" invalidValueTreatment=\"returnInvalid\"/&gt;\n   &lt;MiningField name=\"credit\" usageType=\"active\" invalidValueTreatment=\"returnInvalid\"/&gt;\n   &lt;MiningField name=\"payfreq\" usageType=\"active\" invalidValueTreatment=\"returnInvalid\"/&gt;\n   &lt;MiningField name=\"gmi\" usageType=\"active\" invalidValueTreatment=\"returnInvalid\"/&gt;\n  &lt;/MiningSchema&gt;\n  &lt;Output&gt;\n   &lt;OutputField name=\"Predicted_bad\" feature=\"predictedValue\"/&gt;\n  &lt;/Output&gt;\n  &lt;ParameterList&gt;\n   &lt;Parameter name=\"p0\" label=\"(Intercept)\"/&gt;\n   &lt;Parameter name=\"p1\" label=\"age\"/&gt;\n   &lt;Parameter name=\"p2\" label=\"credit\"/&gt;\n   &lt;Parameter name=\"p3\" label=\"payfreq\"/&gt;\n   &lt;Parameter name=\"p4\" label=\"gmi\"/&gt;\n  &lt;/ParameterList&gt;\n  &lt;FactorList/&gt;\n  &lt;CovariateList&gt;\n   &lt;Predictor name=\"age\"/&gt;\n   &lt;Predictor name=\"credit\"/&gt;\n   &lt;Predictor name=\"payfreq\"/&gt;\n   &lt;Predictor name=\"gmi\"/&gt;\n  &lt;/CovariateList&gt;\n  &lt;PPMatrix&gt;\n   &lt;PPCell value=\"1\" predictorName=\"age\" parameterName=\"p1\"/&gt;\n   &lt;PPCell value=\"1\" predictorName=\"credit\" parameterName=\"p2\"/&gt;\n   &lt;PPCell value=\"1\" predictorName=\"payfreq\" parameterName=\"p3\"/&gt;\n   &lt;PPCell value=\"1\" predictorName=\"gmi\" parameterName=\"p4\"/&gt;\n  &lt;/PPMatrix&gt;\n  &lt;ParamMatrix&gt;\n   &lt;PCell parameterName=\"p0\" df=\"1\" beta=\"14.4782176066955\"/&gt;\n   &lt;PCell parameterName=\"p1\" df=\"1\" beta=\"-0.16633241754673\"/&gt;\n   &lt;PCell parameterName=\"p2\" df=\"1\" beta=\"-0.0125492006930571\"/&gt;\n   &lt;PCell parameterName=\"p3\" df=\"1\" beta=\"0.422786551151072\"/&gt;\n   &lt;PCell parameterName=\"p4\" df=\"1\" beta=\"-0.0005500245399861\"/&gt;\n  &lt;/ParamMatrix&gt;\n &lt;/GeneralRegressionModel&gt;\n&lt;/PMML&gt;\n</code></pre>\n<h3>Attachment 3 - Here is my Python-generated PMML:</h3>\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;PMML xmlns=\"http://www.dmg.org/PMML-4_2\" xmlns:data=\"http://jpmml.org/jpmml-model/InlineTable\" version=\"4.2\"&gt;\n    &lt;Header&gt;\n        &lt;Application name=\"JPMML-SkLearn\" version=\"1.5.8\"/&gt;\n        &lt;Timestamp&gt;2018-10-30T22:10:32Z&lt;/Timestamp&gt;\n    &lt;/Header&gt;\n    &lt;MiningBuildTask&gt;\n        &lt;Extension&gt;PMMLPipeline(steps=[('mapper', DataFrameMapper(default=False, df_out=False,\n        features=[(['age', 'credit', 'payfreq', 'gmi'], [ContinuousDomain(high_value=None, invalid_value_replacement=None,\n         invalid_value_treatment='return_invalid', low_value=None,\n         missing_value_replacement=None, missing_value_treatment='as_is',\n         missing_values=None, outlier_treatment='as_is', with_data=True,\n         with_statistics=True), Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)])],\n        input_df=False, sparse=False)),\n       ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))])&lt;/Extension&gt;\n    &lt;/MiningBuildTask&gt;\n    &lt;DataDictionary&gt;\n        &lt;DataField name=\"bad\" optype=\"categorical\" dataType=\"double\"&gt;\n            &lt;Value value=\"0\"/&gt;\n            &lt;Value value=\"1\"/&gt;\n        &lt;/DataField&gt;\n        &lt;DataField name=\"age\" optype=\"continuous\" dataType=\"double\"&gt;\n            &lt;Interval closure=\"closedClosed\" leftMargin=\"20.0\" rightMargin=\"50.0\"/&gt;\n        &lt;/DataField&gt;\n        &lt;DataField name=\"credit\" optype=\"continuous\" dataType=\"double\"&gt;\n            &lt;Interval closure=\"closedClosed\" leftMargin=\"501.0\" rightMargin=\"699.0\"/&gt;\n        &lt;/DataField&gt;\n        &lt;DataField name=\"payfreq\" optype=\"continuous\" dataType=\"double\"&gt;\n            &lt;Interval closure=\"closedClosed\" leftMargin=\"2.0\" rightMargin=\"4.0\"/&gt;\n        &lt;/DataField&gt;\n        &lt;DataField name=\"gmi\" optype=\"continuous\" dataType=\"double\"&gt;\n            &lt;Interval closure=\"closedClosed\" leftMargin=\"1012.0\" rightMargin=\"4197.0\"/&gt;\n        &lt;/DataField&gt;\n    &lt;/DataDictionary&gt;\n    &lt;RegressionModel functionName=\"classification\" normalizationMethod=\"softmax\" algorithmName=\"glm\" targetFieldName=\"bad\"&gt;\n        &lt;MiningSchema&gt;\n            &lt;MiningField name=\"bad\" usageType=\"target\"/&gt;\n            &lt;MiningField name=\"age\" missingValueReplacement=\"35.05\" missingValueTreatment=\"asMean\"/&gt;\n            &lt;MiningField name=\"credit\" missingValueReplacement=\"622.28\" missingValueTreatment=\"asMean\"/&gt;\n            &lt;MiningField name=\"payfreq\" missingValueReplacement=\"2.74\" missingValueTreatment=\"asMean\"/&gt;\n            &lt;MiningField name=\"gmi\" missingValueReplacement=\"3119.4\" missingValueTreatment=\"asMean\"/&gt;\n        &lt;/MiningSchema&gt;\n        &lt;Output&gt;\n            &lt;OutputField name=\"probability(0)\" optype=\"categorical\" dataType=\"double\" feature=\"probability\" value=\"0\"/&gt;\n            &lt;OutputField name=\"probability(1)\" optype=\"categorical\" dataType=\"double\" feature=\"probability\" value=\"1\"/&gt;\n        &lt;/Output&gt;\n        &lt;ModelStats&gt;\n            &lt;UnivariateStats field=\"age\"&gt;\n                &lt;Counts totalFreq=\"100.0\" missingFreq=\"0.0\" invalidFreq=\"0.0\"/&gt;\n                &lt;NumericInfo minimum=\"20.0\" maximum=\"50.0\" mean=\"35.05\" standardDeviation=\"9.365228240678386\" median=\"40.5\" interQuartileRange=\"18.0\"/&gt;\n            &lt;/UnivariateStats&gt;\n            &lt;UnivariateStats field=\"credit\"&gt;\n                &lt;Counts totalFreq=\"100.0\" missingFreq=\"0.0\" invalidFreq=\"0.0\"/&gt;\n                &lt;NumericInfo minimum=\"501.0\" maximum=\"699.0\" mean=\"622.28\" standardDeviation=\"76.1444784603585\" median=\"662.0\" interQuartileRange=\"150.5\"/&gt;\n            &lt;/UnivariateStats&gt;\n            &lt;UnivariateStats field=\"payfreq\"&gt;\n                &lt;Counts totalFreq=\"100.0\" missingFreq=\"0.0\" invalidFreq=\"0.0\"/&gt;\n                &lt;NumericInfo minimum=\"2.0\" maximum=\"4.0\" mean=\"2.74\" standardDeviation=\"0.9656086163658655\" median=\"2.0\" interQuartileRange=\"2.0\"/&gt;\n            &lt;/UnivariateStats&gt;\n            &lt;UnivariateStats field=\"gmi\"&gt;\n                &lt;Counts totalFreq=\"100.0\" missingFreq=\"0.0\" invalidFreq=\"0.0\"/&gt;\n                &lt;NumericInfo minimum=\"1012.0\" maximum=\"4197.0\" mean=\"3119.4\" standardDeviation=\"1282.4386379082625\" median=\"4028.5\" interQuartileRange=\"2944.0\"/&gt;\n            &lt;/UnivariateStats&gt;\n        &lt;/ModelStats&gt;\n        &lt;RegressionTable targetCategory=\"1\" intercept=\"0.9994024132088255\"&gt;\n            &lt;NumericPredictor name=\"age\" coefficient=\"-0.1252021965856186\"/&gt;\n            &lt;NumericPredictor name=\"credit\" coefficient=\"-8.682780007730786E-4\"/&gt;\n            &lt;NumericPredictor name=\"payfreq\" coefficient=\"1.2605378393614861\"/&gt;\n            &lt;NumericPredictor name=\"gmi\" coefficient=\"1.4681704138387003E-4\"/&gt;\n        &lt;/RegressionTable&gt;\n        &lt;RegressionTable targetCategory=\"0\" intercept=\"0.0\"/&gt;\n    &lt;/RegressionModel&gt;\n&lt;/PMML&gt;\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One solution floating around is to use the Math Formula node to apply the sigmoid function on the output of the PMML Predictor. Have you tried that? </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am learning Python for data science, but my problem is that I still don't understand the difference between Spyder and Jupyter!</p>\n<p>I would like you guys to help me to understand the difference, please; I would appreciate that.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's just a basic summary of the two tools.</p>\n<p>Jupyter is a very popular application used for data analysis. It's an IPython notebook (\"interactive python\"). You can run each block of code separately. For example, I can print a graph using matplotlib. Create a new block of code and print another graph. There are also cool functions like %timeit that test the speed of your code.</p>\n<p>Spyder is an Integrated Development Environment (IDE) for Python like Atom, Visual Studio, etc. I use VS Code and I suggest you install it as well. It's easier to learn and get running. There's also tons of helpful youtube videos due to its popularity.</p>\n<p>I prefer to use Jupyter notebook to analyze data whether it be in pandas dataframes or plots. When I'm developing a program or implementing new code on data I already analyzed, I use a text editor like VS Code.</p>\n<p>There's a lot more to it, but I think that's all you need to know for now. As you gain more experience you'll learn more about the tools and find your preferences. If you want to know more, there a ton of information about them online with people who can probably explain this much better than I can.</p>\n<p>I hope your journey into data science goes well! Just be patient and remember struggling is part of learning. Good luck!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Spyder Pros:</p>\n<ul>\n<li>Code completion</li>\n<li>Code cells: You can create code cells using Spyder.</li>\n<li>Scientific libraries</li>\n<li>PDB debugger</li>\n<li>Help feature</li>\n</ul>\n<p>cons:</p>\n<ul>\n<li>Limited to python only.</li>\n<li>Bad layout not customizable</li>\n</ul>\n<p>Jupyter pros:</p>\n<ul>\n<li>Easy to learn</li>\n<li>Secure and free server - The Jupyter server can be utilized free of charge.</li>\n<li>Keyboard shortcuts makes it easy and fast</li>\n<li>Share Notebook</li>\n</ul>\n<p>cons:</p>\n<ul>\n<li>Not recommended for running long, nonconcurrent errands.</li>\n<li>No IDE integration, no linting, and no code-style adjustment.</li>\n</ul>\n<p>Read more in detail <a href=\"https://ssiddique.info/pycharm-vs-spyder-vs-jupyter.html\" rel=\"noreferrer\">https://ssiddique.info/pycharm-vs-spyder-vs-jupyter.html</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between <code>MinMaxScaler()</code> and <code>StandardScaler()</code>.</p>\n<p><code>mms = MinMaxScaler(feature_range = (0, 1))</code> (Used in a machine learning model)</p>\n<p><code>sc = StandardScaler()</code> (In another machine learning model they used standard-scaler and not min-max-scaler)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>MinMaxScaler(feature_range = (0, 1))</code> will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).</p>\n<p><code>StandardScaler()</code> will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.</p>\n<p>If there are outliers, use <code>RobustScaler()</code>. Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)</p>\n<p>Additional Note: If scaler is used before train_test_split, data leakage will happen. Do use scaler after train_test_split</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From <a href=\"http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\" rel=\"noreferrer\">ScikitLearn site</a>: </p>\n<blockquote>\n<p><code>StandardScaler</code> removes the mean and scales the data to unit variance.\n  However, the outliers have an influence when computing the empirical\n  mean and standard deviation which shrink the range of the feature\n  values as shown in the left figure below. Note in particular that\n  because the outliers on each feature have different magnitudes, the\n  spread of the transformed data on each feature is very different: most\n  of the data lie in the [-2, 4] range for the transformed median income\n  feature while the same data is squeezed in the smaller [-0.2, 0.2]\n  range for the transformed number of households.</p>\n<p>StandardScaler therefore cannot guarantee balanced feature scales in\n  the presence of outliers.</p>\n<p><code>MinMaxScaler</code> rescales the data set such that all feature values are in\n  the range [0, 1] as shown in the right panel below. However, this\n  scaling compress all inliers in the narrow range [0, 0.005] for the\n  transformed number of households.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\nScaling the data means it helps to  Normalize the data within a particular range.</p>\n<p>When MinMaxScaler is used the it is also known as Normalization and it transform all the values in range between (0 to 1)\nformula is x = [(value - min)/(Max- Min)]</p>\n<p>StandardScaler comes under Standardization and its value ranges between (-3 to +3)\nformula is z = [(x - x.mean)/Std_deviation]</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>i am trying to do hyperparemeter search with using scikit-learn's GridSearchCV on XGBoost. During gridsearch i'd like it to early stop, since it reduce search time drastically and (expecting to) have better results on my prediction/regression task. I am using XGBoost via its Scikit-Learn API.</p>\n<pre><code>    model = xgb.XGBRegressor()\n    GridSearchCV(model, paramGrid, verbose=verbose ,fit_params={'early_stopping_rounds':42}, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY)\n</code></pre>\n<p>I tried to give early stopping parameters with using fit_params, but then it throws this error which is basically because of lack of validation set which is required for early stopping:</p>\n<pre><code>/opt/anaconda/anaconda3/lib/python3.5/site-packages/xgboost/callback.py in callback(env=XGBoostCallbackEnv(model=&lt;xgboost.core.Booster o...teration=4000, rank=0, evaluation_result_list=[]))\n    187         else:\n    188             assert env.cvfolds is not None\n    189 \n    190     def callback(env):\n    191         \"\"\"internal function\"\"\"\n--&gt; 192         score = env.evaluation_result_list[-1][1]\n        score = undefined\n        env.evaluation_result_list = []\n    193         if len(state) == 0:\n    194             init(env)\n    195         best_score = state['best_score']\n    196         best_iteration = state['best_iteration']\n</code></pre>\n<p>How can i apply GridSearch on XGBoost with using early_stopping_rounds?</p>\n<p>note: model is working without gridsearch, also GridSearch works without 'fit_params={'early_stopping_rounds':42}</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When using <code>early_stopping_rounds</code> you also have to give <code>eval_metric</code> and <code>eval_set</code> as input parameter for the fit method. Early stopping is done via calculating the error on an evaluation set. The error has to decrease every <code>early_stopping_rounds</code> otherwise the generation of additional trees is stopped early.</p>\n<p>See the <a href=\"http://xgboost.readthedocs.io/en/latest/python/python_api.html\" rel=\"noreferrer\">documentation</a> of xgboosts fit method for details.</p>\n<p>Here you see a minimal fully working example:</p>\n<pre><code>import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\ncv = 2\n\ntrainX= [[1], [2], [3], [4], [5]]\ntrainY = [1, 2, 3, 4, 5]\n\n# these are the evaluation sets\ntestX = trainX \ntestY = trainY\n\nparamGrid = {\"subsample\" : [0.5, 0.8]}\n\nfit_params={\"early_stopping_rounds\":42, \n            \"eval_metric\" : \"mae\", \n            \"eval_set\" : [[testX, testY]]}\n\nmodel = xgb.XGBRegressor()\ngridsearch = GridSearchCV(model, paramGrid, verbose=1 ,\n         fit_params=fit_params,\n         cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX,trainY]))\ngridsearch.fit(trainX,trainY)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>An update to @glao's answer and a response to @Vasim's comment/question, as of sklearn 0.21.3 (note that <code>fit_params</code> has been moved out of the instantiation of <code>GridSearchCV</code> and been moved into the <code>fit()</code> method; also, the import specifically pulls in the sklearn wrapper module from xgboost):</p>\n<pre><code>import xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\ncv = 2\n\ntrainX= [[1], [2], [3], [4], [5]]\ntrainY = [1, 2, 3, 4, 5]\n\n# these are the evaluation sets\ntestX = trainX \ntestY = trainY\n\nparamGrid = {\"subsample\" : [0.5, 0.8]}\n\nfit_params={\"early_stopping_rounds\":42, \n            \"eval_metric\" : \"mae\", \n            \"eval_set\" : [[testX, testY]]}\n\nmodel = xgb.XGBRegressor()\n\ngridsearch = GridSearchCV(model, paramGrid, verbose=1,             \n         cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))\n\ngridsearch.fit(trainX, trainY, **fit_params)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's a solution that works in a Pipeline with GridSearchCV.  The challenge occurs when you have a pipeline that is required to pre-process your training data. For example, when X is a text document and you need TFTDFVectorizer to vectorize it.</p>\n<p><strong>Over-ride the XGBRegressor or XGBClssifier.fit() Function</strong></p>\n<ul>\n<li>This step uses train_test_split() to select the specified number of\nvalidation records from X for the eval_set and then passes the\nremaining records along to fit().</li>\n<li>A new parameter eval_test_size is added to .fit() to control the number of validation records. (see train_test_split <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" rel=\"noreferrer\">test_size</a> documenation)</li>\n<li>**kwargs passes along any other parameters added by the user for the XGBRegressor.fit() function.</li>\n</ul>\n<pre><code>from xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n\nclass XGBRegressor_ES(XGBRegressor):\n    \n    def fit(self, X, y, *, eval_test_size=None, **kwargs):\n        \n        if eval_test_size is not None:\n        \n            params = super(XGBRegressor, self).get_xgb_params()\n            \n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, test_size=eval_test_size, random_state=params['random_state'])\n            \n            eval_set = [(X_test, y_test)]\n            \n            # Could add (X_train, y_train) to eval_set \n            # to get .eval_results() for both train and test\n            #eval_set = [(X_train, y_train),(X_test, y_test)] \n            \n            kwargs['eval_set'] = eval_set\n            \n        return super(XGBRegressor_ES, self).fit(X_train, y_train, **kwargs) \n</code></pre>\n<p><strong>Example Usage</strong></p>\n<p>Below is a multistep pipeline that includes multiple transformations to X. The pipeline's fit() function passes the new evaluation parameter to the XGBRegressor_ES class above as xgbr__eval_test_size=200.  In this example:</p>\n<ul>\n<li>X_train contains text documents passed to the pipeline.</li>\n<li>XGBRegressor_ES.fit() uses train_test_split() to select 200 records from X_train for the validation set and early stopping. (This could also be a percentage such as xgbr__eval_test_size=0.2)</li>\n<li>The remaining records in X_train are passed along to XGBRegressor.fit() for the actual fit().</li>\n<li>Early stopping may now occur after 75 rounds of unchanged boosting for each cv fold in a gridsearch.</li>\n</ul>\n<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectPercentile, f_regression\n   \nxgbr_pipe = Pipeline(steps=[('tfidf', TfidfVectorizer()),\n                     ('vt',VarianceThreshold()),\n                     ('scaler', StandardScaler()),\n                     ('Sp', SelectPercentile()),\n                     ('xgbr',XGBRegressor_ES(n_estimators=2000,\n                                             objective='reg:squarederror',\n                                             eval_metric='mae',\n                                             learning_rate=0.0001,\n                                             random_state=7))    ])\n\nX_train = train_idxs['f_text'].values\ny_train = train_idxs['Pct_Change_20'].values\n</code></pre>\n<p><strong>Example Fitting the Pipeline:</strong></p>\n<pre><code>%time xgbr_pipe.fit(X_train, y_train, \n                    xgbr__eval_test_size=200,\n                    xgbr__eval_metric='mae', \n                    xgbr__early_stopping_rounds=75)\n\n</code></pre>\n<p><strong>Example Fitting GridSearchCV:</strong></p>\n<pre><code>learning_rate = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\nparam_grid = dict(xgbr__learning_rate=learning_rate)\n\ngrid_search = GridSearchCV(xgbr_pipe, param_grid, scoring=\"neg_mean_absolute_error\", n_jobs=-1, cv=10)\ngrid_result = grid_search.fit(X_train, y_train, \n                    xgbr__eval_test_size=200,\n                    xgbr__eval_metric='mae', \n                    xgbr__early_stopping_rounds=75)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to use langchain for my project.</p>\n<p>so I installed it using following command : <code>pip install langchain</code></p>\n<p>but While importing \"langchain\" I am facing following Error:</p>\n<pre><code>File /usr/lib/python3.8/typing.py:774, in _GenericAlias.__subclasscheck__(self, cls)\n    772 if self._special:\n    773     if not isinstance(cls, _GenericAlias):\n--&gt; 774         return issubclass(cls, self.__origin__)\n    775     if cls._special:\n    776         return issubclass(cls.__origin__, self.__origin__)\n\nTypeError: issubclass() arg 1 must be a class\n</code></pre>\n<p>Any one who can solve this error ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So I was trying it for hours and at last I found a solution hope it helps you.</p>\n<p>First, I did this:</p>\n<pre><code>pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n</code></pre>\n<p>Then:</p>\n<pre><code>pip install pydantic -U\n</code></pre>\n<p>After this will throw an error but once again I did:</p>\n<pre><code>pip install pydantic==1.10.11\n</code></pre>\n<p>Then it started working.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>typing-inspect==0.8.0\ntyping_extensions==4.5.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>After installing so many packages got a solution using below package</p>\n<pre><code>!pip install pydantic -U\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is my code that works if I use other activation layers like tanh:</p>\n<pre><code>model = Sequential()\nact = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(Activation(act))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('softplus'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(2, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n</code></pre>\n<p>In this case, it doesn't work and says \"TypeError: 'PReLU' object is not callable\" and the error is called at the model.compile line. Why is this the case? All the non-advanced activation functions works. However, neither of the advanced activation functions, including this one, works.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The correct way to use the advanced activations like PReLU is to use it with <code>add()</code> method and not wrapping it using <code>Activation</code> class. Example:</p>\n<pre><code>model = Sequential()\nact = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(act)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If using the <code>Model</code> API in Keras you can call directly the function inside the Keras <code>Layer</code>. Here's an example:</p>\n<pre><code>from keras.models import Model\nfrom keras.layers import Dense, Input\n# using prelu?\nfrom keras.layers.advanced_activations import PReLU\n\n# Model definition\n# encoder\ninp = Input(shape=(16,))\nlay = Dense(64, kernel_initializer='uniform',activation=PReLU(),\n            name='encoder')(inp)\n#decoder\nout = Dense(2,kernel_initializer='uniform',activation=PReLU(), \n            name='decoder')(lay)\n\n# build the model\nmodel = Model(inputs=inp,outputs=out,name='cae')\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For Keras functional API I think the correct way to combine Dense and PRelu (or any other advanced activation) is to use it like this:</p>\n<pre><code>focus_tns =focus_lr(enc_bidi_tns)\n\nenc_dense_lr = k.layers.Dense(units=int(hidden_size))\nenc_dense_tns = k.layers.PReLU()(enc_dense_lr(focus_tns))\n\ndropout_lr = k.layers.Dropout(0.2)\ndropout_tns = dropout_lr(enc_dense_tns)\n\nenc_dense_lr2 = k.layers.Dense(units=int(hidden_size/4))\nenc_dense_tns2 = k.layers.PReLU()(enc_dense_lr2(dropout_tns)) \n</code></pre>\n<p>of course one should parametrize layers according to the problem</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How to set the size of the figure ploted by ScikitLearn's Confusion Matrix?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\ncm = confusion_matrix(np.arange(25), np.arange(25))\ncmp = ConfusionMatrixDisplay(cm, display_labels=np.arange(25))\ncmp.plot()\n</code></pre>\n<p>The code above shows this figure, which is too tight:</p>\n<p><a href=\"https://i.sstatic.net/2tbGe.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/2tbGe.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can send a <a href=\"https://matplotlib.org/stable/api/axes_api.html\" rel=\"noreferrer\"><code>matplotlib.axes</code></a> object to the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.plot\" rel=\"noreferrer\"><code>.plot</code></a> method of <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\" rel=\"noreferrer\"><code>sklearn.metrics.ConfusionMatrixDisplay</code></a>. Set the size of the figure in <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\" rel=\"noreferrer\"><code>matplotlib.pyplot.subplots</code></a> first.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(np.arange(25), np.arange(25))\ncmp = ConfusionMatrixDisplay(cm, display_labels=np.arange(25))\nfig, ax = plt.subplots(figsize=(10,10))\ncmp.plot(ax=ax)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/yx3rY.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/yx3rY.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was looking for how to adjust the colorbar as someone  pointed out in the commentaries in the answer offered by @Raphael and now want to add how to made this.</p>\n<p>I used the properties of <code>ConfusionMatrixDisplay</code> and guided by this <a href=\"https://stackoverflow.com/a/56900830/15879103\">answer</a> modified the code to:</p>\n<pre><code>import numpy as np\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(np.arange(25), np.arange(25))\ncmp = ConfusionMatrixDisplay(cm, display_labels=np.arange(25))\nfig, ax = plt.subplots(figsize=(10,10))\n\n# Deactivate default colorbar\ncmp.plot(ax=ax, colorbar=False)\n\n# Adding custom colorbar\ncax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\nplt.colorbar(cmp.im_,  cax=cax)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/WgUfZ.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/WgUfZ.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am doing a data cleaning exercise on python and the text that I am cleaning contains Italian words which I would like to remove. I have been searching online whether I would be able to do this on Python using a tool kit like nltk. </p>\n<p>For example given some text : </p>\n<pre><code>\"Io andiamo to the beach with my amico.\"\n</code></pre>\n<p>I would like to be left with : </p>\n<pre><code>\"to the beach with my\" \n</code></pre>\n<p>Does anyone know of a way as to how this could be done?\nAny help would be much appreciated. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use the <code>words</code> corpus from NLTK:</p>\n<pre><code>import nltk\nwords = set(nltk.corpus.words.words())\n\nsent = \"Io andiamo to the beach with my amico.\"\n\" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n         if w.lower() in words or not w.isalpha())\n# 'Io to the beach with my'\n</code></pre>\n<p>Unfortunately, <em>Io</em> happens to be an English word. In general, it may be hard to decide whether a word is English or not.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In MAC OSX it still can show an exception if you try this code. So make sure you download the words corpus manually. Once you <code>import</code> your <code>nltk</code> library, make you might as in mac os it does not download the words corpus automatically. So you have to download it potentially otherwise you will face exception. </p>\n<pre><code>import nltk \nnltk.download('words')\nwords = set(nltk.corpus.words.words())\n</code></pre>\n<p>Now you can perform same execution as previous person directed. </p>\n<pre><code>sent = \"Io andiamo to the beach with my amico.\"\nsent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n</code></pre>\n<p>According to <a href=\"https://www.nltk.org/data.html\" rel=\"noreferrer\">NLTK</a> documentation it doesn't say so. But I got a <a href=\"https://github.com/nltk/nltk/issues/2357\" rel=\"noreferrer\">issue</a> over github and solved that way and it really works. If you don't put the <code>word</code>  parameter there, you OSX can logg off and happen again and again. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have dataframe <code>total_year</code>, which contains three columns (<code>year</code>, <code>action</code>, <code>comedy</code>).</p>\n<p><a href=\"https://i.sstatic.net/u7BTR.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/u7BTR.png\"/></a></p>\n<p>How can I plot two columns (<code>action</code> and <code>comedy</code>) on y-axis?\nMy code plots only one:</p>\n<pre><code>total_year[-15:].plot(x='year', y='action', figsize=(10,5), grid=True)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/svToj.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/svToj.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Several column names may be provided to the <code>y</code> argument of the pandas plotting function. Those should be specified in a <code>list</code>, as follows.</p>\n<pre><code>df.plot(x=\"year\", y=[\"action\", \"comedy\"])\n</code></pre>\n<p>Complete example:</p>\n<pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({\"year\": [1914,1915,1916,1919,1920],\n                   \"action\" : [2.6,3.4,3.25,2.8,1.75],\n                   \"comedy\" : [2.5,2.9,3.0,3.3,3.4] })\ndf.plot(x=\"year\", y=[\"action\", \"comedy\"])\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/WqRpk.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/WqRpk.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>Pandas.DataFrame.plot()</code> per default uses index for plotting <code>X</code> axis, all other <strong>numeric</strong> columns will be used as <code>Y</code> values.</p>\n<p>So setting <code>year</code> column as index will do the trick:</p>\n<pre><code>total_year.set_index('year').plot(figsize=(10,5), grid=True)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ul>\n<li>When using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\" rel=\"nofollow noreferrer\"><code>pandas.DataFrame.plot</code></a>, it's only necessary to specify a column to the <code>x</code> parameter.\n<ul>\n<li>The caveat is, the rest of the columns with <code>numeric</code> values will be used for <code>y</code>.</li>\n<li>The following code contains extra columns to demonstrate. Note, <code>'date'</code> is left as a <code>string</code>. However, if <code>'date'</code> is converted to a <code>datetime</code> <code>dtype</code>, the plot API will also plot the <code>'date'</code> column on the y-axis.</li>\n</ul>\n</li>\n<li>If the dataframe includes many columns, some of which should not be plotted, then specify the <code>y</code> parameter as shown in this <a href=\"https://stackoverflow.com/a/47791257/7758804\">answer</a>, but if the dataframe contains only columns to be plotted, then specify only the <code>x</code> parameter.</li>\n<li><strong>In cases where the index is to be used as the x-axis, it's not necessary to specify <code>x=</code>.</strong></li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\n# test data\ndata = {'year': [1914, 1915, 1916, 1919, 1920],\n        'action': [2.67, 3.43, 3.26, 2.82, 1.75],\n        'comedy': [2.53, 2.93, 3.02, 3.37, 3.45],\n        'test1': ['a', 'b', 'c', 'd', 'e'],\n        'date': ['1914-01-01', '1915-01-01', '1916-01-01', '1919-01-01', '1920-01-01']}\n\n# create the dataframe\ndf = pd.DataFrame(data)\n\n# display(df)\n   year  action  comedy test1        date\n0  1914    2.67    2.53     a  1914-01-01\n1  1915    3.43    2.93     b  1915-01-01\n2  1916    3.26    3.02     c  1916-01-01\n3  1919    2.82    3.37     d  1919-01-01\n4  1920    1.75    3.45     e  1920-01-01\n\n# plot the dataframe\ndf.plot(x='year', figsize=(10, 5), grid=True)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/5qUoH.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/5qUoH.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have written a simple function where I am using the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\" rel=\"nofollow noreferrer\">average_precision_score</a> from <code>scikit-learn</code> to compute average precision.</p>\n<p>My Code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def compute_average_precision(predictions, gold):\n    gold_predictions = np.zeros(predictions.size, dtype=np.int)\n    for idx in range(gold):\n        gold_predictions[idx] = 1\n    return average_precision_score(predictions, gold_predictions)\n</code></pre>\n<p>When the function is executed, it produces the following error.</p>\n<pre class=\"lang-none prettyprint-override\"><code>Traceback (most recent call last):\n  File \"test.py\", line 91, in &lt;module&gt;\n    total_avg_precision += compute_average_precision(np.asarray(probs), len(gold_candidates))\n  File \"test.py\", line 29, in compute_average_precision\n    return average_precision_score(predictions, gold_predictions)\n  File \"/if5/wua4nw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/ranking.py\", line 184, in average_precision_score\n    average, sample_weight=sample_weight)\n  File \"/if5/wua4nw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/base.py\", line 81, in _average_binary_score\n    raise ValueError(\"{0} format is not supported\".format(y_type))\nValueError: continuous format is not supported\n</code></pre>\n<p>If I print the two numpy arrays <code>predictions</code> and <code>gold_predictions</code>, say for one example, it looks alright. [One example is provided below.]</p>\n<pre class=\"lang-none prettyprint-override\"><code>[ 0.40865014  0.26047812  0.07588802  0.26604077  0.10586583  0.17118802\n  0.26797949  0.34618672  0.33659923  0.22075308  0.42288553  0.24908153\n  0.26506338  0.28224747  0.32942101  0.19986877  0.39831917  0.23635269\n  0.34715138  0.39831917  0.23635269  0.35822859  0.12110706]\n[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n</code></pre>\n<p>What I am doing wrong here? What is the meaning of the error?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just taking a look at the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\" rel=\"noreferrer\"><code>sklearn</code> docs</a> </p>\n<blockquote>\n<p>Parameters:   </p>\n<p>y_true : array, shape = [n_samples] or [n_samples, n_classes] True\n  binary labels in binary label indicators.</p>\n<p>y_score : array, shape = [n_samples] or [n_samples, n_classes] Target\n  scores, can either be probability estimates of the positive class,\n  confidence values, or non-thresholded measure of decisions (as\n  returned by “decision_function” on some classifiers).</p>\n</blockquote>\n<p>So your first argument has to be an array of binary labels, but you are passing some sort of float array as the first argument. So I believe you need to reverse the order of the arguments you are passing.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/22419958/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2016-03-24 16:08:16Z\">8 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/22419958/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>As Wikpedia states </p>\n<blockquote>\n<p>The overall goal of the data mining process is to extract information\n  from a data set and transform it into an understandable structure for\n  further use</p>\n</blockquote>\n<p>How is this related with Big Data? Is it correct if I say that Hadoop is doing data mining in a parallel manner? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>Big data is everything</h2>\n<p>Big data is a <em>marketing</em> term, not a technical term. Everything is big data these days. My USB stick is a \"personal cloud\" now, and my harddrive is big data. Seriously. This is a totally unspecific term that is largely defined by what the marketing departments of various very optimistic companies can sell - and the C*Os of major companies buy, in order to make magic happen. Update: and by now, the same applies to <strong>data science</strong>. It's just marketing.</p>\n<h2>Data mining is the old big data</h2>\n<p>Actually, data mining was just as overused... it could mean anything such as </p>\n<ul>\n<li>collecting data (think NSA)</li>\n<li>storing data</li>\n<li>machine learning / AI (which predates the term data mining)</li>\n<li><strong>non-ML data mining</strong> (as in \"knowledge discovery\", where the term data mining was actually coined; but where the focus is on new knowledge, not on learning of existing knowledge)</li>\n<li>business rules and analytics</li>\n<li>visualization</li>\n<li>anything involving data you want to sell for truckloads of money</li>\n</ul>\n<p>It's just that marketing needed a new term. \"Business intelligence\", \"business analytics\", ... they <strong>still keep on selling the same stuff</strong>, it's just rebranded as \"big data\" now.</p>\n<h2>Most \"big\" data mining isn't big</h2>\n<p>Since most methods - at least those that give interesting results - just don't scale, most data \"mined\" isn't actually big. It's clearly much bigger than 10 years ago, but not big as in Exabytes. A survey by KDnuggets had something like 1-10 GB being the average \"largest data set analyzed\". That is not big data by any data management means; it's only large by what can be analyzed using <em>complex</em> methods. (I'm not talking about trivial algorithms such a k-means).</p>\n<h2>Most \"big data\" isn't data mining</h2>\n<p>Now \"Big data\" is real. Google has Big data, and CERN also has big data. Most others probably don't. Data starts being big, when you need 1000 computers just to <em>store</em> it.</p>\n<p>Big data technologies such as Hadoop are also real. They aren't always used sensibly (don't bother to run hadoop clusters less than 100 nodes - as this point you probably can get much better performance from well-chosen non-clustered machines), but of course people write such software.</p>\n<p>But most of what is being done isn't data mining. It's <a href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\" rel=\"noreferrer\">Extract, Transform, Load (ETL)</a>, so it is replacing data warehousing. Instead of using a database with structure, indexes and accelerated queries, the data is just dumped into hadoop, and when you have figured out what to do, you re-read all your data and extract the information you really need, tranform it, and load it into your excel spreadsheet. Because after selection, extraction and transformation, usually it's not \"big\" anymore.</p>\n<h2>Data quality suffers with size</h2>\n<p>Many of the marketing promises of big data will not hold. Twitter produces much less insights for most companies than advertised (unless you are a teenie rockstar, that is); and the Twitter user base is <em>heavily biased</em>. Correcting for such a bias is hard, and needs highly experienced statisticians.</p>\n<p>Bias from data is one problem - if you just collect some random data from the internet or an appliction, it will usually be not representative; in particular not of potential users. Instead, you will be overfittig to the existing heavy-users if you don't manage to cancel out these effects.</p>\n<p>The other big problem is just noise. You have spam bots, but also other tools (think Twitter \"trending topics\" that cause reinforcement of \"trends\") that make the data much noiser than other sources. Cleaning this data is <em>hard</em>, and not a matter of technology but of statistical domain expertise. For example <strong>Google Flu Trends</strong> was repeatedly found to be rather inaccurate. It worked in some of the earlier years (maybe because of overfitting?) but is not anymore of good quality.</p>\n<p>Unfortunately, a lot of big data users pay too little attention to this; which is probably one of the many reasons why most big data projects seem to fail (the others being incompetent management, inflated and unrealistic expectations, and lack of company culture and skilled people).</p>\n<h2>Hadoop != data mining</h2>\n<p>Now for the second part of your question. Hadoop doesn't do data mining. Hadoop manages data storage (via HDFS, a very primitive kind of distributed database) and it schedules computation tasks, allowing you to run the computation on the same machines that store the data. It does <em>not</em> do any complex analysis.</p>\n<p>There are some tools that try to bring data mining to Hadoop. In particular, <strong>Apache Mahout can be called the official Apache attempt to do data mining on Hadoop</strong>. Except that it is mostly a machine learning tool (machine learning != data mining; data mining sometimes uses methods from machine learning). Some parts of Mahout (such as clustering) are far from advanced. The problem is that <strong>Hadoop is good for linear problems, but most data mining isn't linear</strong>. And non-linear algorithms don't just scale up to large data; you need to carefully develop linear-time approximations and live with losses in accuracy - losses that must be smaller than what you would lose by simply working on smaller data.</p>\n<p>A good example of this trade-off problem is k-means. K-means actually is a (mostly) linear problem; so it can be somewhat run on Hadoop. A single iteration is linear, and if you had a good implementation, it would scale well to big data. However, the number of iterations until convergence also grows with data set size, and thus it isn't really linear. However, as this is a statistical method to find \"means\", the results actually do not improve much with data set size. So while you can run k-means on big data, it does not make a whole lot of sense - you could just take a sample of your data, run a highly-efficient single-node version of k-means, and the results will be just as good. Because the extra data just gives you some extra digits of precision of a value that you do not need to be that precise.</p>\n<p>Since this applies to quite a lot of problems, actual data mining on Hadoop doesn't seem to kick off. Everybody tries to do it, and a lot of companies sell this stuff. But it doesn't really work much better than the non-big version. But as long as customers want to buy this, companies will sell this functionality. And as long as it gets you a grant, researchers will write papers on this. Whether it works or not. That's life.</p>\n<p>There are a few cases where these things work. Google search is an example, and Cern. But also image recognition (but not using Hadoop, clusters of GPUs seem to be the way to go there) has recently benefited from an increase in data size. But in any of these cases, you have rather clean data. Google indexes everything; Cern discards any non-interesting data, and only analyzes interesting measurements - there are no spammers feeding their spam into Cern... and in image analysis, you train on preselected relevant images, not on say webcams or random images from the internet (and if so, you treat them as random images, not as representative data).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between big data and Hadoop?</p>\n<p>A: The difference between big data and the open source software program Hadoop is a distinct and fundamental one. The former is an asset, often a complex and ambiguous one, while the latter is a program that accomplishes a set of goals and objectives for dealing with that asset.</p>\n<p>Big data is simply the large sets of data that businesses and other parties put together to serve specific goals and operations. Big data can include many different kinds of data in many different kinds of formats. For example, businesses might put a lot of work into collecting thousands of pieces of data on purchases in currency formats, on customer identifiers like name or Social Security number, or on product information in the form of model numbers, sales numbers or inventory numbers. All of this, or any other large mass of information, can be called big data. As a rule, it’s raw and unsorted until it is put through various kinds of tools and handlers.</p>\n<p>Hadoop is one of the tools designed to handle big data. Hadoop and other software products work to interpret or parse the results of big data searches through specific proprietary algorithms and methods. Hadoop is an open-source program under the Apache license that is maintained by a global community of users. It includes various main components, including a MapReduce set of functions and a Hadoop distributed file system (HDFS).</p>\n<p>The idea behind MapReduce is that Hadoop can first map a large data set, and then perform a reduction on that content for specific results. A reduce function can be thought of as a kind of filter for raw data. The HDFS system then acts to distribute data across a network or migrate it as necessary.</p>\n<p>Database administrators, developers and others can use the various features of Hadoop to deal with big data in any number of ways. For example, Hadoop can be used to pursue data strategies like clustering and targeting with non-uniform data, or data that doesn't fit neatly into a traditional table or respond well to simple queries.</p>\n<p>See the article posted at <a href=\"http://www.shareideaonline.com/cs/what-is-the-difference-between-big-data-and-hadoop/\" rel=\"nofollow\">http://www.shareideaonline.com/cs/what-is-the-difference-between-big-data-and-hadoop/</a></p>\n<p>Thanks\nAnkush</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This answer is really intended to add some specificity to the excellent answer from Anony-Mousse.</p>\n<p>There's a lot of debate over exactly what Big Data is.  Anony-Mousse called out a lot of the issues here around the overuse of terms like analytics, big data, and data mining, but there are a few things I want to provide more detail on.</p>\n<p><strong>Big Data</strong></p>\n<p>For practical purposes, the best definition I've heard of big data is data that is inconvenient or does not function in a traditional relational database. This could be data of 1PB that cannot be worked with or even just data that is 1GB but has 5,000 columns.</p>\n<p>This is a loose and flexible definition. There are always going to be setups or data management tools which can work around it, but, this is where tools like Hadoop, MongoDB, and others can be used more efficiently that prior technology.</p>\n<p>What can we do with data that is this inconvenient/large/difficult to work with? It's difficult to simply look at a spreadsheet and to find meaning here, so we often use data mining and machine learning.</p>\n<p><strong>Data Mining</strong></p>\n<p>This was called out lightly above - my goal here is to be more specific and hopefully to provide more context. Data mining generally applies to somewhat supervised analytic or statistical methods for analysis of data. These may fit into regression, classification, clustering, or collaborative filtering. There's a lot of overlap with machine learning, however, this is still generally driven by a user rather that unsupervised or automated execution, which defines machine learning fairly well.</p>\n<p><strong>Machine Learning</strong></p>\n<p>Often, machine learning and data mining are used interchangeably. Machine learning encompasses a lot of the same areas as data mining but also includes AI, computer vision, and other unsupervised tasks. The primary difference, and this is definitely a simplification, is that user input is not only unnecessary but generally unwanted. The goal is for these algorithms or systems to self-optimize and to improve, rather than an iterative cycle of development.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Applying <code>pandas.to_numeric</code> to a dataframe column which contains strings that represent numbers (and possibly other unparsable strings) results in an error message like this:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-66-07383316d7b6&gt; in &lt;module&gt;()\n      1 for column in shouldBeNumericColumns:\n----&gt; 2     trainData[column] = pandas.to_numeric(trainData[column])\n\n/usr/local/lib/python3.5/site-packages/pandas/tools/util.py in to_numeric(arg, errors)\n    113         try:\n    114             values = lib.maybe_convert_numeric(values, set(),\n--&gt; 115                                                coerce_numeric=coerce_numeric)\n    116         except:\n    117             if errors == 'raise':\n\npandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:53558)()\n\npandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:53344)()\n\nValueError: Unable to parse string\n</code></pre>\n<p>Wouldn't it be helpful to see which value failed to parse? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you can add parameter <code>errors='coerce'</code> for convert bad non numeric values to <code>NaN</code>, then check this values by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isnull.html\" rel=\"noreferrer\"><code>isnull</code></a> and use <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"noreferrer\"><code>boolean indexing</code></a>:</p>\n<pre><code>print (df[pd.to_numeric(df.col, errors='coerce').isnull()])\n</code></pre>\n<p>Sample:</p>\n<pre><code>df = pd.DataFrame({'B':['a','7','8'],\n                   'C':[7,8,9]})\n\nprint (df)\n   B  C\n0  a  7\n1  7  8\n2  8  9\n\nprint (df[pd.to_numeric(df.B, errors='coerce').isnull()])\n   B  C\n0  a  7\n</code></pre>\n<p>Or if need find all string in mixed column - numerice with string values check <code>type</code> of values if is <code>string</code>:</p>\n<pre><code>df = pd.DataFrame({'B':['a',7, 8],\n                   'C':[7,8,9]})\n\nprint (df)\n   B  C\n0  a  7\n1  7  8\n2  8  9\n\nprint (df[df.B.apply(lambda x: isinstance(x, str))])\n   B  C\n0  a  7\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have thought the very same thing, and I don't know if there's a better way, but my current workaround is to search for characters which aren't numbers or periods. This usually turns up the problem. There are cases where multiple periods can cause a problem, but I've found those are rare.</p>\n<pre><code>import pandas as pd\nimport re\n\nnon_numeric = re.compile(r'[^\\d.]+')\n\ndf = pd.DataFrame({'a': [3,2,'NA']})\ndf.loc[df['a'].str.contains(non_numeric)]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-12-02 13:02:29Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/65095614/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Is new macbook m1 suitable for Data Science?</p>\n<p>Do Data Science python libraries such as pandas, numpy, sklearn etc work on the macbook m1 (Apple Silicon) chip and how fast compared to the previous generation intel based macbooks?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This GitHub repository has lots of useful information about the Apple M1 chip and data science in Python <a href=\"https://github.com/neurolabusc/AppleSiliconForNeuroimaging\" rel=\"noreferrer\">https://github.com/neurolabusc/AppleSiliconForNeuroimaging</a>. I have included representative quotes below. This repository focuses on software for brain imaging analysis, but the takeaways are broad.</p>\n<p><em>Updated on 27 September 2021.</em></p>\n<blockquote>\n<h1>TL;DR</h1>\n<p>Unless you are a developer, I would strongly discourage scientists from purchasing an Apple Silicon computer in the short term. Productive work will require core tools to be ported. In the longer term, this architecture could have a profound impact on science. In particular if Apple develops servers that exploit the remarkable power efficiency of their CPUs (competing with AWS Graviton) and leverage the Metal language and GPUs for compute tasks (competing with NVidia's Tesla products and CUDA language).</p>\n<h1>Limitations facing Apple Silicon</h1>\n<p>The infrastructure scientists depend on is not yet available for this architecture. Here are some of the short term limitations:</p>\n<ul>\n<li>Native R can use the unstable R-devel 4.1. However, RStudio will require gdb.</li>\n<li>Julia does not yet natively support Apple Silicon.</li>\n<li>Python natively supports Apple Silicon. However, some modules have issues or are slow. See the NiBabel section below.</li>\n<li>Scientific modules of Python, R, and Julia require a Fortran compiler, which is currently only available in experimental form.</li>\n<li>While Apple's C Clang compiler generates fast native code, many scientific tools will need to wait until gcc and gFortran compilers are available.</li>\n<li>Tools like VirtualBox, VMware Fusion, Boot Camp and Parallels do not yet support Apple Silicon. Many users rely on these tools for using Windows and Linux programs on their macOS computers.</li>\n<li>Docker can support Apple Silicon. However, attempts to run Intel-based containers on Apple Silicon machines can crash as QEMU sometimes fails to run the container. These containers are popular with many neuroimaging tools.</li>\n<li>Homebrew 3.0 supports Apple Silicon. However, many homebrew components do not support Apple Silicon.\nMATLAB is used by many scientific tools, including SPM. While Matlab works in translation, it is not yet available natively (and mex files will need to be recompiled).</li>\n<li>FSL and AFNI do not yet natively support this architecture. While code may work in translation, creating some native tools must wait for compilers and libraries to be updated. This will likely require months.</li>\n<li>The current generation M1 only has four high performance cores. Most neuroimaging pipelines combine sequential tasks that only require a single core (where the M1 excels) as well as parallel tasks. Those parallel tasks could exploit a CPU with more cores (as shown in the pigz and niimath tests below). Bear in mind that this mixture of serial and parallel code faces Amdahls law, with diminishing returns for extra cores.</li>\n<li>The current generation M1 has a maximum of 16 Gb of RAM. Neuroimaging datasets often have large memory demands (especially multi-band accelerated functional, resting-state and diffusion datasets).</li>\n<li>In general, the M1 and Intel-based Macs have identical OpenGL compatibility, with the M1 providing better performance than previous integrated solutions. However, there are corner cases that may break OpenGL tools. Here I describe four limitations. First, OpenGL geometry shaders are not supported (there is no Metal equivalent). Second, the new retina displays support wide color with 16 bitsPerSample that can cause issues for code that assumes 32-bit RGBA textures (such as the text in this Apple example code). Third, textures can be handled differently. Fourth, use of the GL_INT_2_10_10_10_REV data type will cripple performance (tested on macOS 11.2). This is unfortunate, as Apple advocated for this datatype once upon a time. In this case, code msut be changed to use the less compact GL_HALF_FLOAT which is natively supported by the M1 GPU. This impacts neuroimaging scientists visualizing DTI tractography where GPU resources can be overwhelmed.</li>\n</ul>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe that contains</p>\n<pre class=\"lang-none prettyprint-override\"><code>user_id    date       browser  conversion  test  sex  age  country\n   1    2015-12-03       IE        1         0    M   32.0   US\n</code></pre>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn import tree\ndata['date'] = pd.to_datetime(data.date)\ncolumns = [c for c in data.columns.tolist() if c not in [\"test\"]]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(data[columns], data[\"test\"])\n</code></pre>\n<p>I am getting this error:</p>\n<pre class=\"lang-none prettyprint-override\"><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-560-95a8a54aa939&gt; in &lt;module&gt;()\n      4 from sklearn import tree\n      5 clf = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf = (len(data)/100) )\n----&gt; 6 clf = clf.fit(data[columns],data[\"test\"])\n\nC:\\Users\\SnehaPriya\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc in fit(self, X, y, sample_weight, check_input, X_idx_sorted)\n    152         random_state = check_random_state(self.random_state)\n    153         if check_input:\n--&gt; 154             X = check_array(X, dtype=DTYPE, accept_sparse=\"csc\")\n    155             if issparse(X):\n    156                 X.sort_indices()\n\nC:\\Users\\SnehaPriya\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\n    371                                       force_all_finite)\n    372     else:\n--&gt; 373         array = np.array(array, dtype=dtype, order=order, copy=copy)\n    374 \n    375         if ensure_2d:\n\nTypeError: float() argument must be a string or a number\n</code></pre>\n<p>How do I overcome this error?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>A solution which keeps the date(time) column:</p>\n<pre><code>data['date'] = pd.to_numeric(pd.to_datetime(data['date']))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>IIUC you need exclude column <code>date</code> also:</p>\n<pre><code>columns = [c for c in columns if c not in [\"test\", 'date']]\n</code></pre>\n<p>because error:</p>\n<blockquote>\n<p>TypeError: float() argument must be a string or a number, not 'Timestamp'</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h5>Ideas to preserve datetime as features in the model</h5>\n<p>Assuming the dates are relevant only with respect to how much time has passed since the observation, a solution to keep the datetime column as a feature in the model is to convert it into time difference between now and the datetimes.</p>\n<pre class=\"lang-py prettyprint-override\"><code>data['date'] = (pd.Timestamp('now') - pd.to_datetime(data['date'])).dt.total_seconds()\n</code></pre>\n<p>Or you can convert the datetimes into integers straight up.</p>\n<pre class=\"lang-py prettyprint-override\"><code>data['date'] = pd.to_datetime(data['date']).astype('int64')\n</code></pre>\n<hr/>\n<p>N.B. To convert strings to datetime, passing <code>format=</code> makes the conversion run much, much faster (25 times faster). See <a href=\"https://stackoverflow.com/a/75277434/19123103\">this post</a> for the benchmark and see <a href=\"https://stackoverflow.com/a/75304878/19123103\">this post</a> for ideas to pass the format if your datetime column doesn't have a uniform format.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>i have a list of points which are the inertia values of a kmeans algorithm.<br/>\nTo determine the optimum amount of clusters i need to find the point, where this curve starts to flatten.  </p>\n<p><strong>Data example</strong></p>\n<p>Here is how my list of values is created and filled:</p>\n<pre><code>sum_squared_dist = []\nK = range(1,50)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=0)\n    km = km.fit(normalized_modeling_data)\n    sum_squared_dist.append(km.inertia_)\n\nprint(sum_squared_dist)\n</code></pre>\n<p>How can i find a point, where the pitch of this curve increases (the curve is falling, so the first derivation is negative)?</p>\n<p><strong>My approach</strong></p>\n<pre><code>derivates = []\nfor i in range(len(sum_squared_dist)):\n    derivates.append(sum_squared_dist[i] - sum_squared_dist[i-1])\n</code></pre>\n<p>I want to find the optimum number of clusters any given data using the elbow method. Could someone help me how i can find the point where the list of the inertia values starts to flatten?</p>\n<p><strong>Edit</strong><br/>\nDatapoints:</p>\n<pre><code>[7342.1301373073857, 6881.7109460930769, 6531.1657905495022,  \n6356.2255554679778, 6209.8382535595829, 6094.9052166741121, \n5980.0191582610196, 5880.1869867848218, 5779.8957906367368, \n5691.1879324562778, 5617.5153566271356, 5532.2613232619951, \n5467.352265375117, 5395.4493783888756, 5345.3459908298091, \n5290.6769823693812, 5243.5271656371888, 5207.2501206569532, \n5164.9617535255456]\n</code></pre>\n<p>Graph:\n<a href=\"https://i.sstatic.net/gRryK.jpg\" rel=\"noreferrer\"><img alt=\"graph\" src=\"https://i.sstatic.net/gRryK.jpg\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I worked on <a href=\"https://github.com/arvkevi/kneed\" rel=\"noreferrer\">a Python package</a> modeled after the <a href=\"https://raghavan.usc.edu/papers/kneedle-simplex11.pdf\" rel=\"noreferrer\">Kneedle algorithm</a>. It finds <code>x=5</code> as the point where the curve starts to flatten. The documentation and the paper discuss the algorithm for choosing the knee point in more detail.</p>\n<pre><code>y = [7342.1301373073857, 6881.7109460930769, 6531.1657905495022,  \n6356.2255554679778, 6209.8382535595829, 6094.9052166741121, \n5980.0191582610196, 5880.1869867848218, 5779.8957906367368, \n5691.1879324562778, 5617.5153566271356, 5532.2613232619951, \n5467.352265375117, 5395.4493783888756, 5345.3459908298091, \n5290.6769823693812, 5243.5271656371888, 5207.2501206569532, \n5164.9617535255456]\n\nx = range(1, len(y)+1)\n\nfrom kneed import KneeLocator\nkn = KneeLocator(x, y, curve='convex', direction='decreasing')\nprint(kn.knee)\n5\n\nimport matplotlib.pyplot as plt\nplt.xlabel('number of clusters k')\nplt.ylabel('Sum of squared distances')\nplt.plot(x, y, 'bx-')\nplt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/XhP1x.png\" rel=\"noreferrer\"><img alt=\"knee point\" src=\"https://i.sstatic.net/XhP1x.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For all those who want to do this on their own, here is a little and basic implementation.\nIt is highly adapted to my use case (200 clusters as border for the calculation) and the calculation of the distance is very basic and based to point-&gt;point in a 2D space, but it can be adapted to any other amount of figures.<br/>\nI think Kevin's library is technically more up to date and better implemented. </p>\n<pre><code>import KMeansClusterer\nfrom math import sqrt, fabs\nfrom matplotlib import pyplot as plp\nimport multiprocessing as mp\nimport numpy as np\n\nclass ClusterCalculator:\n    m = 0\n    b = 0\n    sum_squared_dist = []\n    derivates = []\n    distances = []\n    line_coordinates = []\n\n    def __init__(self, calc_border, data):\n        self.calc_border = calc_border\n        self.data = data\n\n    def calculate_optimum_clusters(self, option_parser):\n        if(option_parser.multiProcessing):\n            self.calc_mp()\n        else:\n            self.calculate_squared_dist()\n\n        self.init_opt_line()\n        self.calc_distances()\n        self.calc_line_coordinates()\n        opt_clusters = self.get_optimum_clusters()\n        print(\"Evaluated\", opt_clusters, \"as optimum number of clusters\")\n        self.plot_results()\n        return opt_clusters\n\n\n    def calculate_squared_dist(self):\n        for k in range(1, self.calc_border):\n            print(\"Calculating\",k, \"of\", self.calc_border, \"\\n\", (self.calc_border - k), \"to go!\")\n            kmeans = KMeansClusterer.KMeansClusterer(k, self.data)\n            ine = kmeans.calc_custom_params(self.data, k).inertia_\n            print(\"inertia in round\", k, \": \", ine)\n            self.sum_squared_dist.append(ine)\n\n    def init_opt_line(self):\n        self. m = (self.sum_squared_dist[0] - self.sum_squared_dist[-1]) / (1 - self.calc_border)\n        self.b = (1 * self.sum_squared_dist[0] - self.calc_border*self.sum_squared_dist[0]) / (1 - self.calc_border)\n\n    def calc_y_value(self, x_calc):\n        return self.m * x_calc + self.b\n\n    def calc_line_coordinates(self):\n        for i in range(0, len(self.sum_squared_dist)):\n            self.line_coordinates.append(self.calc_y_value(i))\n\n    def calc_distances(self):\n        for i in range(0, self.calc_border):\n            y_value = self.calc_y_value(i)\n            d = sqrt(fabs(self.sum_squared_dist[i] - self.calc_y_value(i)))\n            length_list = len(self.sum_squared_dist)\n            self.distances.append(sqrt(fabs(self.sum_squared_dist[i] - self.calc_y_value(i))))\n        print(\"For border\", self.calc_border, \", calculated the following distances: \\n\", self.distances)\n\n    def get_optimum_clusters(self):\n        return self.distances.index((max(self.distances)))\n\n    def plot_results(self):\n        plp.plot(range(0, self.calc_border), self.sum_squared_dist, \"bx-\")\n        plp.plot(range(0, self.calc_border), self.line_coordinates, \"bx-\")\n        plp.xlabel(\"Number of clusters\")\n        plp.ylabel(\"Sum of squared distances\")\n        plp.show()\n\n    def calculate_squared_dist_sliced_data(self,output, proc_numb, start, end):\n        temp = []\n        for k in range(start, end + 1):\n            kmeans = KMeansClusterer.KMeansClusterer(k, self.data)\n            ine = kmeans.calc_custom_params(self.data, k).inertia_\n            print(\"Process\", proc_numb,\"had the CPU,\", \"calculated\", ine, \"in round\", k)\n            temp.append(ine)\n        output.put((proc_numb, temp))\n\n    def sort_result_queue(self, result):\n        result.sort()\n        result = [r[1] for r in result]\n        flat_list= [item for sl in result for item in sl]\n        return flat_list\n\n    def calc_mp(self):\n        output = mp.Queue()\n        processes = []\n        processes.append(mp.Process(target=self.calculate_squared_dist_sliced_data, args=(output, 1, 1, 50)))\n        processes.append(mp.Process(target=self.calculate_squared_dist_sliced_data, args=(output, 2, 51, 100)))\n        processes.append(mp.Process(target=self.calculate_squared_dist_sliced_data, args=(output, 3, 101, 150)))\n        processes.append(mp.Process(target=self.calculate_squared_dist_sliced_data, args=(output, 4, 151, 200)))\n\n        for p in processes:\n            p.start()\n\n\n        #lock code and wait for all processes to finsish\n        for p in processes:\n            p.join()\n        results = [output.get() for p in processes]\n        self.sum_squared_dist = self.sort_result_queue(results)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am building a model for binary classification problem where each of my  data points is of <strong>300 dimensions</strong> (I am using 300 features). I am using a <em>PassiveAggressiveClassifier</em> from <em>sklearn</em>. The model is performing really well. </p>\n<p><strong>I wish to plot the decision boundary of the model. How can I do so ?</strong></p>\n<p>To get a sense of the data,  I am plotting it in 2D using TSNE. I reduced the dimensions of the data in 2 steps - from 300 to 50, then from 50 to 2 (this is a common recomendation). Below is the code snippet for the same :</p>\n<pre><code>from sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\n\nX_Train_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(X_train)\nX_Train_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_Train_reduced)\n\n#some convert lists of lists to 2 dataframes (df_train_neg, df_train_pos) depending on the label - \n\n#plot the negative points and positive points\nscatter(df_train_neg.val1, df_train_neg.val2, marker='o', c='red')\nscatter(df_train_pos.val1, df_train_pos.val2, marker='x', c='green')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/HzAbZ.jpg\" rel=\"noreferrer\"><img alt=\"Data Plot\" src=\"https://i.sstatic.net/HzAbZ.jpg\"/></a></p>\n<p>I get a decent graph. </p>\n<p><strong>Is there a way that I can add a decision boundary to this plot which represents the actual decision boundary of my model in the 300 dim space ?</strong></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One way is to impose a Voronoi tesselation on your 2D plot, i.e. color it based on proximity to the 2D data points (different colors for each predicted class label). See recent paper by <a href=\"http://dare.uva.nl/document/2/164710\" rel=\"noreferrer\">Migut et al., 2015</a>.</p>\n<p>This is a lot easier than it sounds using a meshgrid and scikit's KNeighborsClassifier (this is an end to end example with the Iris dataset; replace the first few lines with your model/code):</p>\n<pre><code>import numpy as np, matplotlib.pyplot as plt\nfrom sklearn.neighbors.classification import KNeighborsClassifier\nfrom sklearn.datasets.base import load_iris\nfrom sklearn.manifold.t_sne import TSNE\nfrom sklearn.linear_model.logistic import LogisticRegression\n\n# replace the below by your data and model\niris = load_iris()\nX,y = iris.data, iris.target\nX_Train_embedded = TSNE(n_components=2).fit_transform(X)\nprint X_Train_embedded.shape\nmodel = LogisticRegression().fit(X,y)\ny_predicted = model.predict(X)\n# replace the above by your data and model\n\n# create meshgrid\nresolution = 100 # 100x100 background pixels\nX2d_xmin, X2d_xmax = np.min(X_Train_embedded[:,0]), np.max(X_Train_embedded[:,0])\nX2d_ymin, X2d_ymax = np.min(X_Train_embedded[:,1]), np.max(X_Train_embedded[:,1])\nxx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))\n\n# approximate Voronoi tesselation on resolution x resolution grid using 1-NN\nbackground_model = KNeighborsClassifier(n_neighbors=1).fit(X_Train_embedded, y_predicted) \nvoronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\nvoronoiBackground = voronoiBackground.reshape((resolution, resolution))\n\n#plot\nplt.contourf(xx, yy, voronoiBackground)\nplt.scatter(X_Train_embedded[:,0], X_Train_embedded[:,1], c=y)\nplt.show()\n</code></pre>\n<p>Note that rather than precisely plotting your decision boundary, this will just give you an estimate of roughly where the boundary should lie (especially in regions with few data points, the true boundary can deviate from this). It will draw a line between two data points belonging to different classes, but will place it in the middle (there is indeed guaranteed to be a decision boundary between those points in this case, but it does not necessarily have to be in the middle).</p>\n<p>There are also some experimental approaches to better approximate the true decision boundary, e.g. <a href=\"https://github.com/tmadl/highdimensional-decision-boundary-plot\" rel=\"noreferrer\">this one on github</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/46990511/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-04-29 21:19:13Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/46990511/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I have observed that staticians and machine learning scientist generally doesnt follow OOPS for ML/data science projects when using Python (or other languages). </p>\n<p>Mostly it should be due to lack of understanding of best software engineering practises in oops while developing ML code for production. Because they mostly come from math &amp; statistics education background than computer science.</p>\n<p>Days when ML scientist develop ad hoc protype code and another software team make it production ready are over in the industry.</p>\n<p><a href=\"https://i.sstatic.net/zfgJY.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zfgJY.jpg\"/></a></p>\n<p><strong>Questions</strong></p>\n<ol>\n<li>How do we structure code using OOP for ML project?</li>\n<li>Should every major task (from picture above) like data cleaning, feature transformation, grid search, model validation etc. be a individual class? What are the recommended code design practises for ML?</li>\n<li>Any good github links with well strcutured code for reference (may be a well written kaggle solution)</li>\n<li>Should every class like data cleaning have <code>fit()</code>, <code>transform()</code>, <code>fit_transform()</code> function for every process like <code>remove_missing()</code>, <code>outlier_removal()</code>? When this is done why is scikit-learn <code>BaseEstimator</code> be usually inherited?</li>\n<li>What should be the structure of typical config file for ML projects in production? </li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are right about one thing being special about ML: data scientists are generally clever people, so they have no problem in presenting their ideas in code. The problem is that <strong>they tend to create fire&amp;forget code</strong>, because they lack software development craftsmanship - <strong>but ideally this shouldn't be the case</strong>.</p>\n<p>When writing code it shouldn't make any difference what the code is for<sup>1</sup>. <strong>ML is just another domain like anything else, and should follow clean code principles.</strong></p>\n<p><strong>The most important aspect always should be <a href=\"https://en.wikipedia.org/wiki/SOLID\" rel=\"noreferrer\">SOLID</a></strong>. Many important aspects directly follow: maintainability, readability, flexibility, testability, reliability etc. What you can add to this mix of features is risk of change. It doesn't matter whether a piece of code is pure ML, or banking business logic, or audiological algorithm for a hearing instrument. All the same - the implementation will be read by other developers, will contain bugs to fix, will be tested (hopefully) and possibly refactored and extended.</p>\n<p>Let me try to explain this in more detail while addressing some of your questions:</p>\n<p>1,2) You shouldn't think that OOP is the goal in itself. If there is a concept that can be modeled as a class and this will make its usage easy for other developers, it will be <strong>readable</strong>, <strong>easy to extend</strong>, <strong>easy to test</strong>, <strong>easy to avoid bugs</strong> then of course - make it a class. But unless it's needed, you shouldn't follow the <a href=\"https://en.wikipedia.org/wiki/Big_Design_Up_Front\" rel=\"noreferrer\">BDUF</a> antipattern. Start with free functions and evolve into a better interface if needed.</p>\n<p>4) Such complex inheritance hierarchies are typically created to allow implementation to be extensible (see \"O\" from <a href=\"https://en.wikipedia.org/wiki/SOLID\" rel=\"noreferrer\">SOLID</a>). In this case, library users can inherit <code>BaseEstimator</code> and it's easy to see what methods can they override and how this will fit into scikit's existing structure.</p>\n<p>5) Almost the same principles as for code, but with people who will create/edit these config files in mind. What will be the easiest format for them? How to choose parameter names so it will be obvious what do they mean, even for a beginner, who is just starting to use your product?</p>\n<p>All these things should be combined with <em>guessing</em> how likely is it that this piece of code will be changed/extended in the future? If you are sure something should be written in stone, don't worry about all aspects too much (e.g. focus only on readability), and direct your efforts to more critical parts of the implementation.</p>\n<p>To sum up: <strong>think about people who will interact with what you create</strong> in the future. In case of products/config files/user interfaces it should be always \"user first\". In case of code, try to put yourself in the shoes of a future developer who will want to fix/extend/understand your code.</p>\n<hr/>\n<p><sup>1</sup> There are of course some special cases, like code that needs to be formally proven correct or extensively documented because of formal regulations and this main goal imposes some particular constructs/practices.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to drop NA values from a pandas dataframe.</p>\n<p>I have used <code>dropna()</code> (which should drop all NA rows from the dataframe). Yet, it does not work.</p>\n<p>Here is the code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nprison_data = pd.read_csv('https://andrewshinsuke.me/docs/compas-scores-two-years.csv')\n</code></pre>\n<p>That's how you get the data frame. As the following shows, the default <code>read_csv</code> method does indeed convert the NA data points to <code>np.nan</code>.</p>\n<pre><code>np.isnan(prison_data.head()['out_custody'][4])\n\nOut[2]: True\n</code></pre>\n<p>Conveniently, the <code>head()</code> of the DF already contains a NaN values (in the column <code>out_custody</code>), so printing <code>prison_data.head()</code> this, you get:</p>\n<pre><code>   id                name   first         last compas_screening_date   sex  \n\n0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male\n1   3         kevon dixon   kevon        dixon            2013-01-27  Male\n2   4            ed philo      ed        philo            2013-04-14  Male\n3   5         marcu brown   marcu        brown            2013-01-13  Male\n4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male\n\n      dob  age          age_cat              race      ...        \n0  1947-04-18   69  Greater than 45             Other      ...\n1  1982-01-22   34          25 - 45  African-American      ...\n2  1991-05-14   24     Less than 25  African-American      ...\n3  1993-01-21   23     Less than 25  African-American      ...\n4  1973-01-22   43          25 - 45             Other      ...\n\n   v_decile_score  v_score_text  v_screening_date  in_custody  out_custody  \n\n0               1           Low        2013-08-14  2014-07-07   2014-07-14\n1               1           Low        2013-01-27  2013-01-26   2013-02-05\n2               3           Low        2013-04-14  2013-06-16   2013-06-16\n3               6        Medium        2013-01-13         NaN          NaN\n4               1           Low        2013-03-26         NaN          NaN\n\npriors_count.1 start   end event two_year_recid\n0               0     0   327     0              0\n1               0     9   159     1              1\n2               4     0    63     0              1\n3               1     0  1174     0              0\n4               2     0  1102     0              0\n</code></pre>\n<p>However, running <code>prison_data.dropna()</code> does not change the dataframe in any way.</p>\n<pre><code>prison_data.dropna()\nnp.isnan(prison_data.head()['out_custody'][4])\n\n\nOut[3]: True\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>df.dropna()</code> by default returns a new dataset without <code>NaN</code> values. So, you have to assign it to the variable </p>\n<pre><code>df = df.dropna()\n</code></pre>\n<p>if you want it to modify the <code>df</code> inplace, you have to explicitly specify </p>\n<pre><code>df.dropna(inplace= True)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>it wasn't working because there was at least one <code>nan</code> per row </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question needs <a href=\"/help/minimal-reproducible-example\">debugging details</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> Edit the question to include <a href=\"/help/minimal-reproducible-example\">desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem</a>. This will help others answer the question.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2023-04-22 08:30:34Z\">last year</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">The community reviewed whether to reopen this question <span class=\"relativetime\" title=\"2023-07-27 21:25:05Z\">last year</span> and left it closed:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/76028164/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I have been experimenting trying to solve it for weeks. I am using Google <a href=\"https://en.wikipedia.org/wiki/Project_Jupyter#Industry_adoption\" rel=\"noreferrer\">Colaboratory</a> since I got a MacBook Pro with an <a href=\"https://en.wikipedia.org/wiki/Apple_M1\" rel=\"noreferrer\">Apple chip</a> that is not supported by TensorFlow. Here is the Google Colaboratory link with commenting access.  <a href=\"https://colab.research.google.com/drive/1yAr_6q-zEoT-ohdg3ma3j52TDOgd4GBE?usp=sharing\" rel=\"noreferrer\">Click here</a>.</p>\n<p>I have tried using older versions of TensorFlow, but nothing changed, I have the TensorFlow record and training pipeline files ready.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try to install <code>pip install tensorrt</code>.</p>\n<p>Perhaps you need to read this&gt; <a href=\"https://stackoverflow.com/questions/51342408/how-do-i-install-python-packages-in-googles-colab\">How do I install Python packages in Google's Colab?</a></p>\n<p>And check if a GPU is allocated with google colab.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to update previous runs done with MLFlow, ie. changing/updating a parameter value to accommodate a change in the implementation. Typical uses cases:</p>\n<ul>\n<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.</li>\n<li>\"Specialize\" a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.</li>\n<li>Correct a wrong parameter value loggued in the previous runs.</li>\n</ul>\n<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.</p>\n<p>What is the best way to do this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function</p>\n<pre><code>with mlflow.start_run(run_id=\"your_run_id\") as run:\n    mlflow.log_param(\"p1\",\"your_corrected_value\")\n    mlflow.log_metric(\"m1\",42.0) # your corrected metrics\n    mlflow.log_artifact(\"data_sample.html\") # your corrected artifact file\n</code></pre>\n<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=\"https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs\" rel=\"noreferrer\">mlflow.search_runs</a>.</p>\n<p>Source: <a href=\"https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f\" rel=\"noreferrer\">https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>MLflow API does not support updating an existing parameter value, see <a href=\"https://github.com/mlflow/mlflow/issues/3999\" rel=\"nofollow noreferrer\">this</a>.</p>\n<p>However, there are backdoors you can use to achieve the goal of rewriting an existing parameter's value. But use with caution.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Simply speaking, how to apply quantile normalization on a large Pandas dataframe (probably 2,000,000 rows) in Python?</p>\n<p>PS. I know that there is a package named rpy2 which could run R in subprocess, using quantile normalize in R. But the truth is that R cannot compute the     correct result when I use the data set as below:</p>\n<pre><code>5.690386092696389541e-05,2.051450375415418849e-05,1.963190184049079707e-05,1.258362869906251862e-04,1.503352476021528139e-04,6.881341586355676286e-06\n8.535579139044583634e-05,5.128625938538547123e-06,1.635991820040899643e-05,6.291814349531259308e-05,3.006704952043056075e-05,6.881341586355676286e-06\n5.690386092696389541e-05,2.051450375415418849e-05,1.963190184049079707e-05,1.258362869906251862e-04,1.503352476021528139e-04,6.881341586355676286e-06\n2.845193046348194770e-05,1.538587781561563968e-05,2.944785276073619561e-05,4.194542899687506431e-05,6.013409904086112150e-05,1.032201237953351358e-05\n</code></pre>\n<p>Edit:</p>\n<p>What I want:</p>\n<p>Given the data shown above, how to apply quantile normalization following steps in <a href=\"https://en.wikipedia.org/wiki/Quantile_normalization\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Quantile_normalization</a>.</p>\n<p>I found a piece of code in Python declaring that it could compute the quantile normalization:</p>\n<pre><code>import rpy2.robjects as robjects\nimport numpy as np\nfrom rpy2.robjects.packages import importr\npreprocessCore = importr('preprocessCore')\n\n\nmatrix = [ [1,2,3,4,5], [1,3,5,7,9], [2,4,6,8,10] ]\nv = robjects.FloatVector([ element for col in matrix for element in col ])\nm = robjects.r['matrix'](v, ncol = len(matrix), byrow=False)\nRnormalized_matrix = preprocessCore.normalize_quantiles(m)\nnormalized_matrix = np.array( Rnormalized_matrix)\n</code></pre>\n<p>The code works fine with the sample data used in the code, however when I test it with the data given above the result went wrong.</p>\n<p>Since ryp2 provides an interface to run R in python subprocess, I test it again in R directly and the result was still wrong. As a result I think the reason is that the method in R is wrong.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using the example dataset from <a href=\"https://en.wikipedia.org/wiki/Quantile_normalization\" rel=\"noreferrer\">Wikipedia article</a>:</p>\n<pre><code>df = pd.DataFrame({'C1': {'A': 5, 'B': 2, 'C': 3, 'D': 4},\n                   'C2': {'A': 4, 'B': 1, 'C': 4, 'D': 2},\n                   'C3': {'A': 3, 'B': 4, 'C': 6, 'D': 8}})\n\ndf\nOut: \n   C1  C2  C3\nA   5   4   3\nB   2   1   4\nC   3   4   6\nD   4   2   8\n</code></pre>\n<p>For each rank, the mean value can be calculated with the following:</p>\n<pre><code>rank_mean = df.stack().groupby(df.rank(method='first').stack().astype(int)).mean()\n\nrank_mean\nOut: \n1    2.000000\n2    3.000000\n3    4.666667\n4    5.666667\ndtype: float64\n</code></pre>\n<p>Then the resulting Series, <code>rank_mean</code>, can be used as a mapping for the ranks to get the normalized results:</p>\n<pre><code>df.rank(method='min').stack().astype(int).map(rank_mean).unstack()\nOut: \n         C1        C2        C3\nA  5.666667  4.666667  2.000000\nB  2.000000  2.000000  3.000000\nC  3.000000  4.666667  4.666667\nD  4.666667  3.000000  5.666667\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Ok I implemented the method myself of relatively high efficiency.</p>\n<p>After finishing, this logic seems kind of easy but, anyway, I decided to post it here for any one feels confused like I was when I couldn't googled the available code.</p>\n<p>The code is in github: <a href=\"https://github.com/ShawnLYU/Quantile_Normalize\" rel=\"noreferrer\">Quantile Normalize</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One thing worth noticing is that both ayhan and shawn's code use the smaller rank mean for ties, but if you use R package processcore's <code>normalize.quantiles()</code> , it would use the mean of rank means for ties. </p>\n<p>Using the above example:</p>\n<pre><code>&gt; df\n\n   C1  C2  C3\nA   5   4   3\nB   2   1   4\nC   3   4   6\nD   4   2   8\n\n&gt; normalize.quantiles(as.matrix(df))\n\n         C1        C2        C3\nA  5.666667  5.166667  2.000000\nB  2.000000  2.000000  3.000000\nC  3.000000  5.166667  4.666667\nD  4.666667  3.000000  5.666667\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The title says it all.  When you are working <code>R</code> and using <code>RStudio</code>, its really easy and simple to debug something by dropping a <code>browser()</code> call anywhere in your code and seeing what goes wrong.  Is there a way to do that with Python? I'm slowly getting very sick of print statement debugging.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It looks like you are looking for <a href=\"https://pypi.python.org/pypi/ipdb\" rel=\"noreferrer\">ipdb</a></p>\n<p>The basic usage is to set:</p>\n<pre><code>import ipdb\nipdb.set_trace()\n</code></pre>\n<p>in your code to explore; this will take you right to that part of code, so you can explore all the variables at that point.</p>\n<p>For your specific use case: \"Would it be a setting in my Console so that it Opens pdb right before something crashes\" (a comment to another answer), you can use context manager: <code>launch_ipdb_on_exception</code></p>\n<p>For example:</p>\n<pre><code>from ipdb import launch_ipdb_on_exception\n\ndef silly():\n    my_list = [1,2,3]\n    for i in xrange(4):\n        print my_list[i]\n\nif __name__ == \"__main__\":\n    with launch_ipdb_on_exception():\n        silly()\n</code></pre>\n<p>Will take you to <code>ipdb</code> session:</p>\n<pre><code>      5         for i in xrange(4):\n----&gt; 6             print my_list[i]\n      7\n\nipdb&gt; i\n3\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can use python's debugger</p>\n<pre><code>import pdb\npdb.set_trace()\n</code></pre>\n<p>this will pause the script in debug mode </p>\n<p>Example:</p>\n<pre><code>my_file=open('running_config','r')\nword_count={}\nspecial_character_count={}\nimport pdb\npdb.set_trace() &lt;== The code will pause here\nfor config_lines in my_file.readlines():\n    l=config_lines.strip()\n    lines=l.upper()\n</code></pre>\n<p>Console:</p>\n<pre><code>&gt; /home/samwilliams/workspace/parse_running_config/file_operations.py(6)&lt;module&gt;()\n-&gt; for config_lines in my_file.readlines():\n(Pdb) print special_character_count\n{}\n(Pdb) \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since python 3.7 there is now also a build-in function: <a href=\"https://docs.python.org/3.12/library/functions.html#breakpoint\" rel=\"nofollow noreferrer\">breakpoint</a>. So simply call <code>breakpoint()</code> in your code like you would call <code>browser()</code> in R.</p>\n<p>Since it is a build-in function you do not have to first perform an import statement, like the other answers, that you might forget to remove afterwards.</p>\n<hr/>\n<h3>Some additional details on breakpoint().</h3>\n<p>Under the hood, <code>breakpoint</code> calls the function <a href=\"https://docs.python.org/3.12/library/sys.html#sys.breakpointhook\" rel=\"nofollow noreferrer\">sys.breakpointhook</a> which by default would call the debugger <code>pdb.set_trace()</code> from the python debugger library <a href=\"https://docs.python.org/3.12/library/pdb.html#module-pdb\" rel=\"nofollow noreferrer\">pdb</a>.</p>\n<p>But you can configure the function to use any debugger you like, such as the <a href=\"https://pypi.org/project/ipdb/\" rel=\"nofollow noreferrer\">ipython debugger</a> <code>ipyd.set_trace()</code>, via the <a href=\"https://docs.python.org/3.12/using/cmdline.html#envvar-PYTHONBREAKPOINT\" rel=\"nofollow noreferrer\">PYTHONBREAKPOINT</a> environment variable.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using <code>Pipeline</code> and <code>ColumnTransformer</code> modules from <code>sklearn</code> library to perform feature engineering on my dataset.</p>\n<p>The dataset initially looks like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>date</th>\n<th>date_block_num</th>\n<th>shop_id</th>\n<th>item_id</th>\n<th>item_price</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>02.01.2013</td>\n<td>0</td>\n<td>59</td>\n<td>22154</td>\n<td>999.00</td>\n</tr>\n<tr>\n<td>03.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2552</td>\n<td>899.00</td>\n</tr>\n<tr>\n<td>05.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2552</td>\n<td>899.00</td>\n</tr>\n<tr>\n<td>06.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2554</td>\n<td>1709.05</td>\n</tr>\n<tr>\n<td>15.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2555</td>\n<td>1099.00</td>\n</tr>\n</tbody>\n</table>\n</div>\n<pre><code>$&gt; data.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2935849 entries, 0 to 2935848\nData columns (total 6 columns):\n #   Column          Dtype  \n---  ------          -----  \n 0   date            object \n 1   date_block_num  object  \n 2   shop_id         object  \n 3   item_id         object  \n 4   item_price      float64\ndtypes: float64(2), int64(3), object(1)\nmemory usage: 134.4+ MB\n</code></pre>\n<p>Then I have the following transformations:</p>\n<pre><code>num_column_transformer = ColumnTransformer(\n    transformers=[\n        (\"std_scaler\", StandardScaler(), make_column_selector(dtype_include=np.number)),\n    ],\n    remainder=\"passthrough\"\n)\n\nnum_pipeline = Pipeline(\n    steps=[\n        (\"percent_item_cnt_day_per_shop\", PercentOverTotalAttributeWholeAdder(\n            attribute_percent_name=\"shop_id\",\n            attribute_total_name=\"item_cnt_day\",\n            new_attribute_name=\"%_item_cnt_day_per_shop\")\n        ),\n        (\"percent_item_cnt_day_per_item\", PercentOverTotalAttributeWholeAdder(\n            attribute_percent_name=\"item_id\",\n            attribute_total_name=\"item_cnt_day\",\n            new_attribute_name=\"%_item_cnt_day_per_item\")\n        ),\n        (\"percent_sales_per_shop\", SalesPerAttributeOverTotalSalesAdder(\n            attribute_percent_name=\"shop_id\",\n            new_attribute_name=\"%_sales_per_shop\")\n        ),\n        (\"percent_sales_per_item\", SalesPerAttributeOverTotalSalesAdder(\n            attribute_percent_name=\"item_id\",\n            new_attribute_name=\"%_sales_per_item\")\n        ),\n        (\"num_column_transformer\", num_column_transformer),\n    ]\n)\n</code></pre>\n<p>The first four <code>Transformers</code> create four new different numeric variables and the last one applies <code>StandardScaler</code> over all the numerical values of the dataset.</p>\n<p>After executing it, I get the following data:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>0</th>\n<th>1</th>\n<th>2</th>\n<th>3</th>\n<th>4</th>\n<th>5</th>\n<th>6</th>\n<th>7</th>\n<th>8</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>-0.092652</td>\n<td>-0.765612</td>\n<td>-0.173122</td>\n<td>-0.756606</td>\n<td>-0.379775</td>\n<td>02.01.2013</td>\n<td>0</td>\n<td>59</td>\n<td>22154</td>\n</tr>\n<tr>\n<td>-0.092652</td>\n<td>1.557684</td>\n<td>-0.175922</td>\n<td>1.563224</td>\n<td>-0.394319</td>\n<td>03.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2552</td>\n</tr>\n<tr>\n<td>-0.856351</td>\n<td>1.557684</td>\n<td>-0.175922</td>\n<td>1.563224</td>\n<td>-0.394319</td>\n<td>05.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2552</td>\n</tr>\n<tr>\n<td>-0.092652</td>\n<td>1.557684</td>\n<td>-0.17613</td>\n<td>1.563224</td>\n<td>-0.396646</td>\n<td>06.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2554</td>\n</tr>\n<tr>\n<td>-0.092652</td>\n<td>1.557684</td>\n<td>-0.173278</td>\n<td>1.563224</td>\n<td>-0.380647</td>\n<td>15.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2555</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I'd like to have the following output:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>date</th>\n<th>date_block_num</th>\n<th>shop_id</th>\n<th>item_id</th>\n<th>item_price</th>\n<th>%_item_cnt_day_per_shop</th>\n<th>%_item_cnt_day_per_item</th>\n<th>%_sales_per_shop</th>\n<th>%_sales_per_item</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>02.01.2013</td>\n<td>0</td>\n<td>59</td>\n<td>22154</td>\n<td>-0.092652</td>\n<td>-0.765612</td>\n<td>-0.173122</td>\n<td>-0.756606</td>\n<td>-0.379775</td>\n</tr>\n<tr>\n<td>03.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2552</td>\n<td>-0.092652</td>\n<td>1.557684</td>\n<td>-0.175922</td>\n<td>1.563224</td>\n<td>-0.394319</td>\n</tr>\n<tr>\n<td>05.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2552</td>\n<td>-0.856351</td>\n<td>1.557684</td>\n<td>-0.175922</td>\n<td>1.563224</td>\n<td>-0.394319</td>\n</tr>\n<tr>\n<td>06.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2554</td>\n<td>-0.092652</td>\n<td>1.557684</td>\n<td>-0.17613</td>\n<td>1.563224</td>\n<td>-0.396646</td>\n</tr>\n<tr>\n<td>15.01.2013</td>\n<td>0</td>\n<td>25</td>\n<td>2555</td>\n<td>-0.092652</td>\n<td>1.557684</td>\n<td>-0.173278</td>\n<td>1.563224</td>\n<td>-0.380647</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>As you can see, columns <code>5</code>, <code>6</code>, <code>7</code>, and <code>8</code> from the output corresponds to the first four columns in the original dataset. For example, I don't know where the <code>item_price</code> feature lies in the outputted table.</p>\n<ol>\n<li><strong>How could I preserve the column order and names?</strong> After that, I want to do feature engineering over categorical variables and my Transformers make use of the feature column name.</li>\n<li>Am I using correctly the Scikit-Learn API?</li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There's one point to be aware of when dealing with <code>ColumnTransformer</code>, which is reported within the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\" rel=\"noreferrer\">doc</a> as follows:</p>\n<blockquote>\n<p><em><strong>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list</strong></em>.</p>\n</blockquote>\n<p>That's the reason why your <code>ColumnTransformer</code> instance messes things up. Indeed, consider this simplified example which resembles your setting:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndf = pd.DataFrame({\n               'date': ['02.01.2013', '03.01.2013', '05.01.2013', '06.01.2013', '15.01.2013'], \n               'date_block_num': ['0', '0', '0', '0', '0'], \n               'shop_id': ['59', '25', '25', '25', '25'],\n               'item_id': ['22514', '2252', '2252', '2254', '2255'], \n               'item_price': [999.00, 899.00, 899.00, 1709.05, 1099.00]})\n\nct = ColumnTransformer([\n    ('std_scaler', StandardScaler(), make_column_selector(dtype_include=np.number))], \n    remainder='passthrough')\n\npd.DataFrame(ct.fit_transform(df), columns=ct.get_feature_names_out())\n</code></pre>\n<p><a href=\"https://i.sstatic.net/upO3e.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/upO3e.png\"/></a></p>\n<p>As you might notice, the first column in the transformed dataframe turns out to be the <em>numeric</em> one, i.e. the one which undergoes the scaling (and <em>the first in the transformers list</em>).</p>\n<p>Conversely, here's an example of how you can bypass such issue by postponing the scaling on numeric variables after passing through all the string variables and thus ensuring the possibility of getting the columns in your desired order:</p>\n<pre><code>ct = ColumnTransformer([\n    ('pass', 'passthrough', make_column_selector(dtype_include=object)),\n    ('std_scaler', StandardScaler(), make_column_selector(dtype_include=np.number))\n])\n\npd.DataFrame(ct.fit_transform(df), columns=ct.get_feature_names_out())\n</code></pre>\n<p><a href=\"https://i.sstatic.net/0XaKX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0XaKX.png\"/></a></p>\n<p>To complete the picture, here is an attempt to reproduce your Pipeline (though the custom transformer is for sure slightly different from yours):</p>\n<pre><code>from sklearn.base import BaseEstimator, TransformerMixin\n\nclass PercentOverTotalAttributeWholeAdder(BaseEstimator, TransformerMixin):\n\n    def __init__(self, attribute_percent_name='shop_id', new_attribute_name='%_item_cnt_day_per_shop'):\n    self.attribute_percent_name = attribute_percent_name\n    self.new_attribute_name = new_attribute_name\n    \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        df[self.new_attribute_name] = df.groupby(by=self.attribute_percent_name)[self.attribute_percent_name].transform('count') / df.shape[0]\n        return df\n\nct_pipe = ColumnTransformer([\n    ('pass', 'passthrough', make_column_selector(dtype_include=object)),\n    ('std_scaler', StandardScaler(), make_column_selector(dtype_include=np.number))\n    ], verbose_feature_names_out=False)\n\npipe = Pipeline([\n    ('percent_item_cnt_day_per_shop', PercentOverTotalAttributeWholeAdder(\n        attribute_percent_name='shop_id',\n        new_attribute_name='%_item_cnt_day_per_shop')\n    ),\n    ('percent_item_cnt_day_per_item', PercentOverTotalAttributeWholeAdder(\n        attribute_percent_name='item_id',\n        new_attribute_name='%_item_cnt_day_per_item')\n    ),\n    ('column_trans', ct_pipe),\n])\n\npd.DataFrame(pipe.fit_transform(df), columns=pipe[-1].get_feature_names_out())\n</code></pre>\n<p><a href=\"https://i.sstatic.net/B2wON.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/B2wON.png\"/></a></p>\n<p>As a final remark, observe that the <code>verbose_feature_names_out=False</code> parameter ensures that the names of the columns of the transformed dataframe do not show prefixes which refer to the different transformers in <code>ColumnTransformer</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h3>Answer using scikit-learn 1.2.1</h3>\n<p>In <a href=\"https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_2_0.html#pandas-output-with-set-output-api\" rel=\"nofollow noreferrer\">scikit-learn 1.2</a> it's possible to set the output of the <code>ColumnTransformer</code> to a pandas dataframe, avoiding this transformation in a second step. Besides this, in the <a href=\"https://stackoverflow.com/a/70526434/11764049\">answer proposed by @amiola</a> the <code>ColumnTransformer</code> makes use of a passthrough phase to preserve the order of string-type columns with respect to numeric ones, but this only works if all the string-type columns are before the numerics. To show this I use the same example converting the <code>shop_id</code> column to numeric:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndf = pd.DataFrame({\n               'date': ['02.01.2013', '03.01.2013', '05.01.2013', '06.01.2013', '15.01.2013'], \n               'date_block_num': ['0', '0', '0', '0', '0'], \n               'shop_id': [59, 25, 25, 25, 25],\n               'item_id': ['22514', '2252', '2252', '2254', '2255'], \n               'item_price': [999.00, 899.00, 899.00, 1709.05, 1099.00]})\nct = ColumnTransformer([\n         ('pass', 'passthrough', make_column_selector(dtype_include=object)),\n         ('std_scaler', StandardScaler(), make_column_selector(dtype_include=np.number))\n                      ]).set_output(transform='pandas')\nout_df = ct.fit_transform(df)\nout_df\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: right;\"></th>\n<th style=\"text-align: right;\">pass__date</th>\n<th style=\"text-align: right;\">pass__date_block_num</th>\n<th style=\"text-align: right;\">pass__item_id</th>\n<th style=\"text-align: right;\">std_scaler__shop_id</th>\n<th>std_scaler__item_price</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">02.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">22514</td>\n<td style=\"text-align: right;\">2.0</td>\n<td>-0.402369</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">03.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">2252</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td>-0.732153</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">2</td>\n<td style=\"text-align: right;\">05.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">2252</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td>-0.732153</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">3</td>\n<td style=\"text-align: right;\">06.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">2254</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td>1.939261</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">4</td>\n<td style=\"text-align: right;\">15.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">2255</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td>-0.072585</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>As it's possible to see the <code>shop_id</code> column is moved to the end, for the same reason also explained in amiola's answer (i.e. columns are reordered following the order of the transformation in the <code>ColumnTrasnformer</code>). To overcome this issue, you can <a href=\"https://stackoverflow.com/q/13148429/11764049\">reorder dataframe columns</a> after the transformation with <code>verbose_feature_names_out</code> set to <code>False</code> to preserve the same starting column names (beware that those names must be unique, see <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer\" rel=\"nofollow noreferrer\">docs</a>). There's also no need to create a specific passthrough step.</p>\n<pre><code>ct = ColumnTransformer([\n    ('std_scaler', StandardScaler(), make_column_selector(dtype_include=np.number))],\n     remainder='passthrough',\n     verbose_feature_names_out=False).set_output(transform='pandas')\n\nout_df = ct.fit_transform(df)\nout_df = out_df[df.columns]\nout_df\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: right;\"></th>\n<th style=\"text-align: right;\">date</th>\n<th style=\"text-align: right;\">date_block_num</th>\n<th style=\"text-align: right;\">shop_id</th>\n<th style=\"text-align: right;\">item_id</th>\n<th>item_price</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">02.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">2.0</td>\n<td style=\"text-align: right;\">22514</td>\n<td>-0.402369</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">03.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td style=\"text-align: right;\">2252</td>\n<td>-0.732153</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">2</td>\n<td style=\"text-align: right;\">05.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td style=\"text-align: right;\">2252</td>\n<td>-0.732153</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">3</td>\n<td style=\"text-align: right;\">06.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td style=\"text-align: right;\">2254</td>\n<td>1.939261</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">4</td>\n<td style=\"text-align: right;\">15.01.2013</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">-0.5</td>\n<td style=\"text-align: right;\">2255</td>\n<td>-0.072585</td>\n</tr>\n</tbody>\n</table>\n</div> </div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe that looks like this:</p>\n<pre><code>from    to         datetime              other\n-------------------------------------------------\n11      1     2016-11-06 22:00:00          -\n11      1     2016-11-06 20:00:00          -\n11      1     2016-11-06 15:45:00          -\n11      12    2016-11-06 15:00:00          -\n11      1     2016-11-06 12:00:00          -\n11      18    2016-11-05 10:00:00          -\n11      12    2016-11-05 10:00:00          -\n12      1     2016-10-05 10:00:59          -\n12      3     2016-09-06 10:00:34          -\n</code></pre>\n<p>I want to groupby \"from\" and then \"to\" columns and then sort the \"datetime\" in descending order and then finally want to calculate the time difference within these grouped by objects between the current time and the next time. For eg, in this case,\nI would like to have a dataframe like the following:</p>\n<pre><code>from    to     timediff in minutes                                          others\n11      1            120\n11      1            255\n11      1            225\n11      1            0 (preferrably subtract this date from the epoch)\n11      12           300\n11      12           0\n11      18           0\n12      1            25\n12      3            0\n</code></pre>\n<p>I can't get my head around figuring this out!! Is there a way out for this?\nAny help will be much much appreciated!!\nThank you so much in advance!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>df.assign(\n    timediff=df.sort_values(\n        'datetime', ascending=False\n    ).groupby(['from', 'to']).datetime.diff(-1).dt.seconds.div(60).fillna(0))\n</code></pre>\n<p><a href=\"https://i.sstatic.net/NCgJR.png\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/NCgJR.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you need:</p>\n<p><code>groupby</code> with <code>apply</code> <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_values.html\" rel=\"noreferrer\"><code>sort_values</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.diff.html\" rel=\"noreferrer\"><code>diff</code></a>, convert <code>Timedelta</code> to minutes by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.seconds.html\" rel=\"noreferrer\"><code>seconds</code></a> and floor division <code>60</code></p>\n<p><code>fillna</code> and <code>sort_index</code>, remove level <code>2</code> in index</p>\n<pre><code>df = df.groupby(['from','to']).datetime\n       .apply(lambda x: x.sort_values().diff().dt.seconds // 60)\n       .fillna(0)\n       .sort_index()\n       .reset_index(level=2, drop=True)\n       .reset_index(name='timediff in minutes')\n\nprint (df)\n\n   from  to  timediff in minutes \n0    11   1                 120.0\n1    11   1                 255.0\n2    11   1                 225.0\n3    11   1                   0.0\n4    11  12                 300.0\n5    11  12                   0.0\n6    11  18                   0.0\n7    12   3                   0.0\n8    12   3                   0.0\n</code></pre>\n<hr/>\n<pre><code>df = df.join(df.groupby(['from','to'])\n               .datetime\n               .apply(lambda x: x.sort_values().diff().dt.seconds // 60)\n               .fillna(0)\n               .reset_index(level=[0,1], drop=True)\n               .rename('timediff in minutes'))\nprint (df)\n   from  to            datetime other  timediff in minutes\n0    11   1 2016-11-06 22:00:00     -                120.0\n1    11   1 2016-11-06 20:00:00     -                255.0\n2    11   1 2016-11-06 15:45:00     -                225.0\n3    11  12 2016-11-06 15:00:00     -                300.0\n4    11   1 2016-11-06 12:00:00     -                  0.0\n5    11  18 2016-11-05 10:00:00     -                  0.0\n6    11  12 2016-11-05 10:00:00     -                  0.0\n7    12   3 2016-10-05 10:00:59     -                  0.0\n8    12   3 2016-09-06 10:00:34     -                  0.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Almost as above, but without <code>apply</code>:</p>\n<pre><code>result = df.sort_values(['from','to','datetime'])\\\n           .groupby(['from','to'])['datetime']\\\n           .diff().dt.seconds.fillna(0)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am working on a sequential labeling problem with unbalanced classes and I would like to use <code>sample_weight</code> to resolve the unbalance issue. Basically if I train the model for about 10 epochs, I get great results. If I train for more epochs, <code>val_loss</code> keeps dropping, but I get worse results. I'm guessing the model just detects more of the dominant class to the detriment of the smaller classes. </p>\n<p>The model has two inputs, for word embeddings and character embeddings, and the input is one of 7 possible classes from 0 to 6.</p>\n<p>With the padding, the shape of my input layer for word embeddings is <code>(3000, 150)</code> and the input layer for word embeddings is <code>(3000, 150, 15)</code>. I use a 0.3 split for testing and training data, which means <code>X_train</code> for word embeddings is <code>(2000, 150)</code> and <code>(2000, 150, 15)</code> for char embeddings. <code>y</code> contains the correct class for each word, encoded in a one-hot vector of dimension 7, so its shape is <code>(3000, 150, 7)</code>. <code>y</code> is likewise split into a training and testing set. Each input is then fed into a Bidirectional LSTM. </p>\n<p>The output is a matrix with one of the 7 categories assigned for each word of the 2000 training samples, so the size is <code>(2000, 150, 7)</code>.</p>\n<hr/>\n<p>At first, I simply tried to define <code>sample_weight</code> as an <code>np.array</code> of length 7 containing the weights for each class:</p>\n<pre><code>count = [list(array).index(1) for arrays in y for array in arrays]\ncount = dict(Counter(count))\ncount[0] = 0\ntotal = sum([count[key] for key in count])\ncount = {k: count[key] / total for key in count}\ncategory_weights = np.zeros(7)\nfor f in count:\n    category_weights[f] = count[f]\n</code></pre>\n<p>But I get the following error <code>ValueError: Found a sample_weight array with shape (7,) for an input with shape (2000, 150, 7). sample_weight cannot be broadcast.</code></p>\n<p>Looking at the docs, it looks like I should instead be passing <code>a 2D array with shape (samples, sequence_length)</code>. So I create a <code>(3000, 150)</code> array with a concatenation of the weights of every word of each sequence:</p>\n<pre><code>weights = []\n\nfor sample in y:\n    current_weight = []\n    for line in sample:\n        current_weight.append(frequency[list(line).index(1)])\n    weights.append(current_weight)\n\nweights = np.array(weights)\n</code></pre>\n<p>and pass that to the fit function through the <code>sample_weight</code> parameter after having added the <code>sample_weight_mode=\"temporal\"</code> option in <code>compile()</code>.</p>\n<p>I first got an error telling me the dimension was wrong, however after generating the weights for only the training sample, I end up with a <code>(2000, 150)</code> array that I can use to fit my model. </p>\n<hr/>\n<ul>\n<li>Is this a proper way to define sample_weights or am I doing it all wrong ? I can't say I've noticed any improvements from adding the weights, so I must have missed something.</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you are confusing <code>sample_weights</code> and <code>class_weights</code>. Checking the <a href=\"https://keras.io/models/model/#fit\" rel=\"noreferrer\">docs</a> a bit we can see the differences between them:</p>\n<p><strong><code>sample_weights</code> is used to provide a weight for <em>each</em> training sample.</strong> That means that you should pass a 1D array with the same number of elements as your training samples (indicating the weight for each of those samples). In case you are using temporal data you may instead pass a 2D array, enabling you to give weight to each timestep of each sample. </p>\n<p><strong><code>class_weights</code> is used to provide a weight or bias for <em>each</em> output class</strong>. This means you should pass a weight for each class that you are trying to classify. Furthermore, <strong>this parameter expects a dictionary</strong> to be passed to it (not an array, that is why you got that error). For example consider this situation:</p>\n<pre><code>class_weight = {0 : 1. , 1: 50.}\n</code></pre>\n<p>In this case (a binary classification problem) you are giving 50 times as much weight (or \"relevance\") to your samples of class <code>1</code> compared to class <code>0</code>. This way you can compensate for imbalanced datasets. Here is another useful <a href=\"https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\">post</a> explaining more about this and other options to consider when dealing with imbalanced datasets.</p>\n<blockquote>\n<p>If I train for more epochs, val_loss keeps dropping, but I get worse results.</p>\n</blockquote>\n<p>Probably you are over-fitting, and something that may be contributing to that is the imbalanced classes your dataset has, as you correctly suspected. Compensating the class weights should help mitigate this, however there may still be other factors that can cause over-fitting that escape the scope of this question/answer (so make sure to watch out for those after solving this question).</p>\n<hr/>\n<p>Judging by your post, seems to me that what you need is to use <code>class_weight</code> to balance your dataset for training, for which you will need to pass a <em>dictionary</em> indicating the weight ratios between your 7 classes. Consider using <code>sample_weight</code> only if you want to give <em>each</em> sample a custom weight for consideration. </p>\n<p>If you want a more detailed comparison between those two consider checking <a href=\"https://stackoverflow.com/a/48174220/3908170\">this answer</a> I posted on a related question. <strong>Spoiler: <code>sample_weight</code> overrides <code>class_weight</code>, so you have to use one or the other, but not both,</strong> so be careful with not mixing them. </p>\n<hr/>\n<p><strong>Update:</strong> As of the moment of this edit (March 27, 2020), looking at the <a href=\"https://github.com/keras-team/keras/blob/master/keras/engine/training_utils.py#L423\" rel=\"noreferrer\">source code</a> of <code>training_utils.standardize_weights()</code> we can see that it now supports <em>both</em> <code>class_weights</code> and <code>sample_weights</code>:</p>\n<blockquote>\n<p>Everything gets normalized to a single sample-wise (or timestep-wise)\n      weight array. <strong>If both <code>sample_weights</code> and <code>class_weights</code> are provided,\n      the weights are multiplied together.</strong></p>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to install the CUDA toolkit in order to be able to use Thundersvm in my personal computer.\nHowever I keep getting the following message in the GUI installer:\n\"You already have a newer version of the NVIDIA Frameview SDK installed\"</p>\n<p>I read in the CUDA forums that this most probably results from having installed Geforce Experience (which I have installed). So I tried removing it from the Programs and Features windows panel. However I still got the error, so my guess is that the \"Nvidia Corporation\" folder was not removed.</p>\n<p>In the same question, they also suggested performing a custom install. However I could not find any information on how to do a custom install of the CUDA toolkit. I would really appreciate if someone could explain how to do this custom install or safely remove the previous drivers. I thought of using DDU but I read that sometimes it may actually lead to trouble.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had the same problem while I was trying to get TensorFlow to use my NVIDIA GTX1070 GPU for calculations. Here's what allowed me to perform the CUDA Toolkit installation on my Windows 10 machine.</p>\n<p>As the error message in the installer says - you already have a newer Frameview SDK installed. It was the case for me.</p>\n<ol>\n<li>Go to <em>Settings/Uninstall or modify programs</em>.</li>\n<li>Remove the <em>NVIDIA Frameview</em> program. It should be there with GeForce Experience, PhysX, etc.</li>\n</ol>\n<p>Uninstalling only this NVIDIA program didn't cause any driver problems for my machine and I was able to progress through the CUDA Toolkit installation.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently performing multi class SVM with linear kernel using python's scikit library. \nThe sample training data and testing data are as given below:</p>\n<p>Model data:</p>\n<pre><code>x = [[20,32,45,33,32,44,0],[23,32,45,12,32,66,11],[16,32,45,12,32,44,23],[120,2,55,62,82,14,81],[30,222,115,12,42,64,91],[220,12,55,222,82,14,181],[30,222,315,12,222,64,111]]\ny = [0,0,0,1,1,2,2]\n</code></pre>\n<p>I want to plot the decision boundary and visualize the datasets. Can someone please help to plot this type of data.</p>\n<p>The data given above is just mock data so feel free to change the values.\nIt would be helpful if at least if you could suggest the steps that are to followed. \nThanks in advance</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>You have to choose only 2 features to do this. The reason is that you cannot plot a 7D plot. After selecting the 2 features use only these for the visualization of the decision surface.</strong></p>\n<p>(I have also written an article about this here: <a href=\"https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&amp;sk=80f72ab272550d76a0cc3730d7c8af35\" rel=\"noreferrer\">https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&amp;sk=80f72ab272550d76a0cc3730d7c8af35</a>)</p>\n<hr/>\n<p>Now, the next question that you would ask: <em>How can I choose these 2 features?</em>. Well, there are a lot of ways. You could do a <strong>univariate F-value (feature ranking) test</strong> and see what features/variables are the most important. Then you could use these for the plot. Also, we could reduce the dimensionality from 7 to 2 using <em>PCA</em> for example.</p>\n<hr/>\n<p><strong>2D plot for 2 features and using the iris dataset</strong></p>\n<pre><code>from sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\niris = datasets.load_iris()\n# Select 2 features / variable for the 2D plot that we are going to create.\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('y label here')\nax.set_xlabel('x label here')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title(title)\nax.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/653iE.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/653iE.png\"/></a></p>\n<hr/>\n<p><strong>EDIT: Apply PCA to reduce dimensionality.</strong></p>\n<pre><code>from sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.decomposition import PCA\n\niris = datasets.load_iris()\n\nX = iris.data  \ny = iris.target\n\npca = PCA(n_components=2)\nXreduced = pca.fit_transform(X)\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(Xreduced, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = Xreduced[:, 0], Xreduced[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('PC2')\nax.set_xlabel('PC1')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title('Decison surface using the PCA transformed/projected features')\nax.legend()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/BI4FE.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/BI4FE.png\"/></a></p>\n<hr/>\n<p><strong><em>EDIT 1 (April 15th, 2020):</em></strong></p>\n<h2>Case: 3D plot for 3 features and using the iris dataset</h2>\n<pre><code>from sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom mpl_toolkits.mplot3d import Axes3D\n\niris = datasets.load_iris()\nX = iris.data[:, :3]  # we only take the first three features.\nY = iris.target\n\n#make it binary classification problem\nX = X[np.logical_or(Y==0,Y==1)]\nY = Y[np.logical_or(Y==0,Y==1)]\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, Y)\n\n# The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.\n# Solve for w3 (z)\nz = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]\n\ntmp = np.linspace(-5,5,30)\nx,y = np.meshgrid(tmp,tmp)\n\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\nax.plot3D(X[Y==0,0], X[Y==0,1], X[Y==0,2],'ob')\nax.plot3D(X[Y==1,0], X[Y==1,1], X[Y==1,2],'sr')\nax.plot_surface(x, y, z(x,y))\nax.view_init(30, 60)\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/jliMk.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/jliMk.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/\" rel=\"noreferrer\">mlxtend</a>. It's quite clean.</p>\n<p>First do a <code>pip install mlxtend</code>, and then:</p>\n<pre><code>from sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\n\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X, y)\nplot_decision_regions(X, y, clf=svm, legend=2)\nplt.show()\n</code></pre>\n<p>Where <em>X</em> is a two-dimensional data matrix, and <em>y</em> is the associated vector of training labels.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe:</p>\n<pre><code>df = \ncol1  col2  col3 \n1      2     3\n1      4     6\n3      7     2\n</code></pre>\n<p>I want to edit <code>df</code>, such that when the value of col1 is smaller than 2 , take the value from <code>col3</code>.</p>\n<p>So I will get:</p>\n<pre><code>new_df = \ncol1  col2  col3 \n3      2     3\n6      4     6\n3      7     2\n</code></pre>\n<p>I tried to use <code>assign</code> and <code>df.loc</code> but it didn't work.</p>\n<p>What is the best way to do so?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>df['col1'] = df.apply(lambda x: x['col3'] if x['col1'] &lt; x['col2'] else x['col1'], axis=1)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The most eficient way is by using the <code>loc</code> operator:</p>\n<pre class=\"lang-py prettyprint-override\"><code>mask = df[\"col1\"] &lt; df[\"col2\"]\ndf.loc[mask, \"col1\"] = df.loc[mask, \"col3\"]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>df.loc[df[\"col1\"] &lt; 2, \"col1\"] = df[\"col3\"]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using <code>recursive feature elimination with cross validation (rfecv)</code> as a feature selector for <code>randomforest classifier</code> as follows. </p>\n<pre><code>X = df[[my_features]] #all my features\ny = df['gold_standard'] #labels\n\nclf = RandomForestClassifier(random_state = 42, class_weight=\"balanced\")\nrfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(10), scoring='roc_auc')\nrfecv.fit(X,y)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\nfeatures=list(X.columns[rfecv.support_])\n</code></pre>\n<p>I am also performing <code>GridSearchCV</code> as follows to tune the hyperparameters of <code>RandomForestClassifier</code> as follows.</p>\n<pre><code>X = df[[my_features]] #all my features\ny = df['gold_standard'] #labels\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nrfc = RandomForestClassifier(random_state=42, class_weight = 'balanced')\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= k_fold, scoring = 'roc_auc')\nCV_rfc.fit(x_train, y_train)\nprint(CV_rfc.best_params_)\nprint(CV_rfc.best_score_)\nprint(CV_rfc.best_estimator_)\n\npred = CV_rfc.predict_proba(x_test)[:,1]\nprint(roc_auc_score(y_test, pred))\n</code></pre>\n<p>However, I am not clear how to merge feature selection (<code>rfecv</code>) with <code>GridSearchCV</code>.</p>\n<p><strong>EDIT:</strong> </p>\n<p>When I run the answer suggested by @Gambit I got the following error: </p>\n<pre><code>ValueError: Invalid parameter criterion for estimator RFECV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n   estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='gini', max_depth=None, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators='warn', n_jobs=None, oob_score=False,\n            random_state=42, verbose=0, warm_start=False),\n   min_features_to_select=1, n_jobs=None, scoring='roc_auc', step=1,\n   verbose=0). Check the list of available parameters with `estimator.get_params().keys()`.\n</code></pre>\n<p>I could resolve the above issue by using <code>estimator__</code> in the <code>param_grid</code> parameter list.</p>\n<hr/>\n<p>My question now is <strong>How to use the selected features and parameters in <code>x_test</code> to verify if the model works fine with unseen data. How can I obtain the <code>best features</code> and train it with the <code>optimal hyperparameters</code>?</strong></p>\n<p>I am happy to provide more details if needed.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Basically you want to fine tune the hyper parameter of your classifier (with Cross validation) after feature selection using recursive feature elimination (with Cross validation).</p>\n<p>Pipeline object is exactly meant for this purpose of assembling the data transformation and applying estimator.</p>\n<p>May be you could use a different model (<code>GradientBoostingClassifier</code>, etc. ) for your final classification. It would be possible with the following approach:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, \n                                                    random_state=42)\n\n\nfrom sklearn.pipeline import Pipeline\n\n#this is the classifier used for feature selection\nclf_featr_sele = RandomForestClassifier(n_estimators=30, \n                                        random_state=42,\n                                        class_weight=\"balanced\") \nrfecv = RFECV(estimator=clf_featr_sele, \n              step=1, \n              cv=5, \n              scoring = 'roc_auc')\n\n#you can have different classifier for your final classifier\nclf = RandomForestClassifier(n_estimators=10, \n                             random_state=42,\n                             class_weight=\"balanced\") \nCV_rfc = GridSearchCV(clf, \n                      param_grid={'max_depth':[2,3]},\n                      cv= 5, scoring = 'roc_auc')\n\npipeline  = Pipeline([('feature_sele',rfecv),\n                      ('clf_cv',CV_rfc)])\n\npipeline.fit(X_train, y_train)\npipeline.predict(X_test)\n</code></pre>\n<p>Now, you can apply this pipeline (Including feature selection) for test data.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You <em>can</em> do what you want by prefixing the names of the parameters you want to pass to the estimator with <code>'estimator__'</code>.</p>\n<pre><code>X = df[[my_features]]\ny = df[gold_standard]\n\nclf = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\nrfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(3), scoring='roc_auc')\n\nparam_grid = { \n    'estimator__n_estimators': [200, 500],\n    'estimator__max_features': ['auto', 'sqrt', 'log2'],\n    'estimator__max_depth' : [4,5,6,7,8],\n    'estimator__criterion' :['gini', 'entropy']\n}\nk_fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n\nCV_rfc = GridSearchCV(estimator=rfecv, param_grid=param_grid, cv= k_fold, scoring = 'roc_auc')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nCV_rfc.fit(X_train, y_train)\n</code></pre>\n<p>Output on fake data I made:</p>\n<pre><code>{'estimator__n_estimators': 200, 'estimator__max_depth': 6, 'estimator__criterion': 'entropy', 'estimator__max_features': 'auto'}\n0.5653035605690997\nRFECV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n   estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='entropy', max_depth=6, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=200, n_jobs=None, oob_score=False, random_state=0,\n            verbose=0, warm_start=False),\n   min_features_to_select=1, n_jobs=None, scoring='roc_auc', step=1,\n   verbose=0)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You just need to pass the Recursive Feature Elimination Estimator directly into the <code>GridSearchCV</code> object. Something like this should work</p>\n<pre><code>X = df[my_features] #all my features\ny = df['gold_standard'] #labels\n\nclf = RandomForestClassifier(random_state = 42, class_weight=\"balanced\")\nrfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(10), scoring='auc_roc')\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\n#------------- Just pass your RFECV object as estimator here directly --------#\n\nCV_rfc = GridSearchCV(estimator=rfecv, param_grid=param_grid, cv= k_fold, scoring = 'roc_auc')\n\n\nCV_rfc.fit(x_train, y_train)\nprint(CV_rfc.best_params_)\nprint(CV_rfc.best_score_)\nprint(CV_rfc.best_estimator_)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset that contains the NBA Player's average statistics per game. Some player's statistics are repeated because of they've been in different teams in season.</p>\n<p>For example:</p>\n<pre><code>      Player       Pos  Age Tm    G     GS   MP      FG\n8   Jarrett Allen   C   22  TOT  28     10  26.2     4.4\n9   Jarrett Allen   C   22  BRK  12     5   26.7     3.7\n10  Jarrett Allen   C   22  CLE  16     5   25.9     4.9\n</code></pre>\n<p>I want to average Jarrett Allen's stats and put them into a single row. How can I do this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"noreferrer\"><code>groupby</code></a> and use <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html\" rel=\"noreferrer\"><code>agg</code></a> to get the mean. For the non numeric columns, let's take the first value:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.groupby('Player').agg({k: 'mean' if v in ('int64', 'float64') else 'first'\n                          for k,v in df.dtypes[1:].items()})\n</code></pre>\n<p>output:</p>\n<pre><code>              Pos  Age   Tm          G        GS         MP        FG\nPlayer                                                               \nJarrett Allen   C   22  TOT  18.666667  6.666667  26.266667  4.333333\n</code></pre>\n<p>NB. content of the dictionary comprehension:</p>\n<pre><code>{'Pos': 'first',\n 'Age': 'mean',\n 'Tm': 'first',\n 'G': 'mean',\n 'GS': 'mean',\n 'MP': 'mean',\n 'FG': 'mean'}\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>x = [['a', 12, 5],['a', 12, 7], ['b', 15, 10],['b', 15, 12],['c', 20, 1]]\n\nimport pandas as pd\ndf = pd.DataFrame(x, columns=['name', 'age', 'score'])\nprint(df)\nprint('-----------')\n\ndf2 = df.groupby(['name', 'age']).mean()\nprint(df2)\n</code></pre>\n<p>Output:</p>\n<pre><code>  name  age  score\n0    a   12      5\n1    a   12      7\n2    b   15     10\n3    b   15     12\n4    c   20      1\n-----------\n          score\nname age       \na    12       6\nb    15      11\nc    20       1\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Option 1</strong></p>\n<p>If one considers the dataframe that OP shares in the question <code>df</code> the following will do the work</p>\n<pre><code>df_new = df.groupby('Player').agg(lambda x: x.iloc[0] if pd.api.types.is_string_dtype(x.dtype) else x.mean())\n\n[Out]:\n              Pos   Age   Tm          G        GS         MP        FG\nPlayer                                                                \nJarrett Allen   C  22.0  TOT  18.666667  6.666667  26.266667  4.333333\n</code></pre>\n<p>This one uses:</p>\n<ul>\n<li><p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\" rel=\"noreferrer\"><code>pandas.DataFrame.groupby</code></a> to group by the <code>Player</code> column</p>\n</li>\n<li><p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.agg.html\" rel=\"noreferrer\"><code>pandas.core.groupby.GroupBy.agg</code></a> to aggregate the values based on a custom made lambda function.</p>\n</li>\n<li><p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.api.types.is_string_dtype.html\" rel=\"noreferrer\"><code>pandas.api.types.is_string_dtype</code></a> to check if a column is of string type (<a href=\"https://github.com/pandas-dev/pandas/blob/v1.5.0/pandas/core/dtypes/common.py#L545-L583\" rel=\"noreferrer\">see here how the method is implemented</a>)</p>\n</li>\n</ul>\n<p>Let's test it with a new dataframe, <code>df2</code>, with more elements in the <code>Player</code> column.</p>\n<pre><code>import numpy as np\n\ndf2 = pd.DataFrame({'Player': ['John Collins', 'John Collins', 'John Collins', 'Trae Young', 'Trae Young', 'Clint Capela', 'Jarrett Allen', 'Jarrett Allen', 'Jarrett Allen'],\n                    'Pos': ['PF', 'PF', 'PF', 'PG', 'PG', 'C', 'C', 'C', 'C'],\n                    'Age': np.random.randint(0, 100, 9),\n                    'Tm': ['ATL', 'ATL', 'ATL', 'ATL', 'ATL', 'ATL', 'TOT', 'BRK', 'CLE'],\n                    'G': np.random.randint(0, 100, 9),\n                    'GS': np.random.randint(0, 100, 9),\n                    'MP': np.random.uniform(0, 100, 9),\n                    'FG': np.random.uniform(0, 100, 9)})\n\n[Out]:\n          Player Pos  Age   Tm   G  GS         MP         FG\n0   John Collins  PF   71  ATL  75  39  16.123225  77.949756\n1   John Collins  PF   60  ATL  49  49  30.308092  24.788401\n2   John Collins  PF   52  ATL  33  92  11.087317  58.488575\n3     Trae Young  PG   72  ATL  20  91  62.862313  60.169282\n4     Trae Young  PG   85  ATL  61  77  30.248551  85.169038\n5   Clint Capela   C   73  ATL   5  67  45.817690  21.966777\n6  Jarrett Allen   C   23  TOT  60  51  93.076624  34.160823\n7  Jarrett Allen   C   12  BRK   2  77  74.318568  78.755869\n8  Jarrett Allen   C   44  CLE  82  81   7.375631  40.930844\n</code></pre>\n<p>If one tests the operation on <code>df2</code>, one will get the following</p>\n<pre><code>df_new2 = df2.groupby('Player').agg(lambda x: x.iloc[0] if pd.api.types.is_string_dtype(x.dtype) else x.mean())\n\n[Out]:\n              Pos        Age   Tm          G         GS         MP         FG\nPlayer                                                                       \nClint Capela    C  95.000000  ATL  30.000000  98.000000  46.476398  17.987104\nJarrett Allen   C  60.000000  TOT  48.666667  19.333333  70.050540  33.572896\nJohn Collins   PF  74.333333  ATL  50.333333  52.666667  78.181457  78.152235\nTrae Young     PG  57.500000  ATL  44.500000  47.500000  46.602543  53.835455\n</code></pre>\n<hr/>\n<p><strong>Option 2</strong></p>\n<p>Depending on the desired output, assuming that one only wants to group by player (independently of <code>Age</code> or <code>Tm</code>), a simpler solution would be to just group by and pass <code>.mean()</code> as follows</p>\n<pre><code>df_new3 = df.groupby('Player').mean()\n\n[Out]:\n\n                Age          G        GS         MP        FG\nPlayer                                                       \nJarrett Allen  22.0  18.666667  6.666667  26.266667  4.333333\n</code></pre>\n<p><strong>Notes</strong>:</p>\n<ul>\n<li>The output of this previous operation won't display non-numerical columns (apart from the Player name).</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have small problem when exporting static chart using plotly.</p>\n<p>Plotly does not correctly recognize that I have orca installed and I have still error related to missing orca. I try to change the orca directory but it is still not working. Anyone who knows what is wrong?</p>\n<p>My code:</p>\n<pre><code>import plotly.graph_objects as go\nimport orca\nimport plotly\n\n#%%\nfig = go.Figure(data=go.Candlestick(x=pricedata.index,\n                    open=pricedata['bidopen'],\n                    high=pricedata['bidhigh'],\n                    low=pricedata['bidlow'],\n                    close=pricedata['bidclose']),)\n#%%\nfig.show()\n\n#%%\nplotly.io.orca.config.executable = r'C:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\orca'\nplotly.io.orca.config.save()\n\n#%%\n\nfig.write_image(\"images/fig1.png\")\n</code></pre>\n<p>Here is described how to solve it but it does not work for me:</p>\n<p><a href=\"https://plot.ly/python/orca-management/#configuring-the-executable\" rel=\"noreferrer\">https://plot.ly/python/orca-management/#configuring-the-executable</a></p>\n<p>The orca version is 1.5.1</p>\n<p>Thanks.</p>\n<p>B.</p>\n<p>EDIT:</p>\n<p>Error msg:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nc:\\Users\\Kuba\\Documents\\GitHub\\frstalg\\FXCM Stuff\\LiveMyStrategyNOTEBOOK-20191017.py in \n      1 \n----&gt; 2 fig.write_image(\"images/fig1.png\")\n\n~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\basedatatypes.py in write_image(self, *args, **kwargs)\n   2686         import plotly.io as pio\n   2687 \n-&gt; 2688         return pio.write_image(self, *args, **kwargs)\n   2689 \n   2690     # Static helpers\n\n~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\io\\_orca.py in write_image(fig, file, format, scale, width, height, validate)\n   1703     # Do this first so we don't create a file if image conversion fails\n   1704     img_data = to_image(\n-&gt; 1705         fig, format=format, scale=scale, width=width, height=height, validate=validate\n   1706     )\n   1707 \n\n~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\io\\_orca.py in to_image(fig, format, width, height, scale, validate)\n   1480     # Make sure orca sever is running\n   1481     # -------------------------------\n-&gt; 1482     ensure_server()\n   1483 \n   1484     # Handle defaults\n\n~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\io\\_orca.py in ensure_server()\n   1342     # Validate orca executable\n   1343     if status.state == \"unvalidated\":\n-&gt; 1344         validate_executable()\n   1345 \n   1346     # Acquire lock to make sure that we keep the properties of orca_state\n\n~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\io\\_orca.py in validate_executable()\n   1041                 executable=config.executable,\n   1042                 formatted_path=formatted_path,\n-&gt; 1043                 instructions=install_location_instructions,\n   1044             )\n   1045         )\n\nValueError: \nThe orca executable is required to export figures as static images,\nbut it could not be found on the system path.\n\nSearched for executable 'C:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\orca' on the following path:\n    C:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\n    C:\\Program Files\\Microsoft MPI\\Bin\\\n    C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\n    C:\\Program Files (x86)\\Intel\\iCLS Client\\\n    C:\\Program Files\\Intel\\iCLS Client\\\n    C:\\Windows\\system32\n    C:\\Windows\n    C:\\Windows\\System32\\Wbem\n    C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\\n    C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\n    C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL\n    C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL\n    C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT\n    C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT\n    C:\\WINDOWS\\system32\n    C:\\WINDOWS\n    C:\\WINDOWS\\System32\\Wbem\n    C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\\n    C:\\WINDOWS\\System32\\OpenSSH\\\n    C:\\Program Files\\Git\\cmd\n    C:\\Program Files\\dotnet\\\n    C:\\Program Files\\PuTTY\\\n    C:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python37\\Scripts\\\n    C:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python37\\\n    C:\\Users\\Kuba\\AppData\\Local\\Microsoft\\WindowsApps\n    C:\\Users\\Kuba\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\n    C:\\Users\\Kuba\\AppData\\Local\\GitHubDesktop\\bin\n    %USERPROFILE%\\AppData\\Local\\Microsoft\\WindowsApps\n\n    C:\\Users\\Kuba\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\.libs\n\nIf you haven't installed orca yet, you can do so using conda as follows:\n\n    $ conda install -c plotly plotly-orca\n\nAlternatively, see other installation methods in the orca project README at\nhttps://github.com/plotly/orca.\n\nAfter installation is complete, no further configuration should be needed.\n\nIf you have installed orca, then for some reason plotly.py was unable to\nlocate it. In this case, set the `plotly.io.orca.config.executable`\nproperty to the full path of your orca executable. For example:\n\n    &gt;&gt;&gt; plotly.io.orca.config.executable = '/path/to/orca'\n\nAfter updating this executable property, try the export operation again.\nIf it is successful then you may want to save this configuration so that it\nwill be applied automatically in future sessions. You can do this as follows:\n\n    &gt;&gt;&gt; plotly.io.orca.config.save()\n\nIf you're still having trouble, feel free to ask for help on the forums at\nhttps://community.plot.ly/c/api/python\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From the <a href=\"https://plotly.com/python/static-image-export/\" rel=\"noreferrer\">plotly docs</a></p>\n<pre><code>pip install -U kaleido\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>try kaleido, it worked for me.\nIt is an alternative to orca</p>\n<pre><code>conda install -c plotly python-kaleido\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What worked for me on Windows with Pycharm and Python 3.6 is:</p>\n<ul>\n<li><p>Following <a href=\"https://github.com/plotly/orca#installation\" rel=\"noreferrer\">https://github.com/plotly/orca#installation</a> - Method 4: Standalone binaries:</p>\n<ul>\n<li>Download windows-release.zip from <a href=\"https://github.com/plotly/orca/releases\" rel=\"noreferrer\">https://github.com/plotly/orca/releases</a> </li>\n<li>Install the executable</li>\n<li>Right click on the desktop newly created Orca icon to get the path where the application was installed.</li>\n</ul></li>\n<li><p>From the plotly\\io folder (in my case <code>C:\\Users\\ventafri\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\plotly\\io</code>) open <code>_orca.py</code></p></li>\n</ul>\n<p>Substitute:</p>\n<pre><code> # Try to find an executable\n # -------------------------\n # Search for executable name or path in config.executable\n executable = which(config.executable)\n path = os.environ.get(\"PATH\", os.defpath)\n formatted_path = path.replace(os.pathsep, \"\\n    \")\n</code></pre>\n<p>with:</p>\n<pre><code># Try to find an executable\n# -------------------------\n# Search for executable name or path in config.executable\nexecutable = r\"C:\\Users\\ventafri\\AppData\\Local\\Programs\\orca\\orca.exe\"\npath = os.environ.get(\"PATH\", os.defpath)\nformatted_path = path.replace(os.pathsep, \"\\n    \")\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How to compare column names of 2 different Pandas data frame. I want to compare train and test data frames where there are some columns missing in test Data frames??</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.html\" rel=\"noreferrer\"><code>pandas.Index</code></a> objects, including dataframe columns, have useful <code>set</code>-like methods, such as <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.intersection.html\" rel=\"noreferrer\"><code>intersection</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.difference.html\" rel=\"noreferrer\"><code>difference</code></a>.</p>\n<p>For example, given dataframes <code>train</code> and <code>test</code>:</p>\n<pre><code>train_cols = train.columns\ntest_cols = test.columns\n\ncommon_cols = train_cols.intersection(test_cols)\ntrain_not_test = train_cols.difference(test_cols)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How can we change y axis to percent, instead of a fraction using Plotnine library in Python?</p>\n<p>A MWE of a barplot is as follows:</p>\n<pre><code>from plotnine import *\nfrom plotnine.data import mpg\n\np = ggplot(mpg) + geom_bar(aes(x='manufacturer', fill='class'), position='fill')\nprint(p)\n</code></pre>\n<p>Which gives the following figure:</p>\n<p><a href=\"https://i.sstatic.net/wwx4M.png\" rel=\"noreferrer\">Stacked bar chart with y axis as fraction not percent</a></p>\n<p>With ggplot2 in R it is simple, just need to add:</p>\n<pre><code>+ scale_y_continuous(labels = scales::percent)\n</code></pre>\n<p>However I have not been able to find how to do this in Plotnine. </p>\n<p>Any advise?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>similar question raised here: <a href=\"https://github.com/has2k1/plotnine/issues/152\" rel=\"noreferrer\">https://github.com/has2k1/plotnine/issues/152</a></p>\n<pre><code>from plotnine import *\nfrom plotnine.data import mpg\nfrom mizani.formatters import percent_format\n\np = ggplot(mpg) + geom_bar(aes(x='manufacturer', fill='class'), position='fill')\np = p + scale_y_continuous(labels=percent_format())\nprint(p)\n</code></pre>\n<p>other predefined filters can be found here: <a href=\"https://mizani.readthedocs.io/en/stable/formatters.html\" rel=\"noreferrer\">https://mizani.readthedocs.io/en/stable/formatters.html</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>labels</code> parameter accepts a callable that takes the list of break points as input. All you have to do is to convert each item in the list manually:</p>\n<pre><code>scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>An irregular time series <code>data</code> is stored in a <code>pandas.DataFrame</code>. A <code>DatetimeIndex</code> has been set. I need the time difference between consecutive entries in the index. </p>\n<p>I thought it would be as simple as</p>\n<pre><code>data.index.diff()\n</code></pre>\n<p>but got</p>\n<pre><code>AttributeError: 'DatetimeIndex' object has no attribute 'diff'\n</code></pre>\n<p>I tried</p>\n<pre><code>data.index - data.index.shift(1)\n</code></pre>\n<p>but got</p>\n<pre><code>ValueError: Cannot shift with no freq\n</code></pre>\n<p>I do not want to infer or enforce a frequency first before doing this operation. There are large gaps in the time series that would be expanded to large runs of <code>nan</code>. The point is to find these gaps first.</p>\n<p>So, what is a clean way to do this seemingly simple operation? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is no implemented <code>diff</code> function yet for index.</p>\n<p>However, it is possible to convert the index to a <code>Series</code> first by using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.to_series.html\" rel=\"noreferrer\"><code>Index.to_series</code></a>, if you need to preserve the original index. Use the <code>Series</code> constructor with no index parameter if the default index is needed.</p>\n<p>Code example:</p>\n<pre><code>rng = pd.to_datetime(['2015-01-10','2015-01-12','2015-01-13'])\ndata = pd.DataFrame({'a': range(3)}, index=rng)  \nprint(data)\n             a\n 2015-01-10  0\n 2015-01-12  1\n 2015-01-13  2\n\na = data.index.to_series().diff()\nprint(a)\n\n2015-01-10      NaT\n2015-01-12   2 days\n2015-01-13   1 days\ndtype: timedelta64[ns]\n\na = pd.Series(data.index).diff()\nprint(a)\n 0      NaT\n 1   2 days\n 2   1 days\ndtype: timedelta64[ns]\n\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This question is a bit old but anyway...</p>\n<p>I use <code>numpy.diff(data.index)</code> to get the time deltas. Working fine.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I tried out two ways of implementing light GBM. Expect it to return the same value but it didnt.</p>\n<p>I thought <code>lgb.LightGBMRegressor()</code> and <code>lgb.train(train_data, test_data)</code> will return the same accuracy but it didnt. So I wonder why?</p>\n<h1>Function to break the data</h1>\n<pre class=\"lang-py prettyprint-override\"><code>def dataready(train, test, predictvar):\n    included_features = train.columns\n    y_test = test[predictvar].values\n    y_train = train[predictvar].ravel()\n    train = train.drop([predictvar], axis = 1)\n    test = test.drop([predictvar], axis = 1)\n    x_train = train.values\n    x_test = test.values\n    return x_train, y_train, x_test, y_test, train\n</code></pre>\n<h1>This is how i break down the data</h1>\n<pre><code>x_train, y_train, x_test, y_test, train2 = dataready(train, test, 'runtime.min')\ntrain_data = lgb.Dataset(x_train, label=y_train)\ntest_data = lgb.Dataset(x_test, label=y_test)\n</code></pre>\n<h1>predict model</h1>\n<pre><code>lgb1 = LMGBRegressor()\nlgb1.fit(x_train, y_train)\nlgb = lgb.train(parameters,train_data,valid_sets=test_data,num_boost_round=5000,early_stopping_rounds=100)\n</code></pre>\n<p>I expect it to be roughly the same but it is not. As far as I understand, one is a booster and the other is a regressor?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>LGBMRegressor</code> is the <a href=\"https://scikit-learn.org/stable/\" rel=\"noreferrer\">sklearn</a> interface. The <code>.fit(X, y)</code> call is standard sklearn syntax for model training. It is a class object for you to use as part of sklearn's ecosystem (for running pipelines, parameter tuning etc.).</p>\n<p><code>lightgbm.train</code> is the core training API for lightgbm itself.</p>\n<p>XGBoost and many other popular ML training libraries have a similar differentiation (core API uses <code>xgb.train(...)</code> for example with sklearn API using <code>XGBClassifier</code> or <code>XGBRegressor</code>).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to do a naive Bayes and after loading some data into a dataframe in Pandas, the describe function captures the data I want.  I'd like to capture the mean and std from each column of the table but am unsure on how to do that.  I've tried things like:</p>\n<pre><code>df.describe([mean])\ndf.describe(['mean'])\ndf.describe().mean\n</code></pre>\n<p>None are working.  I was able to do something similar in R with summary but don't know how to do in Python.  Can someone lend some advice? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Please try something like this:</p>\n<pre><code>df.describe(include='all').loc['mean']\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You were close. You don't need any <code>include</code> tag. Just rewrite your second approach correctly: <code>df.describe()['mean']</code></p>\n<p>For example:</p>\n<pre><code>import pandas as pd\n\ns = pd.Series([1, 2, 3, 4, 5])\ns.describe()['mean']\n# 3.0\n</code></pre>\n<p>If you want both <code>mean</code> and <code>std</code>, just write <code>df.describe()[['mean', 'std']]</code>. For example, </p>\n<pre><code>s.describe()[['mean', 'std']]\n# mean    3.000000\n# std     1.581139\n# dtype: float64\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you further want to extract specific column data then try:</p>\n<pre><code>df.describe()['FeatureName']['mean']\n</code></pre>\n<p>Replace mean with any other statistic you want to extract</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe which looks like this.</p>\n<p><code>id        YearReleased      Artist     count\n168             2015             Muse      1\n169             2015          Rihanna      3\n170             2015     Taylor Swift      2\n171             2016   Jennifer Lopez      1\n172             2016          Rihanna      3\n173             2016       Underworld      1\n174             2017         Coldplay      1\n175             2017       Ed Sheeran      2</code></p>\n<p>I want to get the maximum count for each year and then get the corresponding Artist name.</p>\n<p>Something like this:</p>\n<p>YearReleased  Artist</p>\n<p>2015          Rihanna<br/>\n2016          Rihanna<br/>\n2017       Ed Sheeran</p>\n<p>I have tried using a loop to iterate over the rows of the dataframe and create another dictionary with key as year and value as artist. But when I try to convert that dictionary to a dataframe, the keys are mapped to columns instead of rows.</p>\n<p>Can somebody guide me to have a better approach to this without having to loop over the dataframe and instead use some inbuilt pandas method to achieve this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Look at <code>idxmax</code></p>\n<pre><code>df.loc[df.groupby('YearReleased')['count'].idxmax()]\nOut[445]: \n    id  YearReleased     Artist  count\n1  169          2015    Rihanna      3\n4  172          2016    Rihanna      3\n7  175          2017  EdSheeran      2\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"noreferrer\">groupby</a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html\" rel=\"noreferrer\">transform</a> :</p>\n<pre><code>idx = df.groupby(['YearReleased'])['count'].transform(max) == df['count']\n</code></pre>\n<p>and then use this indexer:</p>\n<pre><code>df[idx]\nOut[14]: \n    id  YearReleased      Artist  count\n1  169          2015     Rihanna      3\n4  172          2016     Rihanna      3\n7  175          2017  Ed Sheeran      2\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can I think of an ORC file as similar to a CSV file with column headings and row labels containing data? If so, can I somehow read it into a simple pandas dataframe? I am not that familiar with tools like Hadoop or Spark, but is it necessary to understand them just to see the contents of a local ORC file in Python? </p>\n<p>The filename is <code>someFile.snappy.orc</code></p>\n<p>I can see online that <code>spark.read.orc('someFile.snappy.orc')</code> works, but even after <code>import pyspark</code>, it is throwing error. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I haven't been able to find any great options, there are a few dead projects trying to wrap the java reader. However, pyarrow does have an ORC reader that won't require you using pyspark. It's a bit limited but it works.</p>\n<pre><code>import pandas as pd\nimport pyarrow.orc as orc\n\nwith open(filename, 'rb') as file:\n    data = orc.ORCFile(file)\n    df = data.read().to_pandas()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In case <code>import pyarrow.orc as orc</code> does not work (did not work for me in Windows 10), you can read them to Spark data frame then convert to <code>pandas</code>'s data frame</p>\n<pre class=\"lang-py prettyprint-override\"><code>import findspark\nfrom pyspark.sql import SparkSession\n\nfindspark.init()\nspark = SparkSession.builder.getOrCreate()\ndf_spark = spark.read.orc('example.orc')\ndf_pandas = df_spark.toPandas()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Starting from Pandas 1.0.0, there is a built in function for Pandas.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_orc.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/docs/reference/api/pandas.read_orc.html</a></p>\n<pre><code>import pandas as pd\nimport pyarrow.orc \n\ndf = pd.read_orc('/tmp/your_df.orc')\n</code></pre>\n<p>Be sure to read this warning about dependencies. This function might not work on Windows\n<a href=\"https://pandas.pydata.org/docs/getting_started/install.html#install-warn-orc\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/docs/getting_started/install.html#install-warn-orc</a></p>\n<blockquote>\n<p>If you want to use\nread_orc(), it is highly recommended to install pyarrow using conda</p>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset from a number of users (nUsers). Each user is sampled randomly in time (non-constant nSamples for each user). Each sample has a number of features (nFeatures). \nFor example:</p>\n<p>nUsers = 3 ---&gt; 3 users</p>\n<p>nSamples = [32, 52, 21] ---&gt; first user was sampled 32 times second user was sampled 52 times etc.</p>\n<p>nFeatures = 10 ---&gt; constant number of features for each sample.</p>\n<p>I would like the LSTM to produce a current prediction based on the current features and on previous predictions of the same user. \nCan I do that in Keras using LSTM layer? \nI have 2 problems: </p>\n<ol>\n<li>The data has a <strong>different time series</strong> for each user. How do I incorporate this?</li>\n<li>How do I deal with adding the previous predictions into the current time feature space in order to make a current prediction?</li>\n</ol>\n<p>Thanks for your help!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It sounds like each user is a sequence, so, users may be the \"batch size\" for your problem. So at first, <code>nExamples = nUsers</code>.    </p>\n<p>If I understood your problem correctly (predict the next element), you should define a maximum length of \"looking back\". Say you can predict the next element from looking at the 7 previous ones, for instance (and not looking at the entire sequence).   </p>\n<p>For that, you should separate your data like this:</p>\n<pre><code>example 1: x[0] = [s0, s1, s2, ..., s6] | y[0] = s7   \nexample 2: x[1] = [s1, s2, s3, ..., s7] | y[1] = s8\n</code></pre>\n<p>Where <code>sn</code> is a sample with 10 features. \nUsually, it doesn't matter if you mix users. Create these little segments for all users and put everything together. </p>\n<p>This will result in in arrays shaped like</p>\n<pre><code>x.shape -&gt; (BatchSize, 7, 10) -&gt; (BatchSize, 7 step sequences, 10 features)   \ny.shape -&gt; (BatchSize, 10)\n</code></pre>\n<p>Maybe you don't mean predicting the next set of features, but just predicting something. In that case, just replace y for the value you want. That may result in <code>y.shape -&gt; (BatchSize,)</code> if you want just a single result.</p>\n<hr/>\n<p>Now, if you do need the entire sequence for predicting (instead of n previous elements), then you will have to define the maximum length and pad the sequences.</p>\n<p>Suppose your longest sequence, as in your example, is 52. Then:</p>\n<pre><code>x.shape -&gt; (Users, 52, 10).    \n</code></pre>\n<p>Then you will have to \"pad\" the sequences to fill the blanks.<br/>\nYou can for instance fill the beginning of the sequences with zero features, such as:</p>\n<pre><code>x[0] = [s0, s1, s2, ......., s51] -&gt; user with the longest sequence    \nx[1] = [0 , 0 , s0, s1, ..., s49] -&gt; user with a shorter sequence\n</code></pre>\n<p>Or (I'm not sure this works, I never tested), pad the ending with zero values and use the <a href=\"https://keras.io/layers/core/#masking\" rel=\"noreferrer\">Masking Layer</a>, which is what Keras have for \"variable length sequences\". You still use a fixed size array, but internally it will (?) discard the zero values. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/54762690/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2020-09-04 06:27:19Z\">4 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n<p class=\"mb0 mt12\">This post was edited and submitted for review <span class=\"relativetime\" title=\"2021-09-17 23:16:24Z\">3 years ago</span> and failed to reopen the post:</p>\n<blockquote class=\"mb0 mt12\">\n<p>Original close reason(s) were not resolved</p>\n</blockquote>\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/54762690/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I need to know whether coherence score of  0.4 is good or bad? I use LDA as topic modelling algorithm.</p>\n<p>What is the average coherence score in this context?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Coherence measures the relative distance between words within a topic.  There are two major types C_V typically 0 &lt; x &lt; 1  and uMass -14 &lt; x &lt; 14.  It's rare to see a coherence of 1 or +.9 unless the words being measured are either identical words or bigrams.  Like United and States would likely return a coherence score of ~.94 or hero and hero would return a coherence of 1.  The overall coherence score of a topic is the average of the distances between words.  I try and attain a .7 in my LDAs if I'm using c_v I think that is a strong topic correlation.  I would say: </p>\n<ul>\n<li><p>.3 is bad</p>\n<p>.4 is low </p>\n<p>.55 is okay </p>\n<p>.65 might be as good as it is going to get </p>\n<p>.7 is nice </p>\n<p>.8 is unlikely and </p>\n<p>.9 is probably wrong</p></li>\n</ul>\n<p>Low coherence fixes:</p>\n<ul>\n<li><p>adjust your parameters alpha = .1, beta = .01 or .001, random_state = 123, etc </p></li>\n<li><p>get better data</p></li>\n<li><p>at .4 you probably have the wrong number of topics check out <a href=\"https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\" rel=\"noreferrer\">https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/</a> for what is known as the elbow method - it gives you a graph of the optimal number of topics for greatest coherence in your data set.  I'm using mallet which has pretty good coherance here is code to check coherence for different numbers of topics:</p></li>\n</ul>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\r\n    \"\"\"\r\n    Compute c_v coherence for various number of topics\r\n\r\n    Parameters:\r\n    ----------\r\n    dictionary : Gensim dictionary\r\n    corpus : Gensim corpus\r\n    texts : List of input texts\r\n    limit : Max num of topics\r\n\r\n    Returns:\r\n    -------\r\n    model_list : List of LDA topic models\r\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\r\n    \"\"\"\r\n    coherence_values = []\r\n    model_list = []\r\n    for num_topics in range(start, limit, step):\r\n        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\r\n        model_list.append(model)\r\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\r\n        coherence_values.append(coherencemodel.get_coherence())\r\n\r\n    return model_list, coherence_values\r\n# Can take a long time to run.\r\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\r\n# Show graph\r\nlimit=40; start=2; step=6;\r\nx = range(start, limit, step)\r\nplt.plot(x, coherence_values)\r\nplt.xlabel(\"Num Topics\")\r\nplt.ylabel(\"Coherence score\")\r\nplt.legend((\"coherence_values\"), loc='best')\r\nplt.show()\r\n\r\n# Print the coherence scores\r\nfor m, cv in zip(x, coherence_values):\r\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\r\n    \r\n# Select the model and print the topics\r\noptimal_model = model_list[3]\r\nmodel_topics = optimal_model.show_topics(formatted=False)\r\npprint(optimal_model.print_topics(num_words=10))</code></pre>\n</div>\n</div>\n</p>\n<p>I hope this helps :)</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In addition to the excellent answer from Sara:</p>\n<p><a href=\"http://qpleple.com/topic-coherence-to-evaluate-topic-models/\" rel=\"nofollow noreferrer\">UMass coherence</a> measure how often were the two words (Wi, Wj) were seen together in the corpus. It is defined as:</p>\n<pre><code>D(Wi, Wj) = log [ (D(Wi, Wj) + EPSILON) / D(Wi) ]\n</code></pre>\n<p>Where:\nD(Wi, Wj) is how many times word Wi and word Wj appeared together</p>\n<p>D(Wi) is how many times word Wi appeared alone in the corpus</p>\n<p>EPSILON is a small value <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/topic_coherence/direct_confirmation_measure.py\" rel=\"nofollow noreferrer\">(like 1e-12)</a> added to the numerator to avoid 0 values</p>\n<p>If Wi and Wj never appear together, then this results in log(0) which will break the universe. EPSILON value is kind-of a hack to fix this.</p>\n<p>In conclusion, you can get a value from very big negative number all the way till approx 0. Interpretation is the same as Sara wrote, the greater the number the better, where 0 would be obviously wrong.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would just like to add that good or bad is relative to the corpus you are working on and the scores for the other clusters.</p>\n<p>In the link that Sara provided the article shows 33 topics as optimal with a coherence score of ~0.33, but as the author mentions there maybe repeated terms within that cluster. In that case you would have to compare terms/snippets from the optimal cluster decomposition to a lower coherence score to see if the results are more or less interpretable.</p>\n<p>Of course you should adjust the parameters of your model but the score contextually dependent, and I don't think you can necessarily say a specific coherence score clustered your data optimally without first understanding what the data looks like. That said, as Sara mentioned ~1 or ~0 are probably wrong.</p>\n<p>You could compare your model against a benchmark dataset and if it has a higher coherence, then you have a better gauge of how well your model is working.</p>\n<p>This paper was helpful to me: <a href=\"https://rb.gy/kejxkz\" rel=\"nofollow noreferrer\">https://rb.gy/kejxkz</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to implement a custom loss function in scikit learn. I use the following code snippet:</p>\n<pre><code>def my_custom_loss_func(y_true,y_pred):\n   diff3=max((abs(y_true-y_pred))*y_true)\n   return diff3\n\nscore=make_scorer(my_custom_loss_func,greater_ is_better=False)\nclf=RandomForestRegressor()\nmnn= GridSearchCV(clf,score)\nknn = mnn.fit(feam,labm) \n</code></pre>\n<p>What should be the arguments passed into <code>my_custom_loss_func</code>? My label matrix is called <code>labm</code>. I want to calculate the difference between the actual and the predicted output (by the model ) multiplied by the true output. If I use <code>labm</code> in place of <code>y_true</code>, what should I use in place of <code>y_pred</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Okay, there's 3 things going on here:</p>\n<p>1) there is a loss function while training used to tune your models parameters</p>\n<p>2) there is a scoring function which is used to judge the quality of your model</p>\n<p>3) there is hyper-parameter tuning which uses a scoring function to optimize your hyperparameters.</p>\n<p>So... if you are trying to tune hyperparameters, then you are on the right track in defining a \"loss fxn\" for that purpose. If, however, you are trying to tune your whole model to perform well on, lets say, a recall test - then you need a recall optimizer to be part of the training process. It's tricky, but you can do it...</p>\n<p>1) Open up your classifier. Let's use an RFC for example: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>\n<p>2) click [source]</p>\n<p>3) See how it's inheriting from ForestClassifier? Right there in the class definition. Click that word to jump to it's parent definition.</p>\n<p>4) See how this new object is inheriting from ClassifierMixin? Click that.</p>\n<p>5) See how the bottom of that ClassifierMixin class says this?</p>\n<pre><code>from .metrics import accuracy_score\nreturn accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n</code></pre>\n<p>That's your model being trained on accuracy. You need to inject at this point if you want to train your model to be a \"recall model\" or a \"precision model\" or whatever model. This accuracy metric is baked into SKlearn. Some day, a better man than I will make this a parameter which models accept, however in the mean time, you gotta go into your sklearn installation, and tweak this accuracy_score to be whatever you want.</p>\n<p>Best of luck!</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The documentation for make_scorer goes like this:</p>\n<pre><code>sklearn.metrics.make_scorer(score_func, greater_is_better=True, needs_proba=False, \nneeds_threshold=False, **kwargs)\n</code></pre>\n<p>So, it dosen't need you to pass arguments while calling the function.\nIs this what you were asking?</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a pandas dataframe and want to get rid of rows in which the column 'A' is negative. I know 2 ways to do this:</p>\n<pre><code>df = df[df['A'] &gt;= 0]\n</code></pre>\n<p>or</p>\n<pre><code>selRows = df[df['A'] &lt; 0].index\ndf = df.drop(selRows, axis=0)\n</code></pre>\n<p>What is the recommended solution? Why?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The recommended solution is the most eficient, which in this case, is the first one.</p>\n<pre><code>df = df[df['A'] &gt;= 0]\n</code></pre>\n<p>On the second solution</p>\n<pre><code>selRows = df[df['A'] &lt; 0].index\ndf = df.drop(selRows, axis=0)\n</code></pre>\n<p>you are repeating the slicing process. But lets break it to pieces to understand why.</p>\n<p>When you write</p>\n<pre><code>df['A'] &gt;= 0\n</code></pre>\n<p>you are creating a mask, a Boolean Series with an entry for each index of df, whose value is either True or False according to a condition (on this case, if such the value of column 'A' at a given index is greater than or equal to 0).</p>\n<p>When you write</p>\n<pre><code>df[df['A'] &gt;= 0]\n</code></pre>\n<p>you accessing the rows for which your mask (df['A'] &gt;= 0) is True. This is a slicing method supported by Pandas that lets you select certain rows by passing a Boolean Series and will return a view of the original DataFrame with only the entries for which the Series was True.</p>\n<p>Finally, when you write this</p>\n<pre><code>selRows = df[df['A'] &lt; 0].index\ndf = df.drop(selRows, axis=0)\n</code></pre>\n<p>you are repeating the proccess because</p>\n<pre><code>df[df['A'] &lt; 0]\n</code></pre>\n<p>is already slicing your DataFrame (in this case for the rows you want to <strong>drop</strong>). You are then getting those indices, going back to the original DataFrame and explicitly dropping them. No need for this, you already sliced the DataFrame in the first step.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>df = df[df['A'] &gt;= 0]\n</code></pre>\n<p>is indeed the faster solution. Just be aware that it returns a <em>view</em> of the original data frame, not a new data frame. This can lead you into trouble, for example when you want to change its values, as pandas will give you the <code>SettingwithCopyWarning</code>.</p>\n<p>The simple fix of course is what Wen-Ben recommended:</p>\n<pre><code>df = df[df['A'] &gt;= 0].copy()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your question is like this: \"I have two identical cakes, but one has icing. Which has more calories?\"</p>\n<p>The second solution is doing the same thing but twice. A filtering step is enough, there's no need to filter and <em>then</em> redundantly proceed to call a function that does the exact same thing the filtering op from the previous step did.</p>\n<p>To clarify: regardless of the operation, you are still doing the same thing: generating a boolean mask, and then subsequently indexing.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How do these function works? I am using Python3.7 and OpenCv 4.2.0. Thanks in Advance.</p>\n<pre><code>approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt, True), True)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you are looking for a example snippet, below is one:</p>\n<pre><code>import cv2\nimport imutils\n\n# edged is the edge detected image\ncnts = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\ncnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n# loop over the contours\nfor c in cnts:\n    # approximate the contour\n    peri = cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n    # if our approximated contour has four points, then we\n    # can assume that we have found our screen\n    if len(approx) == 4:\n        screenCnt = approx\n        break\n</code></pre>\n<p>In the above snippet, first it finds the contours from a edge detected image, then it sorts the contours to find the five largest contours. Finally it loop over the contours and used <code>cv2.approxPolyDP</code> function to smooth and approximate the quadrilateral. <code>cv2.approxPolyDP</code> works for the cases where there are sharp edges in the contours like a document boundary.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have found what must be dozens of articles on Towards Data Science/ medium/ etc. of people making recommendation engines with imdb data (based on ratings that users gave to movies, what movies should we recommend to those users). \nThese articles begin with 'memory based approaches' of user-based content filtering and item-based content filtering.\nI have been tasked with making a recommendation engine, and since none of the suits <em>really</em> care or know anything about this, I want to do the bare minimum (which seems to be user-based content filtering).</p>\n<p>Problem is, all of my data is binary (no ratings, just based on the items that other users bought, should we recommend items to similar users - this is actually similar to the cartoons that all of the medium articles have stolen from eachother, but none of the medium articles give an example of how to do that).</p>\n<p><strong>All of the articles use Pearson Correlation or cosine similarity to determine user similarity, can I use these approaches with binary dimensions (bought or not), if so how, and if not is there a different way to measure user similarity?</strong></p>\n<p>I am working with python btw. And I was thinking of maybe using Hamming Distance (is there a reason that wouldn't be good)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ul>\n<li>Similarity score based approaches do work even with binary dimension. When you have scores, two similar users may look like [5,3,3,0,1] and [4,3,3,0,0], where as in your case it would be something like [1,1,1,0,1] and [1,1,1,0,0]. </li>\n</ul>\n<pre><code>from scipy.spatial.distance import cosine\n1 - cosine([5,3,2,0,1],[4,3,3,0,0])\n0.961161313666907\n1 - cosine([1,1,1,0,1],[1,1,1,0,0]) \n0.8660254037844386\n</code></pre>\n<ul>\n<li>Another approach is, if you can get the number of times a user bought a product, that count can be used as rating and then similarities can be calculated</li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It seems, that in your situation the best approach would be <a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering\" rel=\"nofollow noreferrer\">collaborative filtering</a>. You don't need scores, everything that you need is a user-item interaction matrix. The simplest algorithm, in this case, is <a href=\"https://en.wikipedia.org/wiki/Matrix_completion#Alternating_least_squares_minimization\" rel=\"nofollow noreferrer\">Alternating Least Square (ALS)</a>.</p>\n<p>There're already a few implementations in python. For instance, <a href=\"https://github.com/benfred/implicit\" rel=\"nofollow noreferrer\">this one</a>. Also, \nthere's an implementation in PySpark <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.recommendation\" rel=\"nofollow noreferrer\">recommendation module</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The data you have is an implicit data which means interactions are not necessarily indicate user's interest it's just interaction. Interaction value of 1 and interaction value of 1000 has no difference in this case they both shows interaction nothing else, such that memory based algorithms are useless here. If you are not familiar with neural networks, then you have to at least use matrix factorization techniques to make a meaningful recommendation using this data, you can start with surprise library \n<a href=\"https://surprise.readthedocs.io/en/stable/\" rel=\"nofollow noreferrer\">here</a> which has a bunch of matrix factorization models.</p>\n<p>It will be better if you use ALS as optimization technique, but SGD will also do the work. If you are ok with deep-learning I can refer to the sources of the best work so far. </p>\n<p>I once used non-negative matrix factorization(NNMF for short) algorithm in surprise for data like yours and the results was good enough.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using Python and I need to split my .csv imported data in two parts, a training and test set, E.G 70% training and 30% test. </p>\n<p>I keep getting various errors, such as <code>'list' object is not callable</code> and so on. </p>\n<p>Is there any easy way of doing this?</p>\n<p>Thanks</p>\n<p>EDIT:</p>\n<p>The code is basic, I'm just looking to split the dataset.</p>\n<pre><code>from csv import reader\nwith open('C:/Dataset.csv', 'r') as f:\n    data = list(reader(f)) #Imports the CSV\n    data[0:1] ( data )\n</code></pre>\n<p><code>TypeError: 'list' object is not callable</code></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <code>pandas</code>:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('C:/Dataset.csv')\ndf['split'] = np.random.randn(df.shape[0], 1)\n\nmsk = np.random.rand(len(df)) &lt;= 0.7\n\ntrain = df[msk]\ntest = df[~msk]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Better practice and maybe more random is to use <code>df.sample</code>:</p>\n<pre><code>from numpy.random import RandomState\nimport pandas as pd\n\ndf = pd.read_csv('C:/Dataset.csv')\nrng = RandomState()\n\ntrain = df.sample(frac=0.7, random_state=rng)\ntest = df.loc[~df.index.isin(train.index)]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should use the <code>read_csv ()</code> function from the pandas module. It reads all your data straight into the dataframe which you can use further to break your data into train and test. Equally, you can use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" rel=\"nofollow noreferrer\"><code>train_test_split()</code></a> function from the scikit-learn module.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am taking my first steps with scikit library and found myself in need of backfilling <strong>only</strong> some columns in my data frame.</p>\n<p>I have read carefully the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\" rel=\"noreferrer\">documentation</a> but I still cannot figure out how to achieve this.</p>\n<p>To make this more specific, let's say I have:</p>\n<pre><code>A = [[7,2,3],[4,np.nan,6],[10,5,np.nan]]\n</code></pre>\n<p>And that I would like to fill in the second column with the mean but <strong>not</strong> the third.  How can I do this with SimpleImputer (or another helper class)?</p>\n<p>An evolution from this, and the natural follow up questions is: how can I fill the second column with the mean and the last column with a constant (only for cells that had no values to begin with, obviously)?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is no need to use the SimpleImputer.<br/>\n<code>DataFrame.fillna()</code> can do the work as well<br/></p>\n<ul>\n<li><p>For the second column, use</p>\n<p><code>column.fillna(column.mean(), inplace=True)</code></p>\n</li>\n<li><p>For the third column, use</p>\n<p><code>column.fillna(constant, inplace=True)</code></p>\n</li>\n</ul>\n<p>Of course, you will need to replace <code>column</code> with your DataFrame's column you want to change and <code>constant</code> with your desired constant.</p>\n<hr/>\n<p><strong>Edit</strong><br/>\nSince the use of <code>inplace</code> is discouraged and will be deprecated, the syntax should be</p>\n<pre><code>column = column.fillna(column.mean())\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Following Dan's advice, an example of using <code>ColumnTransformer</code> and <code>SimpleImputer</code> to backfill the columns is:</p>\n<pre><code>import numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nA = [[7,2,3],[4,np.nan,6],[10,5,np.nan]]\n\ncolumn_trans = ColumnTransformer(\n[('imp_col1', SimpleImputer(strategy='mean'), [1]),\n ('imp_col2', SimpleImputer(strategy='constant', fill_value=29), [2])],\nremainder='passthrough')\n\nprint(column_trans.fit_transform(A)[:, [2,0,1]])\n# [[7 2.0 3]\n#  [4 3.5 6]\n#  [10 5.0 29]]\n</code></pre>\n<p>This approach helps with constructing <a href=\"https://scikit-learn.org/stable/modules/compose.html#\" rel=\"noreferrer\">pipelines</a> which are more suitable for larger applications.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is methode I use, you can replace <code>low_cardinality_cols</code> by cols you want to encode. But this works also justt set value unique to <code>max(df.columns.nunique())</code>.</p>\n<pre><code>#check cardinalité des cols a encoder\nlow_cardinality_cols = [cname for cname in df.columns if df[cname].nunique() &lt; 16 and \n                        df[cname].dtype == \"object\"]\n</code></pre>\n<p>Why thes columns, it's recommanded, to encode only cols with cardinality near 10.</p>\n<pre><code># Replace NaN, if not you'll stuck\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # feel free to use others strategy\ndf[low_cardinality_cols]  = imp.fit_transform(df[low_cardinality_cols])\n\n# Apply label encoder \nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nfor col in low_cardinality_cols:\n    df[col] = label_encoder.fit_transform(df[col])\n    ```\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to get real time predictions using my machine learning model with the help of SageMaker. I want to directly get inferences on my website. How can I use the deployed model for predictions?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sagemaker endpoints are not publicly exposed to the Internet. So, you'll need some way of creating a public HTTP endpoint that can route requests to your Sagemaker endpoint. One way you can do this is with an AWS Lambda function fronted by API gateway. </p>\n<p>I created an example web app that takes webcam images and passes them on to a Sagemaker endpoint for classification. This uses the API Gateway -&gt; Lambda -&gt; Sagemaker endpoint strategy that I described above. You can see the whole example, including instructions for how to set up the Lambda (and the code to put in the lambda) at this GitHub repository: <a href=\"https://github.com/gabehollombe-aws/webcam-sagemaker-inference/\" rel=\"noreferrer\">https://github.com/gabehollombe-aws/webcam-sagemaker-inference/</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use the CLI like this:</p>\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name &lt;endpoint-name&gt; \\\n  --body '{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}' \\\n  --content-type application/json \\\n  --accept application/json \\\n  results\n</code></pre>\n<p>I found it over here in a tutorial about <a href=\"https://aws.amazon.com/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/\" rel=\"nofollow noreferrer\">accessing Sagemaker via API Gateway</a>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can invoke the SageMaker endpoint using API Gateway or Lambda.</p>\n<p><strong>Lambda:</strong></p>\n<p>Use sagemaker aws sdk and invoke the endpoint with lambda.</p>\n<p><strong>API Gateway:</strong></p>\n<p>Use API Gateway and pass parameters to the endpoint with AWS service proxy.</p>\n<p><strong>Documentation with example:</strong></p>\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/\" rel=\"nofollow noreferrer\">https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/</a></p>\n<p>Hope it helps.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've searched the pandas documentation and cookbook recipes and it's clear you can round to the nearest decimal place easily using <code>dataframe.columnName.round(decimalplace)</code>. </p>\n<p>How do you do this with larger numbers?</p>\n<p>Example, I have a column of housing prices and I want them rounded to the nearest 10000 or 1000 or whatever. </p>\n<pre><code>df.SalesPrice.WhatDoIDo(1000)? \n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>By using the notation <code>df.ColumnName.round()</code>, you are actually calling <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.round.html#pandas.Series.round\" rel=\"noreferrer\"><code>pandas.Series.round</code></a>, the documentation of which specifies:</p>\n<blockquote>\n<p>decimals : int</p>\n<p>Number of decimal places to round to (default: 0). If decimals is negative, it specifies the number of positions to the left of the decimal point.</p>\n</blockquote>\n<p>So you can do:</p>\n<pre><code>df = pd.DataFrame({'val':[1,11,130,670]})\ndf.val.round(decimals=-2)\n</code></pre>\n<p>This produces the output:</p>\n<pre><code>0      0\n1      0\n2    100\n3    700\nName: val, dtype: int64\n</code></pre>\n<p><code>decimals=-3</code> rounds to the 1000s, and so on.  Notably, it also works using <code>pandas.DataFrame.round()</code>, though the documentation doesn't tell you:</p>\n<pre><code>df = pd.DataFrame({'val':[1,11,130,670], 'x':[1,11,150,900]})\ndf.round({'val':-2})\n</code></pre>\n<p>This will round the column <code>val</code> to the nearest 100, but leave <code>x</code> alone.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Function <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.round.html\" rel=\"nofollow noreferrer\">round</a> does accept negative values for cases in which you want to specify precision to the left of the decimal point:</p>\n<pre><code>dataframe.columnName.round(-3)\n</code></pre>\n<p>Example:</p>\n<pre><code>&gt;&gt;&gt; pd.Series([1, 500, 500.1, 999, 1500, 1501, 946546]).round(-3)\n0         0.0\n1         0.0\n2      1000.0\n3      1000.0\n4      2000.0\n5      2000.0\n6    947000.0\ndtype: float64\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can try this </p>\n<pre><code>df = pd.DataFrame({'val':[1,11,130,670]})\n10**df.val.astype(str).str.len()\nOut[27]: \n0      10\n1     100\n2    1000\n3    1000\nName: val, dtype: int64\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to a clear picture of how they are interconnected and if the use of one always require the use of the other. If you could give a non-technical definition or explanation of each of them, I would appreciate it.\nPlease do not paste a technical definition of the two. I am not a software engineer or data analyst or data engineer.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>These two paragraphs summarize the difference (from <a href=\"https://medium.com/mlearning-ai/databricks-vs-spark-introduction-comparison-pros-and-cons-35958a1bd7e4#:%7E:text=Spark%20provides%20an%20interface%20similar,and%20share%20Spark%2Dbased%20applications.\" rel=\"noreferrer\">this</a> source) comprehensively:</p>\n<blockquote>\n<p>Spark is a general-purpose cluster computing system that can be used for numerous purposes. Spark provides an interface similar to MapReduce, but allows for more complex operations like queries and iterative algorithms. Databricks is a tool that is built on top of Spark. It allows users to develop, run and share Spark-based applications.</p>\n</blockquote>\n<blockquote>\n<p>Spark is a powerful tool that can be used to analyze and manipulate data. It is an open-source cluster computing framework that is used to process data in a much faster and efficient way. Databricks is a company that uses Apache Spark as a platform to help corporations and businesses accelerate their work. Databricks can be used to create a cluster, to run jobs and to create notebooks. It can be used to share datasets and it can be integrated with other tools and technologies. Databricks is a useful tool that can be used to get things done quickly and efficiently.</p>\n</blockquote>\n<p>In simple words, Databricks has a <em>tool</em> that is built on top of Apache Spark, but it wraps and manipulates it in an intuitive way which is easier for people to use.</p>\n<p>This, in principle, is the same as difference between Hadoop and AWS EMR.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am not able to import <a href=\"http://contrib.scikit-learn.org/categorical-encoding/\" rel=\"noreferrer\">category_encoders</a> module in jupyter notebook in python 3 virtual environment. </p>\n<p>Error </p>\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-15-86725efc8d1e&gt; in &lt;module&gt;()\n      9 from plotly import graph_objs\n     10 from datetime import datetime\n---&gt; 11 import category_encoders as ce\n     12 \n     13 import sklearn\n\nModuleNotFoundError: No module named 'category_encoders'\n</code></pre>\n<p>Output of \"which pip\"</p>\n<pre><code>/opt/virtual_env/py3/bin/pip\n</code></pre>\n<p>Output of \"pip show category_encoders\" is </p>\n<pre><code>Name: category-encoders\nVersion: 1.3.0\nSummary: A collection sklearn transformers to encode categorical variables as numeric\nHome-page: https://github.com/wdm0006/categorical_encoding\nAuthor: Will McGinnis\nAuthor-email: <a class=\"__cf_email__\" data-cfemail=\"384f51545478485d5c59544f4a5d565b505d4a165b5755\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\nLicense: BSD\nLocation: /opt/virtual_env/py3/lib/python3.6/site-packages\nRequires: numpy, pandas, statsmodels, scikit-learn, patsy, scipy\nRequired-by: \n</code></pre>\n<p>Output of \"pip list\"</p>\n<pre><code>Package             Version\n------------------- -------\nabsl-py             0.7.0  \nastor               0.7.1  \nbackcall            0.1.0  \ncategory-encoders   1.3.0  \ncycler              0.10.0 \n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>pip install --upgrade category_encoders</code></p>\n<p>Now,\nRestart your kernel</p>\n<p><code>import category_encoders as...</code> so on and so forth</p>\n<p>Also,\nyou can checkout the sk-learn's preprocessing class CategoricalEncoder...</p>\n<p><code>from sklearn.preprocessing import CategoricalEncoder</code></p>\n<p>get sk-learn from here,</p>\n<p><code>pip install git+git://github.com/scikit-learn/scikit-learn.git</code></p>\n<p>or Simply</p>\n<p><code>pip install sklearn</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was using anaconda environment.</p>\n<p>Initially, I installed <code>category_encoders</code> using pip (within the conda enviornment) and it failed to resolve. Then I uninstalled ``category_encoders` from pip</p>\n<p><code>pip uninstall category-encoders</code></p>\n<p>and installed it using conda:</p>\n<p><code>conda install -c conda-forge category_encoders</code></p>\n<p>That solved the issue.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to decompose a 3D matrix using python library <a href=\"https://github.com/mnick/scikit-tensor\">scikit-tensor</a>. I managed to decompose my Tensor (with dimensions 100x50x5) into three matrices. My question is how can I compose the initial matrix again using the decomposed matrix produced with Tensor factorization? I want to check if the decomposition has any meaning. My code is the following:</p>\n<pre><code>import logging\nfrom scipy.io.matlab import loadmat\nfrom sktensor import dtensor, cp_als\nimport numpy as np\n\n//Set logging to DEBUG to see CP-ALS information\nlogging.basicConfig(level=logging.DEBUG)\nT = np.ones((400, 50))\nT = dtensor(T)\nP, fit, itr, exectimes = cp_als(T, 10, init='random')\n// how can I re-compose the Matrix T? TA = np.dot(P.U[0], P.U[1].T)\n</code></pre>\n<p>I am using the canonical decomposition as provided from the scikit-tensor library function cp_als. Also what is the expected dimensionality of the decomposed matrices?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The CP product of, for example, 4 matrices</p>\n<p><img alt=\"X_{abcd} = \\displaystyle\\sum_{z=0}^{Z}{A_{az} B_{bz} C_{cz} D_{dz} + \\epsilon_{abcd}}\" src=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=X_%7Babcd%7D+%3D+%5Cdisplaystyle%5Csum_%7Bz%3D0%7D%5E%7BZ%7D%7BA_%7Baz%7D+B_%7Bbz%7D+C_%7Bcz%7D+D_%7Bdz%7D%7D+%2B+%5Cepsilon_%7Babcd%7D\"/></p>\n<p>can be expressed using <a href=\"https://en.wikipedia.org/wiki/Einstein_notation\" rel=\"noreferrer\">Einstein notation</a> as</p>\n<p><img alt=\"X_{abcd} = A_{az} B_{bz} C_{cz} D_{dz} + \\epsilon_{abcd}\" src=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=X_%7Babcd%7D+%3D+A_%7Baz%7D+B_%7Bbz%7D+C_%7Bcz%7D+D_%7Bdz%7D+%2B+%5Cepsilon_%7Babcd%7D\"/></p>\n<p>or in numpy as</p>\n<pre><code>numpy.einsum('az,bz,cz,dz -&gt; abcd', A, B, C, D)\n</code></pre>\n<p>so in your case you would use</p>\n<pre><code>numpy.einsum('az,bz-&gt;ab', P.U[0], P.U[1])\n</code></pre>\n<p>or, in your 3-matrix case</p>\n<pre><code>numpy.einsum('az,bz,cz-&gt;abc', P.U[0], P.U[1], P.U[2])\n</code></pre>\n<p><code>sktensor.ktensor.ktensor</code> also have a method <code>totensor()</code> that does exactly this:</p>\n<pre><code>np.allclose(np.einsum('az,bz-&gt;ab', P.U[0], P.U[1]), P.totensor())\n&gt;&gt;&gt; True\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to predict the close price (1 or 0) based on the features present in the 'input_data'. But when I try to run the code I am getting the below error, I am not sure how to fix this. Any help is much appreciated, thanks</p>\n<pre><code>Traceback (most recent call last):\n  File \"F:/Machine Learning/SK_Learn/SVM_Stock.py\", line 71, in &lt;module&gt;\n    estimator.fit(x,y)\n  File \"C:\\Python35\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 210, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File \"C:\\Python35\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 139, in fit\n    **self.filter_sk_params(self.build_fn.__call__))\nTypeError: __call__() missing 1 required positional argument: 'inputs'\n</code></pre>\n<p>Here's the code:</p>\n<pre><code>class SVM_Stock:\n\n    def __init__(self):\n        pass\n\n    def create_model(self):\n\n        model = Sequential()\n        model.add(Dense(14, input_dim=16, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(7, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n        model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n        return model\n\n\nif __name__ == \"__main__\":\n\n    desired_width = 450\n    pd.set_option('display.width', desired_width)\n    pd.set_option('display.max_columns', 17)\n\n    ds = pd.read_csv('F:\\\\Machine Learning\\\\Linear Regression\\\\BIOCON-EQ.csv')\n\n    ds = ds[['Date','Open','High','Low','Close','Volume','Slow VWMA','Fast VWMA']][14:].sort_values('Date')\n\n    ds.loc[ds['Slow VWMA'] &gt; ds['Fast VWMA'], 'Trend UP'] = 1\n    ds.loc[ds['Slow VWMA'] &lt; ds['Fast VWMA'], 'Trend UP'] = 0\n    ds.loc[ds['Slow VWMA'] == ds['Fast VWMA'], 'Trend UP'] = -1\n\n    ds.loc[ds['Slow VWMA'] &lt; ds['Fast VWMA'], 'Trend Down'] = 1\n    ds.loc[ds['Slow VWMA'] &gt; ds['Fast VWMA'], 'Trend Down'] = 0\n    ds.loc[ds['Slow VWMA'] == ds['Fast VWMA'], 'Trend Down'] = -1\n\n    ds.loc[ds['Close'] &gt; ds['Open'], 'Close Price'] = 1\n    ds.loc[ds['Close'] &lt; ds['Open'], 'Close Price'] = 0\n    ds.loc[ds['Close'] == ds['Open'], 'Close Price'] = -1\n\n    input_data = ds[['Date','Open','High','Low','Close','Trend UP', 'Trend \n    Down']]\n    input_data.index = input_data.Date\n    input_data.drop('Date', axis=1, inplace=True)\n    target = ds[['Close Price']]\n\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    x = scaler.fit_transform(input_data)\n    y = target.values.ravel()\n\n    # clf = svm.SVC(gamma=0.1, C=100)\n    # clf.fit(x[:400], y[:400])\n    # print(clf.score(x[:400], y[:400]))\n    #\n    # for i in range(420, len(x)):\n    #     print(\"Prediction :\", clf.predict(x[i].reshape(1, -1)))\n    #     print(i, y[i])\n\n    SS = SVM_Stock()\n    estimator = KerasClassifier(build_fn=SS.create_model(), nb_epoch=10, verbose=0)\n    estimator.fit(x,y)\n\n    '''Cross Validate'''\n    cv_scores = cross_val_score(estimator, x, y, cv=10)\n    print(cv_scores.mean())\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When creating your estimator you should pass the <code>create_model</code> function without calling it (i.e. without brackets):</p>\n<pre><code>estimator = KerasClassifier(build_fn=SS.create_model, nb_epoch=10, verbose=0)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have an example data as:</p>\n<pre><code>datetime             col1    col2    col3\n2021-04-10 01:00:00    25.    50.     50\n2021-04-10 02:00:00.   25.    50.     50\n2021-04-10 03:00:00.   25.    100.    50\n2021-04-10 04:00:00    50.     50.    100\n2021-04-10 05:00:00.   100.    100.   100\n</code></pre>\n<p>I want to create a new column called state, which returns col1 value if col2 and col3 values are  less than or equal to 50 otherwise returns the max value between col1,column2 and column3.</p>\n<p>The expected output is as shown below:</p>\n<pre><code>datetime             col1    col2    col3. state\n2021-04-10 01:00:00    25.    50.     50.   25\n2021-04-10 02:00:00.   25.    50.     50.   25\n2021-04-10 03:00:00.   25.    100.    50.   100\n2021-04-10 04:00:00    50.     50.    100.  100\n2021-04-10 05:00:00.   100.    100.   100.  100\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To improve upon other answer, I would use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\" rel=\"noreferrer\">pandas apply</a> for iterating over rows and calculating new column.</p>\n<pre class=\"lang-py prettyprint-override\"><code>def calc_new_col(row):\n   if row['col2'] &lt;= 50 &amp; row['col3'] &lt;= 50:\n        return row['col1']\n    else:\n        return max(row['col1'], row['col2'], row['col3'])\n\ndf[\"state\"] = df.apply(calc_new_col, axis=1)\n# axis=1 makes sure that function is applied to each row\n\nprint(df)\n            datetime  col1  col2  col3  state\n2021-04-10  01:00:00    25    50    50     25\n2021-04-10  02:00:00    25    50    50     25\n2021-04-10  03:00:00    25   100    50    100\n2021-04-10  04:00:00    50    50   100    100\n2021-04-10  05:00:00   100   100   100    100\n\n</code></pre>\n<p><code>apply</code> helps the code to be cleaner and more reusable.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code># Create a mask:\n\n# Create a mask for the basic condition\nmask1 = ((df['col2'] &lt;= 50) &amp; (df['col3'] &lt;= 50))\n\n# Use loc to select rows where condition is met and input the df['col1'] value in state\ndf.loc[mask1, 'state'] = df['col1']\n\n# Check for rows where condition is not met ~ does that, input the mean in state.\ndf.loc[~mask1, 'state'] = (df['col1'] + df['col2'] + df['col3'])/3\n\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can iterate through the dataframe's rows and check the condition</p>\n<pre><code>values = []\n\nfor ind, row in df.iterrows():\n    if row['col2'] &lt;= 50 &amp; row['col3'] &lt;= 50:\n        values.append(row['col1'])\n    else:\n        values.append(max(row['col1'], row['col2'], row['col3']))\n\ndf['state'] = values\n\nprint(df)\n            datetime  col1  col2  col3  state\n2021-04-10  01:00:00    25    50    50     25\n2021-04-10  02:00:00    25    50    50     25\n2021-04-10  03:00:00    25   100    50    100\n2021-04-10  04:00:00    50    50   100    100\n2021-04-10  05:00:00   100   100   100    100\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In pandas, <strong>axis=0 represent rows</strong> and <strong>axis=1 represent columns</strong>.\nTherefore <strong>to get the sum of values in each row in pandas</strong>, <strong>df.sum(axis=0)</strong> is called.\n<strong>But it returns a sum of values in each columns</strong> and vice-versa. <strong>Why???</strong> </p>\n<pre><code>import pandas as pd\ndf=pd.DataFrame({\"x\":[1,2,3,4,5],\"y\":[2,4,6,8,10]})\ndf.sum(axis=0)\n</code></pre>\n<p>Dataframe:</p>\n<pre><code>   x   y\n0  1   2\n\n1  2   4\n\n2  3   6\n\n3  4   8\n\n4  5  10\n</code></pre>\n<p>Output:</p>\n<pre><code>x    15\n\ny    30\n</code></pre>\n<p>Expected Output:</p>\n<pre><code>0     3\n\n1     6\n\n2     9\n\n3    12\n\n4    15\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think the right way to interpret the <code>axis</code> parameter is what axis you sum 'over' (or 'across'), rather than the 'direction' the sum is computed in. Specifying <code>axis = 0</code> computes the sum over the rows, giving you a total for each column; <code>axis = 1</code> computes the sum across the columns, giving you a total for each row. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was a reading the source code in pandas project, and I think that this come from Numpy, in this library is used in that way(0 sum vertically and 1 horizonally), and additionally Pandas use under the hood numpy in order to make this sum.</p>\n<p>In this <a href=\"https://github.com/pandas-dev/pandas/blob/master/pandas/core/generic.py#L10149\" rel=\"nofollow noreferrer\">link</a> you could check that pandas use <code>numpy.cumsum</code> function to make the sum.\nAnd this <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.cumsum.html\" rel=\"nofollow noreferrer\">link</a> is for numpy documentation.</p>\n<p>If you are looking a way to remember how to use the axis parameter, the 'anant' answer, its a good approach, interpreting the sum <code>over</code> the axis instead <code>across</code>. So when is specified 0 you are computing the sum over the rows(iterating over the index in order to be more pandas doc complaint). When axis is 1 you are iterating over the columns.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Reading the Kubernetes \"Run to Completion\" documentation, it says that jobs can be run in parallel, but is it possible to chain together a series of jobs that should be run in sequential order (parallel and/or non-parallel).</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\" rel=\"noreferrer\">https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/</a></p>\n<hr/>\n<p>Or is it up to the user to keep track of which jobs have finished and triggering the next job using a PubSub messaging service?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have used initContainers under the PodSpec in the past to solve problems like this: <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\" rel=\"noreferrer\">https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</a></p>\n<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n  - name: init-mydb\n    image: busybox\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n</code></pre>\n<p>Take a look here for the chaining of containers using the \"depends\" keyword is also an option:</p>\n<p><a href=\"https://github.com/kubernetes/kubernetes/issues/1996\" rel=\"noreferrer\">https://github.com/kubernetes/kubernetes/issues/1996</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is not possible to manage job workflows with Kubernetes core API objects.</p>\n<ul>\n<li>Argo Workflow looks like an interesting tool to manage workflow inside Kubernetes: <a href=\"https://argoproj.github.io/projects/argo\" rel=\"nofollow noreferrer\">https://argoproj.github.io/projects/argo</a>.\n<ul>\n<li>It looks like it can handle Kubernetes jobs workflow: <a href=\"https://argoproj.github.io/argo/examples/#kubernetes-resources\" rel=\"nofollow noreferrer\">https://argoproj.github.io/argo/examples/#kubernetes-resources</a></li>\n<li>It is in the CNCF incubator: <a href=\"https://www.cncf.io/blog/2020/04/07/toc-welcomes-argo-into-the-cncf-incubator/\" rel=\"nofollow noreferrer\">https://www.cncf.io/blog/2020/04/07/toc-welcomes-argo-into-the-cncf-incubator/</a></li>\n</ul>\n</li>\n</ul>\n<p>Other alternatives include:</p>\n<ul>\n<li><a href=\"https://www.nextflow.io/\" rel=\"nofollow noreferrer\">https://www.nextflow.io/</a></li>\n<li><a href=\"https://www.pachyderm.com/\" rel=\"nofollow noreferrer\">https://www.pachyderm.com/</a></li>\n<li>Airflow: <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/kubernetes.html\" rel=\"nofollow noreferrer\">https://airflow.apache.org/docs/apache-airflow/stable/kubernetes.html</a></li>\n</ul>\n<p>This document might also help: <a href=\"https://www.preprints.org/manuscript/202001.0378/v1/download\" rel=\"nofollow noreferrer\">https://www.preprints.org/manuscript/202001.0378/v1/download</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Overall, no. Check out things like <a href=\"https://airflow.apache.org/\" rel=\"nofollow noreferrer\">Airflow</a> for this. Job objects give you a pretty simple way to run a container until it completes, that's about it. The parallelism is in that you can run multiple copies, it's not a full workflow management system :) </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am looking for some examples which shows the difference between <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.asanyarray.html\" rel=\"noreferrer\"><code>numpy.asanyarray()</code></a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.asarray.html\" rel=\"noreferrer\"><code>numpy.asarray()</code></a>? And at which conditions should I use specifically asanyarray()?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Code for <code>asanyarray</code>:</p>\n<pre><code>return array(a, dtype, copy=False, order=order, subok=True)\n</code></pre>\n<p>for <code>asarray</code>:</p>\n<pre><code>return array(a, dtype, copy=False, order=order)\n</code></pre>\n<p>The only difference is in specifying the <code>subok</code> parameter.  If you are working with subclasses of <code>ndarray</code> you might want to use it.  If you don't know what that means, it probably doesn't matter.</p>\n<p>The defaults for <code>np.array</code> are:</p>\n<pre><code>array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n</code></pre>\n<p>If you are fine tuning a function that is supposed to work with all kinds of numpy arrays (and lists that can be made into arrays), and shouldn't make unnecessary copies, you can use one of these functions.  Otherwise <code>np.array</code>, without or without the extra prameters, works just fine.  As a beginner don't put much effort into understanding these differences.</p>\n<p>===</p>\n<p><code>expand_dims</code> uses both:</p>\n<pre><code>if isinstance(a, matrix):\n    a = asarray(a)\nelse:\n    a = asanyarray(a)\n</code></pre>\n<p>A <code>np.matrix</code> subclass array can only have 2 dimensions, but <code>expand_dims</code> has to change that, so uses <code>asarray</code> to turn the input into a regular <code>ndarray</code>.  Otherwise it uses <code>asanyarray</code>.  That way a subclass like maskedArray remains that class.</p>\n<pre><code>In [158]: np.expand_dims(np.eye(2),1)                                           \nOut[158]: \narray([[[1., 0.]],\n\n       [[0., 1.]]])\nIn [159]: np.expand_dims(np.matrix(np.eye(2)),1)                                \nOut[159]: \narray([[[1., 0.]],\n\n       [[0., 1.]]])\nIn [160]: np.expand_dims(np.ma.masked_array(np.eye(2)),1)                       \nOut[160]: \nmasked_array(\n  data=[[[1., 0.]],\n\n        [[0., 1.]]],\n  mask=False,\n  fill_value=1e+20)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I got confused about computing AUC (area under curve) to evaluate recommendation system result. </p>\n<p>If we have cross validation data like (user, product, rating). How to choose positive sample and negative sample for each user to compute AUC?</p>\n<p>Is it good to choose products occurred for each user in dataset as positive sample, and the rest did not occur in dataset as negative sample? I think this way can not find out those \"real\" negative samples, because user has chance to like those products in negative samples.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>\"A ROC curve plots recall (true positive rate) against fallout (false positive rate) for increasing recommendation set size.\" <a href=\"http://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/Evaluation.pdf\" rel=\"noreferrer\">Schröder, Thiele, and Lehner 2011 (PDF)</a></p>\n<p>In general, you will hold out a portion of your data as testing data. For a particular user, you would train on (for instance) 80% of her data and try to predict which items (out of all items in your dataset) she'll exhibit a preference for based on the remaining 20% of her data.</p>\n<p>Let's say you're building a Top-20 recommender. The 20 items you recommend for a user are the Positive items, and the unrecommended items are Negative. True Positive items are therefore the items that you showed in your Top-N list that match what the user preferred in her held-out testing set. False Positive are the items in your Top-N list that don't match her preferred items in her held-out testing set. True Negative items are those you didn't include in your Top-N recommendations and are items the user didn't have in her preferred items in her held-out testing set. And False Negative are items you didn't include in your Top-N recommendations but do match what the user preferred in her held-out testing set. That's the confusion matrix. Now you can vary the number of items you recommend and calculate the confusion matrix for each, calculate recall and fallout for each, and plot the ROC.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm currently trying to <strong>train a linear model using sklearn in python</strong> but not with mean squared error (MSE) as error measure - but <strong>with mean absolute error (MAE)</strong>. I specificially need a linear model with MAE as requirement from my professor at university.</p>\n<p>I've looked into <strong>sklearn.linear_model.LinearRegression</strong> which since it is an OLS regressor does not provide alternative error measures. </p>\n<p>Hence, I checked the other available regressors and stumbled upon <strong>sklearn.linear_model.HuberRegressor</strong> and <strong>sklearn.linear_model.SGDRegressor</strong>. They both mention MAE as part of their error measures - but do not seem to provide simple MAE. Is there a way to choose the parameters for one of those regressors so that the resulting error measure is a simple MAE? Or is there another regressor in sklearn which I've overlooked? </p>\n<p>Alternatively, is there another (easy to use) python 3.X package which provides what I need?</p>\n<p>Thanks for your help!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In SGD, if you use <code>'epsilon_insensitive'</code> with epsilon=0 it should work as if you used MAE.</p>\n<p>You could also take a look at statsmodels <a href=\"http://www.statsmodels.org/dev/examples/notebooks/generated/quantile_regression.html\" rel=\"noreferrer\">quantile regression</a> (using MAE is also called median regression, and median is a quantile).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to finetune a pre-trained GPT2-model. When applying the respective tokenizer, I originally got the error message:</p>\n<blockquote>\n<p>Using pad_token, but it is not set yet.</p>\n</blockquote>\n<p>Thus, I changed my code to:</p>\n<pre><code>GPT2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nGPT2_tokenizer.pad_token = GPT2_tokenizer.eos_token\n</code></pre>\n<p>When calling the <code>trainer.train()</code> later, I end up with the following error:</p>\n<blockquote>\n<p>AssertionError: Cannot handle batch sizes &gt; 1 if no padding token is\ndefined.</p>\n</blockquote>\n<p>Since I specifically defined the pad_token above, I expect these errors (or rather my fix of the original error and this new error) to be related - although I could be wrong. Is this a known problem that eos_token and pad_token somehow interfer? Is there an easy work-around?</p>\n<p>Thanks a lot!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've been running into a similar problem, producing the same error message you were receiving. I can't be sure if your problem and my problem were caused by the same issue, since I can't see your full stack trace, but I'll post my solution in case it can help you or someone else who comes along.</p>\n<p>You were totally correct to fix the first issue you described with your tokenizer by setting its pad token with the code provided. <strong>However, I also had to set the pad_token_id of my model's configuration to get my GPT2 model to function properly.</strong> I did this in the following way:</p>\n<pre><code># instantiate the configuration for your model, this can be imported from transformers\nconfiguration = GPT2Config()\n# set up your tokenizer, just like you described, and set the pad token\nGPT2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nGPT2_tokenizer.pad_token = GPT2_tokenizer.eos_token\n# instantiate the model\nmodel = GPT2ForSequenceClassification(configuration).from_pretrained(model_name).to(device)\n# set the pad token of the model's configuration\nmodel.config.pad_token_id = model.config.eos_token_id\n</code></pre>\n<p>I suppose this is because the tokenizer and the model function separately, and both need knowledge of the ID being used for the pad token. I can't tell if this will fix your problem (since this post is 6 months old, it may not matter anyway), but hopefully my answer may be able to help someone else.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I know polars does not support index by design, so <code>df.filter(expr).index</code> isn't an option, another way I can think of is by adding a new column before applying any filters, not sure if this is an optimal way for doing so in polars</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.with_columns(pl.Series('index', range(len(df))).filter(expr).index\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <a href=\"https://docs.pola.rs/api/python/dev/reference/dataframe/api/polars.DataFrame.with_row_index.html\" rel=\"nofollow noreferrer\"><code>with_row_index()</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pl.DataFrame([pl.Series(\"a\", [5, 9, 6]), pl.Series(\"b\", [8, 3, 4])])\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>In [20]: df.with_row_index()\nOut[20]: \nshape: (3, 3)\n┌────────┬─────┬─────┐\n│ index  ┆ a   ┆ b   │\n│ ---    ┆ --- ┆ --- │\n│ u32    ┆ i64 ┆ i64 │\n╞════════╪═════╪═════╡\n│ 0      ┆ 5   ┆ 8   │\n│ 1      ┆ 9   ┆ 3   │\n│ 2      ┆ 6   ┆ 4   │\n└────────┴─────┴─────┘\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code># Start from 1 instead of 0.\nIn [21]: df.with_row_index(offset=1)\nOut[21]: \nshape: (3, 3)\n┌────────┬─────┬─────┐\n│ index  ┆ a   ┆ b   │\n│ ---    ┆ --- ┆ --- │\n│ u32    ┆ i64 ┆ i64 │\n╞════════╪═════╪═════╡\n│ 1      ┆ 5   ┆ 8   │\n│ 2      ┆ 9   ┆ 3   │\n│ 3      ┆ 6   ┆ 4   │\n└────────┴─────┴─────┘\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code># Start from 1 and call column \"my_index\".\nIn [22]: df.with_row_index(name=\"my_index\", offset=1)\nOut[22]: \nshape: (3, 3)\n┌──────────┬─────┬─────┐\n│ my_index ┆ a   ┆ b   │\n│ ---      ┆ --- ┆ --- │\n│ u32      ┆ i64 ┆ i64 │\n╞══════════╪═════╪═════╡\n│ 1        ┆ 5   ┆ 8   │\n│ 2        ┆ 9   ┆ 3   │\n│ 3        ┆ 6   ┆ 4   │\n└──────────┴─────┴─────┘\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As of new versions of Polars, <code>with_row_count()</code> is deprecated. Use <code>with_row_index()</code> instead. Following @ghuls answer:</p>\n<pre><code>df.with_row_index(name=\"my_index\", offset=1)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've got a dataframe with a multi index of Year and Month like the following</p>\n<pre><code>     |     |Value\nYear |Month|  \n     |  1  |  3\n1992 |  2  |  5\n     |  3  |  8\n     | ... | ...\n1993 |  1  |  2\n     | ... | ...\n</code></pre>\n<p>I'm trying to select the maximum Value for each year and put that in a DF like this: </p>\n<pre><code>     | Max\nYear |  \n1992 |  5\n1993 |  2\n     | ... \n</code></pre>\n<p>There's not much info on multi-indexes, should I simply do a group by and apply or something similar to make it more simple?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Exactly right:</p>\n<pre><code>df.groupby(level=0).apply(max)\n</code></pre>\n<p>In my sample <code>DataFrame</code>:</p>\n<pre>\n                     0\nCaps Lower            \nA    a     0  0.246490\n           1 -1.265711\n           2 -0.477415\n           3 -0.355812\n           4 -0.724521\n     b     0 -0.409198\n           1 -0.062552\n           2 -0.731789\n           3  1.131616\n           4  0.085248\nB    a     0  0.193948\n           1  2.010710\n           2  0.289300\n           3  0.305373\n           4  1.376965\n     b     0  0.210522\n           1  1.431279\n           2 -0.247171\n           3  0.899074\n           4  0.639926\n</pre>\n<p>Result:</p>\n<pre>\n             0\nCaps          \nA     1.131616\nB     2.010710\n</pre>\n<p>This is how I created the <code>DataFrame</code>, by the way:</p>\n<pre><code>df = pd.DataFrame(np.random.randn(5,4), columns = l)\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps','Lower'])\ndf = pd.DataFrame(df.unstack())\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Simplier solution is <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.max.html\" rel=\"noreferrer\"><code>max</code></a> only:</p>\n<pre><code>#bernie's sample data\ndf = df.max(level=0)\nprint (df)\n             0\nCaps          \nA     1.131616\nB     2.010710\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/48370708/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2018-01-22 06:51:52Z\">6 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/48370708/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Hey I'm new to Pandas and I just came across <code>df.query()</code>.</p>\n<p>Why people would use <code>df.query()</code> when you can directly filter your Dataframes using brackets notation ? The official pandas tutorial also seems to prefer the latter approach.</p>\n<p>With brackets notation :</p>\n<pre><code>df[df['age'] &lt;= 21]\n</code></pre>\n<p>With pandas query method :</p>\n<pre><code>df.query('age &lt;= 21')\n</code></pre>\n<p>Besides some of the stylistic or flexibility differences that have been mentioned, is one canonically preferred - namely for performance of operations on large dataframes?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Consider the following sample DF:</p>\n<pre><code>In [307]: df\nOut[307]:\n  sex  age     name\n0   M   40      Max\n1   F   35     Anna\n2   M   29      Joe\n3   F   18    Maria\n4   F   23  Natalie\n</code></pre>\n<p>There are quite a few good reasons to prefer <code>.query()</code> method.</p>\n<ul>\n<li><p>it might be much shorter and cleaner compared to boolean indexing:</p>\n<pre><code>In [308]: df.query(\"20 &lt;= age &lt;= 30 and sex=='F'\")\nOut[308]:\n  sex  age     name\n4   F   23  Natalie\n\nIn [309]: df[(df['age']&gt;=20) &amp; (df['age']&lt;=30) &amp; (df['sex']=='F')]\nOut[309]:\n  sex  age     name\n4   F   23  Natalie\n</code></pre></li>\n<li><p>you can prepare conditions (queries) programmatically:</p>\n<pre><code>In [315]: conditions = {'name':'Joe', 'sex':'M'}\n\nIn [316]: q = ' and '.join(['{}==\"{}\"'.format(k,v) for k,v in conditions.items()])\n\nIn [317]: q\nOut[317]: 'name==\"Joe\" and sex==\"M\"'\n\nIn [318]: df.query(q)\nOut[318]:\n  sex  age name\n2   M   29  Joe\n</code></pre></li>\n</ul>\n<hr/>\n<p>PS there are also some disadvantages:</p>\n<ul>\n<li>we can't use <code>.query()</code> method for columns containing spaces or columns that consist only from digits</li>\n<li>not all functions can be applied or in some cases we have to use <code>engine='python'</code> instead of default <code>engine='numexpr'</code> (which is faster)</li>\n</ul>\n<p>NOTE: Jeff (one of the main Pandas contributors and a member of Pandas core team) <a href=\"https://github.com/pandas-dev/pandas/issues/6508#issuecomment-283181667\" rel=\"noreferrer\">once said</a>:</p>\n<blockquote>\n<p>Note that in reality .query is just a nice-to-have interface, in fact\n  it has very specific guarantees, meaning its meant to parse like a\n  query language, and not a fully general interface.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Some other interesting usages in the <a href=\"https://pandas.pydata.org/pandas-docs/stable/indexing.html#the-query-method-experimental\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/indexing.html#query-use-cases\" rel=\"nofollow noreferrer\">Reuseable</a></h3>\n<blockquote>\n<p>A use case for query() is <strong>when you have a collection of DataFrame\n  objects that have a subset of column names (or index levels/names) in\n  common.</strong> You can pass the same query to both frames without having to\n  specify which frame you’re interested in querying -- <a href=\"https://pandas.pydata.org/pandas-docs/stable/indexing.html#query-use-cases\" rel=\"nofollow noreferrer\">(Source)</a></p>\n</blockquote>\n<p>Example: </p>\n<pre><code>dfA = pd.DataFrame([[1,2,3], [4,5,6]], columns=[\"X\", \"Y\", \"Z\"])\ndfB = pd.DataFrame([[1,3,3], [4,1,6]], columns=[\"X\", \"Y\", \"Z\"])\nq = \"(X &gt; 3) &amp; (Y &lt; 10)\"\n\nprint(dfA.query(q))\nprint(dfB.query(q))\n\n   X  Y  Z\n1  4  5  6\n   X  Y  Z\n1  4  1  6\n</code></pre>\n<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/indexing.html#query-python-versus-pandas-syntax-comparison\" rel=\"nofollow noreferrer\">More flexible syntax</a></h3>\n<pre><code>df.query('a &lt; b and b &lt; c')  # understand a bit more English\n</code></pre>\n<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/indexing.html#the-in-and-not-in-operators\" rel=\"nofollow noreferrer\">Support <code>in</code> operator and <code>not in</code> (alternative to <code>isin</code>)</a></h3>\n<pre><code>df.query('a in [3, 4, 5]') # select rows whose value of column a is in [2, 3, 4]\n</code></pre>\n<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/indexing.html#special-use-of-the-operator-with-list-objects\" rel=\"nofollow noreferrer\">Special usage of == and != (similar to <code>in</code>/<code>not in</code>)</a></h3>\n<pre><code>df.query('a == [1, 3, 5]') # select whose value of column a is in [1, 3, 5]\n# equivalent to df.query('a in [1, 3, 5]')\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to build a website and deploy it to github pages or heroku. My question is: is it possible to embed a LIVE (where I can run code) Google Colab notebook in the website i'll be hosting?</p>\n<p>I want this embeded colab notebook to execute spark code!!</p>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Even if the answer is a bit late, I came across the same problem and to the conclusion that this is at the moment still not possible.</p>\n<p>There was, however, a feature request opened a while ago <a href=\"https://github.com/googlecolab/colabtools/issues/1225\" rel=\"noreferrer\">here</a>. I think the most you can do is to upvote the issue.</p>\n<h3>Embed without executing</h3>\n<p>There is a half-functioning workaround though...</p>\n<p>You <strong>can</strong> export your already executed notebook as a gist and embed this into your webpage.</p>\n<ol>\n<li>Save Colab as a Gist (Colab -&gt; File -&gt; Save a copy as Github Gist)</li>\n<li>Make the Gist public (Gist -&gt; Edit -&gt; Make public)</li>\n<li>Embed the link into your webpage</li>\n</ol>\n<p>For further instructions, details and screenshots see <a href=\"https://medium.com/@lzhou1110/how-to-embed-google-colaboratory-into-medium-in-3-steps-487b525b103c\" rel=\"noreferrer\">this article</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm preprocessing my data before implementing a machine learning model. Some of the features are with high cardinality, like country and language.</p>\n<p>Since encoding those features as one-hot-vector can produce sparse data, I've decided to look into <a href=\"https://en.wikipedia.org/wiki/Feature_hashing\" rel=\"noreferrer\">the hashing trick</a> and used python's category_encoders like so:</p>\n<pre><code>from category_encoders.hashing import HashingEncoder\nce_hash = HashingEncoder(cols = ['country'])\nencoded = ce_hash.fit_transform(df.country)\nencoded['country'] = df.country\nencoded.head()\n</code></pre>\n<p>When looking at the result, I can see the collisions</p>\n<pre><code>    col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7 country\n0       0      0      1      0      0      0      0      0      US &lt;━┓\n1       0      1      0      0      0      0      0      0      CA.  ┃ US and SE collides \n2       0      0      1      0      0      0      0      0      SE &lt;━┛\n3       0      0      0      0      0      0      1      0      JP\n</code></pre>\n<p>Further investigation lead me to <a href=\"https://www.kaggle.com/discdiver/category-encoders-examples#Hashing\" rel=\"noreferrer\">this Kaggle article</a>. The example of Hashing there include <strong>both X and y</strong>.</p>\n<ul>\n<li>What is the purpose of y, does it help to fight the collision problem?</li>\n<li>Should I add more columns to the encoder and encode more than one feature together (for example country and language)?</li>\n</ul>\n<p>Will appreciate an explanation of how to encode such categories using the hashing trick.</p>\n<p><strong>Update:</strong>\nBased on the comments I got from @CoMartel, Iv'e looked at <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html\" rel=\"noreferrer\">Sklearn FeatureHasher</a> and written the following code to hash the country column:</p>\n<pre><code>from sklearn.feature_extraction import FeatureHasher\nh = FeatureHasher(n_features=10,input_type='string')\nf = h.transform(df.country)\ndf1 = pd.DataFrame(f.toarray())\ndf1['country'] = df.country\ndf1.head()\n</code></pre>\n<p>And got the following output:</p>\n<pre><code>     0    1    2    3    4    5    6    7    8    9 country\n0 -1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0      US\n1 -1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0      US\n2 -1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0      US\n3  0.0 -1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      CA\n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0 -1.0  0.0      SE\n5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      JP\n6 -1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      AU\n7 -1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      AU\n8 -1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0      DK\n9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0 -1.0  0.0      SE\n</code></pre>\n<ul>\n<li>Is that the way to use the library in order to encode high categorical\nvalues?</li>\n<li>Why are some values negative?</li>\n<li>How would you choose the \"right\" <code>n_features</code> value?</li>\n<li>How can I check the collisions ratio?</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>Is that the way to use the library in order to encode high categorical\nvalues?</p>\n</blockquote>\n<p>Yes. There is nothing wrong with your implementation.</p>\n<p>You can think about the hashing trick as a \"<em>reduced size one-hot encoding with a small risk of collision, that you won't need to use if you can tolerate the original feature dimension</em>\".</p>\n<p>This idea was first introduced by <a href=\"http://alex.smola.org/papers/2009/Weinbergeretal09.pdf\" rel=\"noreferrer\">Kilian Weinberger</a>. You can find in their paper the whole analysis of the algorithm theoretically and practically/empirically.</p>\n<hr/>\n<blockquote>\n<p>Why are some values negative?</p>\n</blockquote>\n<p>To avoid collision, a <em><strong>signed</strong></em> hash function is used. That is, the strings are hashed by using the usual <a href=\"https://en.wikipedia.org/wiki/Hash_function\" rel=\"noreferrer\">hash function</a> first (e.g. a string is converted to its corresponding numerical value by summing ASCII value of each char, then modulo <code>n_feature</code> to get an index in (0, <code>n_features</code>]). Then another <em><strong>single-bit output</strong></em> hash function is used. The latter produces <code>+1</code> or <code>-1</code> by definition, where it's added to the index resulted from the first hashing function.</p>\n<p>Pseudo code (it looks like Python, though):</p>\n<pre><code>def hash_trick(features, n_features):\n     for f in features:\n         res = np.zero_like(features)\n         h = usual_hash_function(f) # just the usual hashing\n         index = h % n_features  # find the modulo to get index to place f in res\n         if single_bit_hash_function(f) == 1:  # to reduce collision\n             res[index] += 1\n         else:\n             res[index] -= 1 # &lt;--- this will make values to become negative\n\n     return res \n</code></pre>\n<hr/>\n<blockquote>\n<p>How would you choose the \"right\" n_features value?</p>\n</blockquote>\n<p>As a rule of thumb, and as you can guess, if we hash an extra feature (i.e. #<code>n_feature + 1</code>), the collision is certainly going to happen. Hence, the best case-scenario is when each feature is mapped to a unique hash value -- hopefully. In this case, logically speaking, <code>n_features</code> should be <em><strong>at least</strong></em> equal to the actual number of features/categories (in your particular case, the number of different countries). Nevertheless, please remember that this is the \"best\" case scenario, which is not the case \"mathematically speaking\". Hence, the higher the better <strong>of course</strong>, but how high? See next.</p>\n<hr/>\n<blockquote>\n<p>How can I check the collisions ratio?</p>\n</blockquote>\n<p>If we ignore the second single-bit hash function, the problem is reduced to something called \"Birthday problem for Hashing\".</p>\n<p>This is a big topic. For a comprehensive introduction to this problem, I recommend you read <a href=\"https://en.wikipedia.org/wiki/Birthday_attack\" rel=\"noreferrer\">this</a>, and for some detailed math, I recommend <a href=\"https://crypto.stackexchange.com/a/39644\">this</a> answer.</p>\n<p>In a nutshell, what you need to know is that, the probability of no collisions is <code>exp(-1/2) = 60.65%</code>, that means there is approximately <code>39.35%</code> chance of one collision, at least, to happen.</p>\n<p>So, as a rule of thumb, if we have <code>X</code> countries, there is about <code>40%</code> chance, for at least one collision, if the hash function output range (i.e. <code>n_feature</code> parameter) is <code>X^2</code>. In other words, there is <code>40%</code> chance of collision if the number of countries in your example = <code>square_root(n_features)</code>. As you increase <code>n_features</code> exponentially, the chances of collision is reduced by half. (personally, if it is not for security purposes, but just a plain conversion from string to numbers, it is not worth going too high).</p>\n<p>Side-note for curios readers:  For a large enough hash function output size(e.g. 256 bits), the chances an attacker guess (or avail of) the collision is almost impossible (from a security perspective).</p>\n<hr/>\n<p>Regarding the <code>y</code> parameter, as you've already got in a comment, it is just for compatibility purpose, not used (<code>scikit-learn</code> has this along many other implementations).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe\">Use a list of values to select rows from a Pandas dataframe</a>\n<span class=\"question-originals-answer-count\">\n                                (8 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2023-12-07 18:32:31Z\">10 months ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>I have a dataframe that has a row called \"Hybridization REF\". I would like to filter so that I only get the data for the items that have the same label as one of the items in my list.</p>\n<p>Basically, I'd like to do the following:</p>\n<pre><code>dataframe[dataframe[\"Hybridization REF\"].apply(lambda: x in list)] \n</code></pre>\n<p>but that syntax is not correct.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose \n<code>df</code> is your <code>dataframe</code>,\n<code>lst</code> is our <code>list</code> of labels.</p>\n<pre><code>df.loc[ df.index.isin(lst), : ]\n</code></pre>\n<p>Will display all rows whose index matches any value of the list item. I hope this helps solve your query.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there a numpy dataframe? I am guessing it is pandas dataframe, if so here is the solution.</p>\n<pre><code>df[df['Hybridization REF'].isin(list)]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Update using reindex,</p>\n<pre><code>df.reindex(collist, axis=1)\n</code></pre>\n<p>and</p>\n<pre><code>df.reindex(rowlist, axis=0)\n</code></pre>\n<p>and both:</p>\n<pre><code>df.reindex(index=rowlist, columns=collist)\n</code></pre>\n<hr/>\n<p>You can use .loc or column filtering:</p>\n<pre><code>df = pd.DataFrame(data=np.random.rand(5,5),columns=list('ABCDE'),index=list('abcde'))\n\ndf\n          A         B         C         D         E\na  0.460537  0.174788  0.167554  0.298469  0.630961\nb  0.728094  0.275326  0.405864  0.302588  0.624046\nc  0.953253  0.682038  0.802147  0.105888  0.089966\nd  0.122748  0.954955  0.766184  0.410876  0.527166\ne  0.227185  0.449025  0.703912  0.617826  0.037297\n\ncollist = ['B','D','E']\n\nrowlist = ['a','c']\n</code></pre>\n<p>Get columns in list:</p>\n<pre><code>df[collist]\n</code></pre>\n<p>Output:</p>\n<pre><code>          B         D         E\na  0.174788  0.298469  0.630961\nb  0.275326  0.302588  0.624046\nc  0.682038  0.105888  0.089966\nd  0.954955  0.410876  0.527166\ne  0.449025  0.617826  0.037297\n</code></pre>\n<p>Get rows in list</p>\n<pre><code>df.loc[rowlist]\n\n          A         B         C         D         E\na  0.460537  0.174788  0.167554  0.298469  0.630961\nc  0.953253  0.682038  0.802147  0.105888  0.089966\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/23203638/filter-out-rows-with-more-than-certain-number-of-nan\">Filter out rows with more than certain number of NaN</a>\n<span class=\"question-originals-answer-count\">\n                                (3 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2018-12-18 17:20:47Z\">5 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>I am trying to remove the rows in the data frame with more than 7 null values. Please suggest something that is efficient to achieve this.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If I understand correctly, you need to remove rows only if total nan's in a row is more than <code>7</code>:</p>\n<pre><code>df = df[df.isnull().sum(axis=1) &lt; 7]\n</code></pre>\n<p>This will keep only rows which have <code>nan</code>'s less than 7 in the dataframe, and will remove all having nan's &gt; 7.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>dropna</code> has a <code>thresh</code> argument. Subtract your desired number from the number of columns.</p>\n<blockquote>\n<p>thresh : int, optional Require that many non-NA values.</p>\n</blockquote>\n<pre><code>df.dropna(thresh=df.shape[1]-7, axis=0)\n</code></pre>\n<h3>Sample Data:</h3>\n<pre><code>print(df)\n     0    1     2     3     4     5     6     7\n0  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN\n1  NaN  NaN   NaN   NaN   NaN   NaN   NaN   5.0\n2  6.0  7.0   8.0   9.0   NaN   NaN   NaN   NaN\n3  NaN  NaN  11.0  12.0  13.0  14.0  15.0  16.0\n\ndf.dropna(thresh=df.shape[1]-7, axis=0)\n     0    1     2     3     4     5     6     7\n1  NaN  NaN   NaN   NaN   NaN   NaN   NaN   5.0\n2  6.0  7.0   8.0   9.0   NaN   NaN   NaN   NaN\n3  NaN  NaN  11.0  12.0  13.0  14.0  15.0  16.0\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So I've seen a few answers on here that helped a bit, but my dataset is larger than the ones that have been answered previously. To give a sense of what I'm working with, <a href=\"https://docs.google.com/spreadsheets/d/11Fdg_OywL05MYHwoCEcytEuw9i5Lba9gYsvBEFLO4l8/\" rel=\"noreferrer\">here's a link to the full dataset</a>. I've included a picture of one attempted solution, which was found at this <a href=\"https://stackoverflow.com/questions/48937397/how-to-sort-a-historical-timeline-written-with-python-matplotlib\">link</a>:\n<img alt=\"Example Picture\" src=\"https://i.sstatic.net/7cjB6.png\"/>. </p>\n<p>The issue is that 1. This is difficult to read and 2. I don't know how to flatten it out so that it looks like a traditional timeline. The issue becomes more apparent when I try and work with larger segments, such as this one, which is basically unreadable: \n<img alt=\"It's basically unreadable.\" src=\"https://i.sstatic.net/C3rXs.png\"/>\nHere's the code I used to produce both of these (I just modified the included code in order to change which section of the overall dataset was used).</p>\n<pre><code>event = Xia['EnglishName']\nbegin = Xia['Start']\nend = Xia['Finish']\nlength = Xia['Length']\n\nplt.figure(figsize=(12,6))\nplt.barh(range(len(begin)), (end-begin), .3, left=begin)\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.tick_params(axis='both', which='minor', labelsize=20)\nplt.title('Xia Dynasty', fontsize = '25')\nplt.xlabel('Year', fontsize = '20')\nplt.yticks(range(len(begin)), \"\")\nplt.xlim(-2250, -1750)\nplt.ylim(-1,18)\nfor i in range(18):\n    plt.text(begin.iloc[i] + length.iloc[i]/2, i+.25, event.iloc[i], ha='center', fontsize = '12') \n</code></pre>\n<p>This code semi-works, but I'd prefer if the bars were either closer together or differently colored and all on the same y-value. I appreciate any and all help. I've been trying to figure this out for about two weeks now and am hitting a brick wall.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't know whether you already resolved this problem or not, but, from what I have seen so far from your code and (also borrowing from Evgeny's code) your requirements, the only reason you have the different levels of horizontal bars because you have defined the scalar <em>y</em> of the barh of matplotlib (<code>matplotlib.pyplot.barh(y, width, height=0.8, left=None, *, align='center', **kwargs</code>) as a range. So, each successive stacked bar is being listed on a separate level.</p>\n<p>So, I took the liberty of downloading your dataset and playing around with the code a little bit.</p>\n<p>I created a dataframe from the google dataset and assigned each of the Dynasty (Dynasty_col column) and Age (Age_col column) with a matplotlib CSS color (this is not necessary, but, I find this easier to manage for visualisation):\n<a href=\"https://i.sstatic.net/v1RO7.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/v1RO7.png\"/></a></p>\n<p>Then for the purpose of replicating your Xia Dynasty representation, I just created a subset:\n<a href=\"https://i.sstatic.net/zNynv.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zNynv.png\"/></a></p>\n<p>Following that I kept mostly to what your/Evgeny's code already shows with a few minor changes:</p>\n<pre><code>event = data_set_xia['EnglishName']\nbegin = data_set_xia['Start']\nend = data_set_xia['Finish']\nlength =  data_set_xia['Length']\n</code></pre>\n<p>Here I added a level for naming with a vertical line (you can lengthen or shorten the array [-2, 2, -1, 1] to get different levels of labelling):</p>\n<pre><code>levels = np.tile([-2, 2, -1, 1],\n                 int(np.ceil(len(begin)/4)))[:len(begin)]\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(12,6))\n</code></pre>\n<p>Here I basically add all of the dynasties on the same <em>y</em> scalar (listed as 0), the rest of the line has been modified to correspond to the color of the bars and give an edgecolour.</p>\n<pre><code>plt.barh(0, (end-begin), color=data_set_xia.loc[:,\"Dynasty_col\"], height =0.3 ,left=begin, edgecolor = \"black\")\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.tick_params(axis='both', which='minor', labelsize=20)\nplt.title('Xia Dynasty', fontsize = '25')\nplt.xlabel('Year', fontsize = '20')\n# plt.yticks(range(len(begin)), \"\")\nax = plt.gca()\nax.axes.yaxis.set_visible(False)\nplt.xlim(-2250, -1700)\nplt.ylim(-5,5)\n</code></pre>\n<p>I played around a bit with vertical lines for labels and the labels were associated with the levels to create the plot.</p>\n<pre><code>plt.vlines(begin+length/2, 0, levels, color=\"tab:red\")\nfor i in range(18):\n    plt.text(begin.iloc[i] + length.iloc[i]/2, \n             levels[i]*1.3, event.iloc[i], \n             ha='center', fontsize = '12')\n\nplt.tight_layout()\nplt.show()\n</code></pre>\n<p>This resulted in the following graphs for the Xia dynasty:\n<a href=\"https://i.sstatic.net/MKpIT.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/MKpIT.png\"/></a></p>\n<p>And using a bigger subset, I could generate this other graph too:\n<a href=\"https://i.sstatic.net/M3Sge.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/M3Sge.png\"/></a>\nand\n<a href=\"https://i.sstatic.net/nAuVz.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/nAuVz.png\"/></a></p>\n<p>Now obviously, the longer the number of entries are, the busier and the more cluttered the graphs become and it starts looking a bit ugly, but it is still legible. Also, the code is not \"perfect\", I would clean it up a bit and change some command options like the <em>color</em> in the arguments in barh, but it works for now.</p>\n<p>For an alternate representation, I am adding the code of staggered representation of the different dynasties by time, as some of the dynasties overlap with each other:</p>\n<pre><code>event = data_set_adj['EnglishName']\nbegin = data_set_adj['Start']\nend = data_set_adj['Finish']\nlength =  data_set_adj['Length']\ndynasty = data_set_adj['Dynasty']\ndynasty_col = data_set_adj['Dynasty_col']\n\ndict_dynasty = dict(zip(dynasty.unique(), range(0,4*len(dynasty.unique()),4)))\n\nlevels = np.tile([-1.2,1.2, -0.8, 0.8, -0.4, 0.4],\n                 int(np.ceil(len(begin)/6)))[:len(begin)]\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(20,10))\n\nfor x in range(len(dynasty)):   \n    plt.vlines(begin.iloc[x]+length.iloc[x]/2, dict_dynasty[dynasty.iloc[x]], dict_dynasty[dynasty.iloc[x]]+levels[x], color=\"tab:red\")\n    plt.barh(dict_dynasty[dynasty.iloc[x]], (end.iloc[x]-begin.iloc[x]), color=dynasty_col.iloc[x], height =0.3 ,left=begin.iloc[x], edgecolor = \"black\", alpha = 0.5)\n    if x%2==0:\n        plt.text(begin.iloc[x] + length.iloc[x]/2, \n                 dict_dynasty[dynasty.iloc[x]]+1.6*levels[x], event.iloc[x], \n                 ha='center', fontsize = '8')\n    else:\n        plt.text(begin.iloc[x] + length.iloc[x]/2, \n                 dict_dynasty[dynasty.iloc[x]]+1.25*levels[x], event.iloc[x], \n                 ha='center', fontsize = '8')\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.tick_params(axis='both', which='minor', labelsize=20)\nplt.title('Chinese Dynasties', fontsize = '25')\nplt.xlabel('Year', fontsize = '20')\nax = plt.gca()\nax.axes.yaxis.set_visible(False)\nplt.xlim(900, 1915)\nplt.ylim(-4,28)\n\n\nplt.tight_layout()\nplt.show()\n</code></pre>\n<p>This last part was done hastily, so the code is not the neatest, but the only thing I changed here was update the <em>y</em> scalar of barh based on the dynasties in the data sub-set that I am considering. I have modified the levels and the fontsize for readability, you can play around with the numbers and the code to get the appropriate representations.</p>\n<p>This results in the following representation:\n<a href=\"https://i.sstatic.net/cPNXC.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/cPNXC.png\"/></a></p>\n<p>Also, as I added the Age_col column, you could categorise the whole thing as Pre-Imperial and Imperial (red or blue). I didn't attach any graphs with that for now, but that works if you add a patch of that colour with a different \"zorder\" around the dynasties.</p>\n<p>For zoomable and pannable graphing, I guess using bokeh or some other similar library for plotting would be better, that way, you can keep it uncluttered and also focus on the parts that make sense?</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Something I did similar charting for a little sitcom succession diagram. The code is a bit naive (<a href=\"https://github.com/epogrebnyak/hrange/blob/master/hrange.py\" rel=\"nofollow noreferrer\">placed on github</a>), but on encountering your question I was surprised this is still a problem for people doing similar visualisation. I was hoping there might be specialised library for historic charts. </p>\n<p><a href=\"https://i.sstatic.net/gSaCW.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/gSaCW.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is the code to replicate the original plot, something like this is expected in the question, would allow more time to answer the problem (as opposed to re-creating it).</p>\n<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\nxia = pd.DataFrame([['Da Yu', -2207, -2197], \n                    ['Qi', -2197, -2188], \n                    ['Tai Kang', -2188, -2159]], \n                    columns=['EnglishName', 'Start', 'Finish']) \nevent = xia['EnglishName']\nbegin = xia['Start']\nend = xia['Finish']\nlength =  xia['Finish'] - xia['Start']\n\n\nplt.figure(figsize=(12,6))\nplt.barh(range(len(begin)), (end-begin), .3, left=begin)\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.tick_params(axis='both', which='minor', labelsize=20)\nplt.title('Xia Dynasty', fontsize = '25')\nplt.xlabel('Year', fontsize = '20')\nplt.yticks(range(len(begin)), \"\")\nplt.xlim(-2250, -1750)\nplt.ylim(-1,18)\nfor i in range(3):\n    plt.text(begin.iloc[i] + length.iloc[i]/2, \n             i+.25, event.iloc[i], \n             ha='center', fontsize = '12')\n</code></pre>\n<p>Grievances (to settle what to do next): </p>\n<blockquote>\n<p>The issue is that \n  1. This is difficult to read and \n  2. I don't know how to flatten it out so that it looks like a traditional timeline. The issue becomes more apparent when I try and work with larger segments </p>\n</blockquote>\n<p><a href=\"https://i.sstatic.net/C3rXs.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/C3rXs.png\"/></a></p>\n<blockquote>\n<p>I'd prefer if the bars were either closer together or differently colored and all on the same y-value. </p>\n</blockquote>\n<p>The designs are rather difficult to specifiy in words. If you put on a single line (eg with this <code>plt.barh([1 for _ in begin], (end-begin)-0.5, .3, left=begin)</code>) the text would be overlapping even more unreadable. </p>\n<p>Here is a small code to reproduce the horizontal timeline:</p>\n<pre><code>plt.figure(figsize=(4,2))\nplt.ylim(0.5, 1.5)\nplt.yticks(range(len(begin)), \"\")\n# 0.25 is a stub, it controls for white separator\nplt.barh([1 for _ in begin], (end-begin)-0.25, .3, left=begin)\n</code></pre>\n<p>But where would you expect to place the names?</p>\n<p>If you want different colors, need some rule about what the colors are. A programmer would have said you need a better specification for the task.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Working with census data, I want to replace NaNs in two columns (\"workclass\" and \"native-country\") with the respective modes of those two columns. I can get the modes easily:</p>\n<pre><code>mode = df.filter([\"workclass\", \"native-country\"]).mode()\n</code></pre>\n<p>which returns a dataframe:</p>\n<pre><code>  workclass native-country\n0   Private  United-States\n</code></pre>\n<p>However, </p>\n<pre><code>df.filter([\"workclass\", \"native-country\"]).fillna(mode)\n</code></pre>\n<p>does <strong>not</strong> replace the NaNs in each column with anything, let alone the mode corresponding to that column. Is there a smooth way to do this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you want to impute missing values with the <code>mode</code> in some columns a dataframe <code>df</code>, you can just <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\" rel=\"noreferrer\"><code>fillna</code></a> by <code>Series</code> created by select by position by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html\" rel=\"noreferrer\"><code>iloc</code></a>:</p>\n<pre><code>cols = [\"workclass\", \"native-country\"]\ndf[cols]=df[cols].fillna(df.mode().iloc[0])\n</code></pre>\n<p>Or:</p>\n<pre><code>df[cols]=df[cols].fillna(mode.iloc[0])\n</code></pre>\n<p>Your solution:</p>\n<pre><code>df[cols]=df.filter(cols).fillna(mode.iloc[0])\n</code></pre>\n<p>Sample:</p>\n<pre><code>df = pd.DataFrame({'workclass':['Private','Private',np.nan, 'another', np.nan],\n                   'native-country':['United-States',np.nan,'Canada',np.nan,'United-States'],\n                   'col':[2,3,7,8,9]})\n\nprint (df)\n   col native-country workclass\n0    2  United-States   Private\n1    3            NaN   Private\n2    7         Canada       NaN\n3    8            NaN   another\n4    9  United-States       NaN\n\nmode = df.filter([\"workclass\", \"native-country\"]).mode()\nprint (mode)\n  workclass native-country\n0   Private  United-States\n\ncols = [\"workclass\", \"native-country\"]\ndf[cols]=df[cols].fillna(df.mode().iloc[0])\nprint (df)\n   col native-country workclass\n0    2  United-States   Private\n1    3  United-States   Private\n2    7         Canada   Private\n3    8  United-States   another\n4    9  United-States   Private\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can do it like that:</p>\n<pre><code>df[[\"workclass\", \"native-country\"]]=df[[\"workclass\", \"native-country\"]].fillna(value=mode.iloc[0])\n</code></pre>\n<p>For example, </p>\n<pre><code>    import pandas as pd\nd={\n    'key3': [1,4,4,4,5],\n    'key2': [6,6,4],\n    'key1': [6,4,4],\n}\n\ndf=pd.DataFrame.from_dict(d,orient='index').transpose()\n</code></pre>\n<p>Then <code>df</code> is </p>\n<pre><code>  key3  key2    key1\n0   1   6       6\n1   4   6       4\n2   4   4       4\n3   4   NaN     NaN\n4   5   NaN     NaN\n</code></pre>\n<p>Then by doing:</p>\n<pre><code>l=df.filter([\"key1\", \"key2\"]).mode()\ndf[[\"key1\", \"key2\"]]=df[[\"key1\", \"key2\"]].fillna(value=l.iloc[0])\n</code></pre>\n<p>we get that <code>df</code> is </p>\n<pre><code>  key3  key2    key1\n0   1   6        6\n1   4   6        4\n2   4   4        4\n3   4   6        4\n4   5   6        4\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to convert images to tensor using <code>torchvision.transforms.ToTensor()</code>. After processing, I printed the image but the image was not right. Here is my code:</p>\n<pre><code>trans = transforms.Compose([\n    transforms.ToTensor()])\n\ndemo = Image.open(img) \ndemo_img = trans(demo)\ndemo_array = demo_img.numpy()*255\nprint(Image.fromarray(demo_array.astype(np.uint8)))\n</code></pre>\n<p>The original image is:</p>\n<p><img alt=\"original image\" src=\"https://i.sstatic.net/U2aiK.jpg\"/></p>\n<p>After processing, it looks like:</p>\n<p><img alt=\"after processing\" src=\"https://i.sstatic.net/XnYAt.png\"/></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It seems that the problem is with the channel axis.</p>\n<p>If you look at <code>torchvision.transforms</code> docs, especially on <a href=\"https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html#ToTensor\" rel=\"noreferrer\"><code>ToTensor()</code></a></p>\n<blockquote>\n<p>Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n[0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]</p>\n</blockquote>\n<p>So once you perform the transformation and return to <code>numpy.array</code> your shape is: (C, H, W) and you should change the positions, you can do the following:</p>\n<pre><code>demo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)\n</code></pre>\n<p>This will transform the array to shape (H, W, C) and then when you return to PIL and show it will be the same image.</p>\n<p>So in total:</p>\n<pre><code>import numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\ntrans = transforms.Compose([transforms.ToTensor()])\n\ndemo = Image.open(img) \ndemo_img = trans(demo)\ndemo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)\nprint(Image.fromarray(demo_array.astype(np.uint8)))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nimport findspark\nfrom pyspark.sql.functions import countDistinct\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName(\"usres mobile related information analysis\") \\\n.config(\"spark.submit.deployMode\", \"client\") \\\n.config(\"spark.executor.memory\",\"3g\") \\\n.config(\"spark.driver.maxResultSize\", \"1g\") \\\n.config(\"spark.executor.pyspark.memory\",\"3g\") \\\n.enableHiveSupport() \\\n.getOrCreate()\n</code></pre>\n<p><code>handset_info =</code> <code>ora_tmp.select('some_value','some_value','some_value','some_value','some_value','some_value','some_value')</code></p>\n<p>I configure the spark with 3gb execution memory and 3gb execution pyspark memory.My Database has more than 70 Million row. Show i call the  </p>\n<pre><code> handset_info.show()\n</code></pre>\n<p>method it is showing the top 20 row in between 2-5 second. But when i try to run the following code </p>\n<pre><code>mobile_info_df = handset_info.limit(30)\nmobile_info_df.show()\n</code></pre>\n<p>to show the top 30 rows the it takes too much time(3-4 hour). Is it logical to take that much time. Is there any problem in my configuration.\nConfiguration of my laptop is-</p>\n<ul>\n<li>Core i7(4 core) laptop with 8gb ram</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Spark copies the parameter you passed to <code>limit()</code> to each partition so, in your case, it tries to read 30 rows per partition. I guess you happened to have a huge number of partitions (which is not good in any case). Try <code>df.coalesce(1).limit(30).show()</code> and it should run as fast as <code>df.show()</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your configuration is fine. This huge duration difference is caused by underlying implementation. The difference is that limit() reads all of the 70 million rows before it creates a dataframe with 30 rows. Show() in contrast just takes the first 20 rows of the existing dataframe and has therefore only to read this 20 rows. \nIn case you are just interessted in showing 30 instead of 20 rows, you can call the show() method with 30 as parameter:</p>\n<pre><code>df.show(30, truncate=False)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As you've already experienced, limit() with large data has just terrible performance. Wanted to share a workaround for anyone else with this problem.\nIf the limit count doesn't have to be exact, use sort() or orderBy() to sort a column, and use filter() to grab top k% of the rows.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I plotted data on a barplot using seaborn library. But on the top of the bars, I can see some black lines. Can someone explain me what does it mean?</p>\n<p>Note : the last bar does not have this line as there is only one entry for that case.\n<a href=\"https://i.sstatic.net/JMqGV.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/JMqGV.png\"/></a></p>\n<p><a href=\"https://i.sstatic.net/cNu27.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/cNu27.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is the error bar. </p>\n<blockquote>\n<p>Error bars are graphical representations of the variability of data and used on graphs to indicate the error or uncertainty in a reported measurement.</p>\n</blockquote>\n<p>In your data, it is likely the <a href=\"https://en.wikipedia.org/wiki/Standard_deviation\" rel=\"noreferrer\">Standard Deviation</a> or <code>STD</code> line. </p>\n<p>See <a href=\"https://stackoverflow.com/questions/35193996/what-is-the-overlay-line-on-each-bar-in-a-bar-chart\">here</a>\nAnd <a href=\"https://en.wikipedia.org/wiki/Error_bar\" rel=\"noreferrer\">here</a></p>\n<p><strong>EDIT: ====================</strong></p>\n<p>In response to <code>Petr Novotný</code> - I think you're right. It may br the 95% confidence interval:</p>\n<p>From the <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html\" rel=\"noreferrer\">Documentation</a></p>\n<blockquote>\n<p>ci : float or “sd” or None, optional</p>\n<p>Size of confidence intervals to draw around estimated values. If “sd”, skip bootstrapping and draw the standard deviation of the observations. If None, no bootstrapping will be performed, and error bars will not be drawn.</p>\n</blockquote>\n<p>The difference between Standard Deviation and confidence interval is a subtle one. </p>\n<p><a href=\"https://www.investopedia.com/ask/answers/042415/what-difference-between-standard-error-means-and-standard-deviation.asp\" rel=\"noreferrer\">difference-between-standard-error-means-and-standard-deviation</a></p>\n<p>A confidence interval is something slightly different...\n<a href=\"https://www.graphpad.com/guides/prism/7/statistics/stat_more_about_confidence_interval.htm?toc=0&amp;printWindow\" rel=\"noreferrer\">95% confidence interval</a></p>\n<p>If the parameter \"ci\" is not passed, I <em>believe</em> seaborn bootstraps the STD into a <code>Confidence Interval</code>. By what method I don't know.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is more of a statistics question. They are error bars. They can display either confidence intervals or the standard deviation. The bar plot shows an aggregation of some values. The error bar shows how spread out the original non-aggregated data was (simplified!). That is also the reason why no bar shows up when there was only one value. I wil point you towards the <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html\" rel=\"nofollow noreferrer\">seaborn documentation</a>. Pay extra attention to the \"estimator\" and \"ci\" parameters to start with. From there I enourage you to do your own research.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to load a serialized xgboost model from a pickle file.</p>\n<pre><code>import pickle\ndef load_pkl(fname):\n    with open(fname, 'rb') as f:\n        obj = pickle.load(f)\n    return obj\n\nmodel = load_pkl('model_0_unrestricted.pkl')\n</code></pre>\n<p>while printing the model object, I am getting the following error in linux(AWS Sagemaker Notebook)</p>\n<pre><code>    ~/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py in get_params(self, deep)\n    436             if k == 'type' and type(self).__name__ != v:\n    437                 msg = 'Current model type: {}, '.format(type(self).__name__) + \\\n--&gt; 438                       'type of model in file: {}'.format(v)\n    439                 raise TypeError(msg)\n    440             if k == 'type':\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/base.py in get_params(self, deep)\n    193         out = dict()\n    194         for key in self._get_param_names():\n--&gt; 195             value = getattr(self, key)\n    196             if deep and hasattr(value, 'get_params'):\n    197                 deep_items = value.get_params().items()\n\nAttributeError: 'XGBClassifier' object has no attribute 'use_label_encoder'\n</code></pre>\n<p>Can you please help to fix the issue?</p>\n<p>It is working fine in my local mac.</p>\n<p>Ref: xgboost:1.4.1 installation log (Mac)</p>\n<pre><code>    Collecting xgboost\n  Downloading xgboost-1.4.1-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n     \n</code></pre>\n<p>But not working on AWS</p>\n<p>Ref: xgboost:1.4.1 installation log (SM Notebook, linux machine)</p>\n<pre><code>Collecting xgboost\n  Using cached xgboost-1.4.1-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n</code></pre>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Looks like you upgraded xgboost.\nYou may consider downgrading to 1.2.0 by:</p>\n<pre><code>pip install xgboost==1.2.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I tried testing on notebook running on ubuntu, it seems to work fine, however can you check how are you initializing your classifier ? This is what I tried :</p>\n<pre><code>import numpy as np\nimport pickle\nfrom scipy.stats import uniform, randint\n\nfrom sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold,RandomizedSearchCV, train_test_split\n\nimport xgboost as xgb\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=45)\nxgb_model.fit(X, y)\npickle.dump(xgb_model, open(\"xgb_model.pkl\", \"wb\"))\n</code></pre>\n<p>Load the model back using your function and output it :</p>\n<pre><code>def load_pkl(fname):\n    with open(fname, 'rb') as f:\n        obj = pickle.load(f)\n    return obj\n\nmodel = load_pkl('xgb_model.pkl')\nmodel\n</code></pre>\n<p>Below is the output :</p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n          colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n          importance_type='gain', interaction_constraints='',\n          learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n          min_child_weight=1, missing=nan, monotone_constraints='()',\n          n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=45,\n          reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n          tree_method='exact', validate_parameters=1, verbosity=None)\n</code></pre>\n<p>​</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I suspect the pickled model you are loading in was modified in someway to have that additional method prior to being saved. Either that or as @vbhatt said, you may be modifying some aspect of your classifier prior to loading it in. This has happened to me before when using custom models in Pytorch Lightning.</p>\n<p>If you haven't modified the base model at all, please ensure that you are using the same version from within the notebook as well, could be the venv in the notebook has a different version?</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Consider we only have images as.npy file. Is it possible to resizing images  without converting their to images (because I'm looking for a way that is fast when run the code).\nfor more info, I asked the way without converting to image, I have images but i don't want use those in code, because my dataset is too large and running with images is so slow, on the other hand, Im not sure which size is better for my imeges, So Im looking for a way that first convert images to npy and save .npy file and then preprocess npy file, for example resize the dimension of images. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try <a href=\"https://pillow.readthedocs.io/en/stable/\" rel=\"noreferrer\"><code>PIL</code></a>, maybe it's fast enough for you.</p>\n<pre><code>import numpy as np\nfrom PIL import Image\n\narr = np.load('img.npy')\nimg = Image.fromarray(arr)\nimg.resize(size=(100, 100))\n</code></pre>\n<p>Note that you have to compute the aspect ratio if you want to keep it. Or you can use <a href=\"https://pillow.readthedocs.io/en/stable/reference/Image.html#create-thumbnails\" rel=\"noreferrer\"><code>Image.thumbnail()</code></a>, which can take an antialias filter.</p>\n<p>There's also <a href=\"https://scikit-image.org/\" rel=\"noreferrer\"><code>scikit-image</code></a>, but I suspect it's using <code>PIL</code> under the hood. It works on NumPy arrays:</p>\n<pre><code>import skimage.transform as st\n\nst.resize(arr, (100, 100))\n</code></pre>\n<p>I guess the other option is <a href=\"https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#cv2.resize\" rel=\"noreferrer\">OpenCV</a>. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you are only dealing with <code>numpy</code> arrays, I think <code>slicing</code> would be enough</p>\n<p>Say, the shape of the loaded <code>numpy</code> array is <code>(m, n)</code> (one channel), and the target shape is <code>(a, b)</code>. Then, the stride can be <code>(s1, s2) = (m // a, n // b)</code></p>\n<p>So the original array can be sliced by </p>\n<p><code>new_array = old_array[::s1, ::s2]</code></p>\n<p><strong>EDIT</strong></p>\n<p>To scale up an array is also quite straight forward if you use masks for advanced slicing. For example, the shape of the original array is <code>(m, n)</code>, and the target shape is <code>(a, b)</code>. Then, as an example</p>\n<pre><code>a, b = 300, 200\nm, n = 3, 4\n\noriginal = np.linspace(1, 12, 12).reshape(3, 4)\ncanvas = np.zeros((a, b))\n(s1, s2) = (a // m, b // n) # the scalar\n\n# the two masks  \nmask_x = np.concatenate([np.ones(s1) * ind for ind in range(m)])\nmask_y = np.concatenate([np.ones(s2) * ind for ind in range(n)])\n\n# make sure the residuals are taken into account\nif len(mask_x) &lt; a: mask_x = np.concatenate([mask_x, np.ones(len(mask_x) % a) * (m - 1)])\nif len(mask_y) &lt; b: mask_y = np.concatenate([mask_y, np.ones(len(mask_y) % b) * (n - 1)])\nmask_x = mask_x.astype(np.int8).tolist()\nmask_y = mask_y.astype(np.int8).tolist()\n\ncanvas = original[mask_x, :]\ncanvas = canvas[:, mask_y]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a daily time series dataset that I am using Python SARIMAX method to predict for future. But I do not know how to write codes in python that accounts for multiple seasonalities. As far as I know, SARIMAX takes care of only one seasonality but I want to check for weekly, monthly, and quarterly seasonalities. I know to capture day of the week seasonality, I should create 6 dummy variables, To capture day of the month seasonality, create 30 dummy variables, and To capture month of the year, create 11 dummy variables. But I don't know how to incorporate it with the main SARIMAX function in Python? I mean SARIMAX is just a function that does the autoregressive, moving average and the differencing parts but how should I include multiple seasonality codes in my time series analysis with SARIMAX? So far, I know how to create dummy variables for each category but don't know how to replicate it to the entire dataset? After that I don't know how to write Python codes that do SARIMAX and captures multiple seasonalities at the same time.</p>\n<p>I am in need of help for Python code that can do it.</p>\n<p>Please advise accordingly</p>\n<p>Regards</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, SARIMA model is designed for dealing with a single seasonality. </p>\n<ul>\n<li><p>To make it work for multiple seasonality, it is possible to apply a\nmethod called Fourier terms. </p></li>\n<li><p>Secondly, there is a better method for time series data with multiple\nseasonality effects which is called TBABS. Here is an example that\nincludes codes and explanation for both approaches:\n<a href=\"https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a\" rel=\"noreferrer\">https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a</a></p></li>\n<li><p>Thirdly, you can check <a href=\"https://facebook.github.io/prophet/\" rel=\"noreferrer\">https://facebook.github.io/prophet/</a> which also\nbrings up an easier way to this issue.</p></li>\n<li><p>For a deeper research, you can always google terms \"time series\" with \"multiple seasonality\" or \"multiple seasonal effect\"</p></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One option for handling multiple seasonalities in python is the Multiple Seasonal-Trend decomposition using LOESS (MSTL) functionality from the <a href=\"https://www.statsmodels.org/\" rel=\"nofollow noreferrer\">statsmodels</a> package.  Both <a href=\"https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.MSTL.html\" rel=\"nofollow noreferrer\">MSTL function documentation</a> and an <a href=\"https://www.statsmodels.org/dev/examples/notebooks/generated/mstl_decomposition.html\" rel=\"nofollow noreferrer\">MSTL decomposition notebook</a> are provided.</p>\n<p>Some example code showing decomposition of daily and weekly seasonalities:</p>\n<pre><code>from statsmodels.tsa.seasonal import MSTL\n\n\nres = MSTL(data, periods=(24, 24 * 7)).fit()\nres.plot()\nplt.tight_layout()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/DUpAU.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/DUpAU.png\"/></a></p>\n<p>The <code>res</code> object can now be used for forecasting.  I believe the <code>MSTL</code> implementations are currently limited to additive seasonalities.  There is no support for exogenous regressors.</p>\n<p>The statsmodels <code>MSTL</code> function is based on the <a href=\"https://robjhyndman.com/publications/mstl/\" rel=\"nofollow noreferrer\">work by Rob Hyndman and co.</a> available in the R <a href=\"https://pkg.robjhyndman.com/forecast/reference/mstl.html\" rel=\"nofollow noreferrer\">forecast</a> package.  It uses the <a href=\"https://en.wikipedia.org/wiki/Local_regression\" rel=\"nofollow noreferrer\">LOESS (locally estimated scatterplot smoothing)</a> method.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am training a tensorflow keras sequential model on around 20+ GB <strong>text based categorical data in a postgres db</strong> and i need to give class weights to the model.\nHere is what i am doing.</p>\n<pre><code>class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes, y)\n\nmodel.fit(x, y, epochs=100, batch_size=32, class_weight=class_weights, validation_split=0.2, callbacks=[early_stopping])\n</code></pre>\n<p>Since i can't load the whole thing in memory i figured i can use <strong>fit_generator</strong> method in keras model. </p>\n<p>However how can i <strong>calculate the class weights</strong> on this data? <strong>sklearn</strong> does not provide any special function for this, <strong>is it the right tool for this</strong> ?</p>\n<p>I thought of doing it on multiple <strong>random samples</strong> but is there a better approach where <strong>whole data</strong> can be used ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use the generators and also you can compute the class weights. </p>\n<p>Let's say you have your generator like this</p>\n<pre><code>train_generator = train_datagen.flow_from_directory(\n        'train_directory',\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode = \"categorical\"\n        )\n</code></pre>\n<p>and the class weights for the training set can be computed like this</p>\n<pre><code>class_weights = class_weight.compute_class_weight(\n           'balanced',\n            np.unique(train_generator.classes), \n            train_generator.classes)\n</code></pre>\n<p>[EDIT 1]\nSince you mentioned about postgres sql in the comments, I am adding the prototype answer here. </p>\n<p>first fetch the count for each classes using a separate query from postgres sql and use it to compute the class weights. you can compute it manually. The basic logic is the count of least weighed class gets the value 1, and the rest of the classes get &lt;1 based on the relative count to the least weighed class. </p>\n<p>for example you have 3 classes A,B,C with 100,200,150 then class weights becomes {A:1,B:0.5,C:0.66}</p>\n<p>let compute it manually after fetching the values from postgres sql. </p>\n<p>[Query]</p>\n<pre><code>cur.execute(\"SELECT class, count(*) FROM table group by classes order by 1\")\nrows = cur.fetchall()\n</code></pre>\n<p>The above query will return rows with tuples (class name, count for each class) ordered from least to highest. </p>\n<p>Then the below line will code will create the class weights dictionary</p>\n<pre><code>class_weights = {}\nfor row in rows:\n    class_weights[row[0]]=rows[0][1]/row[1] \n    #dividing the least value the current value to get the weight, \n    # so that the least value becomes 1, \n    # and other values becomes &lt; 1\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sklearn isn't used for large processing like this. Ideally, we must implement it ourselves especially when it's a part of a pipeline you are running regularly.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to plot multiple series with two measurements (so it's actually num_of_time_series x 2 graphs) in one figure using pygal.\nFor instance, suppose mt data is:</p>\n<pre><code>from collections import defaultdict\n\nmeasurement_1=defaultdict(None,[\n  (\"component1\", [11.83, 11.35, 0.55]), \n  (\"component2\", [2.19, 2.42, 0.96]),\n  (\"component3\", [1.98, 2.17, 0.17])])\n\nmeasurement_2=defaultdict(None,[\n  (\"component1\", [34940.57, 35260.41, 370.45]),\n  (\"component2\", [1360.67, 1369.58, 2.69]),\n  (\"component3\", [13355.60, 14790.81, 55.63])])\n\nx_labels=['2016-12-01', '2016-12-02', '2016-12-03']\n</code></pre>\n<p>and the graph rendering code is that:</p>\n<pre><code>from pygal import graph\nimport pygal\ndef draw(measurement_1, measurement_2 ,x_labels):\n  graph = pygal.Line()\n  graph.x_labels = x_labels\n\n  for key, value in measurement_1.iteritems():\n      graph.add(key, value)\n  for key, value in measurement_2.iteritems():\n      graph.add(key, value, secondary=True)\n\n  return graph.render_data_uri()\n</code></pre>\n<p>The Current result is <a href=\"https://i.sstatic.net/ZYDRr.png\" rel=\"nofollow noreferrer\">that</a>.</p>\n<p>The problem in the code above is that it's unclear which graph represents measurement 1 and which represents measurement 2.\n Second, I would like to see each component in a different color(or shape). </p>\n<p>This graph aims to compare one component versus the two others, and to see the correlation between measurement 1 and 2.</p>\n<p>Thanks for the help guys!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I figured out how to distinguish the compared component by a dashed line. The code should look like that:</p>\n<pre><code>from pygal import graph\nimport pygal\n\ndef draw(measurement_1, measurement_2 ,x_labels):\n  graph = pygal.Line()\n  graph.x_labels = x_labels\n\n  for key, value in measurement_1.iteritems():\n     ##\n     if \"component1\":\n        graph.add(key, value, stroke_style={'width': 5, 'dasharray': '3, 6', 'linecap': 'round', 'linejoin': 'round'})\n     else:\n     ##\n        graph.add(key, value)\n  for key, value in measurement_2.iteritems():\n      graph.add(key, value, secondary=True)\n\n  return graph.render_data_uri()\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to plot a scalar field that is structured in a <strong>3D grid</strong> as the one that follows:</p>\n<pre><code>import numpy as np\nfrom mayavi import mlab\n\ndt = 10\nX,Y,Z = np.mgrid[0:dt,0:dt,0:dt]\n\nF = X**2+Y**2+Z**2\n\ntest = mlab.figure(size = (1024,768), bgcolor = (1,1,1), fgcolor = (0, 0, 0))\nsf = mlab.pipeline.scalar_field(X,Y,Z,F)\nvl = mlab.pipeline.volume(sf)\nmlab.outline()\nmlab.axes()\nmlab.title('Can not change font size for this title')\nmlab.xlabel('Only end ticks')\nmlab.ylabel('No major ticks')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/mJEHE.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/mJEHE.png\"/></a></p>\n<p>I would like to do so in Python since I simulate many datasets in this language and I would like to be able to visualize them quickly as I perform sensitivities in my simulation parameters.</p>\n<p>Mayavi seemed to offer pretty standard routines for scientific 3d plotting. However, when it comes to communicate these plots in publications, very basic plot customizations are not available such as major and minor ticks in the axes. Also, those very basic features that are supported do not even work properly to date (e.g. see example in font size <a href=\"https://stackoverflow.com/questions/19825520/enthought-canopy-mayavi-font-size-bug\">bug</a> and <a href=\"https://stackoverflow.com/questions/15077984/does-mayavi-font-size-text-property-work\">here</a>).</p>\n<p>Is there any decent and easy to use scientific <strong>3D</strong> plotting library in Python? I have tried learning vtk but the website examples seem to be obsolete (e.g. <a href=\"http://www.vtk.org/Wiki/VTK/Examples/Python/UnstructuredTransientVolumeRendering\" rel=\"nofollow noreferrer\">volume rendering</a> example that fails to run, I tried editing many lines of code to make it work without luck) and others seem to agree that the documentation is lacking.</p>\n<p>By decent scientific plotting library I mean the following:</p>\n<ul>\n<li>Allows for customizing fonts in axes, labels, titles, etc.</li>\n<li>Can edit axes ticks spacing (with major ticks at the very least).</li>\n<li>Can add colorbars</li>\n<li>Has documentation.</li>\n</ul>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should try matplotlib, if you haven't done so already. It's not difficult to use <code>meshgrid</code> and <code>contour</code> or <code>contourf</code> (you'll find scripts on the web easily enough) to plot data on structured grids. It's even possible to plot on unstructured grids (Check this out: <a href=\"https://grantingram.wordpress.com/plotting-2d-unstructured-data-using-free-software/\" rel=\"nofollow\">https://grantingram.wordpress.com/plotting-2d-unstructured-data-using-free-software/</a> )</p>\n<p>It has your characteristics of a \"decent\" scientific plotting library.</p>\n<p>EDIT:\nWhen you say '3D' plotting, I assumed you wanted a plot of a function of 2 variables, so that its graph is 3D.</p>\n<p>If, however, you have data depending on 3 space variables, I assume you want the ability to display cut planes and such. Then I recommend you output your data to files and use a proper visualization package such as ParaView (which uses VTK) or TecPlot (non-free). You can automate the visualization pipeline through scripting (I believe ParaView supports Python scripting).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Trying to use pyramid's auto arima function and getting nowhere. </p>\n<p>Importing the whole class:</p>\n<pre><code>import pyramid\n\n\n\nstepwise_fit = auto_arima(df.Weighted_Price, start_p=0, start_q=0, max_p=10, max_q=10, m=1,\n                      start_P=0, seasonal=True, trace=True,\n                      error_action='ignore',  # don't want to know if an order does not work\n                      suppress_warnings=True,  # don't want convergence warnings\n                      stepwise=True)  # set to stepwise\n</code></pre>\n<p>I get error message:</p>\n<pre><code>NameError: name 'auto_arima' is not defined\n</code></pre>\n<p>Fine, then let's import that specific package from pyramid.</p>\n<pre><code>from pyramid.arima import auto_arima\n</code></pre>\n<blockquote>\n<p>--------------------------------------------------------------------------- RuntimeError                              Traceback (most recent call\n  last) RuntimeError: module compiled against API version 0xb but this\n  version of numpy is 0xa</p>\n<p>--------------------------------------------------------------------------- ImportError                               Traceback (most recent call\n  last)  in ()\n        1 #trying to import pyramid\n  ----&gt; 2 from pyramid.arima import auto_arima</p>\n<p>/usr/local/lib/python2.7/site-packages/pyramid/arima/<strong>init</strong>.py in\n  ()\n        3 # Author: Taylor Smith \n        4 \n  ----&gt; 5 from .approx import *\n        6 from .arima import *\n        7 from .auto import *</p>\n<p>/usr/local/lib/python2.7/site-packages/pyramid/arima/approx.py in\n  ()\n       16 # and since the platform might name the .so file something funky (like\n       17 # _arima.cpython-35m-darwin.so), import this absolutely and not relatively.\n  ---&gt; 18 from pyramid.arima._arima import C_Approx\n       19 \n       20 <strong>all</strong> = [</p>\n<p>ImportError: numpy.core.multiarray failed to import</p>\n</blockquote>\n<p>After importing numpy, or even after just running the block again, I get this error message when running <code>from pyramid.arima import auto_arima\n</code></p>\n<blockquote>\n<p>--------------------------------------------------------------------------- ImportError                               Traceback (most recent call\n  last)  in ()\n        1 #trying to import pyramid\n  ----&gt; 2 from pyramid import arima</p>\n<p>/usr/local/lib/python2.7/site-packages/pyramid/arima/<strong>init</strong>.py in\n  ()\n        3 # Author: Taylor Smith \n        4 \n  ----&gt; 5 from .approx import *\n        6 from .arima import *\n        7 from .auto import *</p>\n<p>/usr/local/lib/python2.7/site-packages/pyramid/arima/approx.py in\n  ()\n       16 # and since the platform might name the .so file something funky (like\n       17 # _arima.cpython-35m-darwin.so), import this absolutely and not relatively.\n  ---&gt; 18 from pyramid.arima._arima import C_Approx\n       19 \n       20 <strong>all</strong> = [</p>\n<p>ImportError: cannot import name C_Approx</p>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try to install <code>pmdarima</code> by using <code>pip</code>: </p>\n<pre><code>pip install pmdarima\n</code></pre>\n<p>then in your python script use: </p>\n<pre><code>from pmdarima.arima import auto_arima\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Environment: \nWindows 10\nIDE: Pycharm\nPython: 3.6</p>\n<p>In Anaconda, create a new environment and then run:</p>\n<pre><code>pip install pyramid-arima\n</code></pre>\n<p>Now in your python code, you can use: </p>\n<p><code>from pyramid.arima import auto_arima</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>use:</p>\n<pre><code>pip install pyramid-arima\n</code></pre>\n<p>You installed the web framework pyramid which is not your looking for.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a data set with 36k rows. I want to randomly select 9k rows from it using pandas. How do I accomplish this task?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you can use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\" rel=\"noreferrer\"><code>sample</code></a> - <code>9k</code> or <code>25%</code> rows:</p>\n<pre><code>df.sample(n=9000)\n</code></pre>\n<p>Or:</p>\n<pre><code>df.sample(frac=0.25)\n</code></pre>\n<p>Another solution with creating random sample of <code>index</code> by <a href=\"https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html\" rel=\"noreferrer\"><code>numpy.random.choice</code></a> and then select by <code>loc</code> - <code>index</code> has to be unique:</p>\n<pre><code>df = df.loc[np.random.choice(df.index, size=9000)]\n</code></pre>\n<p>Solution if not unique index:</p>\n<pre><code>df = df.iloc[np.random.choice(np.arange(len(df)), size=9000)]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>numpy</code></p>\n<pre><code>i = np.random.permutation(np.arange(len(df)))\nidx = i[:9000]\npd.DataFrame(df.values[idx], df.index[idx])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using SVC classifier with Linear kernel to train my model.\nTrain data: 42000 records</p>\n<pre><code>    model = SVC(probability=True)\n    model.fit(self.features_train, self.labels_train)\n    y_pred = model.predict(self.features_test)\n    train_accuracy = model.score(self.features_train,self.labels_train)\n    test_accuracy = model.score(self.features_test, self.labels_test)\n</code></pre>\n<p>It takes more than 2 hours to train my model.\nAm I doing something wrong? \nAlso, what can be done to improve the time</p>\n<p>Thanks in advance</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are several possibilities to speed up your SVM training. Let <code>n</code> be the number of records, and <code>d</code> the embedding dimensionality. I assume you use <code>scikit-learn</code>.</p>\n<ul>\n<li><p><strong>Reducing training set size</strong>. Quoting the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\" rel=\"noreferrer\">docs</a>:</p>\n<blockquote>\n<p>The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</p>\n</blockquote>\n<p><code>O(n^2)</code> complexity will most likely dominate other factors. Sampling fewer records for training will thus have the largest impact on time. Besides random sampling, you could also try <a href=\"https://en.wikipedia.org/wiki/Instance_selection\" rel=\"noreferrer\">instance selection</a> methods. For example, <a href=\"https://uwaterloo.ca/scholar/sites/ca.scholar/files/mcrowley/files/Benyamin_Writing_1.pdf\" rel=\"noreferrer\">principal sample analysis</a> has been proposed recently.</p></li>\n<li><p><strong>Reducing dimensionality</strong>. As others have hinted at in their comments, embedding dimension also impacts runtime. Computing inner products for the linear kernel is in <code>O(d)</code>. <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction\" rel=\"noreferrer\">Dimensionality reduction</a> can, therefore, also reduce runtime. In <a href=\"https://stats.stackexchange.com/a/24498/226709\">another question</a>, <em>latent semantic indexing</em> was suggested specifically for TF-IDF representations.</p></li>\n<li><strong>Parameters</strong>. Use <code>SVC(probability=False)</code> unless you need the probabilities, because they \"<em>will slow down that method.</em>\" (from the docs).</li>\n<li><strong>Implementation</strong>. To the best of my knowledge, scikit-learn just wraps around LIBSVM and LIBLINEAR. I am speculating here, but you may be able to speed this up by using efficient BLAS libraries, such as in Intel's MKL.</li>\n<li><p><strong>Different classifier</strong>. You may try <code>sklearn.svm.LinearSVC</code>, which is...</p>\n<blockquote>\n<p>[s]imilar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.</p>\n</blockquote>\n<p>Moreover, a scikit-learn dev suggested the <a href=\"https://scikit-learn.org/stable/modules/kernel_approximation.html\" rel=\"noreferrer\"><code>kernel_approximation</code></a> module in a <a href=\"https://stackoverflow.com/a/45110520/6555620\">similar question</a>.</p></li>\n</ul>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had the same issue, but scaling the data solved the problem</p>\n<pre><code># Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can try using accelerated implementations of algorithms - such as scikit-learn-intelex - <a href=\"https://github.com/intel/scikit-learn-intelex\" rel=\"nofollow noreferrer\">https://github.com/intel/scikit-learn-intelex</a></p>\n<p>For SVM you for sure would be able to get higher compute efficiency.</p>\n<p>First install package</p>\n<pre class=\"lang-bash prettyprint-override\"><code>pip install scikit-learn-intelex\n</code></pre>\n<p>And then add in your python script</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearnex import patch_sklearn\npatch_sklearn()\n</code></pre>\n<p><strong>Note that:</strong> \"You have to import scikit-learn <strong>after</strong> these lines. Otherwise, the patching will not affect the original scikit-learn estimators.\"\n(from <a href=\"https://intel.github.io/scikit-learn-intelex/latest/#usage\" rel=\"nofollow noreferrer\">docs</a>)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm loving Altair for creating choropleth maps! My biggest problem, however, is I cannot figure out how to change the size of the legend. I've read through the documentation and tried several things to no avail. </p>\n<p>Here's an example using the <a href=\"https://altair-viz.github.io/gallery/choropleth.html\" rel=\"noreferrer\">unemployment map by county</a> from Altair's docs. I added a 'config' layer to change the font size for the title on both the map and the legend. Note the .configure_legend() part of the code within \"config\". </p>\n<pre><code>counties = alt.topo_feature(data.us_10m.url, 'counties')\nsource = data.unemployment.url\n\nforeground = alt.Chart(counties).mark_geoshape(\n    ).encode(\n    color=alt.Color('rate:Q', sort=\"descending\",  scale=alt.Scale(scheme='plasma'), legend=alt.Legend(title=\"Unemp Rate\", tickCount=6))\n).transform_lookup(\n    lookup='id',\n    from_=alt.LookupData(source, 'id', ['rate'])\n).project(\n    type='albersUsa'\n).properties(\n    title=\"Unemployment Rate by County\",\n    width=500,\n    height=300\n)\n\nconfig = alt.layer(foreground).configure_title(fontSize=20, anchor=\"middle\").configure_legend(titleColor='black', titleFontSize=14) \n\nconfig\n</code></pre>\n<p>Here's what the image should look like:</p>\n<p><a href=\"https://i.sstatic.net/pkFHf.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/pkFHf.png\"/></a></p>\n<p>If I change the size of the map like this:</p>\n<pre><code>counties = alt.topo_feature(data.us_10m.url, 'counties')\nsource = data.unemployment.url\n\nforeground = alt.Chart(counties).mark_geoshape(\n    ).encode(\n    color=alt.Color('rate:Q', sort=\"descending\",  scale=alt.Scale(scheme='plasma'), legend=alt.Legend(title=\"Unemp Rate\", tickCount=6))\n).transform_lookup(\n    lookup='id',\n    from_=alt.LookupData(source, 'id', ['rate'])\n).project(\n    type='albersUsa'\n).properties(\n    title=\"Unemployment Rate by County\",\n    width=900,\n    height=540\n)\n\nconfig = alt.layer(foreground).configure_title(fontSize=20, anchor=\"middle\").configure_legend(titleColor='black', titleFontSize=14) \n\nconfig\n</code></pre>\n<p>The legend stays the same size, so that it now looks tiny in comparison to the map:</p>\n<p><a href=\"https://i.sstatic.net/EdvpJ.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/EdvpJ.png\"/></a></p>\n<p>Alternatively, if I make the map size tiny, the legend will be huge!</p>\n<p><a href=\"https://i.sstatic.net/PmeDC.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/PmeDC.png\"/></a></p>\n<p>I've tried about a dozen different things to no avail. </p>\n<p>Anyone have a solution to this? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As you have seen, the legend has a default size in pixels that is constant regardless of the size of the chart. If you would like to adjust it, you can use the <code>configure_legend()</code> chart method.</p>\n<p>In Altair 3.0 or later, the following arguments are the relevant ones for adjusting the size of the legend gradient:</p>\n<pre><code>chart.configure_legend(\n    gradientLength=400,\n    gradientThickness=30\n) \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The first answer is super close but missing the most important piece for changing the font size in the legend. Use the code snippet below to adjust font size of text in the legend.</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>.configure_legend(\ntitleFontSize=18,\nlabelFontSize=15\n) </code></pre>\n</div>\n</div>\n</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In Convolutional Neural Network (CNN), a filter is select for weights sharing. For example, in the following pictures, a 3x3 window with the stride (distance between adjacent neurons) 1 is chosen.</p>\n<p><img alt=\"\" src=\"https://i.sstatic.net/P5eh4.jpg\"/></p>\n<p><img alt=\"\" src=\"https://i.sstatic.net/4eMG8.png\"/></p>\n<p>So my question is: <strong>How to choose the window size?</strong> If I use 4x4 with the stride being 2, how much difference will it cause? Thanks a lot in advance!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There's no definite answer to this: filter size is one of hyperparameters you generally need to tune. However, there're some useful observations, that may help you. It's often preferred to choose <strong>smaller filters</strong>, but have <strong>greater number</strong> of those.</p>\n<p>Example: four <code>5x5</code> filters have 100 parameters (ignoring bias), while 10 <code>3x3</code> filters have 90 parameters. Through the larger of filters you still can capture the variety of features in the image, but with fewer parameters. More on this <a href=\"http://cs231n.github.io/convolutional-networks/#conv\" rel=\"noreferrer\">here</a>.</p>\n<p>Modern CNNs go even further with this idea and choose consecutive <code>3x1</code> and <code>1x3</code> convolutional layers. This reduces the number of parameters even more, but doesn't affect the performance. See the <a href=\"https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba\" rel=\"noreferrer\">evolution of inception network</a>.</p>\n<p>The choice of stride is also important, but it affects the tensor shape after the convolution, hence the whole network. The general rule is to use <code>stride=1</code> in usual convolutions and preserve the spatial size with padding, and use <code>stride=2</code> when you want to downsample the image.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>We have a news feed, and we want to surface items to the user based on a number of criteria. Certain items will be surfaced because of factor A, another because of factor B, and yet another because of factor C. We can create individual heuristics for each factor, but we then need to combine these heuristics in such a way that it promotes the best content considering each factor while still giving a mix of content from each factor.</p>\n<p>Our naive approach is to load the top <code>n</code> from each factor, take the first of each, and make those the first 3 of the feed. Then take the 2nd from each feed and make that the second 3, and so on and so forth. Ideally, we would have some algorithm for more intelligently ranking these feed items - our first thought was to simply sum the three heuristics and pull the top items using the resulting combined score, but there are no guarantees that the heuristics are evenly-scaled (or are evenly-scaled for that particular user), which could result in one factor dominating over the others in the feed. Is there some more intelligent way of ranking these news feed items (akin to what Facebook does in its pseudo-chronological news feed)?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If your final combined heuristic does not need to be admissible, it can do no harm to use a sum of the original heuristics as your final heuristic. The problem here is that the original heuristics are probably not of the same dimension, for instance A has values ranging from 0 to 100 and B has values from -1 to +1. I suggest using following formula to calculate the combined heuristic for an item, that ignores the dimensions of the particular heuristics:</p>\n<p><code>H = (A - min(A))/(max(A) - min(A)) + (B - min(B))/(max(B) -\nmin(B)) + (C - min(C))/(max(C) - min(C))</code></p>\n<p>Of course, to find the <code>min</code> and <code>max</code> values for each heuristic, you need understanding of the meaning of each individual heuristic. I am not sure this solves your problem, but i hope it does.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to add to the point made by <a href=\"https://stackoverflow.com/users/8131331/arne-van-den-kerchove\">Arne Van Den Kerchove</a> - <a href=\"https://stats.stackexchange.com/questions/156226/andrew-ng-scaling-and-normalizing-expression\">Normalization</a>.</p>\n<p>I would suggest another layer that:   </p>\n<ul>\n<li><p>Defines the new Heuristic direction:</p>\n<p>If optimal A,B,C differ in their direction, e.g. optimal A is low, but optimal B is high. This heuristic is the positive square root of the squares of the normalized factors, so higher is better.</p></li>\n<li><p>Will allow to incorporate user response based on the amount of\nattention (weight) the user assigns to each metrics.  </p></li>\n</ul>\n<p>Here is how I imagine it:</p>\n<pre><code>H = sqrt(\n        alpha(\n            ((A - min(A))/(max(A) - min(A)))^2\n        ) + \n        beta(\n            ((B - min(B))/(max(B) - min(B)))^2\n        ) + \n        gamma(\n            ((C - min(C))/(max(C) - min(C)))^2\n        )\n)\n</code></pre>\n<p>Alpha, beta and gamma are weights and will start as [1,1,1] unless you have knowledge that one of the metrics is preferred.<br/>\nThese weights shall change with each user response.  </p>\n<h2>For example:</h2>\n<p>If a user chooses something that ranks as follows: </p>\n<pre><code>Max(A)= 100 :       21 out of 100  in A - relative value is 0.21\nMax(B)= 10,000 :    1234 out of 10,000 in B - relative value is 0.1234\nMax(C)= 1 :         0.2 out of 1 in C - relative value is 0.2\nWhere all minima are 0.\n</code></pre>\n<p>You can add a fraction of the difference between the relative values to alpha, beta and gamma respectively. This way you will have a dynamic rating that not only calculates the factors as you already do, but also adjusts to what the user cares about.  </p>\n<p>For the example above, if we add the full difference, the new alpha, beta and gamma will be [1.0322,0.9456,1.0222] respectively.<br/>\n(Subtract the average (0.1778) from the relative values [0.21,0.1234,0.2] and add the result to the initial set [1,1,1])</p>\n<h3>This way the new relevant item set will be dictated by the user's cumulative choices.</h3>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You have many categories. Let's say A, B and C.</p>\n<p>Combining everything together and rank it(You mentioned that <em>we would have some algorithm for more intelligently ranking these feed items</em>) without depending on the category.</p>\n<p>Show the first 4-5 items in the ranked list independent of category.</p>\n<p>If you have Sponsored feed items(like facebook), then show the top ranked sponsored feed item( If the ranks are 16,27,39,etc then show 16 after the 5) and likewise.</p>\n<p>Then enter to the category.</p>\n<p>If the user have the ability to subscribe to category, then show posts based on the categories.</p>\n<hr/>\n<p>For example</p>\n<p>A have 10 items say a1...a10</p>\n<p>B have 10 items say b1...b10</p>\n<p>likewise C have 10 items say c1...c10</p>\n<p>If the user opted for mainly category B, then show top ranked in b, then 6th ranked in list, second top ranked in b, from list, etc.</p>\n<hr/>\n<p>After 10-12 items,</p>\n<p>Show the items from each category based on the rank order.</p>\n<p>If the user didn't opted for a specific category, then the ranking order should be maintained to 8-10 items and then chose from each category based on the rank order.</p>\n<h3>Side tip</h3>\n<p>When implementing new algorithm, it will always be helpful if you collect the feedback from user from their experience.</p>\n<p>The user should get their preferred contents first, then the contents that are top in each category.</p>\n<p>For that, always refer the user activities and browsing history of each category and each type of post.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have multi variate time series data, want to detect the anomalies with isolation forest algorithm.\nwant to get best parameters from gridSearchCV, here is the code snippet of gridSearch CV.</p>\n<p>input data set loaded with below snippet.</p>\n<pre><code>df = pd.read_csv(\"train.csv\")\ndf.drop(['dataTimestamp','Anomaly'], inplace=True, axis=1)\nX_train = df\ny_train = df1[['Anomaly']] ( Anomaly column is labelled data).\n</code></pre>\n<p>define the parameters for Isolation Forest.</p>\n<pre><code>clf = IsolationForest(random_state=47, behaviour='new', score=\"accuracy\")\nparam_grid = {'n_estimators': list(range(100, 800, 5)), 'max_samples': list(range(100, 500, 5)), 'contamination': [0.1, 0.2, 0.3, 0.4, 0.5], 'max_features': [5,10,15], 'bootstrap': [True, False], 'n_jobs': [5, 10, 20, 30]}\n\nf1sc = make_scorer(f1_score)\ngrid_dt_estimator = model_selection.GridSearchCV(clf, param_grid,scoring=f1sc, refit=True,cv=10, return_train_score=True)\ngrid_dt_estimator.fit(X_train, y_train)\n</code></pre>\n<p>after executing the fit , got the below error.</p>\n<blockquote>\n<p>ValueError: Target is multiclass but average='binary'. Please choose another average setting.</p>\n</blockquote>\n<p>Can some one guide me what is this about, tried average='weight', but still no luck, anything am doing wrong here.\nplease let me know how to get F-score as well.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You incur in this error because you didn't set the parameter <code>average</code> when transforming the f1_score into a scorer. In fact, as detailed in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" rel=\"noreferrer\">documentation</a>:</p>\n<blockquote>\n<p>average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’,\n  ‘samples’, ‘weighted’] This parameter is required for\n  multiclass/multilabel targets. If None, the scores for each class are\n  returned.</p>\n</blockquote>\n<p>The consequence is that the scorer returns multiple scores for each class in your classification problem, instead of a single measure. The solution is to declare one of the possible values of the <code>average</code> parameter for <code>f1_score</code>, depending on your needs. I therefore refactored the code you provided as an example in order to provide a possible solution to your problem:</p>\n<pre><code>from sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import make_scorer, f1_score\nfrom sklearn import model_selection\nfrom sklearn.datasets import make_classification\n\nX_train, y_train = make_classification(n_samples=500, \n                                       n_classes=2)\n\nclf = IsolationForest(random_state=47, behaviour='new')\n\nparam_grid = {'n_estimators': list(range(100, 800, 5)), \n              'max_samples': list(range(100, 500, 5)), \n              'contamination': [0.1, 0.2, 0.3, 0.4, 0.5], \n              'max_features': [5,10,15], \n              'bootstrap': [True, False], \n              'n_jobs': [5, 10, 20, 30]}\n\nf1sc = make_scorer(f1_score(average='micro'))\n\ngrid_dt_estimator = model_selection.GridSearchCV(clf, \n                                                 param_grid,\n                                                 scoring=f1sc, \n                                                 refit=True,\n                                                 cv=10, \n                                                 return_train_score=True)\ngrid_dt_estimator.fit(X_train, y_train)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Update make_scorer with this to get it working.  </p>\n<pre><code>make_scorer(f1_score, average='micro')\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Parameters you tune are not all <strong>necessary</strong>.<br/>\nFor example:<br/>\n<code>contamination</code> is the rate for abnomaly, you can determin the best value after you fitted a model by tune the threshold on <code>model.score_samples</code></p>\n<p><code>n_jobs</code> is the CPU core you used.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to do some tests with neural network final hidden activation layer outputs using sklearn's <code>MLPClassifier</code> after <code>fit</code>ting the data.</p>\n<p>for example, If I create a classifier, assuming data <code>X_train</code> with labels <code>y_train</code> and two hidden layers of sizes <code>(300,100)</code></p>\n<pre><code>clf = MLPClassifier(hidden_layer_sizes=(300,100))\nclf.fit(X_train,y_train)\n</code></pre>\n<p>I would like to be able to call a function somehow to retrieve the final hidden activation layer vector of length <code>100</code> for use in additional tests. </p>\n<p>Assuming a test set <code>X_test, y_test</code>, normal prediction would be:</p>\n<pre><code>preds = clf.predict(X_test)\n</code></pre>\n<p>But, I would like to do something like:</p>\n<pre><code>activation_layers_for_all_X_test = clf.get_final_activation_output(X_test)\n</code></pre>\n<p>Functions such as <code>get_weights</code> exist, but that would only help me on a per layer basis. Short of doing the transformation myself, is there another methodology to retrieve these final hidden layer activated outputs for the final hidden layer?</p>\n<p>Looking at this diagram as an example:</p>\n<p><img alt=\"\" src=\"https://i2.wp.com/python3.codes/wp-content/uploads/2017/01/XIHOY.jpg\"/></p>\n<p>The output I would like is the <code>Out Layer</code>, i.e. the final activated output from the final hidden layer.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As I said in my comment above, it doesn't look like there's a function to do exactly what you want in <code>sklearn</code> but you can hack the <code>_predict</code> function very easily to make it do what you want. The following code will return all activations, you can edit this to <code>return activations[-2]</code> for just the bit that you're after.</p>\n<pre><code>def get_activations(clf, X):\n        hidden_layer_sizes = clf.hidden_layer_sizes\n        if not hasattr(hidden_layer_sizes, \"__iter__\"):\n            hidden_layer_sizes = [hidden_layer_sizes]\n        hidden_layer_sizes = list(hidden_layer_sizes)\n        layer_units = [X.shape[1]] + hidden_layer_sizes + \\\n            [clf.n_outputs_]\n        activations = [X]\n        for i in range(clf.n_layers_ - 1):\n            activations.append(np.empty((X.shape[0],\n                                         layer_units[i + 1])))\n        clf._forward_pass(activations)\n        return activations\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to create a function which would be the inverse of the np.gradient function.</p>\n<p>Where the Vx,Vy arrays (Velocity component vectors) are the input and the output would be an array of anti-derivatives (Arrival Time) at the datapoints x,y.</p>\n<p>I have data on a (x,y) grid with scalar values (time) at each point.</p>\n<p>I have used the numpy gradient function and linear interpolation to determine the gradient vector <strong>Velocity</strong> (Vx,Vy) at each point (See below).</p>\n<p>I have achieved this by:</p>\n<pre><code> #LinearTriInterpolator applied to a delaunay triangular mesh\n LTI= LinearTriInterpolator(masked_triang, time_array)\n\n #Gradient requested at the mesh nodes:\n (Vx, Vy) = LTI.gradient(triang.x, triang.y)\n</code></pre>\n<p>The first image below shows the velocity vectors at each point, and the point labels represent the time value which formed the derivatives (Vx,Vy)\n<a href=\"https://i.sstatic.net/OOoex.png\" rel=\"nofollow noreferrer\"><img alt=\"Velocity vectors shown at each point with associated time value\" src=\"https://i.sstatic.net/OOoex.png\"/></a></p>\n<p>The next image shows the <strong>resultant scalar value</strong> of the derivatives (Vx,Vy) plotted as a colored contour graph with associated node labels.</p>\n<p><a href=\"https://i.sstatic.net/9MnB2.png\" rel=\"nofollow noreferrer\"><img alt=\"Derivative scalar plot\" src=\"https://i.sstatic.net/9MnB2.png\"/></a></p>\n<p>So my challenge is:</p>\n<p><strong>I need to reverse the process!</strong></p>\n<p>Using the gradient vectors (Vx,Vy) or the resultant scalar value to determine the original Time-Value at that point.</p>\n<p>Is this possible?</p>\n<p>Knowing that the numpy.gradient function is computed using second order accurate central differences in the interior points and either first or second order accurate one-sides (forward or backwards) differences at the boundaries, I am sure there is a function which would reverse this process.</p>\n<p>I was thinking that taking a line derivative between the original point (t=0 at x1,y1) to any point (xi,yi) over the Vx,Vy plane would give me the sum of the velocity components. I could then divide this value by the distance between the two points to get the time taken..</p>\n<p>Would this approach work? And if so, which numpy integrate function would be best applied?</p>\n<p><a href=\"https://i.sstatic.net/9reZY.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/9reZY.png\"/></a></p>\n<p>An example of my data can be found here [http://www.filedropper.com/calculatearrivaltimefromgradientvalues060820]</p>\n<p>Your help would be greatly appreciated</p>\n<p>EDIT:</p>\n<p>Maybe this simplified drawing might help understand where I'm trying to get to..\n<a href=\"https://i.sstatic.net/hqP10.png\" rel=\"nofollow noreferrer\"><img alt=\"SImplified drawing\" src=\"https://i.sstatic.net/hqP10.png\"/></a></p>\n<p>EDIT:</p>\n<p>Thanks to @Aguy who has contibuted to this code.. I Have tried to get a more accurate representation using a meshgrid of spacing 0.5 x 0.5m and calculating the gradient at each meshpoint, however I am not able to integrate it properly. I also have some edge affects which are affecting the results that I don't know how to correct.</p>\n<pre><code>import numpy as np\nfrom scipy import interpolate\nfrom matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#Createmesh grid with a spacing of 0.5 x 0.5\nstepx = 0.5\nstepy = 0.5\nxx = np.arange(min(x), max(x), stepx)\nyy = np.arange(min(y), max(y), stepy)\nxgrid, ygrid = np.meshgrid(xx, yy)\ngrid_z1 = interpolate.griddata((x,y), Arrival_Time, (xgrid, ygrid), method='linear') #Interpolating the Time values\n\n#Formatdata\nX = np.ravel(xgrid)\nY= np.ravel(ygrid)\nzs = np.ravel(grid_z1)\nZ = zs.reshape(X.shape)\n\n#Calculate Gradient\n(dx,dy) = np.gradient(grid_z1) #Find gradient for points on meshgrid\n\nVelocity_dx= dx/stepx #velocity ms/m\nVelocity_dy= dy/stepx #velocity ms/m\n\nResultant = (Velocity_dx**2 + Velocity_dy**2)**0.5 #Resultant scalar value ms/m\n\nResultant = np.ravel(Resultant)\n\n#Plot Original Data F(X,Y) on the meshgrid\nfig = pyplot.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(x,y,Arrival_Time,color='r')\nax.plot_trisurf(X, Y, Z)\nax.set_xlabel('X-Coordinates')\nax.set_ylabel('Y-Coordinates')\nax.set_zlabel('Time (ms)')\npyplot.show()\n\n#Plot the Derivative of f'(X,Y) on the meshgrid\nfig = pyplot.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X,Y,Resultant,color='r',s=0.2)\nax.plot_trisurf(X, Y, Resultant)\nax.set_xlabel('X-Coordinates')\nax.set_ylabel('Y-Coordinates')\nax.set_zlabel('Velocity (ms/m)')\npyplot.show()\n\n#Integrate to compare the original data input\ndxintegral = np.nancumsum(Velocity_dx, axis=1)*stepx\ndyintegral = np.nancumsum(Velocity_dy, axis=0)*stepy\n\nvalintegral = np.ma.zeros(dxintegral.shape)\nfor i in range(len(yy)):\n    for j in range(len(xx)):\n        valintegral[i, j] = np.ma.sum([dxintegral[0, len(xx) // 2], \n    dyintegral[i, len(yy)  // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])\nvalintegral = valintegral * np.isfinite(dxintegral)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/5AoON.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/5AoON.png\"/></a></p>\n<p>Now the np.gradient is applied at every meshnode (dx,dy) = np.gradient(grid_z1)</p>\n<p><a href=\"https://i.sstatic.net/ufApE.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ufApE.png\"/></a></p>\n<p>Now in my process I would analyse the gradient values above and make some adjustments (There is some unsual edge effects that are being create which I need to rectify) and would then integrate the values to get back to a surface which would be very similar to f(x,y) shown above.</p>\n<p>I need some help adjusting the integration function:</p>\n<pre><code>#Integrate to compare the original data input\ndxintegral = np.nancumsum(Velocity_dx, axis=1)*stepx\ndyintegral = np.nancumsum(Velocity_dy, axis=0)*stepy\n\nvalintegral = np.ma.zeros(dxintegral.shape)\nfor i in range(len(yy)):\n    for j in range(len(xx)):\n        valintegral[i, j] = np.ma.sum([dxintegral[0, len(xx) // 2], \n    dyintegral[i, len(yy)  // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])\nvalintegral = valintegral * np.isfinite(dxintegral)\n</code></pre>\n<p>And now I need to calculate the new 'Time' values at the original (x,y) point locations.</p>\n<p>UPDATE (08-09-20) : I am getting some promising results using the help from @Aguy. The results can be seen below (with the blue contours representing the original data, and the red contours representing the integrated values).</p>\n<p>I am still working on an integration approach which can remove the inaccuarcies at the areas of min(y) and max(y)</p>\n<p><a href=\"https://i.sstatic.net/RD3Ce.png\" rel=\"nofollow noreferrer\"><img alt=\"Getting closer\" src=\"https://i.sstatic.net/RD3Ce.png\"/></a></p>\n<pre><code>from matplotlib.tri import (Triangulation, UniformTriRefiner, \nCubicTriInterpolator,LinearTriInterpolator,TriInterpolator,TriAnalyzer)\nimport pandas as pd\nfrom scipy.interpolate import griddata\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import interpolate\n\n#-------------------------------------------------------------------------\n# STEP 1: Import data from Excel file, and set variables\n#-------------------------------------------------------------------------\ndf_initial = pd.read_excel(\nr'C:\\Users\\morga\\PycharmProjects\\venv\\Development\\Trial'\nr'.xlsx')\n</code></pre>\n<p>Inputdata can be found here <a href=\"http://www.filedropper.com/trial-wireup2\" rel=\"nofollow noreferrer\">link</a></p>\n<pre><code>df_initial = df_initial .sort_values(by='Delay', ascending=True) #Update dataframe and sort by Delay\nx = df_initial ['X'].to_numpy() \ny = df_initial ['Y'].to_numpy() \nArrival_Time = df_initial ['Delay'].to_numpy() \n\n# Createmesh grid with a spacing of 0.5 x 0.5\nstepx = 0.5\nstepy = 0.5\nxx = np.arange(min(x), max(x), stepx)\nyy = np.arange(min(y), max(y), stepy)\nxgrid, ygrid = np.meshgrid(xx, yy)\ngrid_z1 = interpolate.griddata((x, y), Arrival_Time, (xgrid, ygrid), method='linear')  # Interpolating the Time values\n\n# Calculate Gradient (velocity ms/m)\n(dy, dx) = np.gradient(grid_z1)  # Find gradient for points on meshgrid\n\n\nVelocity_dx = dx / stepx  # x velocity component ms/m\nVelocity_dy = dy / stepx  # y velocity component ms/m\n\n# Integrate to compare the original data input\ndxintegral = np.nancumsum(Velocity_dx, axis=1) * stepx\ndyintegral = np.nancumsum(Velocity_dy, axis=0) * stepy\n\nvalintegral = np.ma.zeros(dxintegral.shape)  # Makes an array filled with 0's the same shape as dx integral\nfor i in range(len(yy)):\n    for j in range(len(xx)):\n        valintegral[i, j] = np.ma.sum(\n        [dxintegral[0, len(xx) // 2], dyintegral[i, len(xx) // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])\nvalintegral[np.isnan(dx)] = np.nan\nmin_value = np.nanmin(valintegral)\n\nvalintegral = valintegral + (min_value * -1)\n\n##Plot Results\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(x, y, color='black', s=7, zorder=3)\nax.set_xlabel('X-Coordinates')\nax.set_ylabel('Y-Coordinates')\nax.contour(xgrid, ygrid, valintegral, levels=50, colors='red', zorder=2)\nax.contour(xgrid, ygrid, grid_z1, levels=50, colors='blue', zorder=1)\nax.set_aspect('equal')\nplt.show()\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h2>TL;DR;</h2>\n<p>You have multiple challenges to address in this issue, mainly:</p>\n<ul>\n<li>Potential reconstruction (scalar field) from its gradient (vector field)</li>\n</ul>\n<p>But also:</p>\n<ul>\n<li>Observation in a concave hull with non rectangular grid;</li>\n<li>Numerical 2D line integration and numerical inaccuracy;</li>\n</ul>\n<p>It seems it can be solved by choosing an adhoc interpolant and a smart way to integrate (as pointed out by <code>@Aguy</code>).</p>\n<h2>MCVE</h2>\n<p>In a first time, let's build a MCVE to highlight above mentioned key points.</p>\n<h3>Dataset</h3>\n<p>We recreate a scalar field and its gradient.</p>\n<pre><code>import numpy as np\nfrom scipy import interpolate\nimport matplotlib.pyplot as plt\n\ndef f(x, y):\n    return x**2 + x*y + 2*y + 1\n\nNx, Ny = 21, 17\nxl = np.linspace(-3, 3, Nx)\nyl = np.linspace(-2, 2, Ny)\n\nX, Y = np.meshgrid(xl, yl)\nZ = f(X, Y)\nzl = np.arange(np.floor(Z.min()), np.ceil(Z.max())+1, 2)\n\ndZdy, dZdx = np.gradient(Z, yl, xl, edge_order=1)\nV = np.hypot(dZdx, dZdy)\n</code></pre>\n<p>The scalar field looks like:</p>\n<pre><code>axe = plt.axes(projection='3d')\naxe.plot_surface(X, Y, Z, cmap='jet', alpha=0.5)\naxe.view_init(elev=25, azim=-45)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/JBM2N.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/JBM2N.png\"/></a></p>\n<p>And, the vector field looks like:</p>\n<pre><code>axe = plt.contour(X, Y, Z, zl, cmap='jet')\naxe.axes.quiver(X, Y, dZdx, dZdy, V, units='x', pivot='tip', cmap='jet')\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/MNqbf.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/MNqbf.png\"/></a></p>\n<p>Indeed gradient is normal to potential levels. We also plot the gradient magnitude:</p>\n<pre><code>axe = plt.contour(X, Y, V, 10, cmap='jet')\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/y9nDQ.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/y9nDQ.png\"/></a></p>\n<h3>Raw field reconstruction</h3>\n<p>If we naively reconstruct the scalar field from the gradient:</p>\n<pre><code>SdZx = np.cumsum(dZdx, axis=1)*np.diff(xl)[0]\nSdZy = np.cumsum(dZdy, axis=0)*np.diff(yl)[0]\n\nZhat = np.zeros(SdZx.shape)\nfor i in range(Zhat.shape[0]):\n    for j in range(Zhat.shape[1]):\n        Zhat[i,j] += np.sum([SdZy[i,0], -SdZy[0,0], SdZx[i,j], -SdZx[i,0]])\n        \nZhat += Z[0,0] - Zhat[0,0]\n</code></pre>\n<p>We can see the global result is roughly correct, but levels are less accurate where the gradient magnitude is low:</p>\n<p><a href=\"https://i.sstatic.net/DsJYz.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/DsJYz.png\"/></a></p>\n<h2>Interpolated field reconstruction</h2>\n<p>If we increase the grid resolution and pick a specific interpolant (usual when dealing with mesh grid), we can get a finer field reconstruction:</p>\n<pre><code>r = np.stack([X.ravel(), Y.ravel()]).T\nSx = interpolate.CloughTocher2DInterpolator(r, dZdx.ravel())\nSy = interpolate.CloughTocher2DInterpolator(r, dZdy.ravel())\n\nNx, Ny = 200, 200\nxli = np.linspace(xl.min(), xl.max(), Nx)\nyli = np.linspace(yl.min(), yl.max(), Nx)\nXi, Yi = np.meshgrid(xli, yli)\nri = np.stack([Xi.ravel(), Yi.ravel()]).T\n\ndZdxi = Sx(ri).reshape(Xi.shape)\ndZdyi = Sy(ri).reshape(Xi.shape)\n\nSdZxi = np.cumsum(dZdxi, axis=1)*np.diff(xli)[0]\nSdZyi = np.cumsum(dZdyi, axis=0)*np.diff(yli)[0]\n\nZhati = np.zeros(SdZxi.shape)\nfor i in range(Zhati.shape[0]):\n    for j in range(Zhati.shape[1]):\n        Zhati[i,j] += np.sum([SdZyi[i,0], -SdZyi[0,0], SdZxi[i,j], -SdZxi[i,0]])\n        \nZhati += Z[0,0] - Zhati[0,0]\n</code></pre>\n<p>Which definitely performs way better:</p>\n<p><a href=\"https://i.sstatic.net/UIj0E.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/UIj0E.png\"/></a></p>\n<p>So basically, increasing the grid resolution with an adhoc interpolant may help you to get more accurate result. The interpolant also solve the need to get a regular rectangular grid from a triangular mesh to perform integration.</p>\n<h2>Concave and convex hull</h2>\n<p>You also have pointed out inaccuracy on the edges. Those are the result of the combination of the interpolant choice and the integration methodology. The integration methodology fails to properly compute the scalar field when it reach concave region with few interpolated points. The problem disappear when choosing a mesh-free interpolant able to extrapolate.</p>\n<p>To illustrate it, let's remove some data from our MCVE:</p>\n<pre><code>q = np.full(dZdx.shape, False)\nq[0:6,5:11] = True\nq[-6:,-6:] = True\ndZdx[q] = np.nan\ndZdy[q] = np.nan\n</code></pre>\n<p>Then the interpolant can be constructed as follow:</p>\n<pre><code>q2 = ~np.isnan(dZdx.ravel())\nr = np.stack([X.ravel(), Y.ravel()]).T[q2,:]\nSx = interpolate.CloughTocher2DInterpolator(r, dZdx.ravel()[q2])\nSy = interpolate.CloughTocher2DInterpolator(r, dZdy.ravel()[q2])\n</code></pre>\n<p>Performing the integration we see that in addition of classical edge effect we do have less accurate value in concave regions (swingy dot-dash lines where the hull is concave) and we have no data outside the convex hull as Clough Tocher is a mesh-based interpolant:</p>\n<pre><code>Vl = np.arange(0, 11, 1)\naxe = plt.contour(X, Y, np.hypot(dZdx, dZdy), Vl, cmap='jet')\naxe.axes.contour(Xi, Yi, np.hypot(dZdxi, dZdyi), Vl, cmap='jet', linestyles='-.')\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/J6xFO.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/J6xFO.png\"/></a></p>\n<p>So basically the error we are seeing on the corner are most likely due to integration issue combined with interpolation limited to the convex hull.</p>\n<p>To overcome this we can choose a different interpolant such as RBF (Radial Basis Function Kernel) which is able to create data outside the convex hull:</p>\n<pre><code>Sx = interpolate.Rbf(r[:,0], r[:,1], dZdx.ravel()[q2], function='thin_plate')\nSy = interpolate.Rbf(r[:,0], r[:,1], dZdy.ravel()[q2], function='thin_plate')\n\ndZdxi = Sx(ri[:,0], ri[:,1]).reshape(Xi.shape)\ndZdyi = Sy(ri[:,0], ri[:,1]).reshape(Xi.shape)\n</code></pre>\n<p>Notice the slightly different interface of this interpolator (mind how parmaters are passed).</p>\n<p>The result is the following:</p>\n<p><a href=\"https://i.sstatic.net/71SGA.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/71SGA.png\"/></a></p>\n<p>We can see the region outside the convex hull can be extrapolated (RBF are mesh free). So choosing the adhoc interpolant is definitely a key point to solve your problem. But we still need to be aware that extrapolation may perform well but is somehow meaningless and dangerous.</p>\n<h2>Solving your problem</h2>\n<p>The answer provided by <code>@Aguy</code> is perfectly fine as it setups a clever way to integrate that is not disturbed by missing points outside the convex hull. But as you mentioned there is inaccuracy in concave region inside the convex hull.</p>\n<p>If you wish to remove the edge effect you detected, you will have to resort to an interpolant able to extrapolate as well, or find another way to integrate.</p>\n<h3>Interpolant change</h3>\n<p>Using RBF interpolant seems to solve your problem. Here is the complete code:</p>\n<pre><code>df = pd.read_excel('./Trial-Wireup 2.xlsx')\nx = df['X'].to_numpy()\ny = df['Y'].to_numpy()\nz = df['Delay'].to_numpy()\n\nr = np.stack([x, y]).T\n\n#S = interpolate.CloughTocher2DInterpolator(r, z)\n#S = interpolate.LinearNDInterpolator(r, z)\nS = interpolate.Rbf(x, y, z, epsilon=0.1, function='thin_plate')\n\nN = 200\nxl = np.linspace(x.min(), x.max(), N)\nyl = np.linspace(y.min(), y.max(), N)\nX, Y = np.meshgrid(xl, yl)\n\n#Zp = S(np.stack([X.ravel(), Y.ravel()]).T)\nZp = S(X.ravel(), Y.ravel())\nZ = Zp.reshape(X.shape)\n\ndZdy, dZdx = np.gradient(Z, yl, xl, edge_order=1)\n\nSdZx = np.nancumsum(dZdx, axis=1)*np.diff(xl)[0]\nSdZy = np.nancumsum(dZdy, axis=0)*np.diff(yl)[0]\n\nZhat = np.zeros(SdZx.shape)\nfor i in range(Zhat.shape[0]):\n    for j in range(Zhat.shape[1]):\n        #Zhat[i,j] += np.nansum([SdZy[i,0], -SdZy[0,0], SdZx[i,j], -SdZx[i,0]])\n        Zhat[i,j] += np.nansum([SdZx[0,N//2], SdZy[i,N//2], SdZx[i,j], -SdZx[i,N//2]])\n        \nZhat += Z[100,100] - Zhat[100,100]\n\nlz = np.linspace(0, 5000, 20)\naxe = plt.contour(X, Y, Z, lz, cmap='jet')\naxe = plt.contour(X, Y, Zhat, lz, cmap='jet', linestyles=':')\naxe.axes.plot(x, y, '.', markersize=1)\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n</code></pre>\n<p>Which graphically renders as follow:</p>\n<p><a href=\"https://i.sstatic.net/Yck6p.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Yck6p.png\"/></a>\n<a href=\"https://i.sstatic.net/kktBR.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/kktBR.png\"/></a></p>\n<p>The edge effect is gone because of the RBF interpolant can extrapolate over the whole grid. You can confirm it by comparing the result of mesh-based interpolants.</p>\n<h3>Linear</h3>\n<p><a href=\"https://i.sstatic.net/jyZT9.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/jyZT9.png\"/></a>\n<a href=\"https://i.sstatic.net/g86m6.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/g86m6.png\"/></a></p>\n<h3>Clough Tocher</h3>\n<p><a href=\"https://i.sstatic.net/smGCD.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/smGCD.png\"/></a>\n<a href=\"https://i.sstatic.net/bhYl8.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/bhYl8.png\"/></a></p>\n<h3>Integration variable order change</h3>\n<p>We can also try to find a better way to integrate and mitigate the edge effect, eg. let's change the integration variable order:</p>\n<pre><code>Zhat[i,j] += np.nansum([SdZy[N//2,0], SdZx[N//2,j], SdZy[i,j], -SdZy[N//2,j]])\n</code></pre>\n<p>With a classic linear interpolant. The result is quite correct, but we still have an edge effect on the bottom left corner:</p>\n<p><a href=\"https://i.sstatic.net/YNNkh.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/YNNkh.png\"/></a></p>\n<p>As you noticed the problem occurs at the middle of the axis in region where the integration starts and lacks a reference point.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is one approach:</p>\n<p>First, in order to be able to do integration, it's good to be on a regular grid. Using here variable names <code>x</code> and <code>y</code> as short for your <code>triang.x</code> and <code>triang.y</code> we can first create a grid:</p>\n<pre><code>import numpy as np\nn = 200 # Grid density\nstepx = (max(x) - min(x)) / n\nstepy = (max(y) - min(y)) / n\nxspace = np.arange(min(x), max(x), stepx)\nyspace = np.arange(min(y), max(y), stepy)\nxgrid, ygrid = np.meshgrid(xspace, yspace)\n</code></pre>\n<p>Then we can interpolate <code>dx</code> and <code>dy</code> on the grid using the same <code>LinearTriInterpolator</code> function:</p>\n<pre><code>fdx = LinearTriInterpolator(masked_triang, dx)\nfdy = LinearTriInterpolator(masked_triang, dy)\n\ndxgrid = fdx(xgrid, ygrid)\ndygrid = fdy(xgrid, ygrid)\n</code></pre>\n<p>Now comes the integration part. In principle, any path we choose should get us to the same value. In practice, since there are missing values and different densities, the choice of path is very important to get a reasonably accurate answer.</p>\n<p>Below I choose to integrate over <code>dxgrid</code> in the x direction from 0 to the middle of the grid at n/2. Then integrate over <code>dygrid</code> in the y direction from 0 to the i point of interest. Then over <code>dxgrid</code> again from n/2 to the point j of interest. This is a simple way to make sure most of the path of integration is inside the bulk of available data by simply picking a path that goes mostly in the \"middle\" of the data range. Other alternative consideration would lead to different path selections.</p>\n<p>So we do:</p>\n<pre><code>dxintegral = np.nancumsum(dxgrid, axis=1) * stepx\ndyintegral = np.nancumsum(dygrid, axis=0) * stepy\n</code></pre>\n<p>and then (by somewhat brute force for clarity):</p>\n<pre><code>valintegral = np.ma.zeros(dxintegral.shape)\nfor i in range(n):\n    for j in range(n):\n        valintegral[i, j] = np.ma.sum([dxintegral[0, n // 2],  dyintegral[i, n // 2], dxintegral[i, j], - dxintegral[i, n // 2]])\nvalintegral = valintegral * np.isfinite(dxintegral)\n</code></pre>\n<p><code>valintegral</code> would be the result up to an arbitrary constant which can help put the \"zero\" where you want.</p>\n<p>With your data shown here:</p>\n<p><code>ax.tricontourf(masked_triang, time_array)</code>\n<a href=\"https://i.sstatic.net/ztn95.png\" rel=\"nofollow noreferrer\"><img alt=\"ax.tricontourf(masked_triang, time_array)\" src=\"https://i.sstatic.net/ztn95.png\"/></a></p>\n<p>This is what I'm getting reconstructed when using this method:</p>\n<p><code>ax.contourf(xgrid, ygrid, valintegral)</code>\n<a href=\"https://i.sstatic.net/ifzqW.png\" rel=\"nofollow noreferrer\"><img alt=\"ax.contourf(xgrid, ygrid, valintegral)\" src=\"https://i.sstatic.net/ifzqW.png\"/></a></p>\n<p>Hopefully this is somewhat helpful.</p>\n<p>If you want to revisit the values at the original triangulation points, you can use <code>interp2d</code> on the <code>valintegral</code> regular grid data.</p>\n<p>EDIT:</p>\n<p>In reply to your edit, your adaptation above has a few errors:</p>\n<ol>\n<li><p>Change the line <code>(dx,dy) = np.gradient(grid_z1)</code> to <code>(dy,dx) = np.gradient(grid_z1)</code></p>\n</li>\n<li><p>In the integration loop change the <code>dyintegral[i, len(yy)  // 2]</code> term to <code>dyintegral[i, len(xx)  // 2]</code></p>\n</li>\n<li><p>Better to replace the line <code>valintegral = valintegral * np.isfinite(dxintegral)</code> with <code>valintegral[np.isnan(dx)] = np.nan</code></p>\n</li>\n</ol>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using the <a href=\"https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk</a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.</p>\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.</p>\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The entry_point argument here is for a script file that contains the codes you want to run training and prediction.</p>\n<p>You can refer to the <a href=\"https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb\" rel=\"nofollow noreferrer\">example here</a></p>\n<p>And the entry_point script above example uses is <a href=\"https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/scikit_learn_iris.py\" rel=\"nofollow noreferrer\">here</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I get a performance hit to <code>deepcopy</code> once I import an unrelated package, <code>CSV</code>. How can I fix this?</p>\n<pre><code>import BenchmarkTools\nmutable struct GameState\n    gameScore::Vector{Int64}\n    setScore::Vector{Int64}\n    matchScore::Vector{Int64}\n    serve::Int64\nend\nBenchmarkTools.@benchmark deepcopy(GameState([0,0],[0,0],[0,0],-1))\n\nBenchmarkTools.Trial: \n  memory estimate:  1.02 KiB\n  allocs estimate:  10\n  --------------\n  minimum time:     1.585 μs (0.00% GC)\n  median time:      1.678 μs (0.00% GC)\n  mean time:        2.519 μs (27.10% GC)\n  maximum time:     5.206 ms (99.88% GC)\n  --------------\n  samples:          10000\n  evals/sample:     10\n\nimport CSV\n\nBenchmarkTools.@benchmark deepcopy(GameState([0,0],[0,0],[0,0],-1))\n\nBenchmarkTools.Trial: \n  memory estimate:  1.02 KiB\n  allocs estimate:  10\n  --------------\n  minimum time:     6.709 μs (0.00% GC)\n  median time:      7.264 μs (0.00% GC)\n  mean time:        9.122 μs (18.00% GC)\n  maximum time:     13.289 ms (99.87% GC)\n  --------------\n  samples:          10000\n  evals/sample:     5\n</code></pre>\n<p>UPDATE: Code for suggested solution</p>\n<pre><code>import Base:deepcopy\nfunction deepcopy(x::GameState)\n    return GameState([x.gameScore[1], x.gameScore[2]], [x.setScore[1], x.setScore[2]], [x.matchScore[1], x.matchScore[2]],x.serve)\nend\n\nBenchmarkTools.@benchmark deepcopy(GameState([0,0],[0,0],[0,0],-1))\nBenchmarkTools.Trial: \n  memory estimate:  672 bytes\n  allocs estimate:  8\n  --------------\n  minimum time:     184.436 ns (0.00% GC)\n  median time:      199.305 ns (0.00% GC)\n  mean time:        256.366 ns (21.29% GC)\n  maximum time:     102.345 μs (99.52% GC)\n  --------------\n  samples:          10000\n  evals/sample:     656\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There are at least two possibilities:</p>\n<ul>\n<li>Something in your code requires run-time dispatch. Importing CSV adds new methods, thus making the method tables for one or more of your key functions longer. Since run-time dispatch then has more possibilities to evaluate, it makes it slower. <strong>Solution</strong>: make sure your code is <a href=\"https://stackoverflow.com/questions/53924454/how-does-type-stability-make-julia-so-fast\">\"type stable\" (inferrable)</a> wherever possible, then Julia won't need to perform run-time dispatch. See the <a href=\"https://docs.julialang.org/en/latest/manual/performance-tips/\" rel=\"noreferrer\">Performance tips page</a> to get started.</li>\n<li>Something in CSV is doing <a href=\"https://docs.julialang.org/en/v1/manual/style-guide/index.html#Avoid-type-piracy-1\" rel=\"noreferrer\">type-piracy</a>, and its implementation is slowing you down.</li>\n</ul>\n<p>I would place my bets on the former. Use <a href=\"https://github.com/timholy/ProfileView.jl\" rel=\"noreferrer\">ProfileView.jl</a> to easily detect run-time dispatch graphically. If you see a lot of red bars on top when you profile <code>rungame</code>, then you know you've found the source of the problems. Aside from the interaction with CSV, eliminating those red bars might give you huge improvements in performance.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have run a KNN model. Now i want to plot the residual vs predicted value plot. Every example from different websites shows that i have to first run a linear regression model. But i couldn't understand how to do this. Can anyone help? Thanks in advance.\nHere is my model-</p>\n<pre><code>train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\nx_train = train.iloc[:,[2,5]].values\ny_train = train.iloc[:,4].values\nx_validate = validate.iloc[:,[2,5]].values\ny_validate = validate.iloc[:,4].values\nx_test = test.iloc[:,[2,5]].values\ny_test = test.iloc[:,4].values\nclf=neighbors.KNeighborsRegressor(n_neighbors = 6)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_validate)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Residuals are nothing but how much your predicted values differ from actual values. So, it's calculated as actual values-predicted values. In your case, it's <strong>residuals = y_test-y_pred</strong>. Now for the plot, just use this;</p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\n\nplt.scatter(residuals,y_pred)\n\nplt.show()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the question? The residuals are simply <code>y_test-y_pred</code>. Now use <a href=\"https://seaborn.pydata.org/generated/seaborn.regplot.html\" rel=\"nofollow noreferrer\">seaborn's regplot.</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Given a \"split ratio\", I am trying to randomly split up a dataset into two groups. The catch is, that I do not know beforehand how many items the dataset contains. My library receives the data one by one from an input stream and is expected to return the data to two output streams. The resulting two datasets should ideally be exactly split into the given split ratio.</p>\n<p><strong>Illustration:</strong></p>\n<pre><code>                            ┌─► stream A\n input stream ──► LIBRARY ──┤\n                            └─► stream B\n</code></pre>\n<p>For example, given a split ratio of <code>30/70</code>, stream A would be expected to receive 30% of the elements from the input stream and stream B the remaining 70%. The order must remain.</p>\n<hr/>\n<h2>My ideas so far:</h2>\n<p><strong>Idea 1: \"Roll the dice\" for each element</strong></p>\n<p>The obvious approach: For each element the algorithm randomly decides if the element should go into stream A or B. The problem is, that the resulting data sets might be far off from the expected split ratio. Given a split ratio of <code>50/50</code>, the resulting data split might be something far off (could even be <code>100/0</code> for very small data sets). The goal is to keep the resulting split ratio as close as possible to the desired split ratio.</p>\n<p><strong>Idea 2: Use a cache and randomize the cached data</strong></p>\n<p>Another idea is to cache a fixed number of elements before passing them on. This would result in caching 1000 elements and shuffling the data (or their corresponding indices to keep the order stable), splitting them up and passing the resulting data sets on. This should work very well, but I'm unsure if the randomization is really random for large data sets (I imagine there will patterns when looking at the distribution).</p>\n<p>Both algorithms are not optimal, so I hope you can help me.</p>\n<hr/>\n<p><strong>Background</strong></p>\n<p>This is about a layer-based data science tool, where each layer receives data from the previous layer via a stream. This layer is expected to split the data (vectors) up into a training and test set before passing them on. The input data can range from just a few elements to a never ending stream of data (hence, the streams). The code is developed in JavaScript, but this question is more about the algorithm than the actual implementation.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could adjust the probability as it shifts away from the desired rate.</p>\n<p>Here's an example along with tests for various levels of adjusting the probability. As we increase the adjustments, we see the stream splitter deviates less from the ideal ratio, but it also means its less random (knowing the previous values, you can predict the next values).</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>// rateStrictness = 0 will lead to \"rolling the dice\" for each invocations\r\n// higher values of rateStrictness will lead to strong \"correcting\" forces\r\nfunction* splitter(desiredARate, rateStrictness = .5) {\r\n\tlet aCount = 0, bCount = 0;\r\n\r\n\twhile (true) {\r\n\r\n\t\tlet actualARate = aCount / (aCount + bCount);\r\n\t\tlet aRate = desiredARate + (desiredARate - actualARate) * rateStrictness;\r\n\t\tif (Math.random() &lt; aRate) {\r\n\t\t\taCount++;\r\n\t\t\tyield 'a';\r\n\t\t} else {\r\n\t\t\tbCount++;\r\n\t\t\tyield 'b';\r\n\t\t}\r\n\t}\r\n}\r\n\r\nlet test = (desiredARate, rateStrictness) =&gt; {\r\n\tlet s = splitter(desiredARate, rateStrictness);\r\n\tlet values = [...Array(1000)].map(() =&gt; s.next().value);\r\n\tlet aCount = values.map((_, i) =&gt; values.reduce((count, v, j) =&gt; count + (v === 'a' &amp;&amp; j &lt;= i), 0));\r\n\tlet aRate = aCount.map((c, i) =&gt; c / (i + 1));\r\n\tlet deviation = aRate.map(a =&gt; a - desiredARate);\r\n\tlet avgDeviation = deviation.reduce((sum, dev) =&gt; sum + dev, 0) / deviation.length;\r\n\tconsole.log(`inputs: desiredARate = ${desiredARate}; rateStrictness = ${rateStrictness}; average deviation = ${avgDeviation}`);\r\n};\r\n\r\ntest(.5, 0);\r\ntest(.5, .25);\r\ntest(.5, .5);\r\ntest(.5, .75);\r\ntest(.5, 1);\r\ntest(.5, 10);\r\ntest(.5, 100);</code></pre>\n</div>\n</div>\n</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How about rolling the dice twice: First of all decide wether the stream should be chosen randomly or if the ratio should be taken into account. Then for the first case, roll the dice, for the second case take the ratio. Some pseudocode:</p>\n<pre><code>  const toA =\n    Math.random() &gt; 0.5 // 1 -&gt; totally random, 0 -&gt; totally equally distributed\n      ? Math.random() &gt; 0.7\n      :  (numberA / (numberA + numberB) &gt; 0.7);\n</code></pre>\n<p>That's just an idea I came up with, I haven't tried that ...</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a way that combines both of your ideas: It uses a cache. As long as the amount of elements in cache can handle that if the stream ends, we can still approach target distribution, we just roll a dice. If not, we add it to the cache. When input stream ends, we shuffle elements in cache and send them trying to approach distribution. I am not sure if there is any gain in this over just forcing element to go to x if distribution is straying off too much in terms of randomness. </p>\n<p>Beware that this approach does not preserve order from original input stream. A few other things could be added such as cache limit and relaxing distribution error (using 0 here). If you need to preserve order, it can be done by sending cache value and pushing to cache current one instead of just sending current one when there are still elements in cache.</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>let shuffle = (array) =&gt; array.sort(() =&gt; Math.random() - 0.5);\r\n\r\nfunction* generator(numElements) {\r\n  for (let i = 0; i &lt; numElements;i++) yield i; \r\n}\r\n\r\nfunction* splitter(aGroupRate, generator) {\r\n  let cache = [];\r\n  let sentToA = 0;\r\n  let sentToB = 0;\r\n  let bGroupRate = 1 - aGroupRate;\r\n  let maxCacheSize = 0;\r\n  \r\n  let sendValue = (value, group) =&gt; {\r\n      sentToA += group == 0;\r\n      sentToB += group == 1;\r\n      return {value: value, group: group};\r\n  }\r\n  \r\n  function* retRandomGroup(value, expected) {\r\n    while(Math.random() &gt; aGroupRate != expected) {\r\n      if (cache.length) {\r\n        yield sendValue(cache.pop(), !expected);\r\n      } else {\r\n        yield sendValue(value, !expected);\r\n        return;\r\n      } \r\n    }\r\n    yield sendValue(value, expected);\r\n  }\r\n  \r\n  for (let value of generator) {\r\n    if (sentToA + sentToB == 0) {\r\n      yield sendValue(value, Math.random() &gt; aGroupRate);\r\n      continue;\r\n    }\r\n    \r\n    let currentRateA = sentToA / (sentToA + sentToB);\r\n        \r\n    if (currentRateA &lt;= aGroupRate) {\r\n      // can we handle current value going to b group?\r\n      if ((sentToA + cache.length) / (sentToB + sentToA + 1 + cache.length) &gt;= aGroupRate) {\r\n        for (val of retRandomGroup(value, 1)) yield val;\r\n        continue;\r\n      }\r\n    }\r\n    \r\n    if (currentRateA &gt; aGroupRate) {\r\n      // can we handle current value going to a group?\r\n      if (sentToA / (sentToB + sentToA + 1 + cache.length) &lt;= aGroupRate) {\r\n        for (val of retRandomGroup(value, 0)) yield val;\r\n        continue;\r\n      }\r\n    }  \r\n    \r\n    cache.push(value);\r\n    maxCacheSize = Math.max(maxCacheSize, cache.length)\r\n  }\r\n  \r\n  shuffle(cache);\r\n  \r\n  let totalElements = sentToA + sentToB + cache.length;\r\n  \r\n  while (sentToA &lt; totalElements * aGroupRate) {\r\n    yield {value: cache.pop(), group: 0}\r\n    sentToA += 1;\r\n  }\r\n  \r\n  while (cache.length) {\r\n    yield {value: cache.pop(), group: 1}\r\n  }  \r\n  \r\n  yield {cache: maxCacheSize}\r\n}\r\n\r\nfunction test(numElements, aGroupRate) {\r\n  let gen = generator(numElements);\r\n  let sentToA = 0;\r\n  let total = 0;\r\n  let cacheSize = null;\r\n  let split = splitter(aGroupRate, gen);\r\n  for (let val of split) {\r\n    if (val.cache != null) cacheSize = val.cache;\r\n    else {\r\n      sentToA += val.group == 0;\r\n      total += 1\r\n    }\r\n  }\r\n  console.log(\"required rate for A group\", aGroupRate, \"actual rate\", sentToA / total, \"cache size used\", cacheSize);\r\n}\r\n\r\ntest(3000, 0.3)\r\ntest(5000, 0.5)\r\ntest(7000, 0.7)</code></pre>\n</div>\n</div>\n</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to work with Featuretools to develop an automated feature engineering workflow for the customer churn dataset. The end outcome is a function that takes in a dataset and label times for customers and builds a feature matrix that can be used to train a machine learning model.</p>\n<p>As part of this exercise I am trying to  execute the below code for plotting a histogram and got \"<em><strong>TypeError: import_optional_dependency() got an unexpected keyword argument 'errors'</strong></em> \". Please help resolve this TypeError.</p>\n<pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (10, 6)\n\ntrans.loc[trans['actual_amount_paid'] &lt; 250, 'actual_amount_paid'].dropna().plot.hist(bins = 30)\nplt.title('Distribution of Actual Amount Paid')\n</code></pre>\n<p><strong>Below is the full error I received:</strong></p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-32-7e19affd5fc1&gt; in &lt;module&gt;\n      4 plt.rcParams['figure.figsize'] = (10, 6)\n      5 \n----&gt; 6 trans.loc[trans['actual_amount_paid'] &lt; 250, 'actual_amount_paid'].dropna().plot.hist(bins = 30)\n      7 plt.title('Distribution of Actual Amount Paid')\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py in new_method(self, other)\n     63                     break\n     64                 if isinstance(other, cls):\n---&gt; 65                     return NotImplemented\n     66 \n     67         other = item_from_zerodim(other)\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py in __lt__(self, other)\n     35     def __ne__(self, other):\n     36         return self._cmp_method(other, operator.ne)\n---&gt; 37 \n     38     @unpack_zerodim_and_defer(\"__lt__\")\n     39     def __lt__(self, other):\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py in _cmp_method(self, other, op)  \n   4937         --------\n   4938         &gt;&gt;&gt; s = pd.Series(range(3))\n-&gt; 4939         &gt;&gt;&gt; s.memory_usage()\n   4940         152\n   4941 \n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py in comparison_op(left, right, op)\n    248     lvalues = ensure_wrapped_if_datetimelike(left)\n    249     rvalues = ensure_wrapped_if_datetimelike(right)\n--&gt; 250 \n    251     rvalues = lib.item_from_zerodim(rvalues)\n    252     if isinstance(rvalues, list):\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py in _na_arithmetic_op(left, right, op, is_cmp)\n    137 \n    138 def _na_arithmetic_op(left, right, op, is_cmp: bool = False):\n--&gt; 139     \n    140     Return the result of evaluating op on the passed in values.\n    141 \n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in &lt;module&gt;\n     17 from pandas._typing import FuncType\n     18 \n---&gt; 19 from pandas.core.computation.check import NUMEXPR_INSTALLED\n     20 from pandas.core.ops import roperator\n     21 \n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\check.py in &lt;module&gt;\n      1 from pandas.compat._optional import import_optional_dependency\n      2 \n----&gt; 3 ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n      4 NUMEXPR_INSTALLED = ne is not None\n      5 if NUMEXPR_INSTALLED:\n\nTypeError: import_optional_dependency() got an unexpected keyword argument 'errors'\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try to upgrade <code>pandas</code>:</p>\n<pre><code>pip install pandas --upgrade\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am not able to access jupyter lab created on google cloud\n<a href=\"https://i.sstatic.net/z09Tu.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/z09Tu.png\"/></a></p>\n<p>I created one notebook using Google AI platform. I was able to start it and work but suddenly it stopped and I am not able to start it now. I tried building and restarting the jupyterlab, but of no use. I have checked my disk usages as well, which is only 12%.</p>\n<p>I tried the diagnostic tool, which gave the following result:\n<a href=\"https://i.sstatic.net/WhqwC.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/WhqwC.png\"/></a></p>\n<p>but didn't fix it.</p>\n<p>Thanks in advance.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The error might be caused by a corrupted disk partition “/dev/sdb”. After connecting to the notebook instance through SSH, run <code>sudo fsck /dev/sdb</code> to perform a disk check and repair, and then perform a reboot.</p>\n<p>In case this does not help, you can download your data after zipping the required content from the “/home/jupyter/” folder and upload it to a new notebook instance.</p>\n<p>Below is the zip utility command for your reference.</p>\n<p><code>cd /home/</code></p>\n<p><code>sudo zip -r test-1.zip jupyter/</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Its possible that your machine on which jupyter lab is hosted becomes unresponsive. This could be due to many reasons some of which include the notebook causing an out of memory at the OS level, or running something very heavy which causes heavy load on the disk or the CPU.</p>\n<p>A quick fix would be to just restart the machine/Vertex AI notebook.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For me, I faced this issue when storage was full. After increasing it started working.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm not sure what this error means.  This error occurs when I try to calculate <code>acc</code>:</p>\n<pre><code>acc = accuracy.eval(feed_dict = {x: batch_images, y: batch_labels, keep_prob: 1.0})\n</code></pre>\n<p>I've tried looking up solutions, but I couldn't find any online.  Any ideas on what's causing my error?</p>\n<p>Here's a <a href=\"https://github.com/mdlee6/svhn/blob/master/multidigit_prediction.ipynb\" rel=\"noreferrer\">link to my full code</a>.  </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For the people coming for <strong>Tensorflow serving</strong> or <strong>Estimator loading</strong>, this error occurs because the values in the feature dictionary need to be in batches.</p>\n<pre><code>data = {\n        \"signature_name\": \"predict\",\n        \"inputs\": {k:[v] for k,v in inputs.items()}\n    }\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The source code generating this error reads as follows:</p>\n<pre><code>OP_REQUIRES(context, axis &gt;= 0 &amp;&amp; axis &lt; input_dims,\n            errors::InvalidArgument(\"Expected dimension in the range [\",\n                                    -input_dims, \", \", input_dims,\n                                    \"), but got \", dim));\n</code></pre>\n<p>Note that <code>axis</code> is required to be <strong>less than</strong> <code>input_dims</code>, <strong>not less-than-or-equal</strong>.</p>\n<p>This conforms with the syntax <code>[-1,1)</code> in the message: <code>[</code> indicates an inclusive value (such that <code>-1</code> is valid), whereas <code>)</code> indicates an exclusive value (putting <code>1</code> itself outside the range).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had a similar error but the problem for me was that I was trying to use argmax on a 1 dimensional vector. So the shape of my label was (50,) and I was trying to do a tf.argmax(y,1) on that when evaluating. The solution reference is <a href=\"https://stackoverflow.com/questions/44581910/tensorflow-i-get-something-wrong-in-accuracy\">Tensorflow: I get something wrong in accuracy</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am a beginner and getting familiar with pandas .\nIt is throwing an error , When I was trying to create a new column this way :</p>\n<p>drinks['total_servings'] = drinks.loc[: ,'beer_servings':'wine_servings'].apply(calculate,axis=1)</p>\n<p>Below is my code, and I get the following error for line number 9:</p>\n<p>\"<code>Cannot set a DataFrame with multiple columns to the single column total_servings</code>\"</p>\n<p>Any help or suggestion would be appreciated :)</p>\n<pre><code>import pandas as pd\ndrinks = pd.read_csv('drinks.csv')\n\ndef calculate(drinks):\n    return drinks['beer_servings']+drinks['spirit_servings']+drinks['wine_servings']\nprint(drinks)\ndrinks['total_servings'] = drinks.loc[:, 'beer_servings':'wine_servings'].apply(calculate,axis=1)\n\ndrinks['beer_sales'] = drinks['beer_servings'].apply(lambda x: x*2)\ndrinks['spirit_sales'] = drinks['spirit_servings'].apply(lambda x: x*4)\ndrinks['wine_sales'] = drinks['wine_servings'].apply(lambda x: x*6)\ndrinks\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In your code, when function<code>calculate</code> is called with axis=1, it passes each row of the Dataframe as an argument. Here, the function <code>calculate</code> is returning dataframe with multiple columns but you are trying to assigned to a single column, which is not possible. You can try updating your code to this,</p>\n<pre><code>def calculate(each_row):\n    return each_row['beer_servings'] + each_row['spirit_servings'] + each_row['wine_servings']\n\ndrinks['total_servings'] = drinks.apply(calculate, axis=1)\ndrinks['beer_sales'] = drinks['beer_servings'].apply(lambda x: x*2)\ndrinks['spirit_sales'] = drinks['spirit_servings'].apply(lambda x: x*4)\ndrinks['wine_sales'] = drinks['wine_servings'].apply(lambda x: x*6)\n\nprint(drinks)   \n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>so i have pasted my complete code for your reference, i want to know what's the use of ppf and cdf here? can you explain it? i did some research and found out that ppf(percent point function) is an inverse of CDF(comulative distribution function)\nif they really are, shouldn't this code work if i replaced ppf and cdf as 1/cdf and 1/ppf respectively?</p>\n<p>please explain this to me, the difference between the two. and how to and when to use which</p>\n<p>this is, btw, hypothesis testing.\nand sorry for so many comments, just a habit of explaining everything for my future reference.(do point me out if any of my comments is wrong regarding the same)</p>\n<pre><code>ball_bearing_radius = [2.99, 2.99, 2.70, 2.92, 2.88, 2.92, 2.82, 2.83, 3.06, 2.85]\n\n\n\n\nimport numpy as np\n\nfrom math import sqrt\nfrom scipy.stats import norm\n\n# h1 : u != U_0\n# h0 : u = u_0\n#case study : ball bearing example, claim is that radius = 3, do hypothesis testing \nmu_0 = 3\nsigma = 0.1\n\n#collect sample\nsample = ball_bearing_radius\n\n#compute mean\nmean = np.mean(sample)\n\n#compute n\nn = len(sample)\n\n#compute test statistic\nz = (mean - mu_0) /(sigma/sqrt(n))\n\n#set alpha\na = 0.01\n\n#-------------------------\n\n#calculate the z_a/2, by using percent point function of the norm of scipy\n#ppf = percent point function, inverse of CDF(comulative distribution function)\n#also, CDF = pr(X&lt;=x), i.e., probability to the left of the distribution\n\nz_critical = norm.ppf(1-a/2)    #this returns a value for which the probab to the left is 0.975\n\np_value = 2*(1 - norm.cdf(np.abs(z)))\n\np_value = float(\"{:.4f}\".format(p_value))\n\n\nprint('z : ',z)\nprint('\\nz_critical :', z_critical)\nprint('\\nmean :', mean, \"\\n\\n\")\n\n#test the hypothesis\n\nif (np.abs(z) &gt; z_critical):\n    print(\"\\nREJECT THE NULL HYPOTHESIS : \\n p-value = \", p_value, \"\\n Alpha = \", a )\n\nelse:\n    print(\"CANNOT REJECT THE NULL HYPOTHESIS. NOT ENOUGH EVIDENCE TO REJECT IT: \\n p-value = \", p_value, \"\\n Alpha = \", a )\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <code>.cdf()</code> function calculates the probability for a given normal distribution value, while the <code>.ppf()</code> function calculates the normal distribution value for which a given probability is the required value. These are inverse of each other in this particular sense.</p>\n<p>To illustrate this calculation, check the below sample code.</p>\n<pre><code>from scipy.stats import norm\nprint(norm.ppf(0.95))\nprint(norm.cdf(1.6448536269514722))\n</code></pre>\n<p><a href=\"https://i.sstatic.net/aZSLH.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/aZSLH.png\"/></a></p>\n<p>This image with the code above should make it clear for you.</p>\n<p>Thanks!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here I have a list with different length vectors. And I'd want to get a data.frame. I've seen lots of posts about it in SO (see ref), but none of them are as simple as I expected because this is really a common task in data preprocessing. Thank you.</p>\n<p>Here simplest means <code>as.data.frame(aa)</code> if it works. So one function from the base package of R will be great. <code>sapply(aa, \"length&lt;-\", max(lengths(aa)))</code> has four functions actually. </p>\n<p>An example is shown below.</p>\n<p>Input:</p>\n<pre><code>aa &lt;- list(A=c(1, 3, 4), B=c(3,5,7,7,8))\n</code></pre>\n<p>Output:</p>\n<pre><code>A B\n1 3\n3 5\n4 7\nNA 7\nNA 8\n</code></pre>\n<p>A and B are the colnames of the data.frame.</p>\n<p>One answer is <code>sapply(aa, '[', seq(max(sapply(aa, length))))</code>, but it's also complex.</p>\n<p>ref: </p>\n<ol>\n<li><p><a href=\"https://stackoverflow.com/questions/15201305/how-to-convert-a-list-consisting-of-vector-of-different-lengths-to-a-usable-data\">How to convert a list consisting of vector of different lengths to a usable data frame in R?</a></p></li>\n<li><p><a href=\"https://stackoverflow.com/questions/5531471/combining-unequal-columns-in-r\">Combining (cbind) vectors of different length</a></p></li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>We can use</p>\n<pre><code>data.frame(lapply(aa, \"length&lt;-\", max(lengths(aa))))\n</code></pre>\n<p>Or using <code>tidyverse</code></p>\n<pre><code>library(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nenframe(aa) %&gt;%\n    unnest(value)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using tidyverse packages. Place the list in a nested data frame. Extract the <code>name</code> for each vector in the list. Unnest the data frame. Give a row index <code>i</code> for each element in each vector, spread the data in wide format</p>\n<pre><code>    aa &lt;- list(A = c(1, 3, 4), B = c(3, 5, 7, 7, 8))\n    library(tidyverse)\n    data_frame(data = aa) %&gt;% \n        group_by(name = names(data)) %&gt;% \n        unnest() %&gt;%\n        mutate(i = row_number()) %&gt;% \n        spread(name, data)\n    # A tibble: 5 x 3\n          i     A     B\n    * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n    1     1     1     3\n    2     2     3     5\n    3     3     4     7\n    4     4    NA     7\n    5     5    NA     8\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Make this function:</p>\n<pre><code>listToDF &lt;- function(aa){\n  sapply(aa, \"length&lt;-\", max(lengths(aa)))\n }\n</code></pre>\n<p>Then use it, simply:</p>\n<pre><code>listToDF(aa)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using the <code>iris</code> dataset I'm trying to calculate a z score for each of the variables. I have the data in tidy format, by performing the following:</p>\n<pre><code>library(reshape2)\nlibrary(dplyr)\ntest &lt;- iris\ntest &lt;- melt(iris,id.vars = 'Species')\n</code></pre>\n<p>That gives me the following:</p>\n<pre><code>  Species     variable value\n1  setosa Sepal.Length   5.1\n2  setosa Sepal.Length   4.9\n3  setosa Sepal.Length   4.7\n4  setosa Sepal.Length   4.6\n5  setosa Sepal.Length   5.0\n6  setosa Sepal.Length   5.4\n</code></pre>\n<p>But when I try to create a z-score column for each group (e.g. the z-score for Sepal.Length will not be comparable to that of Sepal. Width) using the following:</p>\n<pre><code>test &lt;- test %&gt;% \n  group_by(Species, variable) %&gt;% \n  mutate(z_score = (value - mean(value)) / sd(value))\n</code></pre>\n<p>The resulting z-scores have not been grouped, and are based on all of the data. </p>\n<p>What's the best way to return the z-scores by group using dpylr?</p>\n<p>Many thanks! </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I believe that you were complicating when computing z-scores with <code>mean/sd</code>. Just use function <code>scale</code>.</p>\n<pre><code>test &lt;- test %&gt;% \n  group_by(Species, variable) %&gt;% \n  mutate(z_score = scale(value))\n\ntest\n## A tibble: 600 x 4\n## Groups:   Species, variable [12]\n#   Species     variable value     z_score\n#    &lt;fctr&gt;       &lt;fctr&gt; &lt;dbl&gt;       &lt;dbl&gt;\n# 1  setosa Sepal.Length   5.1  0.26667447\n# 2  setosa Sepal.Length   4.9 -0.30071802\n# 3  setosa Sepal.Length   4.7 -0.86811050\n# 4  setosa Sepal.Length   4.6 -1.15180675\n# 5  setosa Sepal.Length   5.0 -0.01702177\n# 6  setosa Sepal.Length   5.4  1.11776320\n# 7  setosa Sepal.Length   4.6 -1.15180675\n# 8  setosa Sepal.Length   5.0 -0.01702177\n# 9  setosa Sepal.Length   4.4 -1.71919923\n#10  setosa Sepal.Length   4.9 -0.30071802\n## ... with 590 more rows\n</code></pre>\n<p><strong>Edit.</strong><br/>\nFollowing a comment by the OP, I am posting some code to get the rows where <code>Petal.Width</code> has a positive <code>z_score</code>.</p>\n<pre><code>i1 &lt;- which(test$variable == \"Petal.Width\" &amp; test$z_score &gt; 0)\ntest[i1, ]\n## A tibble: 61 x 4\n## Groups:   Species, variable [3]\n#   Species    variable value  z_score\n#    &lt;fctr&gt;      &lt;fctr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n# 1  setosa Petal.Width   0.4 1.461300\n# 2  setosa Petal.Width   0.3 0.512404\n# 3  setosa Petal.Width   0.4 1.461300\n# 4  setosa Petal.Width   0.4 1.461300\n# 5  setosa Petal.Width   0.3 0.512404\n# 6  setosa Petal.Width   0.3 0.512404\n# 7  setosa Petal.Width   0.3 0.512404\n# 8  setosa Petal.Width   0.4 1.461300\n# 9  setosa Petal.Width   0.5 2.410197\n#10  setosa Petal.Width   0.4 1.461300\n## ... with 51 more rows\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your code is giving you z-scores by group. It seems to me these z-scores <em>should</em> be comparable exactly because you've individually scaled each group to mean=0 and sd=1, rather than scaling each value based on the mean and sd of the full data frame. For example:</p>\n<pre><code>library(tidyverse)\n</code></pre>\n<p>First, set up the melted data frame:</p>\n<pre><code>dat = iris %&gt;% \n  gather(variable, value, -Species) %&gt;%\n  group_by(Species, variable) %&gt;% \n  mutate(z_score_group = (value - mean(value)) / sd(value)) %&gt;%   # You can also use scale(value) as pointed out by @RuiBarradas\n  ungroup %&gt;% \n  mutate(z_score_ungrouped = (value - mean(value)) / sd(value)) \n</code></pre>\n<p>Now look at the first three rows and compare with direct calculation:</p>\n<pre><code>head(dat, 3)\n\n#   Species     variable value z_score_group z_score_ungrouped\n# 1  setosa Sepal.Length   5.1     0.2666745         0.8278959\n# 2  setosa Sepal.Length   4.9    -0.3007180         0.7266552\n# 3  setosa Sepal.Length   4.7    -0.8681105         0.6254145\n\n# z-scores by group\nwith(dat, (value[1:3] - mean(value[Species==\"setosa\" &amp; variable==\"Sepal.Length\"])) / sd(value[Species==\"setosa\" &amp; variable==\"Sepal.Length\"]))\n\n# [1]  0.2666745 -0.3007180 -0.8681105\n\n# ungrouped z-scores\nwith(dat, (value[1:3] - mean(value)) / sd(value))\n\n# [1] 0.8278959 0.7266552 0.6254145\n</code></pre>\n<p>Now visualize the z-scores: The first graph below is the raw data. The second is the ungrouped z-scores--we've just rescaled the data to an overall mean=0 and SD=1. The third graph is what your code produces. Each group has been <em>individually</em> scaled to mean=0 and SD=1.</p>\n<pre><code>gridExtra::grid.arrange(\n  grobs=setNames(names(dat)[c(3,5,4)], names(dat)[c(3,5,4)]) %&gt;% \n    map(~ ggplot(dat %&gt;% mutate(group=paste(Species,variable,sep=\"_\")), \n                 aes_string(.x, colour=\"group\")) + geom_density()),\n  ncol=1)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/KkzLX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/KkzLX.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>import pandas as pd\nimport os\nimport glob\n\n\nall_data = pd.DataFrame()\nfor f in glob.glob(\"output/test*.xlsx\")\n    df = pd.read_excel(f)\n    all_data = all_data.append(df, ignore_index=True)\n</code></pre>\n<p>I want to put multiple xlsx files into one xlsx. the excel files are in the output/test folder. The columns are the same, in all but I want concat the rows. the above code doesn't seem to work</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Let <code>all_data</code> be a list.</p>\n<pre><code>all_data = []\nfor f in glob.glob(\"output/test/*.xlsx\"):\n    all_data.append(pd.read_excel(f))\n</code></pre>\n<p>Now, call <code>pd.concat</code>:</p>\n<pre><code>df = pd.concat(all_data, ignore_index=True)\n</code></pre>\n<p>Make sure all column names are the same, otherwise this solution won't work.</p>\n<hr/>\n<p>You could also use a <code>map</code> version of the <code>for</code> loop above:</p>\n<pre><code>g = map(pd.read_excel, glob.glob(\"output/test/*.xlsx\"))\ndf = pd.concat(list(g), ignore_index=True)\n</code></pre>\n<p>Or the <em>list comprhension</em> method as shown in the other answer.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <code>list comprehension</code> + <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\" rel=\"noreferrer\"><code>concat</code></a>:</p>\n<pre><code>all_data = [pd.read_excel(f) for f in glob.glob(\"output/test/*.xlsx\")]\ndf = pd.concat(all_data, ignore_index=True)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm working with MNIST dataset from <a href=\"https://www.kaggle.com/c/digit-recognizer\" rel=\"noreferrer\">Kaggle challange</a> and have troubles preprocessing with data. Furthermore, I don't know what are the best practices and was wondering if you could advise me on that.</p>\n<p>Disclaimer: I can't just use torchvision.datasets.mnist because I need to use Kaggle's data for training and submission. </p>\n<p>In <a href=\"https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\" rel=\"noreferrer\">this</a> tutorial, it was advised to create a Dataset object loading .pt tensors from files, to fully utilize GPU. In order to achieve that, I needed to load the csv data provided by Kaggle and save it as .pt files:</p>\n<pre><code>import pandas as pd\nimport torch\nimport numpy as np\n\n# import data\ndigits_train = pd.read_csv('data/train.csv')\n\ntrain_tensor = torch.tensor(digits_train.drop(label, axis=1).to_numpy(), dtype=torch.int)\nlabels_tensor = torch.tensor(digits_train[label].to_numpy()) \n\nfor i in range(train_tensor.shape[0]):\n    torch.save(train_tensor[i], \"data/train-\" + str(i) + \".pt\")\n\n</code></pre>\n<p>Each <code>train_tensor[i].shape</code> is <code>torch.Size([1, 784])</code></p>\n<p>However, each such .pt file has size of about 130MB.\nA tensor of the same size, with randomly generated integers, has size of 6.6kB.\nWhy are these tensors so huge, and how can I reduce their size?</p>\n<p>Dataset is 42 000 samples. Should I even bother with batching this data? Should I bother with saving tensors to separate files, rather than loading them all into RAM and then slicing into batches? What is the most optimal approach here?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As explained in <a href=\"https://discuss.pytorch.org/t/saving-tensor-with-torch-save-uses-too-much-memory/46865/2\" rel=\"noreferrer\">this discussion</a>, <code>torch.save()</code> saves the whole tensor, not just the slice. You need to explicitly copy the data using <code>clone()</code>.</p>\n<p>Don't worry, at runtime the data is only allocated once unless you explicitly create copies.</p>\n<p>As a general advice: If the data easily fits into your memory, just load it at once. For MNIST with 130 MB that's certainly the case.</p>\n<p>However, I would still batch the data because it converges faster. Look up the advantages of SGD for more details.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>Objective</h1>\n<p>To clarify by having what traits or attributes, I can say an analysis is inferential or predictive.</p>\n<h2>Background</h2>\n<p>Taking a data science course which touches on analyses of Inferential and Predictive. The explanations (what I understood) are</p>\n<ul>\n<li><p>Inferential</p>\n<p>Induct a hypothesis from a small samples in a population, and see it is true in larger/entire population. </p>\n<p>It seems to me it is generalisation. I think induct smoking causes lung cancer or CO2 causes global warming are inferential analyses.</p></li>\n<li><p>Predictive</p>\n<p>Induct a statement of what can happen by measuring variables of an object. </p>\n<p>I think, identify what traits, behaviour, remarks people react favourably and make a presidential candidate popular enough to be the president is a predictive analysis (this is touched in the course as well).</p></li>\n</ul>\n<h2>Question</h2>\n<p>I am bit confused with the two as it looks to me there is a grey area or overlap. </p>\n<p><a href=\"https://en.wikipedia.org/wiki/Bayesian_inference\" rel=\"nofollow\">Bayesian Inference</a> is \"inference\" but I think it is used for prediction such as in a spam filter or fraudulent financial transaction identification. For instance, a bank may use previous observations on variables (such as IP address, originator country, beneficiary account type, etc) and predict if a transaction is fraudulent. </p>\n<p>I suppose the <a href=\"https://en.wikipedia.org/wiki/Tests_of_general_relativity\" rel=\"nofollow\">theory of relativity</a> is an inferential analysis that inducted a theory/hypothesis from observations and thought experimentations, but it also predicted light direction would be bent.</p>\n<p>Kindly help me to understand what are Must Have attributes to categorise an analysis as inferential or predictive.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><a href=\"https://www.d.umn.edu/~kgilbert/ened5560-1/The%20Research%20Question-2015-Leek-1314-5.pdf\" rel=\"noreferrer\">\"What is the question?\" by Jeffery T. Leek, Roger D. Peng</a> has a nice description of the various types of analysis that go into a typical data science workflow. To address your question specifically:</p>\n<blockquote>\n<p>An inferential data analysis quantifies whether an observed pattern\n  will likely hold beyond the data set in hand. This is the most common\n  statistical analysis in the formal scientific literature. An example\n  is a study of whether air pollution correlates with life expectancy at\n  the state level in the United States (9). In nonrandomized\n  experiments, it is usually only possible to determine the existence of\n  a relationship between two measurements, but not the underlying\n  mechanism or the reason for it.</p>\n<p>Going beyond an inferential data analysis, which quantifies the\n  relationships at population scale, a predictive data analysis uses a\n  subset of measurements (the features) to predict another measurement\n  (the outcome) on a single person or unit. Web sites like\n  FiveThirtyEight.com use polling data to predict how people will vote\n  in an election. Predictive data analyses only show that you can\n  predict one measurement from another; they do not necessarily explain\n  why that choice of prediction works.</p>\n</blockquote>\n<p><a href=\"https://i.sstatic.net/z7l3f.png\" rel=\"noreferrer\"><img alt=\"data analysis flowchart\" src=\"https://i.sstatic.net/z7l3f.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There is some gray area between the two but we can still make distinctions.</p>\n<p>Inferential statistics is when you are trying to understand what causes a certain outcome. In such analyses there is a specific focus on the independent variables and you want to make sure you have an interpretable model. For instance,  your example on a study to examine whether smoking causes lung cancer is inferential. Here you are trying to closely examine the factors that lead to lung cancer, and smoking happens to be one of them.</p>\n<p>In predictive analytics you are more interested in using a certain dataset to help you predict future variation in the values of the outcome variable. Here you can make your model as complex as possible to the point that it is not interpretable as long as it gets the job done. A more simplified example is a real estate investment company interested in determining which combination of variables predicts prime price for a certain property so it can acquire them for profit. The potential predictors could be neighborhood income, crime, educational status, distance to a beach, and racial makeup. The primary aim here is to obtain an optimal combination of these variables that provide a better prediction of future house prices. </p>\n<p>Here is where it gets murky. Let's say you conduct a study on middle aged men to determine the risks of heart disease. To do this you measure weight, height, race, income, marital status, cholestrol, education, and a potential serum chemical called \"mx34\" (just making this up) among others. Let's say you find that the chemical is indeed a good risk factor for heart disease. You have now achieved your inferential objective. However, you are satisfied with your new findings and you start to wonder whether you can use these variables to predict who is likely to get heart disease. You want to do this so that you can recommend preventive steps to prevent future heart disease.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The same academic paper I was reading that spurred this question for me also gave an answer (from Leo Breiman, a UC Berkeley statistician):</p>\n<blockquote>\n<p>• Prediction. To be able to predict what the responses are going to be\n  to future input variables;</p>\n<p>• [Inference].<sup>23</sup> To [infer] how nature is associating the response\n  variables to the input variables.</p>\n</blockquote>\n<p>Source: <a href=\"http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf\" rel=\"nofollow noreferrer\">http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose i have the data frame below:</p>\n<p><img alt=\"Selection_101\" src=\"https://user-images.githubusercontent.com/55730940/76497540-25b8ad80-6476-11ea-8cca-7f1a34adbff5.png\"/></p>\n<p>I checked the <a href=\"https://altair-viz.github.io/gallery/line_with_ci.html\" rel=\"noreferrer\">documentation</a> but it's only based on a single column. </p>\n<p>Reproducible code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = np.random.normal(100,5,100)\ndata = pd.DataFrame(x)\nepsilon = 10\ndata.columns = ['x']\ndata['lower'] = x - epsilon\ndata['upper'] = x + epsilon\ndata\n\n</code></pre>\n<p>I'd actually like to use altair, since i like it's interactivity.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can layer a line and an area chart, usng the <code>y</code> and <code>y2</code> encodings to specify the range:</p>\n<pre><code>import altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.random.normal(100,5,100)\nepsilon = 10\ndata = pd.DataFrame({\n    'x': x,\n    'lower': x - epsilon,\n    'upper': x + epsilon\n}).reset_index()\n\nline = alt.Chart(data).mark_line().encode(\n    x='index',\n    y='x'\n)\n\nband = alt.Chart(data).mark_area(\n    opacity=0.5\n).encode(\n    x='index',\n    y='lower',\n    y2='upper'\n)\n\nband + line\n</code></pre>\n<p><a href=\"https://i.sstatic.net/rjgi9.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/rjgi9.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I just started with GridSearchCV in Python, but I am confused what is scoring in this. Somewhere I have seen </p>\n<pre><code>scorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score)\n}\n\ngrid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n                       cv=skf, return_train_score=True, n_jobs=-1)\n</code></pre>\n<p>What is the intent of using these values, i.e. precision, recall, accuracy in scoring? </p>\n<p>Is this used by gridsearch in giving us the optimized parameters based on these scoring values.... like for the best precision score it finds the best parameters or something like that? </p>\n<p>It calculate precision, recall, accuracy for the possible parameters and gives the result, now the question is if this is true, then it select best parameters based on precision, recall or accuracy? Is the above statement true?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are basically correct in your assumptions. This parameter dictionary allows the gridsearch to optimize across each scoring metric and find the best parameters for each score.</p>\n<p>However, you can't then have the gridsearch automatically fit and return the <code>best_estimator_</code>, without choosing which score to use for the <code>refit</code>, it will instead throw the following error:</p>\n<pre><code>ValueError: For multi-metric scoring, the parameter refit must be set to a scorer \nkey to refit an estimator with the best parameter setting on the whole data and make\nthe best_* attributes available for that metric. If this is not needed, refit should \nbe set to False explicitly. True was passed.\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>What is the intent of using these values, i.e. precision, recall, accuracy in scoring?</p>\n</blockquote>\n<p>Just in case your question also includes \"What are precision, recall, and accuracy and why are they used?\"...</p>\n<ul>\n<li>Accuracy = (number of correct predictions)/(total predictions)</li>\n<li>Precision = (true positives)/(true positives + false positives)</li>\n<li>Recall = (true positives)/(true positives + false negatives)</li>\n</ul>\n<p>Where a true positive is a prediction of true that is correct, a false positive is a prediction of true which is incorrect, and a false negative is a prediction of false that is incorrect.</p>\n<p>Recall and Precision are useful metrics when working with unbalanced datasets (i.e., there are a lot of samples with label '0', but much fewer samples with label '1'.</p>\n<p>Recall and Precision also lead into slightly more complicated scoring metrics like F1_score (and Fbeta_score), which are also very useful.</p>\n<p>Here's a <a href=\"https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\" rel=\"nofollow noreferrer\">great article</a> explaining how recall and precision work.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to have a MAPE function, however I was not able to find it in standard packages ... Below, my implementation of this function.</p>\n<pre><code>def mape(actual, predict): \n    tmp, n = 0.0, 0\n    for i in range(0, len(actual)):\n        if actual[i] &lt;&gt; 0:\n            tmp += math.fabs(actual[i]-predict[i])/actual[i]\n            n += 1\n    return (tmp/n)\n</code></pre>\n<p>I don't like it, it's super not optimal in terms of speed. How to rewrite the code to be more Pythonic way and boost the speed?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's one vectorized approach with <a href=\"https://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays\" rel=\"noreferrer\"><code>masking</code></a> -</p>\n<pre><code>def mape_vectorized(a, b): \n    mask = a &lt;&gt; 0\n    return (np.fabs(a[mask] - b[mask])/a[mask]).mean()\n</code></pre>\n<p>Probably a faster one with <code>masking</code> after <code>division</code> computation -</p>\n<pre><code>def mape_vectorized_v2(a, b): \n    mask = a &lt;&gt; 0\n    return (np.fabs(a - b)/a)[mask].mean() \n</code></pre>\n<p>Runtime test -</p>\n<pre><code>In [217]: a = np.random.randint(-10,10,(10000))\n     ...: b = np.random.randint(-10,10,(10000))\n     ...: \n\nIn [218]: %timeit mape(a,b)\n100 loops, best of 3: 11.7 ms per loop\n\nIn [219]: %timeit mape_vectorized(a,b)\n1000 loops, best of 3: 273 µs per loop\n\nIn [220]: %timeit mape_vectorized_v2(a,b)\n1000 loops, best of 3: 220 µs per loop\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My code below is giving me the following error \"IndexError: too many indices for array\". I am quite new to machine learning so I do not have any idea about how to solve this. Any kind of help would be appreciated.</p>\n<pre><code>train = pandas.read_csv(\"D:/...input/train.csv\")\n\n\nxTrain = train.iloc[:,0:54]\nyTrain = train.iloc[:,54:]\n\n\nfrom sklearn.cross_validation import cross_val_score\nclf = LogisticRegression(multi_class='multinomial')\nscores = cross_val_score(clf, xTrain, yTrain, cv=10, scoring='accuracy')\nprint('****Results****')\nprint(scores.mean())\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<h1>Step by Step Explanation of ML (Machine Learning) Code with Pandas Dataframe :</h1>\n<ol>\n<li><p>Seperating Predictor and Target Columns into X and y Respectively.</p></li>\n<li><p>Splitting Training data (X_train,y_train) and Testing Data (X_test,y_test).</p></li>\n<li><p>Calculating Cross-Validated AUC (Area Under the Curve). Got an Error “<strong>IndexError: too many indices for array</strong>” due to <strong>y_train</strong> since it was expecting a 1-D Array but Fetched 2-D Array which is a Mismatch. After <strong>Replacing</strong> the code <strong>'y_train'</strong> with <strong>y_train['y']</strong> code <strong>worked like a Charm</strong>.  </p></li>\n</ol>\n<hr/>\n<pre><code>   # Importing Packages :\n\n   import pandas as pd\n\n   from sklearn.model_selection import cross_val_score\n\n   from sklearn.model_selection import StratifiedShuffleSplit\n\n   # Seperating Predictor and Target Columns into X and y Respectively :\n   # df -&gt; Dataframe extracted from CSV File\n\n   data_X = df.drop(['y'], axis=1) \n   data_y = pd.DataFrame(df['y'])\n\n   # Making a Stratified Shuffle Split of Train and Test Data (test_size=0.3 Denotes 30 % Test Data and Remaining 70% Train Data) :\n\n   rs = StratifiedShuffleSplit(n_splits=2, test_size=0.3,random_state=2)       \n   rs.get_n_splits(data_X,data_y)\n\n   for train_index, test_index in rs.split(data_X,data_y):\n\n       # Splitting Training and Testing Data based on Index Values :\n\n       X_train,X_test = data_X.iloc[train_index], data_X.iloc[test_index]\n       y_train,y_test = data_y.iloc[train_index], data_y.iloc[test_index]\n\n       # Calculating 5-Fold Cross-Validated AUC (cv=5) - Error occurs due to Dimension of **y_train** in this Line :\n\n       classify_cross_val_score = cross_val_score(classify, X_train, y_train, cv=5, scoring='roc_auc').mean()\n\n       print(\"Classify_Cross_Val_Score \",classify_cross_val_score) # Error at Previous Line.\n\n       # Worked after Replacing 'y_train' with y_train['y'] in above Line \n       # where y is the ONLY Column (or) Series Present in the Pandas Data frame \n       # (i.e) Target variable for Prediction :\n\n       classify_cross_val_score = cross_val_score(classify, X_train, y_train['y'], cv=5, scoring='roc_auc').mean()\n\n       print(\"Classify_Cross_Val_Score \",classify_cross_val_score)\n\n       print(y_train.shape)\n\n       print(y_train['y'].shape)\n</code></pre>\n<h1>Output :</h1>\n<pre><code>    Classify_Cross_Val_Score  0.7021433588790991\n    (31647, 1) # 2-D\n    (31647,)   # 1-D\n</code></pre>\n<p><em>Note : <strong>from sklearn.model_selection import cross_val_score</strong>.\ncross_val_score has been imported \nfrom sklearn.model_selection and \nNOT from sklearn.cross_validation which is Deprecated.</em></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The error code you're getting is basically saying you've declared contents for your array that don't fit it.\nI can't see the declaration of your array but I'm assuming it's one dimensional and the program is objecting to you treating it like a 2 dimensional one. </p>\n<p>Just check your declarations are correct and also <strong>test the code by printing the values after you've set them to double check they are what you intend them to be.</strong> </p>\n<p>There are a few existing questions on this subject already so i'll just link one that might be helpful here:\n<a href=\"https://stackoverflow.com/questions/29199585/indexerror-too-many-indices-numpy-array-with-1-row-and-2-columns\">IndexError: too many indices. Numpy Array with 1 row and 2 columns</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are getting this error because you are making target array 'y' 2-D which is actually needed to be 1-D to pass in cross validation function.</p>\n<p>These two cases are different:</p>\n<pre><code>1. y=numpy.zeros(shape=(len(list),1))\n2. y=numpy.zeros(shape=(len(list))) \n</code></pre>\n<p>If you declare y like case 1 then y becomes 2-D. But you needed a 1-D array, hence, use case 2.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm plotting the missions ran by the USAF on North Korea during the Korean War.</p>\n<p>The following is the map with 2800 plots.</p>\n<p><a href=\"https://i.sstatic.net/GGzn9.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/GGzn9.jpg\"/></a></p>\n<p>I have a total of about 7500 plots, but whenever I try to plot above 2800 a blank map renders. I'm rendering on a pc laptop. Would it render if I use a desktop? Or is this a limit with folium?</p>\n<p>I'm not speculating that it's an issue with the data. I'll share the coordinates data in case someone would like to explore it: <a href=\"https://docs.google.com/spreadsheets/d/16OLqtxBz6PIJmq57fX0375zNbkatPg1CQDVBXKAlAew/edit#gid=1204279229\" rel=\"noreferrer\">link</a> to public excel sheet.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As @Bob Haffner suggested you can use FastMarkerCluster from Folium library.\nHere is my code, in my file there is ~500K points.</p>\n<pre><code>import pandas as pd\nimport json\nfrom folium.plugins import FastMarkerCluster\n\nrome_lat, rome_lng = 41.9028, 12.4964\nwith open(\"file_name.json\", 'r') as f:\n  # create a new DataFrame\n  samples = pd.DataFrame(json.loads(f.read()))        \n# init the folium map object\nmy_map = folium.Map(location=[rome_lat, rome_lng], zoom_start=5)\n# add all the point from the file to the map object using FastMarkerCluster\nmy_map.add_child(FastMarkerCluster(samples[['latitude', 'longitude']].values.tolist()))\n# save the map \nmy_map.save(\"save_file.html\")\n</code></pre>\n<p>This code takes ~10ms to render the map.</p>\n<p>For more details example please follow this link:\n<a href=\"https://github.com/bobhaffner/medium_posts/blob/a059ef00be902db8da40cd356cce74d3aa00f5ca/folium_markerclusters/folium_markerclusters.ipynb\" rel=\"noreferrer\">FastMarkerCluster example</a></p>\n<p>Hope this is helpful.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Another option is that we can add a specific number of markers(let's say 3000 markers) on a layer, using <code>folium.map.FeatureGroup()</code> function that will add 3000 markers on a single layer, and we can add that layer to the map using <code>add_child()</code> function, which reduces the number of layers on the map. I got the result for 20,000 Markers and 3000 line string. And is able to load within 40-45 seconds.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Say <code>n_repeats=5</code> and the number of fold is 3 (<code>n_splits=3</code>).</p>\n<p>Does that mean the validator is creating 3-folds for our estimator/model to use every fold (like what KFold is for), then repeating that process for 5 times?</p>\n<p>That means our model will use a total of 5 x 3 = 15 folds?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, you can basically achieve the same effect by calling <code>KFolds.split()</code> <code>n_repeats</code> times in a loop.</p>\n<p>Example setup:</p>\n<pre><code>X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\n</code></pre>\n<p>Then running:</p>\n<pre><code>rkf = RepeatedKFold(n_splits=2, n_repeats=1, random_state=2652124)\nfor train_index, test_index in rkf.split(X):\n  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n</code></pre>\n<p>... produces:</p>\n<pre><code>TRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\n</code></pre>\n<p>... just like <code>KFold(n_splits=2, random_state=2652124)</code> would. Changing to <code>n_repeats=2</code> produces:</p>\n<pre><code>TRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\n</code></pre>\n<p>And so on.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Hello everyone on Stack Overflow. Today, I would like to ask something very different question. </p>\n<p>I am currently working as a data scientist, and I work alot on JupyterLab/Notebook. Couple of my co-workers use Notebook instead of JupyterLab. It seems like there are not much difference between those two (I really like how JupyterLab presents codes in different colors). I searched on the internet, and it says</p>\n<p>\"<em>JupyterLab is the next generation of the Jupyter Notebook</em>\"</p>\n<p>However, some featuers like plotly figures do not work well on JupyterLab but works well on Jupyter Notebook. I do not know why this is happening.</p>\n<p>Can anyone who work on these two tell me the actual differences?</p>\n<p><a href=\"https://i.sstatic.net/nYYoU.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/nYYoU.png\"/></a></p>\n<p>Thank you for your replies!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I used to work with Jupyter Notebook for about 4 years and last year I switched to Jupyter Lab. I think it is a nice improvement, I believe the biggest advantage is the improved user interface: it is much easier to switch between notebooks and everything feels more organized with tabs.</p>\n<p>Concerning functionalities, the JupyterLab improvement is being modular: you can easily write your own plugins if you need it. </p>\n<p>I have no experience using plotly, but I have similar issues with other packages. Everything was fixed in a short time, so I wouldn't worry too much about it.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've create a pipeline as follows (using the <a href=\"https://keras.io/scikit-learn-api/\" rel=\"noreferrer\">Keras Scikit-Learn API</a>)</p>\n<pre><code>estimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\n</code></pre>\n<p>and fit it with</p>\n<pre><code>pipeline.fit(trainX,trainY)\n</code></pre>\n<p>If I predict with <code>pipline.predict(testX)</code>, I (believe) I get standardised predictions.</p>\n<p><strong>How do I predict on <code>testX</code> so that <code>predictedY</code> it at the same scale as the actual (untouched) <code>testY</code> (i.e. NOT standardised prediction, but instead the actual values)?</strong> I see there is an <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.inverse_transform\" rel=\"noreferrer\"><code>inverse_transform</code> method for Pipeline</a>, however appears to be for only reverting a transformed <code>X</code>.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Exactly. The StandardScaler() in a pipeline is only mapping the inputs (trainX) of pipeline.fit(trainX,trainY).</p>\n<p>So, if you fit your model to approximate trainY and you need it to be standardized as well, you should map your trainY as</p>\n<pre><code>scalerY = StandardScaler().fit(trainY)  # fit y scaler\npipeline.fit(trainX, scalerY.transform(trainY))  # fit your pipeline to scaled Y\ntestY = scalerY.inverse_transform(pipeline.predict(testX))  # predict and rescale\n</code></pre>\n<p>The inverse_transform() function maps its values considering the standard deviation and mean calculated in StandardScaler().fit().</p>\n<p>You can always fit your model without scaling Y, as you mentioned, but this can be dangerous depending on your data since it can lead your model to overfit. You have to test it ;)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Question</strong></p>\n<p>In this datafile, the United States is broken up into four regions using the \"REGION\" column. </p>\n<p>Create a query that finds the counties that belong to regions 1 or 2, whose name starts with 'Washington', and whose POPESTIMATE2015 was greater than their POPESTIMATE 2014.</p>\n<p><em>This function should return a 5x2 DataFrame with the columns = ['STNAME', 'CTYNAME'] and the same index ID as the census_df (sorted ascending by index).</em></p>\n<p><strong>CODE</strong></p>\n<pre><code>    def answer_eight():\n    counties=census_df[census_df['SUMLEV']==50]\n    regions = counties[(counties[counties['REGION']==1]) | (counties[counties['REGION']==2])]\n    washingtons = regions[regions[regions['COUNTY']].str.startswith(\"Washington\")]\n    grew = washingtons[washingtons[washingtons['POPESTIMATE2015']]&gt;washingtons[washingtons['POPESTIMATES2014']]]\n    return grew[grew['STNAME'],grew['COUNTY']]\n\noutcome = answer_eight()\nassert outcome.shape == (5,2)\nassert list (outcome.columns)== ['STNAME','CTYNAME']\nprint(tabulate(outcome, headers=[\"index\"]+list(outcome.columns),tablefmt=\"orgtbl\"))\n</code></pre>\n<p><strong>ERROR</strong></p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-77-546e58ae1c85&gt; in &lt;module&gt;()\n      6     return grew[grew['STNAME'],grew['COUNTY']]\n      7 \n----&gt; 8 outcome = answer_eight()\n      9 assert outcome.shape == (5,2)\n     10 assert list (outcome.columns)== ['STNAME','CTYNAME']\n\n&lt;ipython-input-77-546e58ae1c85&gt; in answer_eight()\n      1 def answer_eight():\n      2     counties=census_df[census_df['SUMLEV']==50]\n----&gt; 3     regions = counties[(counties[counties['REGION']==1]) | (counties[counties['REGION']==2])]\n      4     washingtons = regions[regions[regions['COUNTY']].str.startswith(\"Washington\")]\n      5     grew = washingtons[washingtons[washingtons['POPESTIMATE2015']]&gt;washingtons[washingtons['POPESTIMATES2014']]]\n\n/opt/conda/lib/python3.5/site-packages/pandas/core/frame.py in __getitem__(self, key)\n   1991             return self._getitem_array(key)\n   1992         elif isinstance(key, DataFrame):\n-&gt; 1993             return self._getitem_frame(key)\n   1994         elif is_mi_columns:\n   1995             return self._getitem_multilevel(key)\n\n/opt/conda/lib/python3.5/site-packages/pandas/core/frame.py in _getitem_frame(self, key)\n   2066     def _getitem_frame(self, key):\n   2067         if key.values.size and not com.is_bool_dtype(key.values):\n-&gt; 2068             raise ValueError('Must pass DataFrame with boolean values only')\n   2069         return self.where(key)\n   2070 \n\nValueError: Must pass DataFrame with boolean values only\n</code></pre>\n<p>I am clueless. Where am I going wrong?</p>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You're trying to use a different shaped df to mask your df, this is wrong, additionally the way you're passing the conditions is being used incorrectly. When you compare a column or series in a df with a scalar to produce a boolean mask you should pass just the condition, not use this successively.</p>\n<pre><code>def answer_eight():\n    counties=census_df[census_df['SUMLEV']==50]\n    # this is wrong you're passing the df here multiple times\n    regions = counties[(counties[counties['REGION']==1]) | (counties[counties['REGION']==2])]\n    # here you're doing it again\n    washingtons = regions[regions[regions['COUNTY']].str.startswith(\"Washington\")]\n    # here you're doing here again also\n    grew = washingtons[washingtons[washingtons['POPESTIMATE2015']]&gt;washingtons[washingtons['POPESTIMATES2014']]]\n    return grew[grew['STNAME'],grew['COUNTY']]\n</code></pre>\n<p>you want:</p>\n<pre><code>def answer_eight():\n    counties=census_df[census_df['SUMLEV']==50]\n    regions = counties[(counties['REGION']==1]) | (counties['REGION']==2])]\n    washingtons = regions[regions['COUNTY'].str.startswith(\"Washington\")]\n    grew = washingtons[washingtons['POPESTIMATE2015']&gt;washingtons['POPESTIMATES2014']]\n    return grew[['STNAME','COUNTY']]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>def answer_eight():\n    df=census_df[census_df['SUMLEV']==50]\n    #df=census_df\n    df=df[(df['REGION']==1) | (df['REGION']==2)]\n    df=df[df['CTYNAME'].str.startswith('Washington')]\n    df=df[df['POPESTIMATE2015'] &gt; df['POPESTIMATE2014']]\n    df=df[['STNAME','CTYNAME']]\n    print(df.shape)\n    return df.head(5)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have the following code to find the mean of the ages in the dataframe.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>let df = df! [\n    \"name\" =&gt; [\"panda\", \"polarbear\", \"seahorse\"],\n    \"age\" =&gt; [5, 7, 1],\n].unwrap();\n\nlet mean = df\n    .lazy()\n    .select([col(\"age\").mean()])\n    .collect().unwrap();\n\nprintln!(\"{:?}\", mean);\n</code></pre>\n<p>After finding the mean, I want to extract the value as an <code>f64</code>.</p>\n<pre><code>┌──────────┐\n│ age      │\n│ ---      │\n│ f64      │   -----&gt; how to transform into a single f64 of value 4.333333?\n╞══════════╡\n│ 4.333333 │\n└──────────┘\n</code></pre>\n<p>Normally, I would do something like <code>df[0,0]</code> to extract the only value. However, as Polars is not a big proponent of indexing, how would one do it using Rust Polars?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Ok guys I found a couple of ways to do this. Although, I'm not sure if they are the most efficient.</p>\n<pre class=\"lang-rust prettyprint-override\"><code>let df = df! [\n    \"name\" =&gt; [\"panda\", \"polarbear\", \"seahorse\"],\n    \"age\" =&gt; [5, 7, 1],\n]?;\n\nlet mean = df\n    .lazy()\n    .select([col(\"age\").mean()])\n    .collect()?;\n\n// Select the column as Series, turn into an iterator, select the first\n// item and cast it into an f64 \nlet mean1 = mean.column(\"age\")?.iter().nth(0)?.try_extract::&lt;f64&gt;()?;\n\n// Select the column as Series and calculate the sum as f64\nlet mean2 = mean.column(\"age\")?.sum::&lt;f64&gt;()?;\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>mean[\"age\"].max().unwrap()\n</code></pre>\n<p>or</p>\n<pre><code>mean[\"age\"].f64().unwrap().get(0).unwrap()\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm looking for an efficient quantiles algorithm that allows sample values to be \"upserted\" or replaced as the value changes over time.</p>\n<p>Let's say I have values for items <code>1-n</code>.  I'd like to put these into a quantiles algorithm that would efficiently store them.  But then say at some point in the future, the value for <code>item-i</code> gets incremented.  I'd like to remove the original value for <code>item-i</code> and replace it with the updated value.  The specific use case is for a streaming system where the sample values are incrementing over time.</p>\n<p>The closest I've seen to something like this is the <a href=\"https://github.com/tdunning/t-digest\" rel=\"noreferrer\">t-Digest data structure</a>.  It stores sample values efficiently.  The only thing it lacks is the ability to remove and replace a sample value.</p>\n<p>I've also looked at <a href=\"https://datasketches.apache.org/docs/Quantiles/QuantilesOverview.html\" rel=\"noreferrer\">Apache Quantiles Datasketch</a> - it suffers from the same problem - no way to remove and replace a sample.</p>\n<p>edit:  thinking about this more, there wouldn't necessarily need to be a remove of the old value and an insertion of the incremented value.  There might be a way to recalculate internal state more easily if there's a constraint that values can only be updated.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If update time <code>O(log n)</code> and quantile compute time <code>O(log n)</code> are acceptable for you then one of solutions would be to implement any type of self-balanced binary tree (<a href=\"https://en.wikipedia.org/wiki/Splay_tree\" rel=\"nofollow noreferrer\">Splay tree</a>, <a href=\"https://en.wikipedia.org/wiki/AVL_tree\" rel=\"nofollow noreferrer\">AVL-tree</a>, <a href=\"https://en.wikipedia.org/wiki/Red%E2%80%93black_tree\" rel=\"nofollow noreferrer\">Red-Black tree</a>) while keeping a <code>HashMap&lt;Key, Node&gt;</code> in parallel to the tree structure (or if you know that your keys are e.g. numbers <code>0</code> to <code>n-1</code>, then you can just use an array for the same purposes). You will also need to keep a count of nodes in the subtree for each given node (which is possible with all of the mentioned self-balanced trees - it is a small addition to all methods which are doing updates on the nodes such as rotations, etc.).</p>\n<p>Pseudo-code for updating value with key K, new value V would be:</p>\n<pre><code>Node node = find_node_in_hash_map_by_key(K); # O(1)\ndelete_node_keeping_subtree_counts_valid(node); # O(log n)\nadd_new_node_keeping_subtree_counts_valid(K, V); # O(log n)\n</code></pre>\n<p>Getting quantile q will be possible in <code>O(log n)</code> too because of the subtree sizes available in each node, because it pretty much gives you access to i-th element by size in <code>O(log n)</code> time. Pseudocode for that operation would look like:</p>\n<pre><code># i-th element requested\nnode = root\nwhile true:\n    left = node.left_subtree\n    left_count = 0\n    if left is not None:\n        left_count = left.nodes_count\n    if i &lt; left_count:\n        node = left # select i-th element in the left subtree\n    elif i == left_count:\n        return node.value # we have exactly i elements in left subtree, so i-th value is in the current node\n    else:\n        i -= left_count + 1 # select element i - left_count - 1 from the right subtree\n        node = node.right\n</code></pre>\n<p>I'm not aware of a good open-source JAVA solution for this data structure, but writing your own AVL tree is not that difficult (and Splay tree should be the easiest, just their worst case complexity is not <code>O(log n)</code>, but on average they should be good).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I trained a model in keras (regression) on a Linux platform and saved the model with a <code>model.save_weights(\"kwhFinal.h5\")</code></p>\n<p>And then I was hoping to take my complete saved model to Python 3.6 on my Windows 10 laptop and just use it with IDLE:</p>\n<pre><code>from keras.models import load_model\n\n# load weights into new model\nloaded_model.load_weights(\"kwhFinal.h5\")\nprint(\"Loaded model from disk\")\n</code></pre>\n<p>Except I am running into this read only mode ValueError with Keras. Thru <code>pip</code> I installed Keras &amp; Tensorflow on my Windows 10 laptop and researching more online it seems like this <a href=\"https://stackoverflow.com/questions/53212672/read-only-mode-in-keras\">other SO post about the same issue</a>, the answer states:</p>\n<blockquote>\n<p>You have to set and define the architecture of your model and then use\n  model.load_weights</p>\n</blockquote>\n<p>But I don't understand this enough to recreate the code from the answer (link to git gist). This is my Keras script below that I ran on a Linux OS to create the model. Can someone give me a tip on how define architecture so I can use this model to predict on my Windows 10 laptop?</p>\n<pre><code>#https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n#https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n#https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras import backend\nfrom keras.models import model_from_json\nimport os\n\n\n\ndef rmse(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\n# load dataset\ndataset = pd.read_csv(\"joinedRuntime2.csv\", index_col='Date', parse_dates=True)\n\nprint(dataset.shape)\nprint(dataset.dtypes)\nprint(dataset.columns)\n\n# shuffle dataset\ndf = dataset.sample(frac=1.0)\n\n# split into input (X) and output (Y) variables\nX = df.drop(['kWh'],1)\nY = df['kWh']\n\noffset = int(X.shape[0] * 0.7)\nX_train, Y_train = X[:offset], Y[:offset]\nX_test, Y_test = X[offset:], Y[offset:]\n\n\nmodel = Sequential()\nmodel.add(Dense(60, input_dim=7, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(55, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(50, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(45, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(30, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(20, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\nmodel.summary()\n\nmodel.compile(loss='mse', optimizer='adam', metrics=[rmse])\n\n# train model\nhistory = model.fit(X_train, Y_train, epochs=5, batch_size=1,  verbose=2)\n\n# plot metrics\nplt.plot(history.history['rmse'])\nplt.title(\"kWh RSME Vs Epoch\")\nplt.show()\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\nmodel.save_weights(\"kwhFinal.h5\")\nprint(\"[INFO] Saved model to disk\")\n</code></pre>\n<p>On machine learning mastery they demonstrate also saving YML &amp; Json but I am unsure if this will help to define model architecture...</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are saving the weights, not the whole model. A Model is more than just the weights, including architecture, losses, metrics and etc.</p>\n<p>You have two solutions:</p>\n<p>1) Go with saving the weights: in this case, in time of model loading, you will need to recreate your model, load the weight and then compile the model. Your code should be something like this:</p>\n<pre><code>model = Sequential()\nmodel.add(Dense(60, input_dim=7, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(55, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(50, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(45, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(30, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(20, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\nmodel.load_weights(\"kwhFinal.h5\")\nmodel.compile(loss='mse', optimizer='adam', metrics=[rmse])\n</code></pre>\n<p>2)  Save the whole model by this command:</p>\n<pre><code>model.save(\"kwhFinal.h5\")\n</code></pre>\n<p>And during the loading use this command for having your model loaded:</p>\n<pre><code>from keras.models import load_model\nmodel=load_model(\"kwhFinal.h5\")\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Save the model as:</p>\n<pre><code>model.save(\"kwhFinal.h5\")\n</code></pre>\n<p>While loading model, you need to add the custom metric function you defined.</p>\n<pre><code>model=load_model(\"kwhFinal.h5\",custom_objects={'rmse': rmse})\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to understand the use patterns for Dask on a local machine.</p>\n<p>Specifically,</p>\n<ul>\n<li>I have a dataset that fits in memory</li>\n<li>I'd like to do some pandas operations\n\n<ul>\n<li>groupby...</li>\n<li>date parsing</li>\n<li>etc.</li>\n</ul></li>\n</ul>\n<p>Pandas performs these operations via a single core and these operations are taking hours for me. I have 8 cores on my machine and, as such, I'd like to use Dask to parallelize these operations as best as possible.</p>\n<p>My question is as follows: What is the difference between the two way of doing this in Dask:</p>\n<pre><code>import pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n</code></pre>\n<p>(1)</p>\n<pre><code>import dask.dataframe as dd\n\ndf = dd.from_pandas(\n    pd.DataFrame(iris.data, columns=iris.feature_names),\n    npartitions=2\n)\n\ndf.mean().compute()\n</code></pre>\n<p>(2)</p>\n<pre><code>import dask.dataframe as dd\nfrom distributed import Client\n\nclient = Client()\n\ndf = client.persist(\n    dd.from_pandas(\n        pd.DataFrame(iris.data, columns=iris.feature_names),\n        npartitions=2\n    )\n)\n\ndf.mean().compute()\n</code></pre>\n<p>What is the benefit of one use pattern over the other? Why should I use one over the other?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Version (2) has two differences compared to version (1): the choice to use the distributed scheduler, and <code>persist</code>. These are separate factors. There is a lot of documentation about both: <a href=\"https://distributed.readthedocs.io/en/latest/quickstart.html\" rel=\"noreferrer\">https://distributed.readthedocs.io/en/latest/quickstart.html</a>, <a href=\"http://dask.pydata.org/en/latest/dataframe-performance.html#persist-intelligently\" rel=\"noreferrer\">http://dask.pydata.org/en/latest/dataframe-performance.html#persist-intelligently</a> , so this answer can be kept brief.</p>\n<p>1) The distributed scheduler is newer and smarter than the previous threaded and multiprocess schedulers. As the name suggests, it is able to use a cluster, but also works on a single machine. Although the latency when calling <code>.compute()</code> is generally higher, in many ways it is more efficient, has more advanced features such as real-time dynamic programming and more diagnostics such as the dashboard. When created with <code>Client()</code>, you by default get a number of processes equal to the number of cores, but you can choose the number of processes and threads, and get close to the original threads-only situation with <code>Client(processes=False)</code>.</p>\n<p>2) Persisting means evaluating a computation and storing it in memory, so that further computations are faster. You can also persist without the distributed client (<code>dask.persist</code>). It effectively offers to trade memory for performance because you don't need to re-evalute the computation each time you use it for anything that depends on it. In the case where you go on to perform only one computation on the intermediate, as in the example, it should make no difference to performance.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm confused about using cross_val_predict in a test data set.</p>\n<p>I created a simple Random Forest model and used cross_val_predict to make predictions:</p>\n<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import cross_val_predict, KFold\n\nlr = RandomForestClassifier(random_state=1, class_weight=\"balanced\", n_estimators=25, max_depth=6)\nkf = KFold(train_df.shape[0], random_state=1)\npredictions = cross_val_predict(lr,train_df[features_columns], train_df[\"target\"], cv=kf)\npredictions = pd.Series(predictions)\n</code></pre>\n<p>I'm confused on the next step here. How do I use what is learnt above to make predictions on the test data set?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't think <code>cross_val_score</code> or <code>cross_val_predict</code> uses fit before predicting. It does it on the fly. If you look at the <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html#obtaining-predictions-by-cross-validation\" rel=\"nofollow noreferrer\">documentation (section 3.1.1.1)</a>, you'll see that they never mention fit anywhere.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As @DmitryPolonskiy commented, the model has to be trained (with the <code>fit</code> method) before it can be used to <code>predict</code>. </p>\n<pre><code># Train the model (a.k.a. `fit` training data to it).\nlr.fit(train_df[features_columns], train_df[\"target\"])\n# Use the model to make predictions based on testing data.\ny_pred = lr.predict(test_df[feature_columns])\n# Compare the predicted y values to actual y values.\naccuracy = (y_pred == test_df[\"target\"]).mean()\n</code></pre>\n<p><code>cross_val_predict</code> is a method of cross validation, which lets you determine the accuracy of your model. Take a look at <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html\" rel=\"nofollow noreferrer\">sklearn's cross-validation page</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/55161958/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2024-04-02 09:54:09Z\">6 months ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/55161958/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>What is the correct naming convention for files in a data science and machine learning project?</p>\n<p>I believe the file name of the Python classes should be a <strong>noun</strong>. However, I want to make it clear that whether to name the class as a subject noun or object noun.</p>\n<p>Which of these should I use?</p>\n<p>1) The class that outputs plots.</p>\n<p><code>visualization.py</code>, <code>visualizer.py</code>, <code>vis.py</code>, or ... </p>\n<p>2) The class that analyses the dataset and outputs files that contains results.</p>\n<p><code>analysis.py</code>, <code>analyzer.py</code>, or ... </p>\n<p>3) The class that coverts the dataset to <em>pickle files</em>.</p>\n<p><code>preprocessor.py</code>, <code>preprocessing.py</code>, <code>prepare.py</code>, or ...</p>\n<p>(I had checked <a href=\"https://www.python.org/dev/peps/pep-0008/\" rel=\"noreferrer\">PEP8</a> but couldn't find the clearly naming conversion for the file names)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>here in <a href=\"https://www.python.org/dev/peps/pep-0008/#naming-conventions\" rel=\"nofollow noreferrer\">PEP-8</a> naming convention section, YOU will find the correct way. </p>\n<p>it's is also discuss in pep-8 that naming convention is ambiguous.</p>\n<p>so if you want a correct way ( which another organization follows) then go to GitHub ( <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow\" rel=\"nofollow noreferrer\">tensorflow</a> for example ) see how they maintain there naming convention for maintained project. </p>\n<p>you can follow there structure and start doing the project.</p>\n<p>Nothing is fixed. it's all depends on how you want to structure it. Better is it should be, easy to read and maintain. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a big doubt... is see a lot of <a href=\"https://robotwealth.com/how-to-connect-google-colab-to-a-local-jupyter-runtime/\" rel=\"noreferrer\">blog posts</a> where they say that you can use the Colab front-end to edit a local Jupiter Notebook</p>\n<p>However I don't see the point... the actual advantage would be to use something like DataSpell or some local IDE, on a remote Notebook on Colab, and use the Colab Resources to do the computations, so you have:</p>\n<ol>\n<li>IDE level of suggestions (Colab is pretty slow compared to local IDE)</li>\n<li>cloud computing performances and advantages</li>\n</ol>\n<p>Hoever, I don't see any blog talking about this... is there any way to do this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could use something like <a href=\"https://github.com/WassimBenzarti/colab-ssh\" rel=\"nofollow noreferrer\">colab-ssh</a> but Google has cracked down on these tools making the whole process inconvenient in one way or another. I think a better approach would be to just use your IDE as you would and store your notebooks on GitHub. Git-to-Colab is officially supported and you're obviously not doing heavy computing or long-running tasks during development so I don't think there should be an issue. For computationally inexpensive tasks (dataviz?) Codespaces or Datalore should be enough.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The 'find in selection' button is missing from VSCode when working with Jupyter Notebooks. It slows down development so I would like to ask if anybody knows how to activate it?</p>\n<p>First image shows the search/replace when in a python file. Second image shows the missing button when in a notebook.</p>\n<p>Python file:</p>\n<p><a href=\"https://i.sstatic.net/HJIVI.png\" rel=\"noreferrer\"><img alt=\"python file with 'find in selection' functionality\" src=\"https://i.sstatic.net/HJIVI.png\"/></a></p>\n<p>Jypyter Notebook:</p>\n<p><a href=\"https://i.sstatic.net/ejSYi.png\" rel=\"noreferrer\"><img alt=\"Jupyter Notebook / 'find in selection' is missing\" src=\"https://i.sstatic.net/ejSYi.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As Jan 2022, within a cell in a jupyternotebook you can search and replace in selection when pushing <code>F3</code>. As comment before, this function is not available when using Ctrl + H</p>\n<p>More info :</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/71319929/how-to-find-and-replace-text-in-a-single-cell-when-using-jupyter-extension-insid/73109147#73109147\">How to find and replace text in a single cell when using Jupyter extension inside Visual Studio Code</a></li>\n<li><a href=\"https://github.com/microsoft/vscode/issues/141493\" rel=\"nofollow noreferrer\">https://github.com/microsoft/vscode/issues/141493</a></li>\n<li><a href=\"https://github.com/microsoft/vscode/issues/121218\" rel=\"nofollow noreferrer\">https://github.com/microsoft/vscode/issues/121218</a></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have this kind of data : </p>\n<pre><code>ID    x1   x2   x3    x4    x5    x6    x7   x8   x9   x10\n1   -0.18   5 -0.40 -0.26  0.53 -0.66  0.10   2 -0.20    1\n2   -0.58   5 -0.52 -1.66  0.65 -0.15  0.08   3  3.03   -2\n3   -0.62   5 -0.09 -0.38  0.65  0.22  0.44   4  1.49    1\n4   -0.22  -3  1.64 -1.38  0.08  0.42  1.24   5 -0.34    0\n5    0.00   5  1.76 -1.16  0.78  0.46  0.32   5 -0.51   -2\n</code></pre>\n<p>what's the best method for visualizing this data, i'm using matplotlib to visualizing it, and read it from csv using pandas</p>\n<p>thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Visualising data in a high-dimensional space is always a difficult problem. One solution that is commonly used (<a href=\"http://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html\" rel=\"nofollow noreferrer\">and is now available in <code>pandas</code></a>) is to inspect all of the 1D and 2D projections of the data. It doesn't give you all of the information about the data, but that's impossible to visualise unless you can see in 10D! Here's an example of how to do this with pandas (version 0.7.3 upwards):</p>\n<pre><code>import numpy as np \nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\n\n#first make some fake data with same layout as yours\ndata = pd.DataFrame(np.random.randn(100, 10), columns=['x1', 'x2', 'x3',\\\n                    'x4','x5','x6','x7','x8','x9','x10'])\n\n#now plot using pandas \nscatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')\n</code></pre>\n<p>This generates a plot with all of the 2D projections as scatter plots, and KDE histograms of the 1D projections:</p>\n<p><a href=\"https://i.sstatic.net/WJ6aU.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/WJ6aU.png\"/></a></p>\n<p>I also have a pure matplotlib approach to this on <a href=\"https://github.com/anguswilliams91/CornerPlot\" rel=\"nofollow noreferrer\">my github page</a>, which produces a very similar type of plot (it is designed for MCMC output, but is also appropriate here). Here's how you'd use it here:</p>\n<pre><code>import corner_plot as cp\n\ncp.corner_plot(data.as_matrix(),axis_labels=data.columns,nbins=10,\\\n              figsize=(7,7),scatter=True,fontsize=10,tickfontsize=7)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/Q4Iyl.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Q4Iyl.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You may change the plot over the time, for each instant you plot a different \"dimension\" of the dataframe.\nHere an example on how you can do plots that change over the time, you may adjust it for your purposes</p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.grid(True)\nplt.hold(False)\nx = np.arange(-3, 3, 0.01)\n\nfor n in range(15):\n    y = np.sin(np.pi*x*n) / (np.pi*x*n)\n    line, = ax.plot(x, y)\n    plt.draw()\n    plt.pause(0.5)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using google Colab notebook for a project that requires me to plot GPS coordinates on a map. I want to use basemap for this purpose. I tried to import it on the Colab notebook by using<br/>\n<code>from mpl_tools.basemap import Basemap</code>\nand it showed up the following error: </p>\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-24-2cb85a2f9bb7&gt; in &lt;module&gt;()\n----&gt; 1 from mpl_tools.basemap import Basemap\n\nModuleNotFoundError: No module named 'mpl_tools'\n</code></pre>\n<p>I need to install the basemap module in order to use it. I tried <code>!pip install basemap</code> and tried to run it on Colab and that did not work. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>!apt-get install libgeos-3.5.0\n!apt-get install libgeos-dev\n!pip install https://github.com/matplotlib/basemap/archive/master.zip\n</code></pre>\n<p>There is a <a href=\"https://stackoverflow.com/questions/55106691/python-basemap-in-google-colaboratory\">problem with pyproj 2.0.1</a>, so we need to downgrade it:</p>\n<pre><code>!pip install pyproj==1.9.6\n</code></pre>\n<p>restart runtime</p>\n<pre><code>from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre>\n<p>This works for me on Colab on April 7, 2019. I can now set up and plot maps.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just for the record, you can also install <code>basemap</code> directly with <code>pip</code> since the <code>basemap</code> version series 1.3.x because now there are precompiled wheels in PyPI (for Windows and GNU/Linux):</p>\n<pre class=\"lang-sh prettyprint-override\"><code>!pip install basemap\n</code></pre>\n<p>In case you need the high-resolution datasets, you have to install them manually:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>!pip install basemap-data-hires\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's what worked for me as of today (21st June 2022) on Google Colab:</p>\n<pre><code>!pip install matplotlib\n!apt install libproj-dev libgeos-dev\n!pip install https://github.com/matplotlib/basemap/archive/v1.2.0rel.tar.gz\n!pip install pyproj==1.9.6\n\nfrom mpl_toolkits.basemap import Basemap as Basemap\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In scikit-learn's PolynomialFeatures preprocessor, there is an option to include_bias. This essentially just adds a column of ones to the dataframe. I was wondering what the point of having this was. Of course, you can set it to False. But theoretically how does having or not having a column of ones along with the Polynomial Features generated affect Regression.</p>\n<p>This is the explanation in the documentation, but I can't seem to get anything useful out of it relation to why it should be used or not.</p>\n<blockquote>\n<p><strong>include_bias : boolean</strong> </p>\n<p>If True (default), then include a bias column, the feature in which\n  all polynomial powers are zero (i.e. a column of ones - acts as an\n  intercept term in a linear model).</p>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose you want to perform the following regression:</p>\n<pre><code>y ~ a + b x + c x^2\n</code></pre>\n<p>where <code>x</code> is a generic sample. The best coefficients <code>a,b,c</code> are computed via simple matricial calculus. First, let us denote with <code>X = [1 | X | X^2]</code> a matrix with N rows, where N is the number of samples. The first column is a column of 1s, the second column is a column of values <code>x_i</code>, for all the samples i, the third column is a column of values <code>x_i^2</code>, for all samples i. Let us denote with B the following column vector <code>B=[a b c]^T</code> If Y is a column vector of the N target values for all samples i, we can write the regression as</p>\n<pre><code>y ~ X B\n</code></pre>\n<p>The <code>i</code>-th row of this equation is <code>y_i ~ [1 x_i x^2] [a b c]^t = a + b x_i + c x_i^2</code>.</p>\n<p>The goal of training a regression is to find <code>B=[a b c]</code> such that <code>X B</code> be as close as possible to <code>y</code>. </p>\n<p>If you don't add a column of <code>1</code>, you are assuming a-priori that <code>a=0</code>, which might not be correct.</p>\n<p>In practice, when you write Python code, and you use <code>PolynomialFeatures</code> together with <code>sklearn.linear_model.LinearRegression</code>, the latter takes care by default of adding a column of 1s (since in <code>LinearRegression</code> the <code>fit_intercept</code> parameter is <code>True</code> by default), so you don't need to add it as well in <code>PolynomialFeatures</code>. Therefore, in <code>PolynomialFeatures</code> one usually keeps <code>include_bias=False</code>.</p>\n<p>The situation is different if you use <code>statsmodels.OLS</code> instead of <code>LinearRegression</code></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to create a 4x4 FacetGrid in seaborn for 4 boxplots, each of which is split into 3 boxplots based on the iris species in the iris dataset. Currently, my code looks like this:</p>\n<pre><code>sns.set(style=\"whitegrid\")\niris_vis = sns.load_dataset(\"iris\")\n\nfig, axes = plt.subplots(2, 2)\n\nax = sns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=iris, orient='v', \n    ax=axes[0])\nax = sns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=iris, orient='v', \n    ax=axes[1])\nax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, orient='v', \n    ax=axes[2])\nax = sns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=iris, orient='v', \n    ax=axes[3])\n</code></pre>\n<p>However, I'm getting this error from my interpreter:</p>\n<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'boxplot'\n</code></pre>\n<p>I'm confused on where the attribute error is exactly in here. What do I need to change?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Not a direct answer to your error, but if you are going to use seaborn, you should try to stick with \"long\" or \"tidy\" data (<a href=\"https://seaborn.pydata.org/tutorial/data_structure.html#long-form-data\" rel=\"noreferrer\">https://seaborn.pydata.org/tutorial/data_structure.html#long-form-data</a>).</p>\n<p>I'm assuming your original data set is wide (column for each feature of the observation). If you melt the data set like so:</p>\n<pre><code>iris = iris.melt(id_vars='target')\n\nprint(iris.head())\n\n   target           variable  value\n0  setosa  sepal length (cm)    5.1\n1  setosa  sepal length (cm)    4.9\n2  setosa  sepal length (cm)    4.7\n3  setosa  sepal length (cm)    4.6\n4  setosa  sepal length (cm)    5.0\n</code></pre>\n<p>You'll be able to use seaborn's <a href=\"https://seaborn.pydata.org/generated/seaborn.catplot.html\" rel=\"noreferrer\"><code>catplot</code></a> with <code>kind='box'</code></p>\n<pre><code>sns.catplot(\n    data=iris, x='target', y='value',\n    col='variable', kind='box', col_wrap=2\n)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/L7Zow.png\" rel=\"noreferrer\"><img alt=\"boxplot\" src=\"https://i.sstatic.net/L7Zow.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>axes</code> shape is <code>(nrows, ncols)</code>. In this case is: </p>\n<pre><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267f425f8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267f1bb38&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267ec95c0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267ef9080&gt;]],\n      dtype=object)\n</code></pre>\n<p>So, when you do <code>ax=axes[0]</code> you get a array and not the axes. Try:</p>\n<pre><code>fig, axes = plt.subplots(2, 2)\n\nax = sns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=iris, orient='v', \n    ax=axes[0, 0])\nax = sns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=iris, orient='v', \n    ax=axes[0, 1])\nax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, orient='v', \n    ax=axes[1, 0])\nax = sns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=iris, orient='v', \n    ax=axes[1, 1])\n</code></pre>\n<p><a href=\"https://i.sstatic.net/mtjL8.png\" rel=\"nofollow noreferrer\"><img alt=\"example_plot\" src=\"https://i.sstatic.net/mtjL8.png\"/></a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Because as @Lucas points out that axes returns a numpy array of 2D(nrows, ncols), you can flatten this array 1D using: </p>\n<pre><code>axes=axes.flatten()\n</code></pre>\n<p>And, you can keep your same code like this:</p>\n<pre><code>fig, axes = plt.subplots(2, 2)\naxes = axes.flatten()\n\nax = sns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=iris, orient='v', \n    ax=axes[0])\nax = sns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=iris, orient='v', \n    ax=axes[1])\nax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, orient='v', \n    ax=axes[2])\nax = sns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=iris, orient='v', \n    ax=axes[3])\n</code></pre>\n<p>Output:</p>\n<p><a href=\"https://i.sstatic.net/MrxmO.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/MrxmO.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am now handling the missing data. I have missing data in my test and train set. I got a little confused about how to deal with the missing data in the <strong>test</strong> set. If I am imputing by using the \"mean\" method, should I use the mean calculated from the train set or the test set if I want to impute the missing value in the <strong>test</strong> set. \nThank you for helping me!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In general, you should not compute mean or anything other from test set (best way of thinking about test set is that it simply doesn't exist, at least until you have already trained your model). </p>\n<p>Build a transformation pipeline that can handle all the necessary preprocessing steps (impute missing data, standardize, perform desired feature engineering, dimensionality reduction...) on training set and when a new observation comes (we should treat test set as just a new observations that are unavailable during training) apply this pipeline transformations on that new data.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should use <code>train</code> mean for that. You should never infer information from <code>test</code> dataset as that's an information leak.</p>\n<p>Calculating mean of <code>test</code> dataset would give your algoritm info about <code>mean</code> of it (obviously) and would probably falsely improve its score on said.</p>\n<p>In real life you would usually have no way to calculate mean of missing data anyway (think of single incoming example with missing values).</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/33454403/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2015-10-31 23:03:08Z\">8 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/33454403/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>Recently i was doing the Machine Learning course at Coursera by Prof. Andrew Ng. After doing this course i have understand the basics of Machine Learning Algorithms, but i have the following questions:</p>\n<ul>\n<li><p>Where can i find the Real world Machine Learning use case examples?</p></li>\n<li><p>What tools or framework are used in Industry/Production for Machine<br/>\nLearning projects?</p></li>\n<li><p>How Machine Learning models are used or deploy in production?</p></li>\n<li><p>How to become Data Scientist? Or What should i do next?</p></li>\n</ul>\n<p>Any suggestion,books,courses or tutorial links will be highly appreciated.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Congrats for completing the Machine Learning course by Andrew Ng, longtime back I have also done this awesome course.\nAnyway, so I will answer your question one by one, although there are few questions that are interrelated.</p>\n<p><strong>Q-1) Where can I find the Real world Machine Learning use case examples?</strong></p>\n<p>Here are a few links where you can find tutorials of machine learning with real-world use cases:</p>\n<ul>\n<li><p>Machine Learning example Scikit learn: <a href=\"http://scikit-learn.org/stable/tutorial/basic/tutorial.html\" rel=\"nofollow noreferrer\">http://scikit-learn.org/stable/tutorial/basic/tutorial.html</a></p></li>\n<li><p>Machine Learning tutorials H20: <a href=\"https://github.com/h2oai/h2o-tutorials\" rel=\"nofollow noreferrer\">https://github.com/h2oai/h2o-tutorials</a></p></li>\n<li><p>Sagemaker Machine Learning &amp; Deep Learning example: <a href=\"https://github.com/awslabs/amazon-sagemaker-examples\" rel=\"nofollow noreferrer\">https://github.com/awslabs/amazon-sagemaker-examples</a></p></li>\n<li><p><a href=\"https://github.com/Kuntal-G/Machine-Learning\" rel=\"nofollow noreferrer\">Tutorials on real-world Machine Learning Case Studies</a></p></li>\n</ul>\n<p><strong>Q-2) What tools or framework are used in Industry/Production for Machine\nLearning projects?</strong></p>\n<p>There are a wide variety of tools or framework that are used in Industry level like:</p>\n<p><strong><em>Machine Learning:</em></strong></p>\n<ul>\n<li>R (mostly used in academia nowadays)</li>\n<li>Python(Sci-kit learn)</li>\n<li>GraphLab</li>\n<li>Apache Mahout</li>\n<li>Spark MLlib</li>\n<li>H20</li>\n</ul>\n<p><strong><em>Deep Learning:</em></strong></p>\n<ul>\n<li>Tensorflow and Keras</li>\n<li>Mxnet</li>\n<li>Pytorch</li>\n<li>DeepLearning4j</li>\n<li>Analytics Zoo (mainly for deep learning with big data using spark)</li>\n</ul>\n<p>While R,Scikit learn,GraphLab works great on single machine and most popular choices among data scientist or machine learning practitioners, but Mahout, H20 and recently Spark (MLlib) has gained a lot of popularity in this era of Big Data, where you want to do machine learning on large dataset that will not fit on single machine.</p>\n<p>Also, there are some other tools like Weka, Rapid Miner for GUI based workflow of machine learning work.</p>\n<p>Choice of this tool or framework really depends on the factors like project requirement, team members knowledge about the tool/language, also ease of development and scalability of deployment.</p>\n<p><strong>Q-3) How Machine Learning models are used or deploy in production?</strong></p>\n<p>In production, you have to first build a model, validate &amp; evaluate that model, then the model is most finally deployed as web/rest service to be used by other applications/services. Deploying a machine learning model depends on a lot of factors such as-</p>\n<ul>\n<li>Is the model trained offline? Or are you deploying an online learning model?</li>\n<li>How often you will retrain your model?</li>\n<li>How would you test your newer version of the model? - A/B testing or Bandit variation.</li>\n<li>Along with other generic things - latency, throughput, data input/output format etc. </li>\n</ul>\n<p>There are some cloud-based machine learning service provider like Azure ML(<a href=\"https://studio.azureml.net/\" rel=\"nofollow noreferrer\">https://studio.azureml.net/</a>) BigML(<a href=\"https://bigml.com/\" rel=\"nofollow noreferrer\">https://bigml.com/</a>) etc, where you can upload your dataset,do some data processing, train|validate|evaluate your machine learning model and then finally deploy it as web service in the cloud.</p>\n<p>Also all major cloud platform (aws, google cloud, azure) nowadays provide you with a machine learning platform, where you can build your own model, evaluate them and then finally deploy it in the cloud. It gives you the flexibility to build the model with almost all major machine learning or deep learning frameworks, and as per your requirement gives you the flexibility to deploy ( what type of server/containers, number of inference/prediction server, etc).</p>\n<p><strong>Amazon SageMaker:</strong></p>\n<p>[+] <a href=\"https://aws.amazon.com/sagemaker/\" rel=\"nofollow noreferrer\">https://aws.amazon.com/sagemaker/</a></p>\n<p><strong>Google Cloud Machine Learning (ML) Engine:</strong> </p>\n<p>[+] <a href=\"https://cloud.google.com/ml-engine/\" rel=\"nofollow noreferrer\">https://cloud.google.com/ml-engine/</a></p>\n<p><strong>Q-4) How to become Data Scientist? Or What should I do next?</strong></p>\n<p>It's a million dollar question and lots of google serach on this question..haha.. I will try to give you a short and concise answer based on my knowledge. First of all Data Science is much broader field of study,that comprise of the following common steps:</p>\n<ul>\n<li>Business understanding or Questioning Phase</li>\n<li>Data gathering or acquiring</li>\n<li>Data processing and preparation</li>\n<li>Model Building</li>\n<li>Validation and Evaluation</li>\n</ul>\n<p>Along with this, you also need to do Model Retraining to depending upon the change of data variability, or you can deploy online learning model(which will adapt itself based on the data that it is seeing).</p>\n<p>But basic ingredients for anyone to become data scientist/machine learning practitioner is to have the curiosity about data (i.e to understand the data &amp; find valuable knowledge out of it). Neither there is a shortcut to becoming a data scientist nor there is any course that will make you become data scientist overnight. </p>\n<p>There is no predefined role/scope what a data science person should be knowing or doing on a day to day basis in a company. Different industry or company has its own job requirement/description for a data scientist depending on their business problem.</p>\n<p>A good versatile data scientist must have the following skills in order to sustain confidently across  various industry and succeed in his/her career:</p>\n<ul>\n<li><p>Good knowledge in Statistics (including a little bit of Bayesian)- essential during EDA phase.</p></li>\n<li><p>Maths (especially linear algebra,matrix,vector, multivariate calculus): <a href=\"https://www.coursera.org/specializations/mathematics-machine-learning\" rel=\"nofollow noreferrer\">https://www.coursera.org/specializations/mathematics-machine-learning</a></p></li>\n<li><p>Good practical knowledge of machine learning algorithms: <a href=\"https://www.coursera.org/specializations/machine-learning\" rel=\"nofollow noreferrer\">https://www.coursera.org/specializations/machine-learning</a></p></li>\n<li><p>Some deep learning &amp; reinforcement learning knowledge. Stanford Deep Learning course: <a href=\"http://cs231n.stanford.edu/\" rel=\"nofollow noreferrer\">http://cs231n.stanford.edu/</a> and youtube video of this course by Andrej Karpathy- <a href=\"https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk</a> . Also DeepLearning Coursera: <a href=\"https://www.coursera.org/specializations/deep-learning\" rel=\"nofollow noreferrer\">https://www.coursera.org/specializations/deep-learning</a>  and Reinforcement Learning course- Berkley University: <a href=\"http://rail.eecs.berkeley.edu/deeprlcourse/\" rel=\"nofollow noreferrer\">http://rail.eecs.berkeley.edu/deeprlcourse/</a></p></li>\n<li><p>Large dataset analysis through big data tools like Spark and SQL. Machine Learning with Big Data: <a href=\"https://www.coursera.org/learn/machine-learning-applications-big-data\" rel=\"nofollow noreferrer\">https://www.coursera.org/learn/machine-learning-applications-big-data</a></p></li>\n<li><p>Curious mind to explore data and learn new things (to stay up to date with the latest innovation in this domain). </p></li>\n<li><p>And some Business domain knowledge- good to have (Optional)</p></li>\n</ul>\n<hr/>\n<p>The best way is to play with data or do some real-world projects.\nLots of real-world datasets available publicly, you can pick a dataset of your choice of interest. Also, you can test your skill and expertise by participating in Machine Learning and Data Science competition at Kaggle.</p>\n<p>To gain some knowledge about data science, how it works along with some hand on exercise, you can try course online like:</p>\n<p><a href=\"https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-2\" rel=\"nofollow noreferrer\">https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-2</a></p>\n<p><a href=\"https://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x\" rel=\"nofollow noreferrer\">https://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Before scikit-learn 0.20 we could use <code>result.grid_scores_[result.best_index_]</code> to get the standard deviation. (It returned for exemple: <code>mean: 0.76172, std: 0.05225, params: {'n_neighbors': 21}</code>)</p>\n<p>What's the best way in scikit-learn 0.20 to get the standard deviation of the best score ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In newer versions, the <code>grid_scores_</code> is renamed as <code>cv_results_</code>. Following the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" rel=\"noreferrer\">documentation</a>, you need this:</p>\n<blockquote>\n<pre><code>best_index_ : int\n\nThe index (of the cv_results_ arrays) which corresponds to the best &gt; \n  candidate parameter setting.\n\nThe dict at search.cv_results_['params'][search.best_index_] gives the &gt; \n  parameter setting for the best model, that gives the highest mean\n  score (search.best_score_).\n</code></pre>\n</blockquote>\n<p>So in your case, you need</p>\n<ul>\n<li>Best params :- <code>result.cv_results_['params'][result.best_index_]</code> OR <code>result.best_params_</code> </li>\n<li><p>Best mean score :- <code>result.cv_results_['mean_test_score'][result.best_index_]</code> OR <code>result.best_score_</code> </p></li>\n<li><p>Best std :- <code>result.cv_results_['std_test_score'][result.best_index_]</code></p></li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How can I input data into keras? What is the structure? Specifically what is the x_train and y_train if I have more than 2 columns?</p>\n<p>This is the data I want to input:</p>\n<p><a href=\"https://i.sstatic.net/2XvBc.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/2XvBc.png\"/></a></p>\n<p>I am trying to define Xtrain in this example Multi Layer Perceptron Neural Network code Keras has in its documentation. (<a href=\"http://keras.io/examples/\" rel=\"nofollow noreferrer\">http://keras.io/examples/</a>) Here is the code:</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\n\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16)\nscore = model.evaluate(X_test, y_test, batch_size=16)\n</code></pre>\n<p>EDIT (additional information):</p>\n<p>Looking here: <a href=\"https://stackoverflow.com/questions/30383404/what-is-data-type-for-python-keras-deep-learning-package?rq=1\">What is data type for Python Keras deep learning package?</a></p>\n<blockquote>\n<p>Keras uses numpy arrays containing the theano.config.floatX floating point type. This can be configured in your .theanorc file. Typically, it will be float64 for CPU computations and float32 for GPU computations, although you can also set it to float32 when working on the CPU if you prefer. You can create a zero-filled array of the proper type by the command</p>\n</blockquote>\n<pre><code>X = numpy.zeros((4,3), dtype=theano.config.floatX)\n</code></pre>\n<p>Question: Step 1 looks like create a floating point numpy array using my above data from the excel file. What do I do with the winner column?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It all depends on your need.</p>\n<p>It looks like that you want to predict the winner based on the parameters shown in column A - N. Then you should define <code>input_dim</code> to be 14, and  <code>X_train</code> should be an (N,14) numpy array like this:</p>\n<pre><code>[\n   [9278,  37.9, ...],\n   [18594, 36.3, ...],\n   ...\n]\n</code></pre>\n<p>It seems that your prediction set only contains 2 items ( 2 president candidates LOL), so you should encode the answer <code>Y_train</code> in an (N,2) numpy array like this:</p>\n<pre><code>[\n   [1, 0],\n   [1, 0],\n   ...\n   [0, 1],\n   [0, 1],\n   ...\n]\n</code></pre>\n<p>where <code>[1,0]</code> indicates that Barack Obama is the winner and vice versa.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am a newbie to machine learning. I have been struggling with a problem for a few weeks now and I hope someone can help here:</p>\n<p>I have a data set with one continuous variable and the rest are categorical. I managed to encode the categorical variables and would like to build a multi output classifier.</p>\n<p>Here is my data set:\n<a href=\"https://i.sstatic.net/yIGyb.png\" rel=\"nofollow noreferrer\">Snapshot of my data set</a>\nI have these features : A, B\nI would like to predict : C,D,E,F, G</p>\n<p>The data set looks like this: A,B,C,D,E,F,G</p>\n<p>I spent days on the documentation on multi-output classifiers on the scikitlearn and here but none of the documentation seems clear to me.</p>\n<p>Can anyone please point me in the right direction to find some sample code on how to create the classifier and predict with some sample data?</p>\n<p>Thank you in advance\nP.S: I am not using TensorFlow and would appreciate your help for sklearn.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is called <a href=\"https://en.wikipedia.org/wiki/Multi-task_learning\" rel=\"noreferrer\">multi-task learning</a>, which basically means a model that learns several functions, but shares (some or all) weights. It's fairly common, for example one model for image recognition and detection. What you need to do is to define several loss functions (they are called <em>heads</em>).</p>\n<p>Here's a very simple example in tensorflow that learns <code>Y1</code> and <code>Y2</code> from <code>X</code> (from <a href=\"https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html\" rel=\"noreferrer\">this post series</a>):</p>\n<pre><code># Define the Placeholders\nX = tf.placeholder(\"float\", [10, 10], name=\"X\")\nY1 = tf.placeholder(\"float\", [10, 1], name=\"Y1\")\nY2 = tf.placeholder(\"float\", [10, 1], name=\"Y2\")\n\n# Define the weights for the layers\nshared_layer_weights = tf.Variable([10,20], name=\"share_W\")\nY1_layer_weights = tf.Variable([20,1], name=\"share_Y1\")\nY2_layer_weights = tf.Variable([20,1], name=\"share_Y2\")\n\n# Construct the Layers with RELU Activations\nshared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))\nY1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))\nY2_layer_weights = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))\n\n# Calculate Loss\nY1_Loss = tf.nn.l2_loss(Y1,Y1_layer)\nY2_Loss = tf.nn.l2_loss(Y2,Y2_layer)\n</code></pre>\n<p>If you wish to code in pure scikit, see <a href=\"http://scikit-learn.org/stable/modules/multiclass.html\" rel=\"noreferrer\"><code>sklearn.multiclass</code></a> package, they support multioutput classification and multioutput regression. Here's an example of multioutput regression:</p>\n<pre><code>&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; from sklearn.multioutput import MultiOutputRegressor\n&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor\n&gt;&gt;&gt; X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n&gt;&gt;&gt; MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\narray([[-154.75474165, -147.03498585,  -50.03812219],\n       [   7.12165031,    5.12914884,  -81.46081961],\n       [-187.8948621 , -100.44373091,   13.88978285],\n       [-141.62745778,   95.02891072, -191.48204257],\n       [  97.03260883,  165.34867495,  139.52003279],\n       [ 123.92529176,   21.25719016,   -7.84253   ],\n       [-122.25193977,  -85.16443186, -107.12274212],\n       [ -30.170388  ,  -94.80956739,   12.16979946],\n       [ 140.72667194,  176.50941682,  -17.50447799],\n       [ 149.37967282,  -81.15699552,   -5.72850319]])\n</code></pre>\n<p><strong>[Update]</strong></p>\n<p>Here's a <em>complete</em> code that does multi-target classification. Try to run it:</p>\n<pre><code>import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\n\n# The data from your screenshot\n#  A      B   C    D    E   F    G\ntrain_data = np.array([\n  [5, 133.5, 27, 284, 638, 31, 220],\n  [5, 111.9, 27, 285, 702, 36, 230],\n  [5,  99.3, 25, 310, 713, 39, 227],\n  [5, 102.5, 25, 311, 670, 34, 218],\n  [5, 114.8, 25, 312, 685, 34, 222],\n])\n# These I just made up\ntest_data_x = np.array([\n  [5, 100.0],\n  [5, 105.2],\n  [5, 102.7],\n  [5, 103.5],\n  [5, 120.3],\n  [5, 132.5],\n  [5, 152.5],\n])\n\nx = train_data[:, :2]\ny = train_data[:, 2:]\nforest = RandomForestClassifier(n_estimators=100, random_state=1)\nclassifier = MultiOutputClassifier(forest, n_jobs=-1)\nclassifier.fit(x, y)\nprint classifier.predict(test_data_x)\n</code></pre>\n<p>Output (well, looks reasonable to me):</p>\n<pre><code>[[  25.  310.  713.   39.  227.]\n [  25.  311.  670.   34.  218.]\n [  25.  311.  670.   34.  218.]\n [  25.  311.  670.   34.  218.]\n [  25.  312.  685.   34.  222.]\n [  27.  284.  638.   31.  220.]\n [  27.  284.  638.   31.  220.]]\n</code></pre>\n<p>If for some reason this doesn't work or can't be applied in your case, please update the question.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Interestingly, I see a lot of different answers about this both on stackoverflow and other sites:</p>\n<p>While working on my training data set, I imputed missing values of a certain column using a decision tree model. So here's my question. Is it fair to use ALL available data (Training &amp; Test) to make a model for imputation (not prediction) or may I only touch the training set when doing this? Also, once I begin work on my Test set, must I use only my test set data, impute using the same imputation model made in my training set, or can I use all the data available to me to retrain my imputation model?</p>\n<p>I would think so long as I didn't touch my test set for prediction model training, using the rest of the data for things like imputations would be fine. But maybe that would be breaking a fundamental rule. Thoughts?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Do not use any information from the Test set when doing any processing on your Training set. @Maxim and the answer linked to are correct, but I want to augment the answer. </p>\n<p>Imputation attempts to reason from incomplete data to suggest likely values for the missing entries. I think it's helpful to consider the missing values as a form of measurement error (see <a href=\"http://gking.harvard.edu/files/gking/files/measure.pdf\" rel=\"noreferrer\">this article</a> for a useful demonstration of this). As such, there are reasons to believe that the missingness is related to the underlying data generating process. And that process is precisely what you're attempting to replicate (though, of course, imperfectly) with your model. </p>\n<p>If you want your model to generalize well -- don't we all! -- then it is best to make sure that whatever processing you do to the training set is dependent only on the information in the data contained within that set. </p>\n<p>I would even suggest that you consider a three-way split: Test, Training, and Validation sets. The Validation set is further culled from the Training set and used to test model fit against \"itself\" (in the tuning of hyperparameters). This is, in part, what cross validation procedures do in things like <code>sklearn</code> and other pipelines. In this case, I generally conduct the imputation <em>after</em> the CV split, rather than on the full Training set, since I am attempting to evaluate a model on data the model \"knows\" (and the holdout data are a proxy for the unknown/future data). But note that I have not seen this suggested as uniformly as maintaining a complete wall between Test and Training sets. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would agree with <a href=\"https://stats.stackexchange.com/a/95088/130598\">this answer on cross-validated</a>:</p>\n<blockquote>\n<p>The division between training and test set is an attempt to replicate\n  the situation where you have past information and are building a model\n  which you will test on future as-yet unknown information</p>\n</blockquote>\n<p>The way you preprocess the data may affect model performance, in some cases significantly. Test data is a proxy for samples that you don't know. Would you perform imputation differently if you knew all future data? If yes, then using the test data is cheating. If no, there's no need for test data anyway. So it's better not to touch test data at all, until the model is built.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The philosophy behind splitting data into training and test sets is to have the opportunity of validating the model through fresh(ish) data, right?\nSo, by using the same imputer on both train and test sets, you are somehow spoiling the test data, and this may cause overfitting.\nYou CAN use the same approach to impute the missing data on both sets (in your case, the decision tree), however, you should instantiate two different models, and fit each one with its own related data.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to select only factor columns from my data frame. Example is below:</p>\n<pre><code>bank[,apply(bank[,names(bank)!=\"id\"], is.factor)]\n</code></pre>\n<p>But the code behaves strangely. Step by step:</p>\n<pre><code>sapply(bank[,names(bank)!=\"id\"], is.factor)\n</code></pre>\n<p>I get:</p>\n<pre><code>age         sex      region      income     married    children         car \n      FALSE        TRUE        TRUE       FALSE        TRUE       FALSE        TRUE \n   save_act current_act    mortgage         pep      ageBin \n       TRUE        TRUE        TRUE        TRUE        TRUE \n</code></pre>\n<p>Looks OK. Now, I assume that I just pass this matrix of TRUE/FALSE to the next step and get only the columns I need:</p>\n<pre><code>bank[,sapply(bank[,names(bank)!=\"id\"], is.factor)]\n</code></pre>\n<p>But as result I get all the same columns as in original bank dataframe. Nothing is filtered out. I tried it one way or another but can't find a solution. Any advice on what I am doing wrong?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>#DATA\ndf = mtcars\ncolnames(df) = gsub(\"mpg\",\"id\",colnames(df))\ndf$am = as.factor(df$am)\ndf$gear = as.factor(df$gear)\ndf$id = as.factor(df$id)\n\n#Filter out 'id' after selecting factors\ndf[,sapply(df, is.factor) &amp; colnames(df) != \"id\"]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>df = mtcars\ncolnames(df) = gsub(\"mpg\",\"id\",colnames(df))\ndf$am = as.factor(df$am)\ndf$gear = as.factor(df$gear)\ndf$id = as.factor(df$id)\n\nlibrary(dplyr)\ndf %&gt;%  select_if(is.factor) %&gt;% select(-id)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Does shapley support logistic regression models?</p>\n<p>Running the following code i get:</p>\n<pre><code>logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\npredictions = logmodel.predict(X_test)\nexplainer = shap.TreeExplainer(logmodel )\n\nException: Model type not yet supported by TreeExplainer: &lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;\n</code></pre>\n<p>P.S. You are supposed to use a different explainder for different models</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Shap is model agnostic by definition. It looks like you have just chosen an explainer that doesn't suit your model type. I suggest looking at <strong>KernelExplainer</strong> which as described by the creators <a href=\"https://github.com/slundberg/shap\" rel=\"noreferrer\">here</a> is </p>\n<blockquote>\n<p>An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.</p>\n</blockquote>\n<p>The documentation for Shap is mostly solid and has some decent examples. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>explainer = shap.LinearExplainer(logmodel)</code> should work as Logistic Regression is a linear model.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Used this code to genrate corelation table:</p>\n<pre><code>df1.drop(['BC DataPlus', 'AC Glossary'], axis=1).corr(method='pearson').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1) \n</code></pre>\n<p>This is the table generated:\n<a href=\"https://i.sstatic.net/lDk38.png\" rel=\"noreferrer\"><img alt=\"Correlation Table\" src=\"https://i.sstatic.net/lDk38.png\"/></a></p>\n<p>I cant find any way to save this table as image. Thank you.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The question you pose is difficult to answer if taken literally.\nThe difficulty stems from the fact that <code>df.style.render()</code> generates HTML which is then sent to a browser to be rendered as an image. The result may not be exactly the same across all browsers either.</p>\n<p>Python is not directly involved in the generation of the image. So there is no\nstraight-forward Python-based solution.</p>\n<p>Nevertheless, the issue of how to convert HTML to png\n<a href=\"https://github.com/pandas-dev/pandas/issues/14023\" rel=\"noreferrer\">was raised</a> on the pandas developers'\ngithub page and the suggested\nanswer was to <a href=\"https://stackoverflow.com/q/18067021/190597\">use <code>phantomjs</code></a>.  Other ways (that I haven't tested) might be to use\n<a href=\"https://stackoverflow.com/a/5634130/190597\"><code>webkit2png</code></a> or\n<a href=\"https://stackoverflow.com/a/50291677/190597\">GrabzIt</a>.</p>\n<p>We could avoid much of this difficulty, however, if we loosen the interpretation of the question. Instead of trying to produce the exact image generated by <code>df.style</code> (for a particular browser),\nwe could generate a <em>similar</em> image very easily using <a href=\"https://seaborn.pydata.org/\" rel=\"noreferrer\">seaborn</a>:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame(np.random.random((6, 4)), columns=list('ABCD'))\nfig, ax = plt.subplots()\nsns.heatmap(df.corr(method='pearson'), annot=True, fmt='.4f', \n            cmap=plt.get_cmap('coolwarm'), cbar=False, ax=ax)\nax.set_yticklabels(ax.get_yticklabels(), rotation=\"horizontal\")\nplt.savefig('result.png', bbox_inches='tight', pad_inches=0.0)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/6hkzS.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/6hkzS.png\"/></a></p>\n<hr/>\n<p>If you don't want to add the seaborn dependency, you could <a href=\"https://stackoverflow.com/q/35905393/190597\">use matplotlib directly</a> though it takes a few more lines of code:</p>\n<pre><code>import colorsys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame(np.random.random((6, 4)), columns=list('ABCD'))\ncorr = df.corr(method='pearson')\n\nfig, ax = plt.subplots()\ndata = corr.values\nheatmap = ax.pcolor(data, cmap=plt.get_cmap('coolwarm'), \n                    vmin=np.nanmin(data), vmax=np.nanmax(data))\nax.set_xticks(np.arange(data.shape[1])+0.5, minor=False)\nax.set_yticks(np.arange(data.shape[0])+0.5, minor=False)\nax.invert_yaxis()\nrow_labels = corr.index\ncolumn_labels = corr.columns\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(column_labels, minor=False)\n\ndef _annotate_heatmap(ax, mesh):\n    \"\"\"\n    **Taken from seaborn/matrix.py**\n    Add textual labels with the value in each cell.\n    \"\"\"\n    mesh.update_scalarmappable()\n    xpos, ypos = np.meshgrid(ax.get_xticks(), ax.get_yticks())\n    for x, y, val, color in zip(xpos.flat, ypos.flat,\n                                mesh.get_array(), mesh.get_facecolors()):\n        if val is not np.ma.masked:\n            _, l, _ = colorsys.rgb_to_hls(*color[:3])\n            text_color = \".15\" if l &gt; .5 else \"w\"\n            val = (\"{:.3f}\").format(val)\n            text_kwargs = dict(color=text_color, ha=\"center\", va=\"center\")\n            # text_kwargs.update(self.annot_kws)\n            ax.text(x, y, val, **text_kwargs)\n\n_annotate_heatmap(ax, heatmap)\nplt.savefig('result.png', bbox_inches='tight', pad_inches=0.0)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is it possible to convert a string vector into an indexed one using <code>numpy</code> ?</p>\n<p>Suppose I have an array of strings like <code>['ABC', 'DEF', 'GHI', 'DEF', 'ABC']</code> etc. I want it to be changed to an array of integers like <code>[0,1,2,1,0]</code>. Is it possible using numpy? I know that <code>Pandas</code> has a <code>Series</code> class that can do this, courtesy of <a href=\"https://stackoverflow.com/a/37122326/8507120\">this answer</a>. Is there something similar for <code>numpy</code> as well?</p>\n<p>Edit : \n <code>np.unique()</code> returns unique value for all elements. What I'm trying to do is convert the labels in the <a href=\"https://www.kaggle.com/uciml/iris/data\" rel=\"noreferrer\">Iris dataset</a> to indices, such as 0 for <code>Iris-setosa</code>, 1 for <code>Iris-versicolor</code> and 2 for <code>Iris-virginica</code> respectively. Is there a way to do this using <code>numpy</code>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html\" rel=\"noreferrer\"><code>numpy.unique</code></a> with parameter <code>return_inverse=True</code>, but there is difference with handling <code>NaN</code>s - check <a href=\"http://pandas.pydata.org/pandas-docs/stable/reshaping.html#factorizing-values\" rel=\"noreferrer\">factorizing values</a>:</p>\n<pre><code>L = ['ABC', 'DEF', 'GHI', 'DEF', 'ABC']\n\nprint (np.unique(L, return_inverse=True)[1])\n[0 1 2 1 0]\n</code></pre>\n<p>pandas <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html\" rel=\"noreferrer\"><code>factorize</code></a> working nice with list or array too: </p>\n<pre><code>print (pd.factorize(L)[0])\n[0 1 2 1 0]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to find <em>ROC curve</em> and <em>AUROC curve</em> for decision tree. My code was something like</p>\n<pre><code>clf.fit(x,y)\ny_score = clf.fit(x,y).decision_function(test[col])\npred = clf.predict_proba(test[col])\nprint(sklearn.metrics.roc_auc_score(actual,y_score))\nfpr,tpr,thre = sklearn.metrics.roc_curve(actual,y_score)\n</code></pre>\n<p>output:</p>\n<pre><code> Error()\n'DecisionTreeClassifier' object has no attribute 'decision_function'\n</code></pre>\n<p>basically, the error is coming up while finding the <code>y_score</code>. Please explain what is <code>y_score</code> and how to solve this problem?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First of all, the <code>DecisionTreeClassifier</code> <strong>has no</strong> attribute <code>decision_function</code>.</p>\n<p>If I guess from the structure of your code , you saw this <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\" rel=\"nofollow noreferrer\">example</a></p>\n<p>In this case the classifier is not the decision tree but it is the OneVsRestClassifier that supports the decision_function method.</p>\n<p>You can see the available attributes of <code>DecisionTreeClassifier</code> <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" rel=\"nofollow noreferrer\">here</a></p>\n<p>A possible way to do it is to <strong>binarize the classes</strong> and then <strong>compute the auc for each class:</strong></p>\n<p>Example:</p>\n<pre><code>from sklearn import datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy import interp\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n\nclassifier = DecisionTreeClassifier()\n\ny_score = classifier.fit(X_train, y_train).predict(X_test)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n#ROC curve for a specific class here for the class 2\nroc_auc[2]\n</code></pre>\n<p><strong>Result</strong></p>\n<pre><code>0.94852941176470573\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Think that for a decision tree you can use .predict_proba() instead of .decision_function() so you will get something as below:</p>\n<pre><code>y_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n</code></pre>\n<p>Then, the rest of the code will be the same.\nIn fact, the roc_curve function from scikit learn can take two types of input:\n\"Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers).\"\nSee <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\" rel=\"nofollow noreferrer\">here</a> for more details.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a simple question I cant seem to find a strait answer for. </p>\n<p>Suppose I have a data frame with date, open, high, low, close and volume. </p>\n<p>What I am trying to do is first find the current date which I can do with:</p>\n<pre><code>today = pd.datetime.today().date()\n</code></pre>\n<p>My issue comes in selecting the last 20 days of data from the current date. </p>\n<p>I need to select the last 20 rows because I need to then find the highest and lowest values in the close colum of this dataset. </p>\n<p>Any pointers would help a lot. Iv searched google for a while and keep finding different answers. </p>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Setup</strong> </p>\n<pre><code>today = pd.datetime.today().date()\n\ndf = pd.DataFrame(\n    np.random.rand(20, 4),\n    pd.date_range(end=today, periods=20, freq='3D'),\n    columns=['O', 'H', 'L', 'C'])\n\ndf\n\n                   O         H         L         C\n2017-08-01  0.821996  0.894122  0.829814  0.429701\n2017-08-04  0.883512  0.668642  0.524440  0.914845\n2017-08-07  0.035753  0.231787  0.421547  0.163865\n2017-08-10  0.742781  0.293591  0.874033  0.054421\n2017-08-13  0.252422  0.632991  0.547044  0.650622\n2017-08-16  0.316752  0.190016  0.504701  0.827450\n2017-08-19  0.777069  0.533121  0.329742  0.603473\n2017-08-22  0.843260  0.546845  0.600270  0.060620\n2017-08-25  0.834180  0.395653  0.189499  0.820043\n2017-08-28  0.806369  0.850968  0.753335  0.902687\n2017-08-31  0.336096  0.145325  0.876519  0.114923\n2017-09-03  0.590195  0.946520  0.009151  0.832992\n2017-09-06  0.901101  0.616852  0.375829  0.332625\n2017-09-09  0.537892  0.852527  0.082807  0.966297\n2017-09-12  0.104929  0.803415  0.345942  0.245934\n2017-09-15  0.085703  0.743497  0.256762  0.530267\n2017-09-18  0.823960  0.397983  0.173706  0.091678\n2017-09-21  0.211412  0.980942  0.833802  0.763510\n2017-09-24  0.312950  0.850760  0.913519  0.846466\n2017-09-27  0.921168  0.568595  0.460656  0.016313\n</code></pre>\n<hr/>\n<p><strong>Solution</strong><br/>\nUse pandas datetime index slicing.  Very simple and straightforward and what pandas devs intended for this problem.\n<em>Note:</em>  This does <strong>not</strong> care how many rows are within the last 20 days, it just grabs all of them.  This is what I think the OP wanted.    </p>\n<pre><code>df[today - pd.offsets.Day(20):]\n\n                   O         H         L         C\n2017-09-09  0.537892  0.852527  0.082807  0.966297\n2017-09-12  0.104929  0.803415  0.345942  0.245934\n2017-09-15  0.085703  0.743497  0.256762  0.530267\n2017-09-18  0.823960  0.397983  0.173706  0.091678\n2017-09-21  0.211412  0.980942  0.833802  0.763510\n2017-09-24  0.312950  0.850760  0.913519  0.846466\n2017-09-27  0.921168  0.568595  0.460656  0.016313\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you simply want the last 20 lines from a <code>DataFrame</code> you can just use <code>df[-20:]</code>. Instead, if you want to have the date 20 days ago you must make <code>pd.Timedelta(-19, unit='d') + pd.datetime.today().date()</code>.</p>\n<pre><code>In [1]: index = pd.date_range(start=(pd.Timedelta(-30, unit='d')+pd.datetime.today().date()), periods=31)\n\nIn [2]: df = pd.DataFrame(np.random.rand(31, 4), index=index, columns=['O', 'H', 'L', 'C'])\n\nIn [3]: df = df.reset_index().rename(columns={'index': 'Date'})\n\nIn [4]: df\nOut[4]:\n         Date         O         H         L         C\n0  2017-08-28  0.616856  0.518961  0.378005  0.716371\n1  2017-08-29  0.300977  0.652217  0.713013  0.842369\n2  2017-08-30  0.875668  0.232998  0.566047  0.969647\n3  2017-08-31  0.273934  0.086575  0.386617  0.390749\n4  2017-09-01  0.667561  0.336419  0.648809  0.619215\n5  2017-09-02  0.988234  0.563675  0.402908  0.671333\n6  2017-09-03  0.111710  0.549302  0.321546  0.201828\n7  2017-09-04  0.469041  0.736152  0.345069  0.336593\n8  2017-09-05  0.674844  0.276839  0.350289  0.862777\n9  2017-09-06  0.128124  0.968918  0.713846  0.415061\n10 2017-09-07  0.920488  0.252980  0.573531  0.270999\n11 2017-09-08  0.113368  0.781649  0.190273  0.758834\n12 2017-09-09  0.414453  0.545572  0.761805  0.586717\n13 2017-09-10  0.348459  0.830177  0.779591  0.783887\n14 2017-09-11  0.571877  0.230465  0.262744  0.360188\n15 2017-09-12  0.844286  0.821388  0.312319  0.473672\n16 2017-09-13  0.605548  0.570590  0.457141  0.882498\n17 2017-09-14  0.242154  0.066617  0.028913  0.969698\n18 2017-09-15  0.725521  0.742362  0.904866  0.890942\n19 2017-09-16  0.460858  0.749581  0.429131  0.723394\n20 2017-09-17  0.767445  0.452113  0.906294  0.978368\n21 2017-09-18  0.342970  0.702579  0.029031  0.743489\n22 2017-09-19  0.221478  0.339948  0.403478  0.349097\n23 2017-09-20  0.147785  0.633542  0.692545  0.194496\n24 2017-09-21  0.656189  0.419257  0.099094  0.708530\n25 2017-09-22  0.329901  0.087101  0.683207  0.558431\n26 2017-09-23  0.902550  0.155262  0.304506  0.756210\n27 2017-09-24  0.072132  0.045242  0.058175  0.755649\n28 2017-09-25  0.149873  0.340870  0.198454  0.725051\n29 2017-09-26  0.972721  0.505842  0.886602  0.231916\n30 2017-09-27  0.511109  0.990975  0.330336  0.898291\n\nIn [5]: df[-20:]\nOut[5]:\n         Date         O         H         L         C\n11 2017-09-08  0.113368  0.781649  0.190273  0.758834\n12 2017-09-09  0.414453  0.545572  0.761805  0.586717\n13 2017-09-10  0.348459  0.830177  0.779591  0.783887\n14 2017-09-11  0.571877  0.230465  0.262744  0.360188\n15 2017-09-12  0.844286  0.821388  0.312319  0.473672\n16 2017-09-13  0.605548  0.570590  0.457141  0.882498\n17 2017-09-14  0.242154  0.066617  0.028913  0.969698\n18 2017-09-15  0.725521  0.742362  0.904866  0.890942\n19 2017-09-16  0.460858  0.749581  0.429131  0.723394\n20 2017-09-17  0.767445  0.452113  0.906294  0.978368\n21 2017-09-18  0.342970  0.702579  0.029031  0.743489\n22 2017-09-19  0.221478  0.339948  0.403478  0.349097\n23 2017-09-20  0.147785  0.633542  0.692545  0.194496\n24 2017-09-21  0.656189  0.419257  0.099094  0.708530\n25 2017-09-22  0.329901  0.087101  0.683207  0.558431\n26 2017-09-23  0.902550  0.155262  0.304506  0.756210\n27 2017-09-24  0.072132  0.045242  0.058175  0.755649\n28 2017-09-25  0.149873  0.340870  0.198454  0.725051\n29 2017-09-26  0.972721  0.505842  0.886602  0.231916\n30 2017-09-27  0.511109  0.990975  0.330336  0.898291\n\nIn [6]: df[df.Date.isin(pd.date_range(pd.Timedelta(-19, unit='d')+pd.datetime.today().date(), periods=20))]\nOut[6]:\n         Date         O         H         L         C\n11 2017-09-08  0.113368  0.781649  0.190273  0.758834\n12 2017-09-09  0.414453  0.545572  0.761805  0.586717\n13 2017-09-10  0.348459  0.830177  0.779591  0.783887\n14 2017-09-11  0.571877  0.230465  0.262744  0.360188\n15 2017-09-12  0.844286  0.821388  0.312319  0.473672\n16 2017-09-13  0.605548  0.570590  0.457141  0.882498\n17 2017-09-14  0.242154  0.066617  0.028913  0.969698\n18 2017-09-15  0.725521  0.742362  0.904866  0.890942\n19 2017-09-16  0.460858  0.749581  0.429131  0.723394\n20 2017-09-17  0.767445  0.452113  0.906294  0.978368\n21 2017-09-18  0.342970  0.702579  0.029031  0.743489\n22 2017-09-19  0.221478  0.339948  0.403478  0.349097\n23 2017-09-20  0.147785  0.633542  0.692545  0.194496\n24 2017-09-21  0.656189  0.419257  0.099094  0.708530\n25 2017-09-22  0.329901  0.087101  0.683207  0.558431\n26 2017-09-23  0.902550  0.155262  0.304506  0.756210\n27 2017-09-24  0.072132  0.045242  0.058175  0.755649\n28 2017-09-25  0.149873  0.340870  0.198454  0.725051\n29 2017-09-26  0.972721  0.505842  0.886602  0.231916\n30 2017-09-27  0.511109  0.990975  0.330336  0.898291\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed.</b> This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet <a href=\"/help/closed-questions\">Stack Overflow guidelines</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p> We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-12-14 21:49:24Z\">2 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/44621452/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I would like to know if there is an implementation of hierarchical classification in the scikit-learn package or in any other python package. </p>\n<p>Thank you so much in advance.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I couldn't find an implementation of Hierarchical Classification on scikit-learn official documentation. But I found this repository recently. This module is based on scikit-learn's interfaces and conventions. I hope this will be useful. </p>\n<p><a href=\"https://github.com/globality-corp/sklearn-hierarchical-classification\" rel=\"noreferrer\">https://github.com/globality-corp/sklearn-hierarchical-classification</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>From various examples I've found online I still don't quite understand how to create embedding layers from my categorical data for neural network models, especially when I have a mix of numerical and categorical data. For example, taking the data set as below.: </p>\n<pre><code>numerical_df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=['num_1','num_2','num_3'])\n\ncat_df = pd.DataFrame(np.random.randint(0,5,size=(100, 3)), columns=['cat_1','cat_2','cat_3'])\n\ndf = numerical_df.join(cat_df)\n</code></pre>\n<p>I want to create embedding layers for my categorical data and use that in conjunction with my numerical data but from all the examples I've seen its almost like the model just filters the entire dataset through the embedding layer, which is confusing.    </p>\n<p>As an example of my confusion, below is an example from Keras' documentation on sequential models. It's as though they just add the embedding step as the first layer and fit it to the entirety of x_train.</p>\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\n\nmax_features = 1024\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, output_dim=256))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=16, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=16)\n</code></pre>\n<p>So ultimately when it comes to creating embedding matrices, is there one per categorical variable...one for all categorical variables?  And how do I reconcile this with my other data that doesn't need an embedding matrix?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To combine the categorical data with numerical data, your model should use multiple inputs using the functional API.  One for each categorical variable and one for the numerical inputs.  Its up to you how you want to then combine all that data together, but I assume it makes sense to just concatenate everything together and then continue with the rest of your model.</p>\n<pre><code>numerical_in = Input(shape=(3,))\ncat_in       = Input(shape=(3,))\nembed_layer  = Embedding(input_dim=5, output_dim=3, input_length=3)(cat_in)\nembed_layer  = Flatten(embed_layer)\nmerged_layer = concatenate([numerical_in, embed_layer])\noutput       = rest_of_your_model(merged_layer)\nmodel        = Model(inputs=[numerical_in, cat_in], outputs=[output])\n\n...\n\nmodel.fit(x=[numerical_df, cat_df], y=[your_expected_out])\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently conducting some analysis using NTSB aviation accident database. There are cause statements for most of the aviation incidents in this dataset that describe the factors lead to such event. </p>\n<p>One of my objectives here is to try to group the causes, and clustering seems to be a feasible way to solve this kind of problem. I performed the followings prior to the beginning of k-means clustering:</p>\n<ol>\n<li>Stop-word removal, that is, to remove some common functional words in the text</li>\n<li>Text stemming, that is, to remove a word's suffix, and if necessary, transform the term into its simplest form </li>\n<li>Vectorised the documents into TF-IDF vector to scale up the less-common but more-informative words and scale down highly-common but less-informative words</li>\n<li>Applied SVD to reduce the dimensionality of vector</li>\n</ol>\n<p>After these steps k-means clustering is applied to the vector. By using the events that occurred from Jan 1985 to Dec 1990 I get the following result with number of clusters <code>k = 3</code>:</p>\n<p>(Note: I am using Python and sklearn to work on my analysis)</p>\n<pre><code>... some output omitted ... \nClustering sparse data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=3, n_init=1,\n    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n    verbose=True)\nInitialization complete\nIteration  0, inertia 8449.657\nIteration  1, inertia 4640.331\nIteration  2, inertia 4590.204\nIteration  3, inertia 4562.378\nIteration  4, inertia 4554.392\nIteration  5, inertia 4548.837\nIteration  6, inertia 4541.422\nIteration  7, inertia 4538.966\nIteration  8, inertia 4538.545\nIteration  9, inertia 4538.392\nIteration 10, inertia 4538.328\nIteration 11, inertia 4538.310\nIteration 12, inertia 4538.290\nIteration 13, inertia 4538.280\nIteration 14, inertia 4538.275\nIteration 15, inertia 4538.271\nConverged at iteration 15\n\nSilhouette Coefficient: 0.037\nTop terms per cluster:\n**Cluster 0: fuel engin power loss undetermin exhaust reason failur pilot land**\n**Cluster 1: pilot failur factor land condit improp accid flight contribute inadequ**\n**Cluster 2: control maintain pilot failur direct aircraft airspe stall land adequ**\n</code></pre>\n<p>and I generated a plot graph of the data as follows:</p>\n<p><a href=\"https://i.sstatic.net/yFrpO.png\" rel=\"noreferrer\"><img alt=\"Plot result of the k-means clustering\" src=\"https://i.sstatic.net/yFrpO.png\"/></a></p>\n<p>The result doesn't seem like make sense to me. I wonder why all of the clusters contain some common terms like \"pilot\" and \"failure\". </p>\n<p>One possibility that I can think of (but I am not sure if it is valid in this case) is the documents with these common terms are actually located at the very centre of the the plot graph, therefore they can not be efficiently clustered into a right cluster. I believe this problem cannot be addressed by increasing the number of clusters, as I have just done it and this problem persists. </p>\n<p>I just want to know if there is any other factors that could cause the scenario that I am facing? Or more broadly, am I using the right clustering algorithm?</p>\n<p>Thanks SO. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I do not want to be a carrier of bad news, but ...</p>\n<ol>\n<li>Clustering is a very bad exploration technique - mostly because without a clear, task oriented aim, clustering techniques are actually focused on optimization of some mathematical criterions, which rarely have anything to do with what you want to achieve. Thus k-means in particular will look for minimization of the euclidean distances from cluster centers to all points inside a cluster. Is this anyhow related with the task you want to achieve? Usually the answer is \"no\", or in the best case \"I have no idea\".</li>\n<li>Representing documents as bag of words leads to very general look at your data, thus it is not a good approach to distinguish between similar objets. Such an approach can be used to distinguish between texts about guns from texts about hockey, but not specialistic texts from the very same domain (which seems to be the case here)</li>\n<li>In the end - you cannot really evaluate a clustering, and this is the biggest issue. Thus there are no well established techniques of fitting best clustering.</li>\n</ol>\n<p>So, to answer your final questions</p>\n<blockquote>\n<p>I just want to know if there is any other factors that could cause the scenario that I am facing? </p>\n</blockquote>\n<p>There are thousands of such factors. Finding actual, reasonable from the human perspectice, clusters in data is extremely hard. Finding any clusters is exteremely simple - because every clustering technique will find something. But in order to find what is important here one would have to go through whole data exploration here.</p>\n<blockquote>\n<p>Or more broadly, am I using the right clustering algorithm?</p>\n</blockquote>\n<p>Probably not, as k-means is simply a method of minimizing of inner cluster sum of euclidean distances, thus it will not work in most real world scenarios. </p>\n<p>Unfortunately - this is not the kind of problem where you can just ask \"which alogirhtm to use?\" and someone will offer you exact solution.</p>\n<p>You have to dig in your data, figure out:</p>\n<ul>\n<li>way of representation - is tfidf really good? have you preprocessed the vocablurary? Removed meaningless words? Maybe it is wort considering going for some modern word/document representation learning?</li>\n<li>structure in your data - in order to find best model you should visualize your data, investigate, run statistical analysis, try to figure out what is an underlying metric. Is there any reasonable distribution of points? Are these gaussians? Gaussian mixtures? Is your data sparse? </li>\n<li>can you provide some expert knowledge? Maybe you can divide part of dataset yourself? semi-supervised techniques are much better defined then any unsupervised ones, thus you might easily get much better results.</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have this code ,i want to remove the column 'timestamp' from the file :<a href=\"https://www.dropbox.com/s/4cksq4dwgbaqd07/u.data?dl=0\" rel=\"noreferrer\">u.data</a> but can't.It shows the error<br/>\n\"ValueError: labels ['timestamp'] not contained in axis\"\nHow can i correct it</p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.cross_validation import KFold\nfrom sklearn.cross_validation import train_test_split\n\n\n\ndata = pd.read_table('u.data')\ndata.columns=['userID', 'itemID','rating', 'timestamp']\ndata.drop('timestamp', axis=1)\n\n\nN = len(data)\nprint data.shape\nprint list(data.columns)\nprint data.head(10)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One of the biggest problem that one faces and that undergoes unnoticed is that in the u.data file while inserting headers the separation should be exactly the same as the separation between a row of data. For example if a tab is used to separate a tuple then you should not use spaces.<br/><br/> In your u.data file add headers and separate them exactly with as many whitespaces as were used between the items of a row.\nPS: Use sublime text, notepad/notepad++ does not work sometimes.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>\"ValueError: labels ['timestamp'] not contained in axis\"</p>\n</blockquote>\n<p>You don't have headers in the file, so the way you loaded it you got a <code>df</code> where the column names are the first rows of the data. You tried to access colunm <code>timestamp</code> which doesn't exist.</p>\n<p><strong>Your <code>u.data</code> doesn't have headers in it</strong></p>\n<pre><code>$head u.data                   \n196 242 3   881250949\n186 302 3   891717742\n</code></pre>\n<p>So working with column names isn't going to be possible unless add the headers. You can add the headers to the file <code>u.data</code>, e.g. I opened it in a text editor and added the line <code>a   b   c   timestamp</code> at the top of it (this seems to be a tab-separated file, so be careful when added the header not to use spaces, else it breaks the format)</p>\n<pre><code>$head u.data                   \na   b   c   timestamp\n196 242 3   881250949\n186 302 3   891717742\n</code></pre>\n<p>Now your code works and <code>data.columns</code> returns </p>\n<pre><code>Index([u'a', u'b', u'c', u'timestamp'], dtype='object')\n</code></pre>\n<p>And the rest of the trace of your working code is now</p>\n<pre><code>(100000, 4) # the shape\n['a', 'b', 'c', 'timestamp'] # the columns\n     a    b  c  timestamp # the df\n0  196  242  3  881250949\n1  186  302  3  891717742\n2   22  377  1  878887116\n3  244   51  2  880606923\n4  166  346  1  886397596\n5  298  474  4  884182806\n6  115  265  2  881171488\n7  253  465  5  891628467\n8  305  451  3  886324817\n9    6   86  3  883603013\n</code></pre>\n<p><strong>If you don't want to add headers</strong></p>\n<p>Or you can drop the column 'timestamp' using it's index (presumably 3), we can do this using <code>df.ix</code> below it selects all rows, columns index 0 to index 2, thus dropping the column with index 3</p>\n<pre><code>data.ix[:, 0:2]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to achieve a calculation involving geometric progression (split). Is there any effective/efficient way of doing it. The data set has millions of rows.\nI need the column \"Traded_quantity\"</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th>Marker</th>\n<th>Action</th>\n<th>Traded_quantity</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2019-11-05</td>\n<td>09:25</td>\n<td>0</td>\n<td></td>\n<td>0</td>\n</tr>\n<tr>\n<td></td>\n<td>09:35</td>\n<td>2</td>\n<td>BUY</td>\n<td>3</td>\n</tr>\n<tr>\n<td></td>\n<td>09:45</td>\n<td>0</td>\n<td></td>\n<td>0</td>\n</tr>\n<tr>\n<td></td>\n<td>09:55</td>\n<td>1</td>\n<td>BUY</td>\n<td>4</td>\n</tr>\n<tr>\n<td></td>\n<td>10:05</td>\n<td>0</td>\n<td></td>\n<td>0</td>\n</tr>\n<tr>\n<td></td>\n<td>10:15</td>\n<td>3</td>\n<td>BUY</td>\n<td>56</td>\n</tr>\n<tr>\n<td></td>\n<td>10:24</td>\n<td>6</td>\n<td>BUY</td>\n<td>8128</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>turtle = 2\n(User defined)</p>\n<p>base_quantity = 1\n(User defined)</p>\n<pre><code>    def turtle_split(row):\n        if row['Action'] == 'BUY':\n            return base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n        else:\n            return 0\n    df['Traded_quantity'] = df.apply(turtle_split, axis=1).round(0).astype(int)\n</code></pre>\n<h2>Calculation</h2>\n<p>For 0th Row, Traded_quantity should be zero (because the Marker is zero)</p>\n<p>For 1st Row, Traded_quantity should be (1x1) + (1x2) = 3 (Marker 2 will be split into 1 and 1, First 1 will be multiplied with the base_quantity&gt;&gt;1x1, Second 1 will be multiplied with the result from first 1 times turtle&gt;&gt;1x2), then we make a sum of these two numbers)</p>\n<p>For 2nd Row, Traded_quantity should be zero (because the Marker is zero)</p>\n<p>For 3rd Row, Traded_quantity should be (2x2) = 4(Marker 1 will be multiplied with the last split from row 1 time turtle i.e 2x2)</p>\n<p>For 4th Row, Traded_quantity should be zero(because the Marker is zero)</p>\n<p>For 5th Row, Traded_quantity should be (4x2)+(4x2x2)+(4x2x2x2) = 56(Marker 3 will be split into 1,1 and 1, First 1 will be multiplied with the last split from row3 times turtle &gt;&gt;4x2, Second 1 will be multiplied with the result from first 1 with turtle&gt;&gt;8x2), third 1 will be multiplied with the result from second 1 with turtle&gt;&gt;16x2) then we make a sum of these three numbers)</p>\n<p>For 6th Row, Traded_quantity should be (32x2)+(32x2x2)+(32x2x2x2)+(32x2x2x2x2)+(32x2x2x2x2x2) = 8128</p>\n<p>Whenever there will be a BUY, the traded quantity will be calculated using the last batch from Traded_quantity times turtle.</p>\n<p>Turns out the code is generating correct Traded_quantity when there is no zero in Marker. Once there is a gap with a couple of zeros geometric progression will not help, I would require the previous fig(from Cache) to recalculate Traded_q. tried with lru_cache for recursion, didn't work.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This should work</p>\n<pre><code>def turtle_split(row):\n        global base_quantity\n        if row['Action'] == 'BUY':\n            summation = base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n            base_quantity = base_quantity * (turtle ** (row['Marker'] - 1))*turtle\n            return summation\n        else:\n            return 0\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe that I need to group, then subgroup.  From the subgroups I need to return what the subgroup is as well as the unique values for a column.</p>\n<pre><code>df = pandas.DataFrame({'country': pandas.Series(['US', 'Canada', 'US', 'US']),\n                       'gender': pandas.Series(['male', 'female', 'male', 'female']),\n                       'industry': pandas.Series(['real estate', 'shipping', 'telecom', 'real estate']),\n                       'income': pandas.Series([1, 2, 3, 4])})\n\ndef subgroup(g):\n    return g.groupby(['gender'])\n\ns = df.groupby(['country']).apply(subgroup)\n</code></pre>\n<p>From s, how can I compute the uniques of \"industry\" as well as which \"gender\" it's grouped for?</p>\n<pre><code>--------------------------------------------\n| US     | male   | [real estate, telecom] |\n|        |----------------------------------\n|        | female | [real estate]          |\n--------------------------------------------\n| Canada | female | [shipping]             |\n--------------------------------------------\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you dont need to define that function, you can solve your problem with groupby() and unique() solely;</p>\n<p>try:</p>\n<pre><code>df.groupby(['country','gender'])['industry'].unique()\n</code></pre>\n<p>output:</p>\n<pre><code>country  gender\nCanada   female                [shipping]\nUS       female             [real estate]\n         male      [real estate, telecom]\nName: industry, dtype: object\n</code></pre>\n<p>hope it helps!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to use pandera library (I am very new with this) for pandas dataframe validation.\nWhat I want to do is to ignore the rows which are not valid as per the schema.\nHow can I do that?</p>\n<p>for example:\npandera schema looks like below:</p>\n<pre><code>import pandera as pa\nimport pandas as pd\n\nschema: pa.DataFrameSchema = pa.DataFrameSchema(columns={\n  'Col1': pa.Column(str),\n  'Col2': pa.Column(float, checks=pa.Check(lambda x: (0 &lt;= x &lt;= 1)), nullable=True),\n})\n\ndf: pd.DataFrame = pd.DataFrame({\n    \"Col1\": [\"1\", \"2\", \"3\", nan],\n    \"Col2\": [0.3, 0.4, 5, 0.2],\n})\n</code></pre>\n<p>What I want to do is when I apply validation on the <code>df</code> I get a result:</p>\n<pre><code>   Col1  Col2\n0     1   0.3\n1     2   0.4\n</code></pre>\n<p>The other rows with error dropped.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>pandera author here!</p>\n<p>Currently you have to use a <code>try except</code> block with lazy validation. The <code>SchemaErrors.failure_cases</code> df doesn't always have an index in certain cases, like if the column's type is incorrect. The index only applies to checks that produce an index-aligned boolean dataframe/series.</p>\n<p>By default the <code>check_fn</code> function fed into <code>pa.Check</code> should take a pandas Series as input. I fixed your custom check like so:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import pandera as pa\nimport pandas as pd\nimport numpy as np\n\nschema: pa.DataFrameSchema = pa.DataFrameSchema(columns={\n  'Col1': pa.Column(str),\n  'Col2': pa.Column(\n      float, checks=pa.Check(lambda series: series.between(0, 1)), nullable=True\n    ),\n})\n\ndf: pd.DataFrame = pd.DataFrame({\n    \"Col1\": [\"1\", \"2\", \"3\", np.nan],\n    \"Col2\": [0.3, 0.4, 5, 0.2],\n})\n\ntry:\n    schema(df, lazy=True)\nexcept pa.errors.SchemaErrors as exc:\n    filtered_df = df[~df.index.isin(exc.failure_cases[\"index\"])]\n\nprint(f\"filtered df:\\n{filtered_df}\")\n</code></pre>\n<p>Output:</p>\n<pre><code>filtered df:\n  Col1  Col2\n0    1   0.3\n1    2   0.4\n</code></pre>\n<p>To check value ranges I'd recommend using the built-in <a href=\"https://pandera.readthedocs.io/en/stable/reference/generated/methods/pandera.checks.Check.in_range.html#pandera.checks.Check.in_range\" rel=\"nofollow noreferrer\"><code>pa.Check.in_range</code></a> check.</p>\n<p>In other cases, just be aware of the <code>element_wise=True</code> kwarg in <a href=\"https://pandera.readthedocs.io/en/stable/reference/generated/pandera.checks.Check.html#pandera.checks.Check\" rel=\"nofollow noreferrer\">pa.Check</a>, it modifies the expected type signature of the <code>check_fn</code> arg.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I tried to compare the two, one is <code>pandas.unique()</code> and another one is <code>numpy.unique()</code>, and I found out that the latter actually surpass the first one.<br/>\nI am not sure whether the excellency is linear or not.</p>\n<p>Can anyone please tell me why such a difference exists, with regards to the code implementation? In what case should I use which?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>np.unique() is treating the data as an array, so it goes through every value individually then identifies the unique fields.</p>\n<p>whereas, pandas has pre-built metadata which contains this information and pd.unique() is simply calling on the metadata which contains 'unique' info, so it doesn't have to calculate it again. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>New to pandas development. How do I forward fill a DataFrame with the value contained in one previously seen column?</p>\n<p>Self-contained example:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nO = [1, np.nan, 5, np.nan]\nH = [5, np.nan, 5, np.nan]\nL = [1, np.nan, 2, np.nan]\nC = [5, np.nan, 2, np.nan]\ntimestamps = [\"2017-07-23 03:13:00\", \"2017-07-23 03:14:00\", \"2017-07-23 03:15:00\", \"2017-07-23 03:16:00\"]\ndict = {'Open': O, 'High': H, 'Low': L, 'Close': C}\ndf = pd.DataFrame(index=timestamps, data=dict)\nohlc = df[['Open', 'High', 'Low', 'Close']]\n</code></pre>\n<p>This yields the following DataFrame:</p>\n<pre><code>print(ohlc)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   NaN   NaN  NaN    NaN\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   NaN   NaN  NaN    NaN\n</code></pre>\n<p>I want to go from that last DataFrame to something like this:</p>\n<pre><code>                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n</code></pre>\n<p>So that the previously-seen value in 'Close' forward fills entire rows until there's a new populated row seen. It's simple enough to fill column 'Close' like so: </p>\n<pre><code>column2fill = 'Close'\nohlc[column2fill] = ohlc[column2fill].ffill()\nprint(ohlc)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   NaN   NaN  NaN    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   NaN   NaN  NaN    2.0\n</code></pre>\n<p>But is there a way to fill across the  03:14:00 and 03:16:00 rows with the 'Close' value of those rows? And is there a way to do it in one step using one forward fill instead of filling the 'Close' column first?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It seems you need <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>assign</code></a> with <code>ffill</code> and then <code>bfill</code> per row by <code>axis=1</code>, but necessary full <code>NaN</code>s rows:</p>\n<pre><code>df = ohlc.assign(Close=ohlc['Close'].ffill()).bfill(axis=1)\nprint (df)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n</code></pre>\n<p>What is same as:</p>\n<pre><code>ohlc['Close'] = ohlc['Close'].ffill()\ndf = ohlc.bfill(axis=1)\nprint (df)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to deploy a SageMaker endpoint and it gets stuck in \"Creating\" stage indefinitely. Below is my Dockerfile and training / serving script. The model trains without any issue. Only the Endpoint deployment gets stuck in the \"Creating\" stage.</p>\n<p>Below is the folder structure</p>\n<p><strong>Folder structure</strong></p>\n<pre><code>|_code\n   |_train_serve.py\n|_Dockerfile\n</code></pre>\n<p>Below is the Dockerfile</p>\n<p><strong>Dockerfile</strong></p>\n<pre><code># ##########################################################\n\n# Adapt your container (to work with SageMaker)\n# # https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-training-container.html\n# # https://hub.docker.com/r/huanjason/scikit-learn/dockerfile\n\nARG REGION=us-east-1\n\nFROM python:3.7\n\nRUN apt-get update &amp;&amp; apt-get -y install gcc\n\nRUN pip3 install \\\n        # numpy==1.16.2 \\\n        numpy \\\n        # scikit-learn==0.20.2 \\\n        scikit-learn \\\n        pandas \\\n        # scipy==1.2.1 \\\n        scipy \\\n        mlflow\n\nRUN rm -rf /root/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\n\n# Install sagemaker-training toolkit to enable SageMaker Python SDK\nRUN pip3 install sagemaker-training\n\nENV PATH=\"/opt/ml/code:${PATH}\"\n\n# Copies the training code inside the container\nCOPY  /code /opt/ml/code\n\n# Defines train_serve.py as script entrypoint\nENV SAGEMAKER_PROGRAM train_serve.py\n</code></pre>\n<p>Below is the script used for training and serving the model</p>\n<p><strong>train_serve.py</strong></p>\n<pre><code>import os\nimport ast\nimport warnings\nimport sys\nimport json\nimport ast\nimport argparse\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom urllib.parse import urlparse\nimport logging\nimport pickle\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\nif __name__ =='__main__':\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='kc_house_data_train.csv')\n    parser.add_argument('--test-file', type=str, default='kc_house_data_test.csv')\n    parser.add_argument('--features', type=str)  # we ask user to explicitly name features\n    parser.add_argument('--target', type=str) # we ask user to explicitly name the target\n\n    args, _ = parser.parse_known_args()\n\n    warnings.filterwarnings(\"ignore\")\n    np.random.seed(40)\n\n    # Reading training and testing datasets\n    logging.info('reading training and testing datasets')\n    logging.info(f\"{args.train} {args.train_file} {args.test} {args.test_file}\")\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    logging.info(args.features.split(','))\n    logging.info(args.target)\n    train_x = np.array(train_df[args.features.split(',')]).reshape(-1,1)\n    test_x = np.array(test_df[args.features.split(',')]).reshape(-1,1)\n    train_y = np.array(train_df[args.target]).reshape(-1,1)\n    test_y = np.array(test_df[args.target]).reshape(-1,1)  \n\n    reg = linear_model.LinearRegression()\n\n    reg.fit(train_x, train_y)\n    predicted_price = \n    reg.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_price)\n\n    logging.info(f\"        Linear model: (features={args.features}, target={args.target})\")\n    logging.info(f\"            RMSE: {rmse}\")\n    logging.info(f\"            MAE: {mae}\")\n    logging.info(f\"            R2: {r2}\")\n\n    model_path = os.path.join(args.model_dir, \"model.pkl\")\n    logging.info(f\"saving to {model_path}\")          \n    logging.info(args.model_dir)\n    with open(model_path, 'wb') as path:\n        pickle.dump(reg, path)\n\n\ndef model_fn(model_dir):\n    with open(os.path.join(model_dir, \"model.pkl\"), \"rb\") as input_model:\n        model = pickle.load(input_model)\n    return model\n    \ndef predict_fn(input_object, model):\n    _return = model.predict(input_object)\n    return _return\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One way of investigating this is to attempt to use the same model via the AWS console as part of a Batch Transform, as this flow seems to give better error messaging and diagnostics versus Inference Endpoint creation.</p>\n<p>In my case, this made me realise that the IAM role that was associated with the model upon its creation no longer existed. I'd overlooked this because because the roles were CDK-managed and at some point got removed, but the Models were created dynamically via Step Functions pipelines.</p>\n<p>Anyway, deploying with a non-existent role would lead to the SageMaker endpoint remaining in the \"Creating\" state for a few hours, before failing with \"Request to service failed. If failure persists after retry, contact customer support\", and there would be no CloudWatch logs. Re-creating the model with a valid role fixed the issue.</p>\n<p>Apologies if the above does not apply to the OP, who reports the same problem but with a different setup that I am not familiar with. I am just sharing my outcome with a similar problem which brought me to this page, in case it helps anyone in future.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm reading a book on Data Science for Python and the author applies 'sigma-clipping operation' to remove outliers due to typos. However the process isn't explained at all.</p>\n<p>What is sigma clipping? Is it only applicable for certain data (eg. in the book it's used towards birth rates in US)?</p>\n<p>As per the text:</p>\n<pre><code>quartiles = np.percentile(births['births'], [25, 50, 75]) #so we find the 25th, 50th, and 75th percentiles\nmu = quartiles[1] #we set mu = 50th percentile\nsig = 0.74 * (quartiles[2] - quartiles[0]) #???\n\nThis final line is a robust estimate of the sample mean, where the 0.74 comes \nfrom the interquartile range of a Gaussian distribution.\n</code></pre>\n<p>Why 0.74? Is there a proof for this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>This final line is a robust estimate of the sample mean, where <strong>the 0.74 comes \n  from the interquartile range of a Gaussian distribution</strong>.</p>\n</blockquote>\n<p>That's it, really...</p>\n<p>The code tries to estimate sigma using the interquartile range to make it robust against outliers. 0.74 is a correction factor. Here is how to calculate it:</p>\n<pre><code>p1 = sp.stats.norm.ppf(0.25)  # first quartile of standard normal distribution\np2 = sp.stats.norm.ppf(0.75)  # third quartile\nprint(p2 - p1)  # 1.3489795003921634\n\nsig = 1  # standard deviation of the standard normal distribution  \nfactor = sig / (p2 - p1)\nprint(factor)  # 0.74130110925280102\n</code></pre>\n<p>In the standard normal distribution <code>sig==1</code> and the interquartile range is <code>1.35</code>. So <code>0.74</code> is the correction factor to turn the interquartile range into sigma. Of course, this is only true for the normal distribution.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose you have a set of data. Compute its median <code>m</code> and its standard deviation <code>sigma</code>. Keep only the data that falls in the range (<code>m-a*sigma</code>,<code>m+a*sigma</code>) for some value of <code>a</code>, and discard everything else. This is one iteration of sigma clipping. Continue to iterate a predetermined number of times, and/or stop when the relative reduction in the value of sigma is small.</p>\n<p>Sigma clipping is geared toward removing outliers, to allow for a more robust (i.e. resistant to outliers) estimation of, say, the mean of the distribution. So it's applicable to data where you expect to find outliers.</p>\n<p>As for the 0.74, it comes from the interquartile range of the Gaussian distribution, as per the text.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The answers here are accurate and reasonable, but don't quite get to the heart of your question:</p>\n<blockquote>\n<p>What is sigma clipping? Is it only applicable for certain data?</p>\n</blockquote>\n<p>If we want to use mean (mu) and standard deviation (sigma) to figure out a threshold for ejecting extreme values in situations where we have a reason to suspect that those extreme values are mistakes (and not just very high/low values), we don't want to calculate mu/sigma using the dataset which includes these mistakes.</p>\n<p>Sample problem: you need to compute a threshold for a temperature sensor to indicate when the temperature is \"High\" - but sometimes the sensor gives readings that are impossible, like \"surface of the sun\" high.</p>\n<p>Imagine a series that looks like this:</p>\n<p><code>thisSeries = np.array([1,2,3,4,1,2,3,4,5,3,4,5,3, 500, 1000])</code></p>\n<p>Those last two values look like obvious mistakes - but if we use a typical stats function like a Normal PPF, it's going to implicitly assume that those outliers belong in the distribution, and perform its calculation accordingly:</p>\n<p><code>st.norm.ppf(.975, thisSeries.mean(), thisSeries.std())</code></p>\n<p><code>631.5029013468446</code></p>\n<p>So using a two-sided 5% outlier threshold (meaning we will reject the lower and upper 2.5%), it's telling me that 500 is not an outlier.   Even if I use a one-sided threshold of .95 (reject the upper 5%), it will give me 546 as the outlier limit, so again, 500 is regarded as non-outlier.</p>\n<p>Sigma-clipping works by focusing on the inter-quartile range and using median instead of mean, so the thresholds won't be calculated under the influence of the extreme values.</p>\n<pre><code>thisDF = pd.DataFrame(thisSeries, columns=[\"value\"])\nintermed=\"value\"\nfactor=5\nquartiles = np.percentile(thisSeries, [25, 50, 75])\nmu, sig = quartiles[1], 0.74 * (quartiles[2] - quartiles[0])\nqueryString = '({} &lt; @mu - {} * @sig) | ({} &gt; @mu + {} * @sig)'.format(intermed, factor, intermed, factor)\nprint(mu + 5 * sig)\n  10.4\n\nprint(thisDF.query(queryString))\n 500\n1000\n</code></pre>\n<p>At factor=5, both outliers are correctly isolated, and the threshold is at a reasonable 10.4 - reasonable, given that the 'clean' part of the series is [1,2,3,4,1,2,3,4,5,3,4,5,3].   ('factor' in this context is a scalar applied to the thresholds)</p>\n<p>To answer the question, then: sigma clipping is a method of identifying outliers which is immune from the deforming effects of the outliers themselves, and though it can be used in many contexts, it excels in situations where you suspect that the extreme values are not merely high/low values that should be considered part of the dataset, but rather that they are errors.</p>\n<p>Here's an illustration of the difference between extreme values that are part of a distribution, and extreme values that are possibly errors, or just so extreme as to deform analysis of the rest of the data.</p>\n<p><a href=\"https://i.sstatic.net/E6OrV.png\" rel=\"nofollow noreferrer\"><img alt=\"Normally distributed data has extreme values that do not deform analysis of the rest of the data\" src=\"https://i.sstatic.net/E6OrV.png\"/></a></p>\n<p>The data above was generated synthetically, but you can see that the highest values in this set are not deforming the statistics.</p>\n<p>Now here's a set generated the same way, but this time with some artificial outliers injected (above 40):</p>\n<p><a href=\"https://i.sstatic.net/PsLvI.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/PsLvI.png\"/></a></p>\n<p>If I sigma-clip this, I can get back to the original histogram and statistics, and apply them usefully to the dataset.</p>\n<p>But where sigma-clipping really shines is in real world scenarios, in which faulty data is common.    Here's an example that uses real data - historical observations of my heart-rate monitor.   Let's look at the histogram without sigma-clipping:</p>\n<p><a href=\"https://i.sstatic.net/Cgmr4.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Cgmr4.png\"/></a></p>\n<p>I'm a pretty chill dude, but I know for a fact that my heart rate is never zero.   Sigma-clipping handles this easily, and we can now look at the real distribution of heart-rate observations:</p>\n<p><a href=\"https://i.sstatic.net/AZijR.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/AZijR.png\"/></a></p>\n<p>Now, you may have some domain knowledge that would enable you to manually assert outlier thresholds or filters.   This is one final nuance to why we might use sigma-clipping - in situations where data is being handled entirely by automation, or we have no domain knowledge relating to the measurement or how it's taken, then we don't have any informed basis for filter or threshold statements.</p>\n<p>It's easy to say that a heart rate of 0 is not a valid measurement - but what about 10?   What about 200?   And what if heart-rate is one of thousands of different measurements we're taking.   In such cases, maintaining sets of manually defined thresholds and filters would be overly cumbersome.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've just discovered the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\" rel=\"noreferrer\">Pipeline</a> feature of scikit-learn, and I find it very useful for testing different combinations of preprocessing steps before training my model. </p>\n<p>A pipeline is a chain of objects that implement the <code>fit</code> and <code>transform</code> methods. Now, if I wanted to add a new preprocessing step, I used to write a  class that inherits from <code>sklearn.base.estimator</code>. However, I'm thinking that there must be a simpler method. Do I really need to wrap every function I want to apply in an estimator class? </p>\n<p>Example:</p>\n<pre><code>class Categorizer(sklearn.base.BaseEstimator):\n    \"\"\"\n    Converts given columns into pandas dtype 'category'.\n    \"\"\"\n\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y):\n        return self\n\n\n    def transform(self, X):\n        for column in self.columns:\n            X[column] = X[column].astype(\"category\")\n        return X\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For a general solution (working for many other use cases, not just transformers, but also simple models etc.), you can write your own <strong>decorator</strong> if you have state-free functions (which do not implement fit), for example by doing:</p>\n<pre><code>class TransformerWrapper(sklearn.base.BaseEstimator):\n\n    def __init__(self, func):\n        self._func = func\n\n    def fit(self, *args, **kwargs):\n        return self\n\n    def transform(self, X, *args, **kwargs):\n        return self._func(X, *args, **kwargs)\n</code></pre>\n<p>and now you can do</p>\n<pre><code>@TransformerWrapper\ndef foo(x):\n  return x*2\n</code></pre>\n<p>which is the equivalent of doing</p>\n<pre><code>def foo(x):\n  return x*2\n\nfoo = TransformerWrapper(foo)\n</code></pre>\n<p>which is what sklearn.preprocessing.FunctionTransformer is doing under the hood.</p>\n<p>Personally I find decorating simpler, since you have a nice separation of your preprocessors from the rest of the code, but it is up to you which path to follow.</p>\n<p>In fact you should be able to decorate with sklearn function by</p>\n<pre><code>from sklearn.preprocessing import FunctionTransformer\n\n@FunctionTransformer\ndef foo(x):\n  return x*2\n</code></pre>\n<p>too.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html\" rel=\"noreferrer\"><code>sklearn.preprocessing.FunctionTransformer</code></a> class can be used to instantiate a scikit-learn transformer (which can be used e.g. in a pipeline) from a user provided function.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think it's worth to mention that <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html\" rel=\"nofollow noreferrer\"><code>sklearn.preprocessing.FunctionTransformer(..., validate=True)</code></a> has a <code>validate=False</code> parameter:</p>\n<blockquote>\n<p><strong>validate</strong> : <code>bool</code>, optional <code>default=True</code></p>\n<p>Indicate that the input <code>X</code> array should be checked before calling <code>func</code>. If validate is false, there will be no input validation. <strong>If it\n  is true, then <code>X</code> will be converted to a 2-dimensional NumPy array or\n  sparse matrix</strong>. If this conversion is not possible or <code>X</code> contains <code>NaN</code> or\n  infinity, an exception is raised.</p>\n</blockquote>\n<p>So if you are going to pass non-numerical features to <code>FunctionTransformer</code> make sure that you explicitly set <code>validate=False</code>, otherwise it'll fail with the following exception:</p>\n<pre><code>ValueError: could not convert string to float: 'your non-numerical value'\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm using a combination of sklearn and Keras running with Theano as its back-end. I'm using the following code-</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.constraints import maxnorm\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nimport time\nfrom datetime import timedelta\nfrom __future__ import division\n\nseed = 7\nnp.random.seed(seed)\n\nY = data['Genre']\ndel data['Genre']\nX = data\n\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n\nX = X.as_matrix().astype(\"float\")\n\ncalls=[EarlyStopping(monitor='acc', patience=10), ModelCheckpoint('C:/Users/1383921/Documents/NNs/model', monitor='acc', save_best_only=True, mode='auto', period=1)]\n\ndef create_baseline(): \n    # create model\n    model = Sequential()\n    model.add(Dense(18, input_dim=9, init='normal', activation='relu'))\n    model.add(Dense(9, init='normal', activation='relu'))\n    model.add(Dense(12, init='normal', activation='softmax'))\n    # Compile model\n    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\nnp.random.seed(seed)\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=300, batch_size=16, verbose=2)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold, fit_params={'mlp__callbacks':calls})\nprint(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n</code></pre>\n<p>The result when I start running this last part is-</p>\n<pre><code>Epoch 1/10\n...\nEpoch 2/10\n</code></pre>\n<p>etc.</p>\n<p>It's supposed to be <code>Epoch 1/300</code> and it works just fine when I run it on a different notebook.</p>\n<p>What do you guys think is happening? <code>np_epoch=300</code>...</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What Keras version is this? If its greater than 2.0, then nb_epoch was changed to just epochs. Else it defaults to 10.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In Keras 2.0 the <code>nb_epoch</code> parameter was renamed to <code>epochs</code> so when you set <code>epochs=300</code> it runs 300 epochs. If you use <code>nb_epoch=300</code> it will default to 10 instead.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Another solution to your problem</strong>: Forget about nb_epoch (doesn't work). Pass epochs inside fit_params:</p>\n<pre><code>results = cross_val_score(pipeline, X, encoded_Y, cv=kfold, \n          fit_params={'epochs':300,'mlp__callbacks':calls})\n</code></pre>\n<p>And that would work. fit_params goes straight into the Fit method and it will get the right epochs.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a CSV file with a data field which contains data like bellow</p>\n<blockquote>\n<p>POLYGON ((79.87749999947846 6.997500000409782, 79.88249999947845\n6.997500000409782, 79.88249999947845 7.002500000409782, 79.87749999947846 7.002500000409782, 79.87749999947846 6.997500000409782))</p>\n</blockquote>\n<p>I want to draw a polygon by using this data field in qgis. How can i do this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For example,I have a csv with two columns \"<strong>Id</strong>\" and \"<strong>geom</strong>\" that geom have your POLYGON example,</p>\n<p><a href=\"https://i.sstatic.net/yFxwV.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/yFxwV.png\"/></a></p>\n<p>Go to <strong><em>layer-&gt;Add Layer-&gt;Add delimited text Layer</em></strong> and browse your csv and the geometry field combobox select the column that have your wkt data,in my case is \"<strong>geom</strong>\" and <strong>Geometry definition</strong> select (<strong>WKT</strong>) option</p>\n<p><a href=\"https://i.sstatic.net/tG3oO.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/tG3oO.png\"/></a></p>\n<p>The result is:</p>\n<p><a href=\"https://i.sstatic.net/fr5b7.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/fr5b7.png\"/></a></p>\n<p>In another way, using Python:</p>\n<pre><code>uri ='file:///C://Users//fjraga//Desktop//test.csv?delimiter=%s&amp;crs=epsg:4326&amp;wktField=%s' % (\",\", \"geom\")\nlyr = QgsVectorLayer(uri, 'Test','delimitedtext')\nQgsMapLayerRegistry.instance().addMapLayer(lyr)\n</code></pre>\n<p>But if you only want load this WKT geometry using QGIS python console,try with this:</p>\n<pre><code>wkt = \"POLYGON ((79.87749999947846 6.997500000409782, 79.88249999947845 6.997500000409782, 79.88249999947845 7.002500000409782, 79.87749999947846 7.002500000409782, 79.87749999947846 6.997500000409782))\"\n\ntemp = QgsVectorLayer(\"Polygon?crs=epsg:4326\", \"result\", \"memory\")\nQgsMapLayerRegistry.instance().addMapLayer(temp)\n\ntemp.startEditing()\ngeom = QgsGeometry()\ngeom = QgsGeometry.fromWkt(wkt)\nfeat = QgsFeature()\nfeat.setGeometry(geom)\ntemp.dataProvider().addFeatures([feat])\ntemp.commitChanges()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You copy your text into the clipboard.</p>\n<p>And then:</p>\n<ol>\n<li>Open QGIS</li>\n<li>Open the \"Edit\" Menu</li>\n<li>Enter the \"Insert Objects as\"/\"Insert Features as\" sub-menu</li>\n<li>Choose either vector or temporary layer</li>\n<li>Select the correct coordinate system</li>\n</ol>\n<p>And you are done.</p>\n<p>It is as simple as that.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to fine-tune my word2vec model. I have two datasets, <code>data1</code> and <code>data2</code>.</p>\n<p>What I did so far is:</p>\n<pre><code>model = gensim.models.Word2Vec(\n        data1,\n        size=size_v,\n        window=size_w,\n        min_count=min_c,\n        workers=work)\nmodel.train(data1, total_examples=len(data1), epochs=epochs)\n\nmodel.train(data2, total_examples=len(data2), epochs=epochs)\n</code></pre>\n<p>Is this correct? Do I need to store learned weights somewhere?</p>\n<p>I checked <a href=\"https://stackoverflow.com/questions/46244286/fine-tuning-pre-trained-word2vec-google-news/55751018#55751018\">this answer</a> and <a href=\"https://www.kaggle.com/kfujikawa/word2vec-fine-tuning\" rel=\"nofollow noreferrer\">this one</a> but I couldn’t understand how it’s done.</p>\n<p>Can someone explain to me the steps to follow?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Note you <strong>don't</strong> need to call <code>train()</code> with <code>data1</code> if you already provided <code>data1</code> at the time of model instantiation. The model will have already done its own internal <code>build_vocab()</code> and <code>train()</code> on the supplied corpus, using the default number of <code>epochs</code> (5) if you haven't specified one in the instantiation. </p>\n<p>\"Fine-tuning\" is not a simple process with reliable steps assured to improve the model. It's very error-prone. </p>\n<p>In particular, if words in <code>data2</code> aren't already known to the model, they'll be ignored. (There's an option to call <code>build_vocab()</code> with the parameter <code>update=True</code> to expand the known vocabulary, but such words aren't really on full equal footing with earlier words.)</p>\n<p>If <code>data2</code> includes some words, but not others, only those in <code>data2</code> get updated via the additional training – which may essentially pull those words <em>out</em> of comparable alignment from other words that only appeared in <code>data1</code>. (Only the words trained together, in an interleaved shared training session, will go through the \"push-pull\" that in the end leaves them in useful arrangments.)</p>\n<p>The safest course for incremental training would be to shuffle <code>data1</code> and <code>data2</code> together, and do the continued training on all the data: so that all words get new interleaved training together.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you train <code>w2v</code> model with gensim it stores the <code>vocab</code> and <code>index</code> of each word.<br/>\n<code>gensim</code> uses this info to map a word to its vector.</p>\n<p>If you are going to finetune an already existing <code>w2v</code> model you need to make sure that your vocab is consistent.</p>\n<p>See attached piece of code.</p>\n<pre><code>import os\nimport pickle\nimport numpy as np\nimport gensim\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.models.callbacks import CallbackAny2Vec\nimport operator\n\nos.mkdir(\"model_dir\")\n\n# class EpochSaver(CallbackAny2Vec):\n#     '''Callback to save model after each epoch.'''\n#     def __init__(self, path_prefix):\n#         self.path_prefix = path_prefix\n#         self.epoch = 0\n\n#     def on_epoch_end(self, model):\n#         list_of_existing_files = os.listdir(\".\")\n#         output_path = 'model_dir/{}_epoch{}.model'.format(self.path_prefix, self.epoch)\n#         try:\n#             model.save(output_path)\n#         except:\n#             model.wv.save_word2vec_format('model_dir/model_{}.bin'.format(self.epoch), binary=True)\n#         print(\"number of epochs completed = {}\".format(self.epoch))\n#         self.epoch += 1\n#         list_of_total_files = os.listdir(\".\")\n\n# saver = EpochSaver(\"my_finetuned\")\n\n\n\n\n\n# function to load vectors from existing model.\n# I am loading glove vectors from a text file, benefit of doing this is that I get complete vocab of glove as well.\n# If you are using a previous word2vec model I would recommed save that in txt format.\n# In case you decide not to do it, you can tweak the function to get vectors for words in your vocab only.\ndef load_vectors(token2id, path,  limit=None):\n    embed_shape = (len(token2id), 300)\n    freqs = np.zeros((len(token2id)), dtype='f')\n\n    vectors = np.zeros(embed_shape, dtype='f')\n    i = 0\n    with open(path, encoding=\"utf8\", errors='ignore') as f:\n        for o in f:\n            token, *vector = o.split(' ')\n            token = str.lower(token)\n            if len(o) &lt;= 100:\n                continue\n            if limit is not None and i &gt; limit:\n                break\n            vectors[token2id[token]] = np.array(vector, 'f')\n            i += 1\n\n    return vectors\n\n\nembedding_name = \"glove.840B.300d.txt\"\ndata = \"&lt;training data(new line separated tect file)&gt;\"\n\n# Dictionary to store a unique id for each token in vocab( in my case vocab contains both my vocab and glove vocab)\ntoken2id = {}\n\n# This dictionary will contain all the words and their frequencies.\nvocab_freq_dict = {}\n\n# Populating vocab_freq_dict and token2id from my data.\nid_ = 0\ntraining_examples = []\nfile = open(\"{}\".format(data),'r', encoding=\"utf-8\")\nfor line in file.readlines():\n    words = line.strip().split(\" \")\n    training_examples.append(words)\n    for word in words:\n        if word not in vocab_freq_dict:\n            vocab_freq_dict.update({word:0})\n        vocab_freq_dict[word] += 1\n        if word not in token2id:\n            token2id.update({word:id_})\n            id_ += 1\n\n# Populating vocab_freq_dict and token2id from glove vocab.\nmax_id = max(token2id.items(), key=operator.itemgetter(1))[0]\nmax_token_id = token2id[max_id]\nwith open(embedding_name, encoding=\"utf8\", errors='ignore') as f:\n    for o in f:\n        token, *vector = o.split(' ')\n        token = str.lower(token)\n        if len(o) &lt;= 100:\n            continue\n        if token not in token2id:\n            max_token_id += 1\n            token2id.update({token:max_token_id})\n            vocab_freq_dict.update({token:1})\n\nwith open(\"vocab_freq_dict\",\"wb\") as vocab_file:\n    pickle.dump(vocab_freq_dict, vocab_file)\nwith open(\"token2id\", \"wb\") as token2id_file:\n    pickle.dump(token2id, token2id_file)\n\n\n\n# converting vectors to keyedvectors format for gensim\nvectors = load_vectors(token2id, embedding_name)\nvec = KeyedVectors(300)\nvec.add(list(token2id.keys()), vectors, replace=True)\n\n# setting vectors(numpy_array) to None to release memory\nvectors = None\n\nparams = dict(min_count=1,workers=14,iter=6,size=300)\n\nmodel = Word2Vec(**params)\n\n# using build from vocab to build the vocab\nmodel.build_vocab_from_freq(vocab_freq_dict)\n\n# using token2id to create idxmap\nidxmap = np.array([token2id[w] for w in model.wv.index2entity])\n\n# Setting hidden weights(syn0 = between input layer and hidden layer) = your vectors arranged accoring to ids\nmodel.wv.vectors[:] = vec.vectors[idxmap]\n\n# Setting hidden weights(syn0 = between hidden layer and output layer) = your vectors arranged accoring to ids\nmodel.trainables.syn1neg[:] = vec.vectors[idxmap]\n\n\nmodel.train(training_examples, total_examples=len(training_examples), epochs=model.epochs)\noutput_path = 'model_dir/final_model.model'\nmodel.save(output_path)\n\n</code></pre>\n<p>Comment if you have any doubts.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<blockquote>\n<p>Is this correct?</p>\n</blockquote>\n<p>Yes, it is. You need to make sure that data2's words in vocabulary provided by data1. If it isn't the words - that isn't presented in vocabulary - will be lost.</p>\n<p>Note that the weights that will be computed by</p>\n<p><code>model.train(data1, total_examples=len(data1), epochs=epochs)</code></p>\n<p>and</p>\n<p><code>model.train(data2, total_examples=len(data2), epochs=epochs)</code></p>\n<p><strong>isn't equal to</strong></p>\n<p><code>model.train(data1+data2, total_examples=len(data1+data2), epochs=epochs)</code></p>\n<blockquote>\n<p>Do I need to store learned weights somewhere?</p>\n</blockquote>\n<p>No, you don't need to.</p>\n<p>But if you want you can save weights as a file so you can use them later.</p>\n<pre><code>model.save(\"word2vec.model\")\n</code></pre>\n<p>And you load them by</p>\n<pre><code>model = Word2Vec.load(\"word2vec.model\")\n</code></pre>\n<p>(<a href=\"https://radimrehurek.com/gensim/models/word2vec.html\" rel=\"nofollow noreferrer\">source</a>)</p>\n<blockquote>\n<p>I need to fine tune my word2vec model.</p>\n</blockquote>\n<p>Note that \"Word2vec training is an unsupervised task, there’s no good way to <strong>objectively evaluate the result</strong>. Evaluation <strong>depends</strong> on your <strong>end application</strong>.\" (<a href=\"https://rare-technologies.com/word2vec-tutorial/\" rel=\"nofollow noreferrer\">source</a>) But there's some evaluations that you can look-up <a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"nofollow noreferrer\">here</a> (<em>\"How to measure quality of the word vectors\"</em> section)</p>\n<p>Hope that helps!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have used a custom metric for light gbm but early stopping work for log loss which is the objective function how can I fix that or change early stopping to work for eval metric.</p>\n<pre class=\"lang-py prettyprint-override\"><code>def evaluate_macroF1_lgb(truth, predictions):  \n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nlg = LGBMClassifier(n_estimators=1000)\n\nlg.fit(x_train,y_train,eval_set=(x_test,y_test),eval_metric=evaluate_macroF1_lgb,early_stopping_rounds=25)\n\n</code></pre>\n<p>I expected it to run for 1000 iteration or less but it ran for 25 as the log loss was not improving but f1 metric is improving.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Update</p>\n<p>I had found a solution we can set metric=\"custom\" in LGBM classifier then it will use the eval metric.</p>\n<pre class=\"lang-py prettyprint-override\"><code>\nlg = LGBMClassifier(n_estimators=1000,metric=\"custom\")\n\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have df with column names: 'a', 'b', 'c' ... 'z'.</p>\n<pre><code>print(my_df.columns)\nIndex(['a', 'b', 'c', ... 'y', 'z'],\n  dtype='object', name=0)\n</code></pre>\n<p>I have function which determine which columns should be displayed. For example:</p>\n<pre><code>start = con_start()\nstop = con_stop()\nprint(my_df.columns &gt;= start) &amp; (my_df &lt;= stop)\n</code></pre>\n<p>My result is:</p>\n<pre><code>[False False ... False False False False  True  True\nTrue  True False False]\n</code></pre>\n<p>My goal is display dataframe only with columns that satisfy my condition.\nIf start = 'a' and stop = 'b', I want to have:</p>\n<pre><code>0                                      a              b         \nindex1       index2                                                  \nNew York     New York           0.000000       0.000000          \nCalifornia   Los Angeles   207066.666667  214466.666667     \nIllinois     Chicago       138400.000000  143633.333333     \nPennsylvania Philadelphia   53000.000000   53633.333333      \nArizona      Phoenix       111833.333333  114366.666667 \n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use slicing to achieve this with .loc:</p>\n<pre><code> df.loc[:,'a':'b']\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to make this robust and with as few assumptions as possible.</p>\n<p><strong><em>option 1</em></strong><br/>\nuse <em><code>iloc</code></em> with array slicing<br/>\n<strong>Assumptions:</strong></p>\n<ul>\n<li><code>my_df.columns.is_unique</code> evaluates to <code>True</code></li>\n<li>columns are already in order</li>\n</ul>\n<hr/>\n<pre><code>start = df.columns.get_loc(con_start())\nstop = df.columns.get_loc(con_stop())\n\ndf.iloc[:, start:stop + 1]\n</code></pre>\n<p><strong><em>option 2</em></strong><br/>\nuse <em><code>loc</code></em> with boolean slicing<br/>\n<strong>Assumptions:</strong></p>\n<ul>\n<li>column values are comparable</li>\n</ul>\n<hr/>\n<pre><code>start = con_start()\nstop = con_stop()\n\nc = df.columns.values\nm = (start &lt;= c) &amp; (stop &gt;= c)\n\ndf.loc[:, m]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Generate a list of colums to display:</p>\n<pre><code>cols = [x for x in my_df.columns if start &lt;= x &lt;= stop]\n</code></pre>\n<p>Use only these columns in your DataFrame:</p>\n<pre><code>my_df[cols]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/66435852/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2021-03-02 20:19:33Z\">3 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/66435852/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I am working on a data science regression problem with around 90,000 rows on train set and 8500 on test set. There are 9 categorical columns and no missing data. for this case, I am applied a catboostregressor which given me the pretty good R2(98.51) and MAE (3.77). Other nodels LGBM, XGBOOST performed under catboost.</p>\n<p>Now I would like to increase the R2 value and decrease the MAE for more accurate results. That's what the demand too.</p>\n<p>I have tuned many times by adding 'loss_function': ['MAE'], 'l2_leaf_reg':[3], 'random_strength': [4], 'bagging_temperature':[0.5] with different values but the performance is the same.</p>\n<p>Can anyone help me how to boost the R2 value by minimizing MAE and MSE ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Simple method -</strong><br/></p>\n<p>You can use Scikit-Learn's <code>GridSearchCV</code> to find the best hyperparameters for your <code>CatBoostRegressor</code> model. You can pass a dictionary of hyperparameters, and <code>GridSearchCV</code> will loop through all the hyperparameters and tell you which parameters are best. You can use it like this -</p>\n<pre><code>from sklearn.model_selection import GridSearchCV\n\nmodel = CatBoostRegressor()\nparameters = {'depth' : [6,8,10],\n              'learning_rate' : [0.01, 0.05, 0.1],\n              'iterations'    : [30, 50, 100]\n              }\n\ngrid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1)\ngrid.fit(X_train, y_train)\n</code></pre>\n<p><strong>Another method -</strong><br/><br/>\nNow-a-days, models are complex and have a lot of parameters to tune. People are using Bayesian Optimization techniques, like Optuna, to tune hyperparameters. You can use Optuna to tune <code>CatBoostClassifier</code> like this:</p>\n<pre><code>!pip install optuna\nimport catboost\nimport optuna\n\ndef objective(trial):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.3),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 15),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n    }\n    \n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_uniform(\"subsample\", 0.1, 1)\n\n    gbm = catboost.CatBoostClassifier(**param, iterations = 10000)\n\n    gbm.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = 0, early_stopping_rounds = 100)\n\n    preds = gbm.predict(X_val)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_val, pred_labels)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 200, show_progress_bar = True)\n</code></pre>\n<p>This method take a lot of time (1-2 hrs, maybe). This method is best to use when you have a lot of parameters to tune. Else, use Grid Search CV.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm very confused about these two methods which are:\nstack() and unstack()\nI know that I should use them in the case of multi-Indexes however, I need to know the following:</p>\n<p>1- I don't know where I should use stack or unstack<br/>\n2- why I should use them</p>\n<p>when I use \"pivot\" what I understand is that the pivot converts Dataframe to  be the unstack form, if that is correct so, I need to know why when I use the following line code it raises an error:</p>\n<pre><code>data.stack(level=1) # IndexError: Too many levels: Index has only 1 level, not 2\n</code></pre>\n<p>but when I do that following it runs:</p>\n<pre><code>data.unstack().stack(level=1)\n</code></pre>\n<p>sometimes, I see that stack has kwargs like so, level=-1 I don't know when I have to place \"-1\" and what does that mean</p>\n<p>I know that I misunderstand a lot of stuff but I'm very confused</p>\n<p>so, any help to understand these terms, please?</p>\n<p>thx in advance</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is an attempt at a canonical answer on the differences between <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> and <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.unstack.html\" rel=\"noreferrer\"><code>unstack</code></a>. For a complete guide on reshaping, pandas's official documentation on <a href=\"https://pandas.pydata.org/docs/user_guide/reshaping.html\" rel=\"noreferrer\">reshaping and pivot tables</a> is a must read.</p>\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> and <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.unstack.html\" rel=\"noreferrer\"><code>unstack</code></a> perform roughly the same operation, but they operate on different logical levels: <strong>columns</strong> and <strong>index levels</strong>, respectively.</p>\n<p>I will use this example dataframe as input:</p>\n<pre><code>df = pd.DataFrame({'col1': list('ABCABC'),\n                   'col2': list('aaabbb'),\n                   'col3': list('uvwxyz'),\n                  })\n</code></pre>\n<pre><code>  col1 col2 col3\n0    A    a    u\n1    B    a    v\n2    C    a    w\n3    A    b    x\n4    B    b    y\n5    C    b    z\n</code></pre>\n<h2>Using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> on columns</h2>\n<p><code>pandas.DataFrame.pivot</code> operates on columns</p>\n<p><em>NB. when the <code>index</code> argument if left unused, it will use the current index.</em></p>\n<pre><code>df.pivot(index='col1', columns='col2', values='col3')\n</code></pre>\n<pre><code>col2  a  b\ncol1      \nA     u  x\nB     v  y\nC     w  z\n</code></pre>\n<h2>Using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.unstack.html\" rel=\"noreferrer\"><code>unstack</code></a> on MultiIndexes</h2>\n<p>There are two use cases here whether the input is a Series or a DataFrame.</p>\n<h3><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.unstack.html\" rel=\"noreferrer\"><code>pandas.Series.unstack</code></a></h3>\n<p>We will generate first a Series with MultIndex from the initial DataFrame:</p>\n<pre><code>series = df.set_index(['col1', 'col2'])['col3']\n</code></pre>\n<pre><code>col1  col2\nA     a       u\nB     a       v\nC     a       w\nA     b       x\nB     b       y\nC     b       z\nName: col3, dtype: object\n</code></pre>\n<p>We see that the data is very similar to the original DataFrame, but <code>col1</code> and <code>col2</code> are now index levels, and the data itself is now one-dimensional (i.e., a Series)</p>\n<p>Now, we can apply <code>unstack</code> to pivot by default the right-most (last) index level as columns to generate a DataFrame. There are many ways to specify the index level to unstack so all these options are equivalent:</p>\n<pre><code>series.unstack()\n</code></pre>\n<pre><code>series.unstack('col2') # by level name\n</code></pre>\n<pre><code>series.unstack(1) # by level position from the left\n</code></pre>\n<pre><code>series.unstack(-1) # by level position from the end (-1 = last)\n</code></pre>\n<pre><code>col2  a  b\ncol1      \nA     u  x\nB     v  y\nC     w  z\n</code></pre>\n<p>This means that <code>df.pivot(index='col1', columns='col2', values='col3')</code> and <code>df.set_index(['col1', 'col2'])['col3'].unstack()</code> are logically equivalent.</p>\n<h3><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.unstack.html\" rel=\"noreferrer\"><code>pandas.DataFrame.unstack</code></a></h3>\n<p>The DataFrame version of <code>unstack</code> is very similar to the Series's one, with the exception that, as the data is already two-dimensional, it will create an extra level of index for the columns.</p>\n<pre><code>df.set_index(['col1', 'col2']).unstack(level='col2')\n</code></pre>\n<pre><code>     col3   \ncol2    a  b\ncol1        \nA       u  x\nB       v  y\nC       w  z\n</code></pre>\n<p>Here again, the same output can be obtained using <code>pivot</code>, by passing a list-encapsulated column name to <code>values</code>:</p>\n<pre><code>df.pivot(index='col1', columns='col2', values=['col3'])\n</code></pre>\n<pre><code>     col3   \ncol2    a  b\ncol1        \nA       u  x\nB       v  y\nC       w  z\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a date column called <strong>day</strong> such as <code>2019/07/22</code> if I want to create a custom field that translates that date to the actual day of week it is such as <strong>Sunday</strong> or <strong>Monday</strong> how is this possible? I cant seem to find a method that works for presto sql. </p>\n<p>Thanks for looking</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use the <a href=\"https://trino.io/docs/current/functions/datetime.html#format_datetime\" rel=\"noreferrer\"><code>format_datetime</code></a> function to extract the day of week from a date or timestamp:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>SELECT format_datetime(day, 'E')\nFROM (\n  VALUES DATE '2019-07-22'\n) t(day)\n</code></pre>\n<p>produces:</p>\n<pre><code> _col0\n-------\n Mon\n</code></pre>\n<p>If you want the full name of the day, use <code>format_datetime(day, 'EEEE')</code>:</p>\n<pre><code> _col0\n-------\n Monday\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was wondering if it is possible to groupby one column while counting the values of another column that fulfill a condition. Because my dataset is a bit weird, I created a similar one:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\nraw_data = {'name': ['John', 'Paul', 'George', 'Emily', 'Jamie'], \n            'nationality': ['USA', 'USA', 'France', 'France', 'UK'],     \n            'books': [0, 15, 0, 14, 40]}  \ndf = pd.DataFrame(raw_data, columns = ['name', 'nationality', 'books'])\n</code></pre>\n<p>Say, I want to groupby the nationality and count the number of people that don't have any books (books == 0) from that country.</p>\n<p>I would therefore expect something like the following as output:</p>\n<pre><code>nationality\nUSA      1\nFrance   1\nUK       0\n</code></pre>\n<p>I tried most variations of groupby, using filter, agg but don't seem to get anything that works.</p>\n<p>Thanks in advance,\nBBQuercus :)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>IIUC:</p>\n<pre><code>df.books.eq(0).astype(int).groupby(df.nationality).sum()\n\nnationality\nFrance    1\nUK        0\nUSA       1\nName: books, dtype: int64\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use:</p>\n<pre><code>df.groupby('nationality')['books'].apply(lambda x: x.eq(0).any().astype(int))\n</code></pre>\n<hr/>\n<pre><code>nationality\nFrance    1\nUK        0\nUSA       1\nName: books, dtype: int64\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>How to upload two large (5GB) each csv file in local system Jupyter Notebook using python pandas. Please suggest any configuration to handle big csv files for data analysis ?</p>\n<pre><code>Local System Configuration:\nOS: Windows 10\nRAM: 16 GB\nProcessor: Intel-Core-i7\n</code></pre>\n<p>Code:</p>\n<pre><code>dpath = 'p_flg_tmp1.csv'\npdf = pd.read_csv(dpath, sep=\"|\") \n\nError:\nMemoryError: Unable to allocate array\n</code></pre>\n<p>or</p>\n<pre><code>pd.read_csv(po_cust_data, sep=\"|\", low_memory=False)\n\nError:\nParserError: Error tokenizing data. C error: out of memory\n</code></pre>\n<p>How to handle two bigger csv file in local system for data analysis? please suggested better configuration if possible in local system using python pandas.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you do not need to process everything at once you can use chunks:</p>\n<pre><code>reader = pd.read_csv('tmp.sv', sep='|', chunksize=4000)   \nfor chunk in reader:\n     print(chunk)\n</code></pre>\n<p>see the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#iterating-through-files-chunk-by-chunk\" rel=\"nofollow noreferrer\">Documentation</a> of Pandas for further information.</p>\n<p>If you need to process everything at once and chunking really isnt an option you have only two options left</p>\n<ol>\n<li>Increase RAM of your system</li>\n<li>Switch to another data storage type</li>\n</ol>\n<p>A csv file takes an enormous amount of memory in RAM, see <a href=\"http://%20https://tablecruncher.com/blog/tutorial/work-with-large-files/\" rel=\"nofollow noreferrer\">this article</a> for more information even if it is for another software it gives a good idea about the problem:</p>\n<blockquote>\n<p>Memory Usage</p>\n<p>You can estimate the memory usage of your CSV file with this simple\n  formula:</p>\n<pre><code>memory = 25 * R * C + F \n</code></pre>\n<p>where R is the number of rows, C the number of columns and F the file size in bytes.</p>\n<p>One of my test files is 524 MB large, contains 10 columns in 4.4\n  million rows. Using the formula from above the RAM usage will be about\n  1.6 GB:</p>\n<pre><code>memory = 25 * 4,400,000 * 10 + 524,000,000 = 1,624,000,000 bytes\n</code></pre>\n<p>While this file is opened in Tablecruncher the Activity Monitor\n  reports 1.4 GB RAM used, so the formula represents a rather accurate\n  guess.</p>\n</blockquote>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use chunk to read data partially.</p>\n<pre><code>dpath = 'p_flg_tmp1.csv'\n\nfor pdf in pd.read_csv(dpath, sep=\"|\", chunksize=1000):\n    *do something here*\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Following up my previous question: <a href=\"https://stackoverflow.com/questions/56041988/sorting-datetime-objects-by-hour-to-a-pandas-dataframe-then-visualize-to-histog\">Sorting datetime objects by hour to a pandas dataframe then visualize to histogram</a></p>\n<p>I need to plot 3 bars for one X-axis value representing viewer counts. Now they show those under one minute and above. I need one showing the overall viewers. I have the Dataframe but I can't seem to make them look right. With just 2 bars I have no problem, it looks just like I would want it with two bars:\n<a href=\"https://i.sstatic.net/Ecvfn.png\" rel=\"noreferrer\"><img alt=\"I need to plot 3 bars for one X-axis value representing viewer counts.\" src=\"https://i.sstatic.net/Ecvfn.png\"/></a></p>\n<p>The relevant part of the code for this:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Time and date stamp variables\nallviews = int(df['time'].dt.hour.count())\ndate = str(df['date'][0].date())\nhours = df_hist_short.index.tolist()\nhours[:] = [str(x) + ':00' for x in hours]\n</code></pre>\n<p>The hours variable that I use to represent the X-axis may be problematic, since I convert it to string so I can make the hours look like <code>23:00</code> instead of just the pandas index output <code>23</code> etc. I have seen examples where people add or subtract values from the X to change the bars position.</p>\n<pre class=\"lang-py prettyprint-override\"><code>fig, ax = plt.subplots(figsize=(20, 5))\nshort_viewers = ax.bar(hours, df_hist_short['time'], width=-0.35, align='edge')\nlong_viewers = ax.bar(hours, df_hist_long['time'], width=0.35, align='edge')\n</code></pre>\n<p>Now I set the <code>align='edge'</code> and the two width values are absolutes and negatives. But I have no idea how to make it look right with 3 bars. I didn't find any positioning arguments for the bars. Also I have tried to work with the plt.hist() but I couldn't get the same output as with the plt.bar() function.</p>\n<p>So as a result I wish to have a 3rd bar on the graph shown above on the left side, a bit wider than the other two.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In pure matplotlib, instead of using the width parameter to position the bars as you've done, you can adjust the x-values for your plot:</p>\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Make some fake data:\nn_series = 3\nn_observations = 5\nx = np.arange(n_observations)\ndata = np.random.random((n_observations,n_series))\n\n\n# Plotting:\n\nfig, ax = plt.subplots(figsize=(20,5))\n\n# Determine bar widths\nwidth_cluster = 0.7\nwidth_bar = width_cluster/n_series\n\nfor n in range(n_series):\n    x_positions = x+(width_bar*n)-width_cluster/2\n    ax.bar(x_positions, data[:,n], width_bar, align='edge')\n</code></pre>\n<p><a href=\"https://i.sstatic.net/sval8.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/sval8.png\"/></a></p>\n<p>In your particular case, <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html\" rel=\"noreferrer\">seaborn</a> is probably a good option. You should (almost always) try keep your data in <a href=\"https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/\" rel=\"noreferrer\">long-form</a> so instead of three separate data frames for short, medium and long, it is much better practice to keep a single data frame and add a column that labels each row as short, medium or long. Use this new column as the <code>hue</code> parameter in Seaborn's <code>barplot</code></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>pandas</code> will do this alignment for you, if you make the bar plot in one step rather than two (or three). Consider this example (adapted from the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\" rel=\"noreferrer\">docs</a> to add a third bar for each animal).</p>\n<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\nspeed = [0.1, 17.5, 40, 48, 52, 69, 88]\nlifespan = [2, 8, 70, 1.5, 25, 12, 28]\nheight = [1, 5, 20, 3, 30, 6, 10]\nindex = ['snail', 'pig', 'elephant',\n         'rabbit', 'giraffe', 'coyote', 'horse']\ndf = pd.DataFrame({'speed': speed,\n                   'lifespan': lifespan,\n                   'height': height}, index=index)\nax = df.plot.bar(rot=0)\n\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/KT4kh.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/KT4kh.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Question about TensorFlow:</p>\n<p>I was looking at the video and model on the site, and it appeared to only have SGD as an algorithm for machine learning. I was wondering if other algorithms are also included in tensorflow, such as L-BFGS.</p>\n<p>Thank you for your responses.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>TensorFlow's jargon for the algorithms such as Stochastic Gradient Descent (SGD) is optimizer. Following are the optimizers supported by TensorFlow:</p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#GradientDescentOptimizer\" rel=\"nofollow noreferrer\">GradientDescentOptimizer</a></li>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#AdadeltaOptimizer\" rel=\"nofollow noreferrer\">AdadeltaOptimizer</a></li>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#AdagradOptimizer\" rel=\"nofollow noreferrer\">AdagradOptimizer</a></li>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#AdamOptimizer\" rel=\"nofollow noreferrer\">AdamOptimizer</a></li>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#FtrlOptimizer\" rel=\"nofollow noreferrer\">FtrlOptimizer</a></li>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#MomentumOptimizer\" rel=\"nofollow noreferrer\">MomentumOptimizer</a></li>\n<li><a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#RMSPropOptimizer\" rel=\"nofollow noreferrer\">RMSPropOptimizer</a></li>\n</ul>\n<p>You can also use the <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface\" rel=\"nofollow noreferrer\">TensorFlow SciPy Optimizer interface</a> which gives you access to optimizers like the L-BFGS.</p>\n<p>Further, <a href=\"https://www.tensorflow.org/api_guides/python/train\" rel=\"nofollow noreferrer\">here</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/opt\" rel=\"nofollow noreferrer\">here</a> are all the available TensorFlow optimizers you could use.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I get the following error when I train LightGBM model:</p>\n<pre><code># Train the model\nimport lightgbm as lgb\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_val = lgb.Dataset(x_test, y_test)\n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\n\nmodel = lgb.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)\n\ny_pred = model.predict(test_data)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you used cut or qcut functions for binning and did not encode later (one-hot encoding, label encoding ..). this may be the cause of the error. Try to use an encoding.</p>\n<p>I hope it works.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had what might be the same problem. </p>\n<p>Post the whole traceback to make sure.</p>\n<p>For me it was a problem serializing to JSON, which LightGBM does under the hood to save the booster for later use. </p>\n<p>Check your dataset for any date/datetime columns, or anything that remotely looks like a date, and either drop it or convert to something JSON can handle.</p>\n<p>Mine had all been converted to categorical dtype by some Pandas code I had poorly written, and I usually do the initial GBM run fairly fast-n-dirty to see what variables show up as important. LightGBM let me make the data binaries for training (i.e. it would have thrown an error if they were datetime or timedelta dtypes before letting me run anything). It would run the training just fine, report an AUC, then fail after the last training step when it was dumping the categoricals to JSON. It was maddening, with a cryptic traceback.</p>\n<p>Hope this helps.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset which contains gender as Male and Female. I have converted male to 1 and female to 0 using pandas functionality which has now data type int8. now I wanted to normalize columns such as weight and height. So what should be done with the gender column: should it be normalized or not. I am planning to use it in for a linear regression.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So I think you are mixing up normalization with standardization. </p>\n<p><strong>Normalization:</strong> </p>\n<p>rescales your data into a range of [0;1] </p>\n<p><strong>Standardization:</strong> </p>\n<p>rescales your data to have a mean of 0 and a standard deviation of 1. </p>\n<p><strong>Back to your question:</strong> </p>\n<p>For your gender column your points are already ranging between 0 and 1. Therefore your data is already \"normalized\". So your question should be if you can standarize your data and the answer is: yes you could, but it doesn't really make sense. This question was already discussed here: <a href=\"https://stats.stackexchange.com/questions/59392/should-you-ever-standardise-binary-variables\">Should you ever standardise binary variables?</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If I have data containing 5 categories (A,B,C,D,E) and a dataset of customers where each customer can belong to one, many or none of the categories. How can I take a data set like this:</p>\n<pre><code>id, categories\n1 , [A,C]\n2 , [B]\n3 , []\n4 , [D,E]\n</code></pre>\n<p>and transform the categories column to one hot encoded vectors, like this</p>\n<pre><code>id, categories, encoded\n1 , [A,C]     , [1,0,1,0,0]\n2 , [B]       , [0,1,0,0,0]\n3 , []        , [0,0,0,0,0]\n4 , [D,E]     , [0,0,0,1,1]\n</code></pre>\n<p>Has anyone found a simple way to do this in spark? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To achieve your desired output you can extend <a href=\"https://stackoverflow.com/users/4351210/stephen-carman\">Stephen Carman</a> answer with Spark's UDF (<em>user-defined functions</em>):</p>\n<pre><code>// Prepare training documents from a list of (id, text, label) tuples.\nval data = spark.createDataFrame(Seq(\n  (0L, Seq(\"A\", \"B\")),\n  (1L, Seq(\"B\")),\n  (2L, Seq.empty),\n  (3L, Seq(\"D\", \"E\"))\n)).toDF(\"id\", \"categories\")\n\n// Get distinct tags array\nval tags = data\n  .flatMap(r ⇒ r.getAs[Seq[String]](\"categories\"))\n  .distinct()\n  .collect()\n  .sortWith(_ &lt; _)\n\nval cvmData = new CountVectorizerModel(tags)\n  .setInputCol(\"categories\")\n  .setOutputCol(\"sparseFeatures\")\n  .transform(data)\n\nval asDense = udf((v: Vector) ⇒ v.toDense)\n\ncvmData\n  .withColumn(\"features\", asDense($\"sparseFeatures\"))\n  .select(\"id\", \"categories\", \"features\")\n  .show()\n</code></pre>\n<p>Which will give you desired output</p>\n<pre><code>+---+----------+-----------------+\n| id|categories|         features|\n+---+----------+-----------------+\n|  0|    [A, B]|[1.0,1.0,0.0,0.0]|\n|  1|       [B]|[0.0,1.0,0.0,0.0]|\n|  2|        []|[0.0,0.0,0.0,0.0]|\n|  3|    [D, E]|[0.0,0.0,1.0,1.0]|\n+---+----------+-----------------+\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Something very easy to do, which is somewhat the same is using a CountVectorizerModel</p>\n<pre><code>val df = spark.createDataFrame(Seq(\n  (1, Seq(\"A\",\"C\")),\n  (2, Seq(\"B\")),\n  (3, Seq()),\n  (4, Seq(\"D\",\"E\")))\n).toDF(\"id\", \"category\")\n\nval cvm = new CountVectorizerModel(Array(\"A\",\"B\",\"C\",\"D\",\"E\"))\n  .setInputCol(\"category\")\n  .setOutputCol(\"features\")\n\ncvm.transform(df).show()\n\n/*\n+---+--------+-------------------+\n| id|category|           features|\n+---+--------+-------------------+\n|  1|  [A, C]|(5,[0,2],[1.0,1.0])|\n|  2|     [B]|      (5,[1],[1.0])|\n|  3|      []|          (5,[],[])|\n|  4|  [D, E]|(5,[3,4],[1.0,1.0])|\n+---+--------+-------------------+\n*/\n</code></pre>\n<p>This isn't exactly like what you had wanted, but the feature vector will tell you what Categories exist in your data. For instance in row 1, [0,2] corresponds to the first and 3rd element of the dictionary, or \"A\" and \"C\" as written there.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am having trouble when I try to download the dependencies needed to run <code>graphlab</code>. I do <code>import graphlab</code> I get the following:</p>\n<pre><code>ACTION REQUIRED: Dependencies libstdc++-6.dll and libgcc_s_seh-1.dll not found.\n\n1. Ensure user account has write permission to C:\\Users\\DANISUAR\\AppData\\Local\\Continuum\\Miniconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\n2. Run graphlab.get_dependencies() to download and install them.\n3. Restart Python and import graphlab again.\n\nBy running the above function, you agree to the following licenses.\n\n* libstdc++: https://gcc.gnu.org/onlinedocs/libstdc++/manual/license.html\n* xz: http://git.tukaani.org/?p=xz.git;a=blob;f=COPYING\n</code></pre>\n<p>So I try to run <code>graphlab.get_dependencies()</code> and I get the following error:</p>\n<pre><code>In [2]: gl.get_dependencies()\n\nBy running this function, you agree to the following licenses.\n\n* libstdc++: https://gcc.gnu.org/onlinedocs/libstdc++/manual/license.html\n* xz: http://git.tukaani.org/?p=xz.git;a=blob;f=COPYING\n\nDownloading xz.\nExtracting xz.\nDownloading gcc-libs.\nExtracting gcc-libs.\nxz: c:\\users\\danisuar\\appdata\\local\\temp\\tmpcdpyzp.xz: File format not recognized\n---------------------------------------------------------------------------\nCalledProcessError                        Traceback (most recent call last)\n&lt;ipython-input-2-5349b2d86a08&gt; in &lt;module&gt;()\n----&gt; 1 gl.get_dependencies()\n\nC:\\Users\\DANISUAR\\AppData\\Local\\Continuum\\Miniconda2\\envs\\gl-env\\lib\\site-packag\nes\\graphlab\\dependencies.pyc in get_dependencies()\n 45     prev_cwd = os.getcwd()\n 46     os.chdir(dllarchive_dir)\n---&gt; 47     subprocess.check_call([xz, '-d', dllarchive_file])\n 48     dllarchive_tar = tarfile.open(os.path.splitext(dllarchive_file)[0])\n 49     dllarchive_tar.extractall()\n\nC:\\Users\\DANISUAR\\AppData\\Local\\Continuum\\Miniconda2\\envs\\gl-env\\lib\\subprocess.pyc in check_call(*popenargs, **kwargs)\n539         if cmd is None:\n540             cmd = popenargs[0]\n--&gt; 541         raise CalledProcessError(retcode, cmd)\n542     return 0\n543\n\nCalledProcessError: Command '['c:\\\\users\\\\danisuar\\\\appdata\\\\local\\\\temp\\\\tmpf1habd\\\\bin_x86-64\\\\xz.exe', '-d', 'c:\\\\users\\\\danisuar\\\\appdata\\\\local\\\\temp\\\\tmpcdpyzp.xz']' returned non-zero exit status 1\n</code></pre>\n<p>I am using an Anaconda environment with Python 2.7 and Windows 7.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The first step is to install all the graph packages using the procedures listed in <a href=\"https://turi.com/download/install-graphlab-create-command-line.html\" rel=\"noreferrer\"><strong>this link</strong></a> using PIP installer.\nVerify the successful installation of GraphLab by typing:</p>\n<blockquote>\n<p>import graphlab </p>\n</blockquote>\n<p>The following errors may appear as given in this image:</p>\n<p><a href=\"https://i.sstatic.net/haGAa.png\" rel=\"noreferrer\"><img alt=\"Image\" src=\"https://i.sstatic.net/haGAa.png\"/></a></p>\n<p>Then, you can run <code>graphlab.get_dependencies()</code> in Python's terminal.</p>\n<p>Verify the installation again in Python's terminal using:</p>\n<blockquote>\n<p>import graphlab</p>\n</blockquote>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am using the below command while installing surprise package. I have got error messages while installing and I am not able to understand. I need help to install this package successfully.\n<em><strong>pip install scikit-surprise</strong></em></p>\n<p>one of the error code in the last says that Microsoft visual C++ 14 or greater is required but I have 14 version installed so the requirement should be satisfied.\n<a href=\"https://i.sstatic.net/Mx6Ew.png\" rel=\"noreferrer\">screen shot from control panel</a></p>\n<pre><code>(base) C:\\Users\\S Vishal&gt;pip install scikit-surprise\nCollecting scikit-surprise\n  Using cached scikit-surprise-1.1.1.tar.gz (11.8 MB)\nRequirement already satisfied: joblib&gt;=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-surprise) (0.17.0)\nRequirement already satisfied: numpy&gt;=1.11.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.19.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.5.2)\nRequirement already satisfied: six&gt;=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.15.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: 'C:\\ProgramData\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\S Vishal\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9wlw55w2\\\\scikit-surprise\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\S Vishal\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9wlw55w2\\\\scikit-surprise\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\S Vishal\\AppData\\Local\\Temp\\pip-wheel-8n8d7erc'\n       cwd: C:\\Users\\S Vishal\\AppData\\Local\\Temp\\pip-install-9wlw55w2\\scikit-surprise\\\n  Complete output (49 lines):\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\\lib.win-amd64-3.8\n  creating build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\accuracy.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\builtin_datasets.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\dataset.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\dump.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\reader.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\trainset.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\utils.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\__init__.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\__main__.py -&gt; build\\lib.win-amd64-3.8\\surprise\n  creating build\\lib.win-amd64-3.8\\surprise\\model_selection\n  copying surprise\\model_selection\\search.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n  copying surprise\\model_selection\\split.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n  copying surprise\\model_selection\\validation.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n  copying surprise\\model_selection\\__init__.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n  creating build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\algo_base.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\baseline_only.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\knns.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\predictions.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\random_pred.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\__init__.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  running egg_info\n  writing scikit_surprise.egg-info\\PKG-INFO\n  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n  writing requirements to scikit_surprise.egg-info\\requires.txt\n  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n  copying surprise\\similarities.c -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\similarities.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\n  copying surprise\\prediction_algorithms\\co_clustering.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\matrix_factorization.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\optimize_baselines.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\slope_one.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\co_clustering.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  copying surprise\\prediction_algorithms\\slope_one.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: scikit-surprise\n    Running setup.py install for scikit-surprise ... error\n    ERROR: Command errored out with exit status 1:\n     command: 'C:\\ProgramData\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\S Vishal\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9wlw55w2\\\\scikit-surprise\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\S Vishal\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9wlw55w2\\\\scikit-surprise\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\S Vishal\\AppData\\Local\\Temp\\pip-record-p24eovcb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\ProgramData\\Anaconda3\\Include\\scikit-surprise'\n         cwd: C:\\Users\\S Vishal\\AppData\\Local\\Temp\\pip-install-9wlw55w2\\scikit-surprise\\\n    Complete output (49 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\\lib.win-amd64-3.8\n    creating build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\accuracy.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\builtin_datasets.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\dataset.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\dump.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\reader.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\trainset.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\utils.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\__init__.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\__main__.py -&gt; build\\lib.win-amd64-3.8\\surprise\n    creating build\\lib.win-amd64-3.8\\surprise\\model_selection\n    copying surprise\\model_selection\\search.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n    copying surprise\\model_selection\\split.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n    copying surprise\\model_selection\\validation.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n    copying surprise\\model_selection\\__init__.py -&gt; build\\lib.win-amd64-3.8\\surprise\\model_selection\n    creating build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\algo_base.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\baseline_only.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\knns.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\predictions.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\random_pred.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\__init__.py -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    running egg_info\n    writing scikit_surprise.egg-info\\PKG-INFO\n    writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\\entry_points.txt\n    writing requirements to scikit_surprise.egg-info\\requires.txt\n    writing top-level names to scikit_surprise.egg-info\\top_level.txt\n    reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n    copying surprise\\similarities.c -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\similarities.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\n    copying surprise\\prediction_algorithms\\co_clustering.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\matrix_factorization.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\optimize_baselines.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\slope_one.c -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\co_clustering.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\matrix_factorization.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\optimize_baselines.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    copying surprise\\prediction_algorithms\\slope_one.pyx -&gt; build\\lib.win-amd64-3.8\\surprise\\prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n    ----------------------------------------\nERROR: Command errored out with exit status 1: 'C:\\ProgramData\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\S Vishal\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9wlw55w2\\\\scikit-surprise\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\S Vishal\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9wlw55w2\\\\scikit-surprise\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\S Vishal\\AppData\\Local\\Temp\\pip-record-p24eovcb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\ProgramData\\Anaconda3\\Include\\scikit-surprise' Check the logs for full command output.\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Have you tried installing with Anaconda?</p>\n<p>Launch your Anaconda command prompt and run:</p>\n<pre><code>conda install -c conda-forge scikit-surprise\n</code></pre>\n<p>I hope this sorts out your issue.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't understand why it works with different scenarios, but not with this one.\nBasically, some gentleman helped me out <a href=\"https://stackoverflow.com/questions/47745106/for-loop-in-function-while-true-difficulty-to-understand\">HERE</a> with improving my code to scrape weather, which works perfectly. I then tried to do the same to scrape an ETH value which is in a span tag <code>&lt;span class=\"text-large2\" data-currency-value=\"\"&gt;$196.01&lt;/span&gt;</code>. So, I followed the same technique in the code, replaced the fields, and was hoping for it to work.</p>\n<p>The code is here:</p>\n<pre><code>import requests\nfrom BeautifulSoup import BeautifulSoup\nimport time\n\nurl = 'https://coinmarketcap.com/currencies/litecoin/'\n\ndef ltc():\n    while (True):\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content)\n        price_now = int(soup.find(\"div\", {\"class\": \"col-xs-6 col-sm-8 col-md-4 text-left\"}).find(\n        \"span\", {\"class\": \"text-large2\"}).getText())\n        print(u\"LTC price is: {}{}\".format(price_now))\n        # if less than 150\n        if 150 &gt; price_now:\n            print('Price is Low')\n        # if more than 200\n        elif 200 &lt; price_now:\n            print('Price is high')\n\nif __name__ == \"__main__\":\n    ltc()\n</code></pre>\n<p>The output looks like this:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test2.py\", line 24, in &lt;module&gt;\n    ltc()\n  File \"test2.py\", line 13, in ltc\n    \"span\", {\"class\": \"text-large2\"}).getText())\nValueError: invalid literal for int() with base 10: '196.01'\n</code></pre>\n<p>Then, I finally tried it this way; but from here I get false positives, but no errors. It prints whatever it wants </p>\n<pre><code>import requests\nfrom bs4 import BeautifulSoup\nimport time\n\nurl = 'https://coinmarketcap.com/currencies/litecoin/'\n\ndef liteCoin():\n    while (True):\n        response = requests.get(url)\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n        value = soup.find('span', {'class': 'text-large2'})\n        print(''.join(value.stripped_strings))\n        if 150 &gt; value:         # if less than 150\n            print('Price is Low!')\n        elif 200 &lt; value:       # if more than 200\n            print('Price is High')\n        else:\n            print('N/A')\n        time.sleep(5)\n\nif __name__ == \"__main__\":\n    liteCoin()\n</code></pre>\n<p>Would the problem be that the value of the ETH has a <code>$</code> sign inside the <code>span tag</code>? And, that way the program doesn't know what to do with string?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First, let's simplify your sample program:</p>\n<pre><code>&gt;&gt;&gt; int('196.01')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: invalid literal for int() with base 10: '196.01'\n</code></pre>\n<p>One cannot convert the string <code>'196.01'</code> to an integer number.</p>\n<p>Try this:</p>\n<pre><code>&gt;&gt;&gt; int(float('196.01'))\n196\n</code></pre>\n<p>Moving from the simple back to the complex, we can do this:</p>\n<pre><code>#UNTESTED\nprice_now = int(float(soup.find(\"div\", {\"class\": \"col-xs-6 col-sm-8 col-md-4 text-left\"}).find(\n    \"span\", {\"class\": \"text-large2\"}).getText()))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to understand types in Python you are getting a float not an int and you need to cast the float to a string to print it. So there are two changes needed.</p>\n<pre><code>    price_now = float(soup.find(\"div\", {\"class\": \"col-xs-6 col-sm-8 col-md-4 text-left\"}).find(\"span\", {\"class\": \"text-large2\"}).getText())\n    print(u\"LTC price is: {}\".format(str(price_now)))\n</code></pre>\n<p>Outputs:</p>\n<pre><code>LTC price is: 195.44\nLTC price is: 195.44\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a machine learning problem and I don't know if there is a theoretical solution to it.</p>\n<p>I have <strong>labeled</strong> data (let's call it dataset <strong>D1</strong>) to build a random forest classification model and it performs well.</p>\n<p>Now my main interest is to apply this model on another dataset <strong>D2</strong> which has zero labels, meaning I cannot use it for training. The only way to measure performance on <strong>D2</strong> is to check the proportions of classes predicted from it.</p>\n<p>Problem: <strong>D2</strong> is skewed compared to <strong>D1</strong> (features don't have the same mean or fit the same distribution). Because of this, the model applied to <strong>D2</strong> gives heavily skewed results towards one class. I know this is normal because the majority of <strong>D2</strong> is similar to a small subset of <strong>D1</strong>.</p>\n<p>But is there any way to correct that skewness? I know from the nature of my problem the proportions of classes predicted should be less biased. I've tried normalization but it doesn't really help.</p>\n<p>I feel I'm not thinking straight :3</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Intressting question. My answer on this has three parts.</p>\n<p><strong>Disclaimer:</strong> There is no free lunch. Hence, you can never by sure without checking the performance on the real test set labels. Worst case you have a concept drift in your problem which makes it impossible to predict your target class. However, there are solutions which can provide pretty good results</p>\n<p><strong>For notation:</strong></p>\n<p>The features are denoted by <code>X</code> the target variable by <code>Y</code> and the classifier learned by <code>f(X) |-&gt; Y</code>. The distribution of <code>X</code> in <code>D1</code> by <code>P(X|D1)</code> (abusing notation a bit)</p>\n<p><strong>Class distribution in Testset</strong></p>\n<p>You \"postulated that one could use the distribution in the predicted variables (\"check the proportions of classes predicted from it.\"). This however can merely be an indication. I'm building classifiers in industry to predict that a machine will fail (predictive maintenance). There are a lot of engineers trying to make my input data skew, this is making the machines producing the data more reliable. However, this is not a problem, as one class basically disappears. However, the classifiers are still valid.</p>\n<p>There is a very simple way on the question \"how to fix\" the distribution in target labels on the test set. The idea would basically be to classify all test instances according to the predicted labels and sample (with replacement) the data points in accordance to the desired target variable distribution. You could then try to check the distribution on the features <code>X</code> but this wouldn't tell you too much. </p>\n<p>Can the skewness be a problem? Indeed it can as a Classifier typically tries to minimise the <code>accuracy</code> of <code>F1</code> measure or some other statistical property. If you would know in advance the distribution in the <code>D2</code> you could provide a cost function which minimises the costs under this distribution. These costs can be used to resample the training data as mentioned in the other answer, however, some learning algorithms also have more elaborate techniques to incorporate this information.</p>\n<p><strong>Outlier Detection</strong></p>\n<p>One question is whether you can detect that something has changed in the inputs <code>X</code>. This is pretty important as this can indicate that you have had the wrong data. You can apply fairly simple tests like for example the mean and distribution in all dimensions. However, this ignores dependencies between the variables. </p>\n<p>For the following two illustration im using the iris dataset\n<a href=\"https://i.sstatic.net/ENZq7.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ENZq7.png\"/></a></p>\n<p>Two techniques jump to my mind which allow you to detect that something in the data has changed. The first technique relies on the PCA transformation. Only for numerical but there are similar ideas for categorial features. PCA allows you to transform your input data into a lower dimensional space. this is \n<code>PCA(X,t)=PCA([X1,...,Xn],t)=[Cond1,...,Condm]=Cond</code> with a projection <code>t</code>\nWhere typically with <code>n&lt;&lt;m</code> this transformation is still reversible such that <code>PCA^1(Cond,t) = X'</code> and the Error <code>MSE(X,X')</code> is small. To detect a problem you can monitor this error and once it increases you can say you mistrust your predictions. </p>\n<p>If I build a PCA on all data from <code>versicolor</code> and <code>virginica</code> and plot the error in reconstruction two dimension (PCA on all iris dimensions) I get</p>\n<p><a href=\"https://i.sstatic.net/PjwY5.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/PjwY5.png\"/></a></p>\n<p>however if versicolor is the new data the results are less convincing. </p>\n<p><a href=\"https://i.sstatic.net/K239P.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/K239P.png\"/></a></p>\n<p>However, a PCA (or smth. similar) is done for numerical data anyhow, hence, it can give good indication without much overhead.</p>\n<p>A second technique I'm aware of is based on so called One class Support Vector machines. Where a normal support vector machine would build a classifier which tries to separat two target classes <code>Y</code>. A one class support vector machine tries to separate seen from unseen data. Using this techniques is fairly attractive if you use a Support Vector Machine for classification. You would basically get two classifications. The first one says the target data and the second one whether similar data has been seen before.</p>\n<p>If I build a one-class classifier on <code>setosa</code> and <code>virginca</code> and color by novelty I get the following graph:</p>\n<p><a href=\"https://i.sstatic.net/CUmqD.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/CUmqD.png\"/></a></p>\n<p>As can be seen Data from <code>versicolor</code> appears to be quit suspicious. In that case it is a new class. However, if we would assume these are instances of virginica they are drifting dangerously close to the hyperplane. </p>\n<p><strong>Semi-supervised learning and Transductive</strong></p>\n<p>To solve your underlying problem. The idea of Transductive Learning, a special case of semi supervised learning might be intressting. In Semi supervise learning the training set consists of two parts. The labeled data and the unlabelled data. Semi-sup-l uses all this data to build the classifier. Transductive Learning is a special case where the unlabelled data is your test data <code>D2</code>. The idea was given by Vapnik as \"don't try to solve a more complicated problem [building a classifier for all possible data] when you want to solve a simpler problem [predicting labels for <code>D2</code>]\"</p>\n<p><strong>APENDIX</strong></p>\n<p>RCODE for plots</p>\n<pre><code>ggplot(iris)+aes(x=Petal.Width,y=Petal.Length,color=Species)+geom_point()+stat_ellipse()\nlibrary(e1071)\niris[iris$Species %in% c(\"virginica\",\"setosa\"),]\n\nocl &lt;- svm(iris[iris$Species %in% c(\"virginica\",\"setosa\"),3:4],type=\"one-classification\")\ncoloring &lt;- predict(ocl,iris[,3:4],decision.values=TRUE)\n\nggplot(iris)+aes(x=Petal.Width,y=Petal.Length,color=coloring)+geom_point()+stat_ellipse()\nggplot(iris)+aes(x=Petal.Width,y=Petal.Length)+geom_point(color=rgb(red=0.8+0.1*attr(coloring,\"decision.values\"),green=rep(0,150),blue=1-(0.8+0.1*attr(coloring,\"decision.values\"))))\n\npca &lt;- prcomp(iris[,3:4])\n\n#pca &lt;- prcomp(iris[iris$Species %in% c(\"virginica\",\"setosa\"),1:4], retx = TRUE,  scale = TRUE)\npca &lt;- prcomp(iris[iris$Species %in% c(\"virginica\",\"setosa\"),1:4], retx = TRUE,  scale = TRUE,tol=0.2)\n  pca &lt;- prcomp(iris[iris$Species %in% c(\"virginica\",\"versicolor\"),1:4], retx = TRUE,  scale = TRUE,tol=0.4)\n  predicted &lt;-predict(pca,iris[,1:4])\n  inverted &lt;- t(t(predicted %*% t(pca$rotation)) * pca$scale + pca$center)\n  ggplot(inverted[,3:4]-iris[,3:4])+aes(x=Petal.Width,y=Petal.Length,color=iris$\n                                    Species)+geom_point()+stat_ellipse()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There could be a number of factors that could lead to this skewed outcome:</p>\n<p>You seem to indicate that D2 IS skewed by comparison to D1, and so the heavily skewed results could possibly be an expected outcome (Perhaps the D2 Dataset is heavily focused on a regional part of the problem space where one class is dominant).  Depending on the nature of the data, this could be a valid outcome.</p>\n<p>Perhaps D1 is overtrained on a particular class.  You could try training on fewer cases in the class to encourage classification to one of the other classes to determine the outcome.  I don't know how many training or testing cases that you have, but if it is large and there are more of that classes labels in the training data than the others, perhaps this could be leading to overclassification.</p>\n<p>Perhaps you could also manipulate the training data to be closer to the means of D2 to see what impact it would have on classification.  I've never tried this before though.</p>\n<p>I hope this helps in some way.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If the dataset contains features some of which are Categorical Variables and some of the others are continuous variable Decision Tree is better than Linear Regression,since Trees can accurately divide the data based on Categorical Variables. Is there any situation where Linear regression outperforms Random Forest?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>There for sure have to be situations where Linear Regression outperforms Random Forests, but I think the more important thing to consider is the complexity of the model.</p>\n<p>Linear Models have very few parameters, Random Forests a lot more. That means that Random Forests will overfit more easily than a Linear Regression.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Key advantages of linear models over tree-based ones are:</p>\n<ul>\n<li>they can extrapolate (e.g. if labels are between 1-5 in train set, tree based model will never predict 10, but linear will)</li>\n<li>could be used for anomaly detection because of extrapolation</li>\n<li>interpretability (yes, tree based models have feature importance, but it's only a proxy, weights in linear model are better)</li>\n<li>need less data to get good results</li>\n<li>have strong online learning implementations (Vowpal Wabbit), which is crucial to work with giant datasets with a lot of features (e.g. texts)</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose I have dataframe df1 which includes two columns - A &amp; B. Value of A represents the lower range and value of B represents the upper range.</p>\n<pre><code>  A     B\n10.5  20.5\n30.5  40.5\n50.5  60.5\n</code></pre>\n<p>I've another dataframe which includes two columns - C &amp; D containing a different range of numbers.</p>\n<pre><code>  C     D\n12.34  15.90\n13.68  19.13\n33.5   35.60\n35.12  38.76\n50.6   59.1\n</code></pre>\n<p>Now I want to list all the pairs from df2 that fall under the groups (between the lower and upper range) in the df1. </p>\n<p>Final output should be like this - </p>\n<pre><code>     Key                Values\n(10.5, 20.5)  [(12.34, 15.90), (13.68, 19.13)]\n(30.5, 40.5)  [(33.5, 35.60), (35.12, 38.76)]\n(50.5, 60.5)  [(50.6, 59.1)]\n</code></pre>\n<p>The solution should be efficient as I have 5000 groups of range and 85000 numbers from different range.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It is not blazing fast (~ 30 secs) on my computer) but could easily be accelerated with the <code>multiprocessing</code> package if you have multiple cores.</p>\n<p>Generating data : </p>\n<pre><code>def get_fake(n):\n    df = pd.DataFrame(np.random.rand(n * 2).reshape(-1, 2))\n    df.loc[:, 1] += 1\n    return df\n\ndf1 = get_fake(200)\ndf2 = get_fake(90000)\n</code></pre>\n<p>Then for the processing part :</p>\n<pre><code>from collections import defaultdict\nresult = defaultdict(list)\nfor index, start, stop in df1.itertuples():\n    subdf = df2[(start &lt; df2.iloc[:, 0]) &amp; (df2.iloc[:, 1] &lt; stop)]\n    result[(start, stop)] += subdf.values.tolist()\n</code></pre>\n<p>Result is a dict but could easily be converted to a Series if necessary.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It will be easy if you use <code>interval index</code> i.e </p>\n<pre><code>idx = pd.IntervalIndex.from_arrays(df['A'],df['B'])\nkeys = df.values.tolist()\nvalues = df2.groupby(df.loc[idx.get_indexer(df2['C'])].index).apply(lambda x : x.values)\n\nnew_df = pd.DataFrame({'key': keys , 'value': values})\n\n          key                            value\n0  [10.5, 20.5]  [[12.34, 15.9], [13.68, 19.13]]\n1  [30.5, 40.5]   [[33.5, 35.6], [35.12, 38.76]]\n2  [50.5, 60.5]                   [[50.6, 59.1]]\n</code></pre>\n<p>Accessing data based on interval index will give you the keys so you can groupby and aggregate i.e </p>\n<pre><code>df.loc[idx.get_indexer(df2['C'])]\n     A     B\n0  10.5  20.5\n0  10.5  20.5\n1  30.5  40.5\n1  30.5  40.5\n2  50.5  60.5\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>one solution is to use <code>apply</code> such as:</p>\n<pre><code># first create your output DF with the keys from your df with A and B\ndf = pd.DataFrame({'Key':[(a,b) for a,b in df1.itertuples(index=False)]})\n# define a function to find the range in df2 within the range from the Keys column\ndef find_range( key, df_2):\n    mask = (key[0] &lt;= df_2['C']) &amp; (key[1] &gt;= df_2['D'])\n    return [(c,d) for c,d in df_2[mask].itertuples(index=False)]\n#now create the column Values with apply\ndf['Values'] = df['Key'].apply(find_range, args=(df2,))\n# output\n            Key                           Values\n0  (10.5, 20.5)  [(12.34, 15.9), (13.68, 19.13)]\n1  (30.5, 40.5)                   [(33.5, 35.6)]\n</code></pre>\n<p>Note: I assume in your data the column C is always lower than D, if not, you have to change the mask in the function to check if both C and D are within key[0] and key[1]. Also, I did not have all your input, so Values for the row number 1 is different that what you show, but just an input difference.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to use <code>StratifiedKFold</code> to create train/test/val splits for use in a non-sklearn machine learning work flow. So, the DataFrame needs to be split and then stay that way.</p>\n<p>I'm trying to do it like the following, using <code>.values</code> because I'm passing pandas DataFrames:</p>\n<pre><code>skf = StratifiedKFold(n_splits=3, shuffle=False)\nskf.get_n_splits(X, y)\n\nfor train_index, test_index, valid_index in skf.split(X.values, y.values):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index,  \"VALID:\", valid_index)\n    X_train, X_test, X_valid = X.values[train_index], X.values[test_index], X.values[valid_index]\n    y_train, y_test, y_valid = y.values[train_index], y.values[test_index], y.values[valid_index]\n</code></pre>\n<p>This fails with: </p>\n<pre class=\"lang-none prettyprint-override\"><code>ValueError: not enough values to unpack (expected 3, got 2).\n</code></pre>\n<p>I read through all of the <code>sklearn</code> docs and ran the example code, but did not gain a better understanding of how to use stratified k fold splits outside of a <code>sklearn</code> cross-validation scenario. </p>\n<p>EDIT:</p>\n<p>I also tried like this:</p>\n<pre><code># Create train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n\n# Create validation split from train split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.05)\n</code></pre>\n<p>Which seems to work, although I imagine I'm messing with the stratification by doing so.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm not exactly sure if this question is about KFold or just stratified splits, but I wrote this quick wrapper for StratifiedKFold with a cross validation set.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.model_selection import StratifiedKFold, train_test_split\n\nclass StratifiedKFold3(StratifiedKFold):\n\n    def split(self, X, y, groups=None):\n        s = super().split(X, y, groups)\n        for train_indxs, test_indxs in s:\n            y_train = y[train_indxs]\n            train_indxs, cv_indxs = train_test_split(train_indxs,stratify=y_train, test_size=(1 / (self.n_splits - 1)))\n            yield train_indxs, cv_indxs, test_indxs\n</code></pre>\n<p>It can be used like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>X = np.random.rand(100)\ny = np.random.choice([0,1],100)\ng = KFold3(10).split(X,y)\ntrain, cv, test = next(g)\ntrain.shape, cv.shape, test.shape\n&gt;&gt; ((80,), (10,), (10,))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>StratifiedKFold can only be used to split your dataset into two parts per fold. You are getting an error because the <code>split()</code> method will only yield a tuple of train_index and test_index (see <a href=\"https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/model_selection/_split.py#L94\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/model_selection/_split.py#L94</a>).</p>\n<p>For this use case you should first split your data into validation and rest, and then split the rest again into test and train like such:</p>\n<pre><code>X_rest, X_val, y_rest, y_val = train_test_split(X, y, test_size=0.2, train_size=0.8, stratify=y)\nX_train, X_test, y_train, y_test = train_test_split(X_rest, y_rest, test_size=0.25, train_size=0.75, stratify=y_rest)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm attempting to do a grid search to optimize my model but it's taking far too long to execute. My total dataset is only about 15,000 observations with about 30-40 variables. I was successfully able to run a random forest through the gridsearch which took about an hour and a half but now that I've switched to SVC it's already ran for over 9 hours and it's still not complete. Below is a sample of my code for the cross validation:</p>\n<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.svm import SVC\n\nSVM_Classifier= SVC(random_state=7)\n\n\n\nparam_grid = {'C': [0.1, 1, 10, 100],\n              'gamma': [1,0.1,0.01,0.001],\n              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n              'degree' : [0, 1, 2, 3, 4, 5, 6]}\n\ngrid_obj = GridSearchCV(SVM_Classifier,\n                        \n                        return_train_score=True,\n                        param_grid=param_grid,\n                        scoring='roc_auc',\n                        cv=3,\n                       n_jobs = -1)\n\ngrid_fit = grid_obj.fit(X_train, y_train)\nSVMC_opt = grid_fit.best_estimator_\n\nprint('='*20)\nprint(\"best params: \" + str(grid_obj.best_estimator_))\nprint(\"best params: \" + str(grid_obj.best_params_))\nprint('best score:', grid_obj.best_score_)\nprint('='*20)\n\n</code></pre>\n<p>I have already reduced the cross validation from 10 to 3, and I'm using n_jobs=-1 so I'm engaging all of my cores. Is there anything else I'm missing that I can do here to speed up the process?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Unfortunately, SVC's fit algorithm is O(n^2) at best, so it indeed is extremely slow. Even the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\" rel=\"nofollow noreferrer\">documentation</a> suggests to use LinearSVC above ~10k samples and you are right in that ballpark.</p>\n<p>Maybe try to increase the kernel <code>cache_size</code>. I would suggest timing a single SVC fit with different cache sizes to see whether you can gain something.</p>\n<p><strong>EDIT:</strong> by the way, you are needlessly computing a lot of SVC fits with different <code>degree</code> parameter values, where that will be ignored (all the kernels but <code>poly</code>). I suggest splitting the runs for <code>poly</code> and the other kernels, you will save a lot of time.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>While exploring LinearSVC might be a good choice (and you should clean up the parameter combinations as noted in the other answer), you could also use a GPU accelerated SVC estimator in RAPIDS <a href=\"https://github.com/rapidsai/cuml\" rel=\"nofollow noreferrer\">cuML</a> on a GPU-enabled cloud instance of your choice (or locally if you have an NVIDIA GPU). This estimator can be dropped directly into your <code>GridSearchCV</code> function if you use the default <code>n_jobs=1</code>. (Disclaimer: I work on this project).</p>\n<p>For example, I ran the following on my local machine [0]:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport cuml\nfrom sklearn.svm import SVC\n\nX, y = sklearn.datasets.make_classification(n_samples=15000, n_features=30)\n%timeit _ = SVC().fit(X, y).predict(X)\n%timeit _ = cuml.svm.SVC().fit(X, y).predict(X)\n8.68 s ± 64.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n366 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p>[0] System</p>\n<ul>\n<li>CPU: Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz, CPU(s): 12</li>\n<li>GPU: Quadro RTX 8000</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When I change/add a variable to my <code>config.py</code> file and then try to import it to my Jupyter Notebook I get:</p>\n<p><code>ImportError: cannot import name 'example_var' from 'config'</code></p>\n<blockquote>\n<p>config.py:</p>\n</blockquote>\n<pre><code>example_var = 'example'\n</code></pre>\n<blockquote>\n<p>jp_notebook.ipynb:</p>\n</blockquote>\n<pre><code>from config import example_var\n\nprint(example_var)\n</code></pre>\n<p>But after I restart the Jupyter Kernel it works fine until I modify the <code>config.py</code> file again. I read somewhere that it's because jupyter already cached that import. Is there any other way to delete that cache so I don't have to restart the kernel every time I make a change in the <code>config.py</code> file. Thanks for help in advance.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\" rel=\"noreferrer\">autoreload</a> to reload modules every new cell execution.</p>\n<pre><code>%load_ext autoreload\n%autoreload 2\nfrom config import example_var\n\nprint(example_var)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I arranged my Jupyter notebooks into: <code>data.ipynb</code>, <code>methods.ipynb</code> and <code>results.ipynb</code>. How can I selectively import cells from <em>data</em> and <em>methods</em> notebooks for use in the <em>results</em> notebook?</p>\n<p>I know of <code>nbimporter</code> and <code>ipynb</code> but neither of those offers selective import of variables. There is an option to import definitions - including variables that are uppercase - but this does not work for me as I would have to convert most of the variables in my notebooks to uppercase.</p>\n<p>I would rather import everything except for two or three cells that take a long time to evaluate. Ideally, I would like to defer the execution of some assignments to the very moment I access them (lazy evaluation) - but I understand that it might be difficult to implement.</p>\n<p>Here is the overview, in pseudocode (each line repesents a cell):</p>\n<p><code>data.ipynb</code>:</p>\n<pre><code>raw_data = load_data()\ndataset = munge(raw_data)\ndescribe(dataset)             # I want this line to be skipped at import\n</code></pre>\n<p><code>methods.ipynb</code>:</p>\n<pre><code>import data\nmethod = lambda x: x * x\n# showcase how the method works on a subset of the dataset\nmethod(data.dataset[:5])      # I want this line to be skipped at import\n</code></pre>\n<p><code>results.ipynb</code>:</p>\n<pre><code>import data\nimport methods\nresult = methods.method(data.dataset)\ndescribe(result)\n</code></pre>\n<p>The motivation is that my real <em>data</em> and <em>methods</em> notebooks:</p>\n<ul>\n<li>are way much longer and complicated, hence I want to use an import system</li>\n<li>there are only a couple of cells that take more than seconds to evaluate</li>\n</ul>\n<p>also, the <em>methods</em> notebook cannot be replaced with <code>methods.py</code> file. In fact, I have such a file which contains the implementation details of my method. The notebook is more of a place to specify default parameters, showcase how my method works and explain example results.</p>\n<p>This question is essentially a combination of:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/19564625/how-can-i-import-from-another-ipython-notebook\">How can I import from another ipython-notebook?</a>, and</li>\n<li><a href=\"https://stackoverflow.com/questions/26494747/simple-way-to-choose-which-cells-to-run-in-ipython-notebook-during-run-all\">Simple way to choose which cells to run in ipython notebook during run all</a></li>\n</ul>\n<p>I read through answers to both and none satisfied my requirements.</p>\n<p>In my answer below I present my solution that uses custom cell magics and monkey-patching. However, I would prefer a solution which allows specifying which cells/expressions to exclude/include not in the notebook of origin (e.g. <code>data.ipynb</code>) but in the target one (e.g. in <code>methods.ipynb</code>).</p>\n<p>For example, it could use regular expressions:</p>\n<pre><code># all variables starting with 'result' would be ignored\nnbimporter.options['exclude'] = '^result.*'\n</code></pre>\n<p>or (even better) lazy evaluation:</p>\n<pre><code># only `a` and `b` would be evaluated and imported\nfrom data import a, b\n</code></pre>\n<p>All ideas will be appreciated!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So far I've been monkey-patching <code>nbimporter</code> and selecting cells to exclude using cell magic:</p>\n<pre><code>from IPython.core import magic\n\n@magic.register_cell_magic\ndef skip_on_import(args, cell):\n    get_ipython().ex(cell)\n</code></pre>\n<p>The code used to monkey-patch of cell remover:</p>\n<pre><code>import ast\n\nclass SkippingTransformer(ast.NodeTransformer):\n    # usage:\n    # import nbimporter \n    # nbimporter.CellDeleter = SkippingTransformer\n\n    def visit(self, node):\n        if (\n            isinstance(node, ast.Expr)\n            and isinstance(node.value, ast.Call)\n            and isinstance(node.value.func, ast.Attribute)\n            and node.value.func.attr == 'run_cell_magic'\n            and node.value.args[0].s == 'skip_on_import'\n        ):\n            return\n        return node\n</code></pre>\n<p>And an actual example, <code>data.ipynb</code>:</p>\n<p><a href=\"https://i.sstatic.net/SMdnf.png\" rel=\"noreferrer\"><img alt=\"data.ipynb\" src=\"https://i.sstatic.net/SMdnf.png\"/></a></p>\n<p>And <code>methods.ipynb</code> (the exception at the end is intended - it means success!):</p>\n<p><a href=\"https://i.sstatic.net/OZQw2.png\" rel=\"noreferrer\"><img alt=\"methods.ipynb\" src=\"https://i.sstatic.net/OZQw2.png\"/></a></p>\n<p><strong>Edit</strong>: I published the above code as a part of <a href=\"https://github.com/krassowski/jupyter-helpers\" rel=\"noreferrer\">jupyter-helpers</a> some time ago. Using this package one simply needs to import the importer in the importing notebook:</p>\n<pre><code>from jupyter_helpers.selective_import import notebooks_importer \n</code></pre>\n<p>and the cell-magic can be imported in the imported notebook with:</p>\n<pre><code>from jupyter_helpers.selective_import import skip_on_import\n</code></pre>\n<p>Here is example imported notebook: <a href=\"https://github.com/krassowski/jupyter-helpers/blob/master/demos/Data.ipynb\" rel=\"noreferrer\">Data.ipynb</a> and example importing notebook: <a href=\"https://github.com/krassowski/jupyter-helpers/blob/master/demos/Results.ipynb\" rel=\"noreferrer\">Results.ipynb</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Working in Jupyter with Pandas DataSeries I have a dataset with rows like this:</p>\n<pre><code>color: white\nengineType: diesel\nmake: Ford\nmanufacturingYear: 2004\naccidentCount: 123\n</code></pre>\n<p>What I need to do is to plot charts of accident counts (y-axis) by manufacturing year (x-axis) for all permutations of color/engineType/make. Any ideas how to proceed with this?</p>\n<p>To speed things up I have this initial setup:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport random\n\n\ncolors = ['white', 'black','silver']\nengineTypes = ['diesel', 'petrol']\nmakes = ['ford', 'mazda', 'subaru']\nyears = range(2000,2005)\n\nrowCount = 100\n\ndef randomEl(data):\n    rand_items = [data[random.randrange(len(data))] for item in range(rowCount)]\n    return rand_items\n\n\ndf = DataFrame({\n    'color': Series(randomEl(colors)),\n    'engineType': Series(randomEl(engineTypes)),\n    'make': Series(randomEl(makes)),\n    'year': Series(randomEl(years)),\n    'accidents': Series([int(1000*random.random()) for i in range(rowCount)])\n})\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can get the number of accidents by unique <code>color</code>, <code>engineType</code>, and <code>make</code> combinations using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"noreferrer\"><code>groupby()</code></a>:</p>\n<pre><code>accident_counts = df.groupby(['color', 'engineType', 'make'])['accidents'].sum()\n</code></pre>\n<p><a href=\"https://matplotlib.org/\" rel=\"noreferrer\">Matplotlib</a> is one way of plotting the results:</p>\n<pre><code>import matplotlib.pyplot as plt\naccident_counts.plot(kind='bar')\nplt.show()\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was learning about non-linear clustering algorithms and I came across this 2-D graph. I was wondering which clustering alogirthm and combination of hyper-parameters will cluster this data well.</p>\n<p><img alt=\"Plot\" src=\"https://i.sstatic.net/e1dlD.png\"/></p>\n<p>Just like a human will cluster those 5 spikes. I want my algorithm to do it.\nI tried KMeans but it was only clustering horizontly or vertically. I started using GMM but couldn't get the hyper-parameters right for the desired clustering.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If it doesn't work, always try to improve the preprocessing first. Algorithms such as k-means are very sensitive to scaling, so that is something that needs to be chosen carefully.</p>\n<p>GMM is clearly your first choice here. It may be worth trying out different tools. R's Mclust is very slow. Sklearn's GMM is sometimes unstable. ELKI is a bit harder to get started with, but its EM gave me the best results usually.</p>\n<p>Apart from GMM, it likely is worth trying out <strong>correlation clustering</strong>. These algorithms assume there is some manifold (e.g., a line) on which a cluster exists. Examples include ORCLUS, LMCLUS, CASH, 4C, ... But in my opinion these mostly work for synthetic toy data.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I will suggest trying out <a href=\"https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec\" rel=\"nofollow noreferrer\">hierarchical clustering</a>. In the Agglomerative approach, you will assign individual clusters to each point, and then combine clusters based on their distances from each other.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>DBSCAN</code> or <code>GMM</code> should work well to cluster this type of data. </p>\n<p>It is one of the few clustering algorithms that does not classify the data into <code>circular clusters</code></p>\n<p><strong>Clustering with DBSCAN</strong></p>\n<p><a href=\"https://cdn-images-1.medium.com/max/1600/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" rel=\"nofollow noreferrer\"><img alt=\"DBSCAN\" src=\"https://cdn-images-1.medium.com/max/1600/1*tc8UF-h0nQqUfLC8-0uInQ.gif\"/></a></p>\n<p><strong>Clustering with GMM</strong></p>\n<p><a href=\"https://i.sstatic.net/vHyij.gif\" rel=\"nofollow noreferrer\"><img alt=\"GMM\" src=\"https://i.sstatic.net/vHyij.gif\"/></a></p>\n<p>Also please do give <a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\" rel=\"nofollow noreferrer\">this blog</a> a read. It will explain the different clustering techniques.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I came across the below line of code, which gives an error when '.index' is not present in it.</p>\n<pre><code>print(df.drop(df[df['Quantity'] == 0].index).rename(columns={'Weight': 'Weight (oz.)'}))\n</code></pre>\n<p>What is the purpose of '.index' while using drop in pandas?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As explained in the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\" rel=\"noreferrer\">documentation</a>, you can use <code>drop</code> with <code>index</code>:</p>\n<pre><code>   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n\ndf.drop([0, 1]) # Here 0 and 1 are the index of the rows\n</code></pre>\n<p>Output:</p>\n<pre><code>   A  B   C   D\n2  8  9  10  11\n</code></pre>\n<p>In this case it will drop the first 2 rows.\nWith <code>.index</code> in your example, you find the rows where <code>Quantity=0</code>and retrieve their index(and then use like in the documentation)</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Im new to Data Science and Analysis. \nAfter going through a lot of kernels on Kaggle, I made a model that predicts the price of a property. Ive tested this model using my training data, but now I want to run it on my test data. Ive got a test.csv file and I want to use it. How do I do that?\nWhat i previously did with my training dataset:</p>\n<pre><code>#loading my train dataset into python\ntrain = pd.read_csv('/Users/sohaib/Downloads/test.csv')\n\n#factors that will predict the price\ntrain_pr = ['OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\n\n#set my model to DecisionTree\nmodel = DecisionTreeRegressor()\n\n#set prediction data to factors that will predict, and set target to SalePrice\nprdata = train[train_pr]\ntarget = train.SalePrice\n\n#fitting model with prediction data and telling it my target\nmodel.fit(prdata, target)\n\nmodel.predict(prdata.head())\n</code></pre>\n<p>Now what I tried to do is, copy the whole code, and change the \"train\" with \"test\", and \"predate\" with \"testprdata\", and I thought it will work, but sadly no. I know I'm doing something wrong with this, idk what it is.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>As long as you process the train and test data exactly the same way, that <code>predict</code> function will work on either data set.  So you'll want to load both the train and test sets, <code>fit</code> on the train, and <code>predict</code> on either just the test or both the train and test.</p>\n<p>Also, note the file you're reading is the <code>test</code> data.  Assuming your file is named properly, even though you named the variable to be <code>train</code>, you are currently training on your test data.</p>\n<pre><code>#loading my train dataset into python\ntrain = pd.read_csv('/Users/sohaib/Downloads/train.csv')\ntest = pd.read_csv('/Users/sohaib/Downloads/test.csv')\n\n#factors that will predict the price\ndesired_factors = ['OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\n\n#set my model to DecisionTree\nmodel = DecisionTreeRegressor()\n\n#set prediction data to factors that will predict, and set target to SalePrice\ntrain_data = train[desired_factors]\ntest_data = test[desired_factors]\ntarget = train.SalePrice\n\n#fitting model with prediction data and telling it my target\nmodel.fit(train_data, target)\n\nmodel.predict(test_data.head())\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You are already using the trained model for prediction (<code>model.predict(prdata.head())</code>). If you want to use that model to predict on other test data, simply supply the other test data instead of <code>prdata.head()</code>. For example, you can use the model to predict all samples from <code>prdata</code> by removing <code>.head()</code> which restricts the DataFrame to the first 5 rows (but you just used this data to train the model; it's just an example).</p>\n<p>Keep in mind, you still need a model to make predictions. Typically, you'll train a model and then present it with test data. Changing all of the references of <code>train</code> to <code>test</code> will not work, because you will not have a model for making predictions based on your test data unless you've saved it from training and restored it prior to presenting it with test data.</p>\n<p>In your code, you are actually using your <code>test.csv</code> data file to <em>train</em> your model as you pass the data to the <code>model.fit</code> method. Typically, you will not train your model with data intended for testing.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>New to ARIMA and attempting to model a dataset in Python using auto ARIMA.\nI'm using auto-ARIMA as I believe it will be better at defining the values of p, d and q however the results are poor and I need some guidance.\nPlease see my reproducible attempts below</p>\n<p>Attempt as follows:</p>\n<pre><code>    # DEPENDENCIES\n    import pandas as pd \n    import numpy as np \n    import matplotlib.pyplot as plt\n    import pmdarima as pm \n    from pmdarima.model_selection import train_test_split \n    from statsmodels.tsa.stattools import adfuller\n    from pmdarima.arima import ADFTest\n    from pmdarima import auto_arima\n    from sklearn.metrics import r2_score \n\n# CREATE DATA\ndata_plot = pd.DataFrame(data removed)\n\n# SET INDEX\ndata_plot['date_index'] = pd.to_datetime(data_plot['date']\ndata_plot.set_index('date_index', inplace=True)\n\n# CREATE ARIMA DATASET\narima_data = data_plot[['value']]\narima_data\n\n# PLOT DATA\narima_data['value'].plot(figsize=(7,4))\n</code></pre>\n<p>The above steps result in a dataset that should look like this.\n<a href=\"https://i.sstatic.net/ku2YH.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ku2YH.png\"/></a></p>\n<pre><code># Dicky Fuller test for stationarity \nadf_test = ADFTest(alpha = 0.05)\nadf_test.should_diff(arima_data)\n</code></pre>\n<p>Result = 0.9867 indicating non-stationary data which should be handled by appropriate over of differencing later in auto arima process.</p>\n<pre><code># Assign training and test subsets - 80:20 split \n\nprint('Dataset dimensions;', arima_data.shape)\ntrain_data = arima_data[:-24]\ntest_data = arima_data[-24:]\nprint('Training data dimension:', train_data.shape, round((len(train_data)/len(arima_data)*100),2),'% of dataset')\nprint('Test data dimension:', test_data.shape, round((len(train_data)/len(arima_data)*100),2),'% of dataset')\n\n# Plot training &amp; test data\nplt.plot(train_data)\nplt.plot(test_data)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/0HvUn.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0HvUn.png\"/></a></p>\n<pre><code> # Run auto arima\n    arima_model = auto_arima(train_data, start_p=0, d=1, start_q=0,\n    max_p=5, max_d=5, max_q=5,\n    start_P=0, D=1, start_Q=0, max_P=5, max_D=5,\n    max_Q=5, m=12, seasonal=True,\n    stationary=False,\n    error_action='warn', trace=True,\n    suppress_warnings=True, stepwise=True,\n    random_state=20, n_fits=50)\n        \n    print(arima_model.aic())\n</code></pre>\n<p>Output suggests best model is <code>'ARIMA(1,1,1)(0,1,0)[12]'</code> with AIC 1725.35484</p>\n<pre><code>#Store predicted values and view resultant df\n\nprediction = pd.DataFrame(arima_model.predict(n_periods=25), index=test_data.index)\nprediction.columns = ['predicted_value']\nprediction\n\n# Plot prediction against test and training trends \n\nplt.figure(figsize=(7,4))\nplt.plot(train_data, label=\"Training\")\nplt.plot(test_data, label=\"Test\")\nplt.plot(prediction, label=\"Predicted\")\nplt.legend(loc='upper right')\nplt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/RlyZz.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/RlyZz.png\"/></a></p>\n<pre><code># Finding r2 model score\n    test_data['predicted_value'] = prediction \n    r2_score(test_data['value'], test_data['predicted_value'])\n</code></pre>\n<p>Result: -6.985</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is <code>auto_arima</code> a method done by you? It depends how you differentiate and what you do there. Did you check the autocorrelation and partial autocorrelation to know which repeating time lags you have there?</p>\n<p>Also, it seems you have some seasonality patterns every year, you could try a SARIMA model if you are not doing it already.</p>\n<p>To try a SARIMA model you have to:</p>\n<ol>\n<li>Stationarized the data, in this case by differentiation you can convert the moving mean a stationary one.</li>\n</ol>\n<pre><code>data_stationarized = train_data.diff()[1:]\n</code></pre>\n<ol start=\"2\">\n<li>Check the autocorrelation and partial autocorrelation to check the seasonality.\nYou can use the library <code>statsmodels</code> for this.</li>\n</ol>\n<pre><code>import statsmodels.api as sm\nsm.graphics.tsa.plot_acf(data_stationarized);\n</code></pre>\n<p><a href=\"https://i.sstatic.net/kEjz1.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/kEjz1.png\"/></a></p>\n<p>You can see that the most prominent flag is the twelfth flag, so as the granularity of the data is by month, that means there is prominent seasonality pattern every 12 months.</p>\n<p>We can check the partial autocorrelation to confirm it too:</p>\n<pre class=\"lang-py prettyprint-override\"><code>sm.graphics.tsa.plot_pacf(data_stationarized);\n</code></pre>\n<p><a href=\"https://i.sstatic.net/0GM0i.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/0GM0i.png\"/></a></p>\n<p>Again the most prominent flag is the twelfth one.</p>\n<ol start=\"3\">\n<li>Fit the model with a seasonality order of 12. There are more parameters to explain which can be adjusted to have better results, but then this post will be very long.</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>model = sm.tsa.SARIMAX(endog=train_data, order=(2,0,0), seasonal_order=(2,0,0,12))\nmodel_fit = model.fit()\n</code></pre>\n<ol start=\"4\">\n<li>Evaluate the results</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.metrics import mean_squared_error\n\ny_pred = model_fit.forecast(steps=24)\n\n# when squared=False then is equals to RMSE\nmean_squared_error(y_true=test_data.values, y_pred=y_pred, squared=False)\n</code></pre>\n<p>This outputs <code>12063.88</code>, which you can use to compare different results more rigorously.</p>\n<p>For a graphical check:</p>\n<pre class=\"lang-py prettyprint-override\"><code>prediction = pd.DataFrame(model_fit.forecast(steps=25), index=test_data.index)\nprediction.columns = ['predicted_value']\nprediction\n\n# Plot prediction against test and training trends\n\nplt.figure(figsize=(7,4))\nplt.plot(train_data, label=\"Training\")\nplt.plot(test_data, label=\"Test\")\nplt.plot(prediction, label=\"Predicted\")\nplt.legend(loc='upper right')\nplt.xticks([])\nplt.yticks([])\nplt.show();\n</code></pre>\n<p><a href=\"https://i.sstatic.net/pTZN4.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/pTZN4.png\"/></a></p>\n<p>Now you can see that the predictions get closer to the expected values.</p>\n<p>You could continue fine tuning the order and seasonal order to get even better results, I will advice to check the <a href=\"https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\" rel=\"nofollow noreferrer\">docs of statsmodel</a>.</p>\n<p>Another advice it's to analyze the autocorrelation and partial autocorrelation of the residuals to check if your model is capturing all of the patterns. You have them in the <code>model_fit</code> object.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>ARIMA has assumptions which need to be checked before applying it to the data . One of them is data Need to be stationary i.e it should not have trend or seasonality . You  can check trend through plotting , which is visible in your graph thent it has upwards trend .</p>\n<p>1.You can seasonality also from graph or use Dicker fuller test to check hypothesis.</p>\n<pre><code>import statsmodels.tsa.stattools as ts\nts.adfuller(data.col) \n</code></pre>\n<p>Check this answer , how to perform and read ad fuller test has been well explained .\n<a href=\"https://stackoverflow.com/questions/47349422/how-to-interpret-adfuller-test-results\">How to interpret adfuller test results?</a></p>\n<ol start=\"2\">\n<li>Always check the ACF and PACF plots and at which lags are lying beyound the limits , shows autocorrelation. Check the whether the Stationarity exits</li>\n</ol>\n<p>As explained by Jose , differencing can be done to Stationarize the data.</p>\n<p>SARIMA Algorithms considers the Seasonal components (p,d,q) and (S,P',D',Q') and also the exogenous varaibles .</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have the following table. Some values are NaNs. Let's assume that columns are highly correlated. Taking <code>row 0</code> and <code>row 5</code> I say that value in <code>col2</code> will be <code>4.0</code>. Same situation for <code>row 1</code> and <code>row 4</code>. But in case of <code>row 6</code>, there is no perfectly matching sample so I should take most similar row - in this case, <code>row 0</code> and change NaN to <code>3.0</code>.\nHow should I approach it? Is there any pandas function that can do this?</p>\n<pre><code>example = pd.DataFrame({\"col1\": [3, 2, 8, 4, 2, 3, np.nan], \n                        \"col2\": [4, 3, 6, np.nan, 3, np.nan, 5], \n                        \"col3\": [7, 8, 9, np.nan, np.nan, 7, 7], \n                        \"col4\": [7, 8, 9, np.nan, np.nan, 7, 6]})\n</code></pre>\n<p>Output:</p>\n<pre><code>    col1    col2    col3    col4\n0   3.0     4.0     7.0     7.0\n1   2.0     3.0     8.0     8.0\n2   8.0     6.0     9.0     9.0\n3   4.0     NaN     NaN     NaN\n4   2.0     3.0     NaN     NaN\n5   3.0     NaN     7.0     7.0\n6   NaN     5.0     7.0     6.0\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a hard question , involved <code>numpy</code> broadcast  , and <code>groupby</code> +  <code>transform</code> , I am using <code>first</code> here , since <code>first</code> will pick up the first not <code>NaN</code> value </p>\n<pre><code>s=df.values\nt=np.all((s==s[:,None])|np.isnan(s),-1)\nidx=pd.DataFrame(t).where(t).stack().index\n# we get the pair for each row\ndf=df.reindex(idx.get_level_values(1))\n# reorder our df to the idx we just get \ndf.groupby(level=[0]).transform('first').groupby(level=1).first()\n# using two times groupby with first , get what we need .\nOut[217]: \n   col1  col2  col3  col4\n0   3.0   4.0   7.0   7.0\n1   2.0   3.0   8.0   8.0\n2   8.0   6.0   9.0   9.0\n3   4.0   NaN   NaN   NaN\n4   2.0   3.0   8.0   8.0\n5   3.0   4.0   7.0   7.0\n6   NaN   5.0   7.0   6.0\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between Kernel Ridge (from sklearn.kernel_ridge) with polynomial kernel and using PolynomialFeatures + Ridge (from sklearn.linear_model)? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The difference is in feature computation. <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\" rel=\"nofollow noreferrer\"><code>PolynomialFeatures</code></a> explicitly computes polynomial combinations between the input features up to the desired degree while <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html\" rel=\"nofollow noreferrer\"><code>KernelRidge(kernel='poly')</code></a> only considers a polynomial kernel (<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html\" rel=\"nofollow noreferrer\">a polynomial representation of feature dot products</a>) which will be expressed in terms of the original features. <a href=\"https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf\" rel=\"nofollow noreferrer\">This document</a> provides a good overview in general.</p>\n<p>Regarding the computation we can inspect the relevant parts from the source code:</p>\n<ul>\n<li><strong>Ridge Regression</strong>\n<ul>\n<li>The actual computation starts <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/linear_model/ridge.py#L406\" rel=\"nofollow noreferrer\">here</a> (for the default settings); you can compare with equation (5) in the above linked document. The computation involves computing the dot product between feature vectors (the kernel), then the dual coefficients (<em>alpha</em>) and finally a dot product with the feature vectors in order to obtain the weights.</li>\n</ul></li>\n<li><strong>Kernel Ridge</strong>\n<ul>\n<li>Similarly <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/kernel_ridge.py#L166\" rel=\"nofollow noreferrer\">computes the dual coefficients</a> and stores them (instead of computing some weights). This is because when making predictions, again <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/kernel_ridge.py#L193\" rel=\"nofollow noreferrer\">the kernel between training and prediction samples is computed</a>. The <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/kernel_ridge.py#L194\" rel=\"nofollow noreferrer\">result is then dotted with the dual coefficients</a>.</li>\n</ul></li>\n</ul>\n<p>The computation of the (training) kernel follows a similar procedure: compare <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/linear_model/ridge.py#L406\" rel=\"nofollow noreferrer\"><code>Ridge</code></a> and <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/metrics/pairwise.py#L727\" rel=\"nofollow noreferrer\"><code>KernelRidge</code></a>. The major difference is that <code>Ridge</code> explicitly considers the dot product between whatever (polynomial) features it has received while for <code>KernelRidge</code> these polynomial features are <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/metrics/pairwise.py#L759\" rel=\"nofollow noreferrer\">generated implicitly during the computation</a>. For example consider a single feature <code>x</code>; with <code>gamma = coef0 = 1</code> the <code>KernelRidge</code> computes <code>(x**2 + 1)**2 == (x**4 + 2*x**2 + 1)</code>. If you consider now <code>PolynomialFeatures</code> this will provide features <code>x**2, x, 1</code> and the corresponding dot product is <code>x**4 + x**2 + 1</code>. Hence the dot product differs by a term <code>x**2</code>. Of course we could rescale the poly-features to have <code>x**2, sqrt(2)*x, 1</code> while with <code>KernelRidge(kernel='poly')</code> we don't have this kind of flexibility. On the other hand the difference probably doesn't matter (in most cases).</p>\n<p>Note that also the computation of the dual coefficients is performed in a similar manner: <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/linear_model/ridge.py#L408\" rel=\"nofollow noreferrer\"><code>Ridge</code></a> and <a href=\"https://github.com/scikit-learn/scikit-learn/blob/e1c3c228b28c0fd1b1c6763a3da579d21bee63d1/sklearn/kernel_ridge.py#L166\" rel=\"nofollow noreferrer\"><code>KernelRidge</code></a>. Finally <code>KernelRidge</code> keeps the dual coefficients while <code>Ridge</code> directly computes the weights.</p>\n<p>Let's see a small example:</p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.utils.extmath import safe_sparse_dot\n\nnp.random.seed(20181001)\n\na, b = 1, 4\nx = np.linspace(0, 2, 100).reshape(-1, 1)\ny = a*x**2 + b*x + np.random.normal(scale=0.2, size=(100,1))\n\npoly = PolynomialFeatures(degree=2, include_bias=True)\nxp = poly.fit_transform(x)\nprint('We can see that the new features are now [1, x, x**2]:')\nprint(f'xp.shape: {xp.shape}')\nprint(f'xp[-5:]:\\n{xp[-5:]}', end='\\n\\n')\n# Scale the `x` columns so we obtain similar results.\nxp[:, 1] *= np.sqrt(2)\n\nridge = Ridge(alpha=0, fit_intercept=False, solver='cholesky')\nridge.fit(xp, y)\n\nkrr = KernelRidge(alpha=0, kernel='poly', degree=2, gamma=1, coef0=1)\nkrr.fit(x, y)\n\n# Let's try to reproduce some of the involved steps for the different models.\nridge_K = safe_sparse_dot(xp, xp.T)\nkrr_K = krr._get_kernel(x)\nprint('The computed kernels are (alomst) similar:')\nprint(f'Max. kernel difference: {np.abs(ridge_K - krr_K).max()}', end='\\n\\n')\nprint('Predictions slightly differ though:')\nprint(f'Max. difference: {np.abs(krr.predict(x) - ridge.predict(xp)).max()}', end='\\n\\n')\n\n# Let's see if the fit changes if we provide `x**2, x, 1` instead of `x**2, sqrt(2)*x, 1`.\nxp_2 = xp.copy()\nxp_2[:, 1] /= np.sqrt(2)\nridge_2 = Ridge(alpha=0, fit_intercept=False, solver='cholesky')\nridge_2.fit(xp_2, y)\nprint('Using features \"[x**2, x, 1]\" instead of \"[x**2, sqrt(2)*x, 1]\" predictions are (almost) the same:')\nprint(f'Max. difference: {np.abs(ridge_2.predict(xp_2) - ridge.predict(xp)).max()}', end='\\n\\n')\nprint('Interpretability of the coefficients changes though:')\nprint(f'ridge.coef_[1:]: {ridge.coef_[0, 1:]}, ridge_2.coef_[1:]: {ridge_2.coef_[0, 1:]}')\nprint(f'ridge.coef_[1]*sqrt(2): {ridge.coef_[0, 1]*np.sqrt(2)}')\nprint(f'Compare with: a, b = ({a}, {b})')\n\nplt.plot(x.ravel(), y.ravel(), 'o', color='skyblue', label='Data')\nplt.plot(x.ravel(), ridge.predict(xp).ravel(), '-', label='Ridge', lw=3)\nplt.plot(x.ravel(), krr.predict(x).ravel(), '--', label='KRR', lw=3)\nplt.grid()\nplt.legend()\nplt.show()\n</code></pre>\n<p>From which we obtain:</p>\n<pre><code>We can see that the new features are now [x, x**2]:\nxp.shape: (100, 3)\nxp[-5:]:\n[[1.         1.91919192 3.68329762]\n [1.         1.93939394 3.76124885]\n [1.         1.95959596 3.84001632]\n [1.         1.97979798 3.91960004]\n [1.         2.         4.        ]]\n\nThe computed kernels are (alomst) similar:\nMax. kernel difference: 1.0658141036401503e-14\n\nPredictions slightly differ though:\nMax. difference: 0.04244651134471766\n\nUsing features \"[x**2, x, 1]\" instead of \"[x**2, sqrt(2)*x, 1]\" predictions are (almost) the same:\nMax. difference: 7.15642822779472e-14\n\nInterpretability of the coefficients changes though:\nridge.coef_[1:]: [2.73232239 1.08868872], ridge_2.coef_[1:]: [3.86408737 1.08868872]\nridge.coef_[1]*sqrt(2): 3.86408737392841\nCompare with: a, b = (1, 4)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/w6VNb.png\" rel=\"nofollow noreferrer\"><img alt=\"Example plot\" src=\"https://i.sstatic.net/w6VNb.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I developed a custom dataset by using the PyTorch dataset class. The code is like that:</p>\n<pre><code>class CustomDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root_path, transform=None):\n        self.path = root_path\n        self.mean = mean\n        self.std = std\n        self.transform = transform\n        self.images = []\n        self.masks = []\n\n        for add in os.listdir(self.path):\n            # Some script to load file from directory and appending address to relative array\n            ...\n\n        self.masks.sort()\n        self.images.sort()\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, item):\n        image_address = self.images[item]\n        mask_address = self.masks[item]\n\n\n\n        if self.transform is not None:\n            augment = self.transform(image=np.asarray(Image.open(image_address, 'r', None)),\n                                     mask=np.asarray(Image.open(mask_address, 'r', None)))\n            image = Image.fromarray(augment['image'])\n            mask = augment['mask']\n\n        if self.transform is None:\n            image = np.asarray(Image.open(image_address, 'r', None))\n            mask = np.asarray(Image.open(mask_address, 'r', None))\n\n        # Handle Augmentation here\n\n        return image, mask\n</code></pre>\n<p>Then I created an object from this class and passed it to torch.utils.data.DataLoader. Although this works well with DataLoader but with torch.utils.data.DataLoader2 I got a problem. The error is this:</p>\n<blockquote>\n<p><code>dataloader = torch.utils.data.DataLoader2(dataset=dataset, batch_size=2, pin_memory=True, num_workers=4)</code></p>\n<blockquote>\n<p>Exception: thread parallelism mode is not supported for old DataSets</p>\n</blockquote>\n</blockquote>\n<p>My question is why DataLoader2 module was added to PyTorch what is different with DataLoader and what are its benefits?</p>\n<p>PyTorch Version: <code>1.10.1</code></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You should definitely not use it <code>DataLoader2</code>.</p>\n<p><code>torch.utils.data.DataLoader2</code> (actually <code>torch.utils.data.dataloader_experimental.DataLoader2</code>)\nwas added as an experimental \"feature\" as a future replacement for <code>DataLoader</code>. It is defined <a href=\"https://github.com/pytorch/pytorch/blob/d90012689f5b99bbc289da598ad982f829687288/torch/utils/data/dataloader_experimental.py#L63\" rel=\"nofollow noreferrer\">here</a>. Currently, it is only accessible on the <strong>master</strong> branch (unstable) and is of course not documented on the official pages.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here's my code:</p>\n<pre><code>from matplotlib.pyplot import imread\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import convolve\n\n\nk3 = np.array([ [-1, -1, -1], [-1, 8, -1], [-1, -1, -1] ])\nimg = imread(\"lena.jpg\")\nchannels = []\nfor channel in range(3):\n    res = convolve(img[:,:,channel], k3)\n    channels.append(res)\n\nimg = np.dstack((channels[0], channels[1], channels[2]))\nplt.imshow(img)\nplt.show()\n</code></pre>\n<p><code>k3</code> filter suppose to be an edge detection filter. Instead, I'm getting a weird image looks like white noise. </p>\n<p>Why?</p>\n<p>Here's the output:</p>\n<p><a href=\"https://i.sstatic.net/IzyCB.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/IzyCB.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>img</code> is likely an 8-bit unsigned integer. Convolving with the Laplace mask as you do, output values are likely to exceed the valid range of [0,255]. When assigning, for example, a -1 to such an image, the value written will be 254. That leads to an output as shown in the question.</p>\n<p>With this particular filter, it is important to convert the image to a signed type first, for example a 16-bit signed integer or a floating-point type.</p>\n<pre><code>img = img.astype(np.int16)\n</code></pre>\n<p>PS: Note that <a href=\"https://stackoverflow.com/a/51414532/7328782\">Laplace is not an edge detector</a>!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to Deploy an Recommendation Engine as mentioned in <a href=\"http://docs.prediction.io/templates/recommendation/quickstart/\" rel=\"nofollow\">quick start guide</a>.\nI completed the steps up to build the engine. Now I want to train the Recommendation Engine. I did as mentioned in quick start guide. (execute <code>pio train</code>). Then I got the lengthy error log and I couldn't paste all here. So I am putting first few rows of the error.</p>\n<pre><code>[INFO] [Console$] Using existing engine manifest JSON at /home/PredictionIO/PredictionIO-0.9.6/bin/MyRecommendation/manifest.json\n[INFO] [Runner$] Submission command: /home/PredictionIO/PredictionIO-0.9.6/vendors/spark-1.5.1-bin-hadoop2.6/bin/spark-submit --class io.prediction.workflow.CreateWorkflow --jar/PredictionIO/PredictionIO-0.9.6/bin/MyRecommendation/target/scala-2.10/template-scala-parallel-recommendation_2.10-0.1-SNAPSHOT.jar,file:/home/PredictionIO/PredictionIO-0.9.6/bndation/target/scala-2.10/template-scala-parallel-recommendation-assembly-0.1-SNAPSHOT-deps.jar --files file:/home/PredictionIO/PredictionIO-0.9.6/conf/log4j.properties --driver/home/PredictionIO/PredictionIO-0.9.6/conf:/home/PredictionIO/PredictionIO-0.9.6/lib/postgresql-9.4-1204.jdbc41.jar:/home/PredictionIO/PredictionIO-0.9.6/lib/mysql-connector-jav file:/home/PredictionIO/PredictionIO-0.9.6/lib/pio-assembly-0.9.6.jar --engine-id qokYFr4rwibijNjabXeVSQKKFrACyrYZ --engine-version ed29b3e2074149d483aa85b6b1ea35a52dbbdb9a --et file:/home/PredictionIO/PredictionIO-0.9.6/bin/MyRecommendation/engine.json --verbosity 0 --json-extractor Both --env PIO_ENV_LOADED=1,PIO_STORAGE_REPOSITORIES_METADATA_NAME=pFS_BASEDIR=/root/.pio_store,PIO_HOME=/home/PredictionIO/PredictionIO-0.9.6,PIO_FS_ENGINESDIR=/root/.pio_store/engines,PIO_STORAGE_SOURCES_PGSQL_URL=jdbc:postgresql://localhost/pGE_REPOSITORIES_METADATA_SOURCE=PGSQL,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=PGSQL,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=pio_event,PIO_STORAGE_SOURCES_PGSQL_PASSWORD=pio,PIURCES_PGSQL_TYPE=jdbc,PIO_FS_TMPDIR=/root/.pio_store/tmp,PIO_STORAGE_SOURCES_PGSQL_USERNAME=pio,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_model,PIO_STORAGE_REPOSITORIES_EVENTDGSQL,PIO_CONF_DIR=/home/PredictionIO/PredictionIO-0.9.6/conf\n[INFO] [Engine] Extracting datasource params...\n[INFO] [WorkflowUtils$] No 'name' is found. Default empty String will be used.\n[INFO] [Engine] Datasource params: (,DataSourceParams(MyApp3,None))\n[INFO] [Engine] Extracting preparator params...\n[INFO] [Engine] Preparator params: (,Empty)\n[INFO] [Engine] Extracting serving params...\n[INFO] [Engine] Serving params: (,Empty)\n[WARN] [Utils] Your hostname, test-digin resolves to a loopback address: 127.0.1.1; using 192.168.2.191 instead (on interface p5p1)\n[WARN] [Utils] Set SPARK_LOCAL_IP if you need to bind to another address\n[INFO] [Remoting] Starting remoting\n[INFO] [Remoting] Remoting started; listening on addresses :[akka.tcp://<a class=\"__cf_email__\" data-cfemail=\"16656677647d52647f60736456272f243827202e382438272f27\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>:56574]\n[WARN] [MetricsSystem] Using default name DAGScheduler for source because spark.app.id is not set.\n[INFO] [Engine$] EngineWorkflow.train\n[INFO] [Engine$] DataSource: duo.DataSource@6088451e\n[INFO] [Engine$] Preparator: duo.Preparator@1642eeae\n[INFO] [Engine$] AlgorithmList: List(duo.ALSAlgorithm@a09303)\n[INFO] [Engine$] Data sanity check is on.\n[INFO] [Engine$] duo.TrainingData does not support data sanity check. Skipping check.\n[INFO] [Engine$] duo.PreparedData does not support data sanity check. Skipping check.\n[WARN] [BLAS] Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n[WARN] [BLAS] Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n[WARN] [LAPACK] Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n[WARN] [LAPACK] Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.StackOverflowError\njava.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\nscala.collection.immutable.$colon$colon.writeObject(List.scala:379)\nsun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\njava.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\njava.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\njava.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\njava.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\njava.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n</code></pre>\n<p>what can I do to overcome this isssue?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your error says <code>java.lang.StackOverflowError</code> for that you can reduce the <code>numIterations parameter</code> in <code>engine.json</code> file. Refer <a href=\"https://stackoverflow.com/questions/34133172/predictionio-engine\">this</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My general problem is that I have a dataframe where columns correspond to feature values. There is also a date column in the dataframe. Each feature column may have missing NaN values. I want to fill a column with some fill logic such as \"fill_mean\" or \"fill zero\". </p>\n<p>But I do not want to just apply the fill logic to the whole column because if one of the earlier values is a NaN, I do not want the average i fill for this specific NaN to be tainted by what the average was later on, when the model should have no knowledge about. Essentially it's the common problem of not leaking information about the future to your model - specifically when trying to fill my time series.</p>\n<p>Anyway, I have simplified my problem to a few lines of code. This is my simplified attempt at the above general problem:</p>\n<pre><code>#assume ts_values is a time series where the first value in the list is the oldest value and the last value in the list is the most recent.\nts_values = [17.0, np.NaN, 12.0, np.NaN, 18.0]\nnan_inds = np.argwhere(np.isnan(ts_values))\nfor nan_ind in nan_inds:\n    nan_ind_value = nan_ind[0]\n    ts_values[nan_ind_value] = np.mean(ts_values[0:nan_ind_value])\n</code></pre>\n<p>The output of the above script is:</p>\n<pre><code>[17.0, 17.0, 12.0, 15.333333333333334, 18.0]\n</code></pre>\n<p>which is exactly what I would expect. </p>\n<p>My only issue with this is that it will be linear time with respect to the number of NaNs in the data set. Is there a way to do this in constant or log time where I don't iterate through the nan index values. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>if you want <code>nan</code> value replaced with a rolling mean (full window) on pandas series <code>s</code>, noting from <a href=\"https://stackoverflow.com/users/7964527/wenyoben\">WeNYoBen</a> that this does not continue the rolling mean calculation during the fill. (so your 15.3 becomes a 12.0).</p>\n<pre><code>s.fillna(s.expanding(1).mean())\n</code></pre>\n<p>If you would like the rolling mean to update as nans are filled, this in-place <a href=\"https://numba.pydata.org/\" rel=\"nofollow noreferrer\"><code>numba</code></a> solution may help </p>\n<pre><code>import numpy as np\nimport numba\nfrom numba import jit\n\n\n@jit(nopython=True)\ndef rolling_fill(a): \n    for i, e in enumerate(a):\n        if np.isnan(e):\n            a[i] = np.mean(a[:i])\n\nts_values = np.array([17.0, np.NaN, 12.0, np.NaN, 18.0])\nrolling_fill(ts_values)\nprint(ts_values)\n</code></pre>\n<p>which gives </p>\n<pre><code>[17.         17.         12.         15.33333333 18.        ]\n</code></pre>\n<p>you could probably improve this by keeping a sum and not calling <code>.mean</code> everytime. </p>\n<p><strong>Time Complexity</strong></p>\n<p>This is not <code>log</code> or <code>constant</code> time as you must interpolate at most <code>n-2</code> missing items from an array of length <code>n</code> which is <code>O(n)</code> - but it should be plenty optimized (by avoiding iteration in native python) and you can not do <em>theoretically</em> better, but lower level implementations of the above will make this dramatically faster. </p>\n<hr/>\n<p><strong>EDIT</strong>: I originally misread and thought you were asking about interpolation</p>\n<p>You would like to <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html\" rel=\"nofollow noreferrer\"><code>interpolate</code></a> the series, and pandas support this directly.</p>\n<pre><code>&gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 5])\n&gt;&gt;&gt; s\n0    0.0\n1    1.0\n2    NaN\n3    5.0\ndtype: float64\n&gt;&gt;&gt; s.interpolate()\n0    0.0\n1    1.0\n2    3.0\n3    5.0\ndtype: float64\n</code></pre>\n<p>Or if you do not want to use <code>pandas</code> because your example is an <code>ndarray</code>, then use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.interp.html\" rel=\"nofollow noreferrer\"><code>numpy.interp</code></a> accordingly. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm an avid R user and am learning python along the way. One of the example code that I can easily run in R is perplexing me in Python.</p>\n<p>Here's the original data (constructed within R):</p>\n<pre><code>library(tidyverse)\n\n\ndf &lt;- tribble(~name, ~age, ~gender, ~height_in,\n        \"john\",20,'m',66,\n        'mary',NA,'f',62,\n        NA,38,'f',68,\n        'larry',NA,NA,NA\n)\n</code></pre>\n<p>The output of this looks like this:</p>\n<pre><code>df\n\n# A tibble: 4 x 4\n  name    age gender height_in\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 john     20 m             66\n2 mary     NA f             62\n3 NA       38 f             68\n4 larry    NA NA            NA\n</code></pre>\n<p>I want to do 3 things:</p>\n<ol>\n<li>I want to replace the NA values in columns that are characters with the value \"zz\"</li>\n<li>I want to replace the NA values in columns that are numeric with the value 0</li>\n<li>I want to convert the character columns to factors.</li>\n</ol>\n<p>Here's how I did it in R (again, using the tidyverse package):</p>\n<pre><code>tmp &lt;- df %&gt;%\n  mutate_if(is.character, function(x) ifelse(is.na(x),\"zz\",x)) %&gt;%\n  mutate_if(is.character, as.factor) %&gt;%\n  mutate_if(is.numeric, function(x) ifelse(is.na(x), 0, x))\n</code></pre>\n<p>Here's the output of the dataframe tmp:</p>\n<pre><code>tmp\n\n# A tibble: 4 x 4\n  name    age gender height_in\n  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;\n1 john     20 m             66\n2 mary      0 f             62\n3 zz       38 f             68\n4 larry     0 zz             0\n</code></pre>\n<p>I'm familiar with if() and else() statements within Python. What I don't know is the correct and most readable way of executing the above code within Python. I'm guessing that there is no mutate_if equivalent in the pandas package. My question is what is the similar code that I can use in python that mimics the mutate_if, is.character, is.numeric, and as.factor functions found within tidyverse and R?</p>\n<p>On a side note, I'm not as interested in speed/efficiency of code execution, but rather readability - which is why I really enjoy tidyverse. I would be grateful for any tips or suggestions.</p>\n<p><strong>Edit 1: adding code to create a pandas dataframe</strong></p>\n<p>Here is the code I used to create the dataframe within Python. This may assist others in getting started.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nmy_dict = {\n    'name' : ['john','mary', np.nan, 'larry'],\n    'age' : [20, np.nan, 38,  np.nan],\n    'gender' : ['m','f','f', np.nan],\n    'height_in' : [66, 62, 68, np.nan]\n}\n\ndf = pd.DataFrame(my_dict)\n</code></pre>\n<p>The output of this should be similar:</p>\n<pre><code>print(df)\n    name   age gender  height_in\n0   john  20.0      m       66.0\n1   mary   NaN      f       62.0\n2    NaN  38.0      f       68.0\n3  larry   NaN    NaN        NaN\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Well, after some sleep, I think I have it figured out.</p>\n<p>Here's the code I used to take the pandas dataframe and apply the comparable mutate_if functions I mentioned earlier to get the same results.</p>\n<pre><code># fill in the missing values (similar to mutate_if from tidyverse)\ndf1 = df.select_dtypes(include=['double']).fillna(0)\ndf2 = df.select_dtypes(include=['object']).fillna('zz').astype('category')\n\ndf = pd.concat([df2.reset_index(drop = True), df1], axis = 1)\n\nprint(df)\n    name gender   age  height_in\n0   john      m  20.0       66.0\n1   mary      f   0.0       62.0\n2     zz      f  38.0       68.0\n3  larry     zz   0.0        0.0\n\n# check again for the data types\ndf.dtypes\nname         category\ngender       category\nage           float64\nheight_in     float64\ndtype: object\n</code></pre>\n<p>The catch is that I had to 'break' apart the original dataframe, apply the changes (i.e., fill in the missing values and change data types), and then recombine the columns (i.e., put the data frame back together).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Attempt at an old question; it seems a combination of <code>replace</code>(for the string characters) and <code>fillna</code>(for the numeric) could suffice here:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.replace({None:'zz'}).fillna(0, downcast='infer') \n     name  age gender  height_in\n0   john   20      m         66\n1   mary    0      f         62\n2     zz   38      f         68\n3  larry    0     zz          0\n</code></pre>\n<p>To convert <code>name</code> to categorical dtype, <code>assign</code> or <a href=\"https://pyjanitor-devs.github.io/pyjanitor/\" rel=\"nofollow noreferrer\">pyjanitor</a>'s <a href=\"https://pyjanitor-devs.github.io/pyjanitor/api/functions/#janitor.functions.encode_categorical.encode_categorical\" rel=\"nofollow noreferrer\">encode_categorical</a> are possible options:</p>\n<pre class=\"lang-py prettyprint-override\"><code>(df.replace({None:'zz'})\n   .fillna(0, downcast='infer') \n   .assign(name = lambda df: df.astype('category')\n)\n</code></pre>\n<p>With pyjanitor:</p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install git+https://github.com/pyjanitor-devs/pyjanitor.git\nimport pandas as pd\nimport janitor\n(df.replace({None:'zz'})\n   .fillna(0, downcast='infer') \n   .encode_categorical('name')\n)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have some time series data with three separate colums (Date, Time, kW) that looks like this:</p>\n<pre><code>Date     Time        kW\n3/1/2011 12:15:00 AM 171.36\n3/1/2011 12:30:00 AM 181.44\n3/1/2011 12:45:00 AM 175.68\n3/1/2011 1:00:00 AM 180.00\n3/1/2011 1:15:00 AM 175.68\n</code></pre>\n<p>And reading the csv file directly from Pandas I can parse the Date &amp; Time:</p>\n<pre><code>df= pd.read_csv('C:\\\\Users\\\\desktop\\\\master.csv', parse_dates=[['Date', 'Time']])\n</code></pre>\n<p>Which appears to work nicely, but the problem is I want to create another data frame in Pandas to represent the numerical value of the month. If I do a: </p>\n<pre><code>df['month'] = df.index.month\n</code></pre>\n<p>An error is thrown:</p>\n<p><code>AttributeError: 'Int64Index' object has no attribute 'month'</code></p>\n<p>I am also hoping to create additional dataframes to represent time stampt day, minute, hour... Any tips greatly appreciated..</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use datetime accessor and extract month</p>\n<pre><code>df['month'] = df['Date_Time'].dt.month\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So I currently have a dataframe that looks like:</p>\n<p><img alt=\"Current Dataframe\" src=\"https://i.sstatic.net/QW7hp.png\"/></p>\n<p>And I want to add a completely new column called \"Predictors\" with only one cell that contains an array.  </p>\n<p>So [0, 'Predictors'] should contain an array and everything below that cell in the same column should be empty.</p>\n<p>Here's my attempt, I tried to create a separate dataframe that just contained the \"Predictors\" column, and tried appending it to the current dataframe, but I get: 'Length mismatch: Expected axis has 3 elements, new values have 4 elements.'</p>\n<p>How do I append a single cell containing an array to my dataframe?</p>\n<pre><code># create a list and dataframe to hold the names of predictors\ndataframe=dataframe.drop(['price','Date'],axis=1)  \npredictorsList = dataframe.columns.get_values().tolist()\npredictorsList = np.array(predictorsList, dtype=object)\n\n# Combine actual and forecasted lists to one dataframe\ncombinedResults = pd.DataFrame({'Actual': actual, 'Forecasted': forecasted})\n\npredictorsDF = pd.DataFrame({'Predictors': [predictorsList]})\n\n# Add Predictors to dataframe\n#combinedResults.at[0, 'Predictors'] = predictorsList\npd.concat([combinedResults,predictorsDF], ignore_index=True, axis=1)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could fill the rest of the cells in the desired column with <code>NaN</code>, but they will not \"empty\". To do that, use <code>pd.merge</code> on both indexes:</p>\n<p><strong>Setup</strong></p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n     'Actual': [18.442, 15.4233, 20.6217, 16.7, 18.185], \n     'Forecasted': [19.6377, 13.1665, 19.3992, 17.4557, 14.0053]\n})\n\narr = np.zeros(3)\ndf_arr = pd.DataFrame({'Predictors': [arr]})\n</code></pre>\n<p><strong>Merging df and df_arr</strong></p>\n<pre><code>result = pd.merge(\n    df,\n    df_arr,\n    how='left',\n    left_index=True, # Merge on both indexes, since right only has 0...\n    right_index=True # all the other rows will be NaN\n)\n</code></pre>\n<p><strong>Results</strong></p>\n<pre><code>&gt;&gt;&gt; print(result)\n    Actual  Forecasted       Predictors\n0  18.4420     19.6377  [0.0, 0.0, 0.0]\n1  15.4233     13.1665              NaN\n2  20.6217     19.3992              NaN\n3  16.7000     17.4557              NaN\n4  18.1850     14.0053              NaN\n\n&gt;&gt;&gt; result.loc[0, 'Predictors']\narray([0., 0., 0.])\n\n&gt;&gt;&gt; result.loc[1, 'Predictors'] # actually contains a NaN value\nnan \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You need to change the object type of the column (in your case <code>Predictors</code>) first</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\n\ndf=pd.DataFrame(np.arange(20).reshape(5,4), columns=list('abcd'))\ndf=df.astype(object)  # this line allows the signment of the array\ndf.iloc[1,2] = np.array([99,99,99])\nprint(df)\n</code></pre>\n<p>gives</p>\n<pre><code>    a   b             c   d\n0   0   1             2   3\n1   4   5  [99, 99, 99]   7\n2   8   9            10  11\n3  12  13            14  15\n4  16  17            18  19\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm sorry if the title of the question is not that clear, I could not sum up the problem in one line. </p>\n<p>Here are the simplified datasets for an explanation. Basically, the number of categories in the training set is much larger than the categories in the test set, because of which there is a difference in the number of columns in the test and training set after OneHotEncoding. How can I handle this problem?</p>\n<p><strong>Training Set</strong></p>\n<pre><code>+-------+----------+\n| Value | Category |\n+-------+----------+\n| 100   | SE1      |\n+-------+----------+\n| 200   | SE2      |\n+-------+----------+\n| 300   | SE3      |\n+-------+----------+\n</code></pre>\n<p><strong>Training set after OneHotEncoding</strong></p>\n<pre><code>+-------+-----------+-----------+-----------+\n| Value | DummyCat1 | DummyCat2 | DummyCat3 |\n+-------+-----------+-----------+-----------+\n| 100   | 1         | 0         | 0         |\n+-------+-----------+-----------+-----------+\n| 200   | 0         | 1         | 0         |\n+-------+-----------+-----------+-----------+\n| 300   | 0         | 0         | 1         |\n+-------+-----------+-----------+-----------+\n</code></pre>\n<p><strong>Test Set</strong></p>\n<pre><code>+-------+----------+\n| Value | Category |\n+-------+----------+\n| 100   | SE1      |\n+-------+----------+\n| 200   | SE1      |\n+-------+----------+\n| 300   | SE2      |\n+-------+----------+\n</code></pre>\n<p><strong>Test set after OneHotEncoding</strong></p>\n<pre><code>+-------+-----------+-----------+\n| Value | DummyCat1 | DummyCat2 |\n+-------+-----------+-----------+\n| 100   | 1         | 0         |\n+-------+-----------+-----------+\n| 200   | 1         | 0         |\n+-------+-----------+-----------+\n| 300   | 0         | 1         |\n+-------+-----------+-----------+\n</code></pre>\n<p>As you can notice, the training set after the OneHotEncoding is of shape <code>(3,4)</code> while the test set after OneHotEncoding is of shape <code>(3,3)</code>.\nBecause of this, when I do the following code (<code>y_train</code> is a column vector of shape <code>(3,)</code>)</p>\n<pre><code>from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train, y_train)\n\nx_pred = regressor.predict(x_test)\n</code></pre>\n<p>I get the error at the predict function. As you can see, the dimensions in the error are quite large, unlike the basic examples.</p>\n<pre><code>  Traceback (most recent call last):\n\n  File \"&lt;ipython-input-2-5bac76b24742&gt;\", line 30, in &lt;module&gt;\n    x_pred = regressor.predict(x_test)\n\n  File \"/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\", line 256, in predict\n    return self._decision_function(X)\n\n  File \"/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\", line 241, in _decision_function\n    dense_output=True) + self.intercept_\n\n  File \"/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\", line 140, in safe_sparse_dot\n    return np.dot(a, b)\n\nValueError: shapes (4801,2236) and (4033,) not aligned: 2236 (dim 1) != 4033 (dim 0)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You have to transform the <code>x_test</code> the same way in which x_train was transformed.</p>\n<pre><code>x_test = onehotencoder.transform(x_test)\nx_pred = regressor.predict(x_test)\n</code></pre>\n<p>Make sure use the same <code>onehotencoder</code> object which was used to <code>fit()</code> on x_train.</p>\n<p>I'm assuming that you are currently using fit_transform() on test data.\nDoing <code>fit()</code> or <code>fit_transform()</code> forgets the previously learnt data and re-fits the oneHotEncoder. It will now think that there are only two distinct values present in the column and hence will change the shape of output.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm working on a data science project for my intro to Data Science class, and we've decided to tackle a problem relating to desalination plants in california: \"Where should we place k plants to minimize the distance to zip codes?\" </p>\n<p>The data that we have so far is, zip, city, county, pop, lat, long, amount of water. </p>\n<p>The issue is, I can't find any resources on how to force the centroid to be constrained to staying on the coast. What we've thought of so far is:\nUse a normal kmeans algorithm, but move the centroid to the coast once clusters have settled (bad)\nUse a normal kmeans algorithm with weights, making the coastal zips have infinite weight (I've been told this isn't a great solution)</p>\n<p>What do you guys think? </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>K-means does not minimize distances.</p>\n<p>It minimizes <strong>squared errors</strong>, which is <em>quite</em> different.\nThe difference is roughly that of the median, and the mean in 1 dimensional data. The error can be massive.</p>\n<p>Here is a counter example, assuming we have the coordinates:</p>\n<pre><code>-1 0\n+1 0\n 0 -1\n 0 101\n</code></pre>\n<p>The center chosen by k-means would be 0,25. The optimal location is 0,0.\nThe sum of distances by k-means is &gt; 152, the optimum location has distance 104. So here, the centroid is almost 50% worse than the optimum! But the centroid (= multivariate mean) is what k-means uses!</p>\n<h2>k-means does not minimize the Euclidean distance!</h2>\n<p>This is one variant how \"k-means is sensitive to outliers\".</p>\n<p>It does not get better if you try to constrain it to place \"centers\" on the coast only...</p>\n<p>Also, you may want to at least use Haversine distance, because in California, 1 degree north != 1 degree east, because it's not at the Equator.</p>\n<p>Furthermore, you likely should <em>not</em> make the assumption that every location requires its own pipe, but rather they will be connected like a tree. This greatly reduces the cost.</p>\n<p>I strongly suggest to treat this as a generic optimization problem, rather than k-means. K-means is an optimization too, but it may optimize the wrong function for your problem...</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would approach this by setting possible points that could be centers, i.e. your coastline.<br/>\nI think this is close to <a href=\"https://stackoverflow.com/users/8054875/nathaniel-saul\">Nathaniel Saul's</a> first comment.<br/>\nThis way, for each iteration, instead of choosing a mean, a point out of the possible set would be chosen by proximity to the cluster.   </p>\n<p>I’ve simplified the conditions to only 2 data columns (lon. and lat.) but you should be able to extrapolate the concept. For simplicity, to demonstrate, I based this on code from <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\" rel=\"nofollow noreferrer\">here</a>.</p>\n<h2>In this example, the purple dots are places on the coastline. If I understood correctly, the optimal Coastline locations should look something like this:</h2>\n<p><img alt=\"Coastline Optimum\" src=\"https://raw.githubusercontent.com/Alex-Chervony/kmeans/master/Figure_1.png\" title=\"tooltip\"/></p>\n<h2>See code below:</h2>\n<pre><code>#! /usr/bin/python3.6\n\n# Code based on:\n# https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n##### Simulation START #####\n# Generate possible points.\ndef possible_points(n=20):\n    y=list(np.linspace( -1, 1, n ))\n    x=[-1.2]\n    X=[]\n    for i in list(range(1,n)):\n        x.append(x[i-1]+random.uniform(-2/n,2/n) )\n    for a,b in zip(x,y):\n        X.append(np.array([a,b]))\n    X = np.array(X)\n    return X\n\n# Generate sample\ndef init_board_gauss(N, k):\n    n = float(N)/k\n    X = []\n    for i in range(k):\n        c = (random.uniform(-1, 1), random.uniform(-1, 1))\n        s = random.uniform(0.05,0.5)\n        x = []\n        while len(x) &lt; n:\n            a, b = np.array([np.random.normal(c[0], s), np.random.normal(c[1], s)])\n            # Continue drawing points from the distribution in the range [-1,1]\n            if abs(a) &lt; 1 and abs(b) &lt; 1:\n                x.append([a,b])\n        X.extend(x)\n    X = np.array(X)[:N]\n    return X\n##### Simulation END #####    \n\n# Identify points for each center.\ndef cluster_points(X, mu):\n    clusters  = {}\n    for x in X:\n        bestmukey = min([(i[0], np.linalg.norm(x-mu[i[0]])) \\\n                    for i in enumerate(mu)], key=lambda t:t[1])[0]\n        try:\n            clusters[bestmukey].append(x)\n        except KeyError:\n            clusters[bestmukey] = [x]\n    return clusters\n\n# Get closest possible point for each cluster.\ndef closest_point(cluster,possiblePoints):\n    closestPoints=[]\n    # Check average distance for each point.\n    for possible in possiblePoints:\n        distances=[]\n        for point in cluster:\n            distances.append(np.linalg.norm(possible-point))\n            closestPoints.append(np.sum(distances)) # minimize total distance\n            # closestPoints.append(np.mean(distances))\n    return possiblePoints[closestPoints.index(min(closestPoints))]\n\n# Calculate new centers.\n# Here the 'coast constraint' goes.\ndef reevaluate_centers(clusters,possiblePoints):\n    newmu = []\n    keys = sorted(clusters.keys())\n    for k in keys:\n        newmu.append(closest_point(clusters[k],possiblePoints))\n    return newmu\n\n# Check whether centers converged.\ndef has_converged(mu, oldmu):\n    return (set([tuple(a) for a in mu]) == set([tuple(a) for a in oldmu]))\n\n# Meta function that runs the steps of the process in sequence.\ndef find_centers(X, K, possiblePoints):\n    # Initialize to K random centers\n    oldmu = random.sample(list(possiblePoints), K)\n    mu = random.sample(list(possiblePoints), K)\n    while not has_converged(mu, oldmu):\n        oldmu = mu\n        # Assign all points in X to clusters\n        clusters = cluster_points(X, mu)\n        # Re-evaluate centers\n        mu = reevaluate_centers(clusters,possiblePoints)\n    return(mu, clusters)\n\n\nK=3\nX = init_board_gauss(30,K)\npossiblePoints=possible_points()\nresults=find_centers(X,K,possiblePoints)\n\n# Show results\n\n# Show constraints and clusters\n# List point types\npointtypes1=[\"gx\",\"gD\",\"g*\"]\n\nplt.plot(\n    np.matrix(possiblePoints).transpose()[0],np.matrix(possiblePoints).transpose()[1],'m.'\n    )\n\nfor i in list(range(0,len(results[0]))) :\n    plt.plot(\n        np.matrix(results[0][i]).transpose()[0], np.matrix(results[0][i]).transpose()[1],pointtypes1[i]\n        )\n\npointtypes=[\"bx\",\"yD\",\"c*\"]\n# Show all cluster points\nfor i in list(range(0,len(results[1]))) :\n    plt.plot(\n        np.matrix(results[1][i]).transpose()[0],np.matrix(results[1][i]).transpose()[1],pointtypes[i]\n        )\nplt.show()\n</code></pre>\n<p>Edited to minimize total distance.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a large data set 45421 * 12 (rows * columns) which contains all categorical variables. There are no numerical variables in my dataset. I would like to use this dataset to build unsupervised clustering model, but before modeling I would like to know the best feature selection model for this dataset. \nAnd I am unable to plot elbow curve to this dataset. I am giving range k = 1-1000 in k-means elbow method but it's not giving any optimal clusters plot and taking 8-10 hours to execute. If any one suggests a better solution to this issue it will be a great help.</p>\n<p>Code: </p>\n<pre><code>data = {'UserName':['infuk_tof', 'infus_llk', 'infaus_kkn', 'infin_mdx'], \n       'UserClass':['high','low','low','medium','high'], \n       'UserCountry':['unitedkingdom','unitedstates','australia','india'], \n       'UserRegion':['EMEA','EMEA','APAC','APAC'], \n       'UserOrganization':['INFBLRPR','INFBLRHC','INFBLRPR','INFBLRHC'], \n       'UserAccesstype':['Region','country','country','region']} \n\ndf = pd.DataFrame(data) \n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For categorical data like this, K-means is not the appropriate clustering algorithm. You may want to look for a K-modes method, which unfortunately not currently included in scikit-learn package. You may want to look at this package for kmodes available on github: <a href=\"https://github.com/nicodv/kmodes\" rel=\"nofollow noreferrer\">https://github.com/nicodv/kmodes</a> which follows much of the syntax you're used to from scikit-learn.</p>\n<p>For more, please see the discussion here: <a href=\"https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\">https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataset which includes over 100 countries in it. I want to include these in an XGBoost model to make a classification prediction. I know that One Hot Encoding is the go-to process for this, but I would rather do something that wont increase the dimensionality so much and will be resilient to new values, so I'm trying binary classification using the <code>category_encoders</code> package. <a href=\"http://contrib.scikit-learn.org/categorical-encoding/binary.html\" rel=\"noreferrer\">http://contrib.scikit-learn.org/categorical-encoding/binary.html</a></p>\n<p>Using this encoding helped my model out over using basic one-hot encoding, but how do I get back to the original labels after encoding?</p>\n<p>I know about the <code>inverse_transform</code> method, but that functions on the whole data frame. I need a way where I can put in a binary, or integer value and get back the original value.</p>\n<p>Here's some example data taken from: <a href=\"https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\" rel=\"noreferrer\">https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159</a></p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport category_encoders as ce\n\n# make some data\ndf = pd.DataFrame({\n 'color':[\"a\", \"c\", \"a\", \"a\", \"b\", \"b\"], \n 'outcome':[1, 2, 3, 2, 2, 2]})\n\n# split into X and y\nX = df.drop('outcome', axis = 1)\ny = df.drop('color', axis = 1)\n\n# instantiate an encoder - here we use Binary()\nce_binary = ce.BinaryEncoder(cols = ['color'])\n\n# fit and transform and presto, you've got encoded data\nce_binary.fit_transform(X, y)\n</code></pre>\n<p><img alt=\"output\" src=\"https://cdn-images-1.medium.com/max/800/1*fVZyy09cfDzYQTvILCqtEQ.png\"/></p>\n<p>I'd like to pass the values <code>[0,0,1]</code> or <code>1</code> into a function and get back <code>a</code> as a value.</p>\n<p>The main reason for this is for looking at the feature importances of the model. I can get feature importances based on a column, but this will give me back a column id rather than the underlying value of a category that is the most important.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Please note that the article you reference suggests using the Binary Encoder for ordinal data only - that is, discrete data that has an order associated with it (small, medium, large), not nominal data (Red, White, Blue).</p>\n<p>If you decide to use a Binary encoder, the order in which colors (or countries) are encoded will impact your performance.  For example, assume red=001, white=010, and blue=011. When you apply an ML algorithm, it will see that red and blue have a feature in common (feature 3).  This is probably not what you want.  </p>\n<p>In terms of applying the inverse transformation, you'll need to apply the inverse transformation to [0,0,1] in your example above, not \"1\".  \"1\" is meaningless without context.  You should be able to apply the inverse transformation to a single record (row) in your data, but not a single column.  The inverse scaler will need to will operate on an object with the output dimension of the transformer.  </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I read many documentations related to this, but can't find something what I am looking for.</p>\n<p>I want to plot walking paths between two points. Is it possible? if not, is there any other library in python for this purpose ?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Of course, you can. Use <a href=\"https://python-visualization.github.io/folium/modules.html#folium.vector_layers.PolyLine\" rel=\"noreferrer\"><code>PolyLine</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import folium\n\nm = folium.Map(location=[40.720, -73.993],\n              zoom_start=15)\n\nloc = [(40.720, -73.993),\n       (40.721, -73.996)]\n\nfolium.PolyLine(loc,\n                color='red',\n                weight=15,\n                opacity=0.8).add_to(m)\nm\n</code></pre>\n<p>and you get:</p>\n<p><a href=\"https://i.sstatic.net/Uti8Q.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Uti8Q.jpg\"/></a></p>\n<hr/>\n<p><strong>EDIT 1</strong></p>\n<p>In order to draw a <strong>walking path</strong> between two points, you can use a combination of <a href=\"https://github.com/gboeing/osmnx\" rel=\"noreferrer\"><code>OSMnx</code></a> and <a href=\"https://networkx.github.io/\" rel=\"noreferrer\"><code>networkx</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import osmnx as ox\nimport networkx as nx\n\nox.config(log_console=True,\n          use_cache=True)\n\nG_walk = ox.graph_from_place('Manhattan Island, New York City, New York, USA',\n                             network_type='walk')\n\norig_node = ox.get_nearest_node(G_walk,\n                                (40.748441, -73.985664))\n\ndest_node = ox.get_nearest_node(G_walk,\n                                (40.748441, -73.4))\n\nroute = nx.shortest_path(G_walk, orig_node, dest_node, weight='length')\n\nfig, ax = ox.plot_graph_route(G_walk,\n                              route,\n                              node_size=0,\n                              save=True,\n                              file_format='svg',\n                              filename='test')\n</code></pre>\n<p>and you get:</p>\n<p><a href=\"https://i.sstatic.net/zbLuX.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/zbLuX.png\"/></a></p>\n<hr/>\n<p><strong>EDIT 2</strong></p>\n<p>For a folium-type map you can use <a href=\"https://osmnx.readthedocs.io/en/stable/osmnx.html#osmnx.plot.plot_route_folium\" rel=\"noreferrer\"><code>plot_route_folium</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import osmnx as ox\nimport networkx as nx\n\nox.config(log_console=True, use_cache=True)\n\nG_walk = ox.graph_from_place('Manhattan Island, New York City, New York, USA',\n                             network_type='walk')\n\norig_node = ox.get_nearest_node(G_walk,\n                                (40.748441, -73.985664))\n\ndest_node = ox.get_nearest_node(G_walk,\n                                (40.748441, -73.4))\n\nroute = nx.shortest_path(G_walk,\n                         orig_node,\n                         dest_node,\n                         weight='length')\n\nroute_map = ox.plot_route_folium(G_walk, route)\n\nroute_map.save('route.html')\n</code></pre>\n<p>and you get a useful html file:</p>\n<p><a href=\"https://i.sstatic.net/KckJm.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/KckJm.jpg\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've created a simple linear regression model to predict S&amp;P 500 closing prices. then calculated the Mean Absolute Error (MAE) and got an MAE score of 1290. Now, I don't want to know if this is right or wrong but I want to know what MAE of 1290 is telling me about my model.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>To be honest \"in general\" it tells you nearly nothing. The value is quite arbitrary, and only if you understand exactly your data you can draw any conclusions. </p>\n<p>MAE stands for Mean Absolute Error, thus if yours is 1290 it means, that if you randomly choose a data point from your data, then, you would expect your prediction to be 1290 away from the true value. Is it good? Bad? Depends on the scale of your output. If it is in millions, then the error this big is nothing, and the model is good. If your output values are in the range of thousands, this is horrible. </p>\n<p>If I understand correctly S&amp;P 500 closing prices are numbers between 0 and 2500 (for last 36 years), thus error of 1290 looks like your model learned nothing. This is pretty much like a constant model, always answering \"1200\" or something around this value.  </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>MAE obtained with a model should always be verified against a baseline model.</p>\n<p>A frequently used baseline is median value assignment. Calculate the MAE for the case when all your predictions are always equal to the median of your target variable vector, then see for yourself if your model's MAE is significantly below that. If it is — congrats.</p>\n<p>Note that, in this case the baseline MAE will depend on the target distribution. If your test sample contains lots of instances that are really close to the median, then it will be almost impossible to get a model with a MAE better than the baseline. Thus, MAE should only be used when your test sample is sufficiently diverse. In the extreme case of only 1 instance in the test sample you will get the baseline MAE=0, which will always be no worse than any model you may come up with.</p>\n<p>This issue with MAE is especially notable, when you get a MAE for your total sample and then want to check how it changes across different subsamples. Say, you have a model that predicts yearly income based on education, age, marital status etc. You get a MAE of $1.2k, the baseline MAE is $5k, so you conclude that your model is pretty good. Then you want to check how the model deals with bottom-earners and get a MAE of $1.7k with a baseline of $0.5k. The same is likely to occur, if you inspect the errors in the 18-22yo demographics.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have used a fresh anaconda install to download and install all the required modules for osnmx library but I got the following error:\n<a href=\"https://i.sstatic.net/qNi2K.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/qNi2K.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am the developer of OSMnx. There is a growing amount of misinformation and confusion in this thread, so I will give you a definitive answer.</p>\n<p><strong>Just follow the documented <a href=\"https://osmnx.readthedocs.io/en/stable/#installation\" rel=\"noreferrer\">installation instructions</a></strong> to install the latest release of OSMnx:</p>\n<pre><code>conda config --prepend channels conda-forge\nconda create -n ox --strict-channel-priority osmnx\n</code></pre>\n<p>If you install an old version of pyproj + a new version of OSMnx, or an old version of OSMnx + a new version of pyproj, you will get package conflicts such as the <code>ImportError</code> above. The same problems can crop up if you just conda install it or pip install it without following the documented installation instructions. Required dependency versions can be seen <a href=\"https://github.com/gboeing/osmnx/blob/master/requirements.txt\" rel=\"noreferrer\">here</a>. Make sure you have 64-bit python (anaconda/miniconda) installed. OSMnx is pure Python and thus its installation alone is trivial, <em>but</em>, it depends on geopandas which itself has tricky dependencies to install. If you follow the documented installation instructions, it should all be smooth and seamless.</p>\n<p>To summarize:</p>\n<ul>\n<li><strong>do</strong> just follow the simple <a href=\"https://osmnx.readthedocs.io/en/stable/#installation\" rel=\"noreferrer\">installation instructions</a> in the OSMnx documentation</li>\n<li><strong>don't</strong> just run <code>conda install osmnx</code></li>\n<li><strong>don't</strong> just run <code>pip install osmnx</code></li>\n<li><strong>don't</strong> run <code>pip install -U pyproj psutil</code></li>\n<li><strong>don't</strong> use pip at all unless you have already installed all of OSMnx's dependencies and confirmed they are all working properly (note: this is nontrivial)</li>\n</ul>\n<p>If you follow the installation instructions in the documentation and still have trouble, please report what you did <strong>step by step with full details at the <a href=\"https://github.com/conda-forge/osmnx-feedstock/issues\" rel=\"noreferrer\">feedstock</a></strong> so we can reproduce it and quickly fix it!</p>\n<p>Again, the <a href=\"https://osmnx.readthedocs.io/en/stable/#installation\" rel=\"noreferrer\">installation instructions</a> are in the documentation.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have had the same issue and turned out that it did not like the latest release of osmnx (0.11.3). It could be that that version is unstable as its new (9th January 2020).</p>\n<p>I have sort out the issue by uninstalling the osmnx 0.11.3</p>\n<pre><code>conda uninstall osmnx\n</code></pre>\n<p>and forcing to install the osmnx 0.11 version</p>\n<pre><code>pip install osmnx==0.11\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question needs to be more <a href=\"/help/closed-questions\">focused</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it focuses on one problem only by <a href=\"/posts/41658185/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2017-01-15 21:11:35Z\">7 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/41658185/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I read a csv file, using <code>DictReader</code>.</p>\n<p>I have a list of dictionaries:</p>\n<p>eg:</p>\n<pre><code>a = [{'Name':'A','Class':'1'},{'Name':'B','Class':'1'},{'Name':'C','Class':'2'}]\n</code></pre>\n<p>I want to count the number of entries in the list that have 'Class' == 1.</p>\n<p>Is it possible to do it without a loop?</p>\n<p>EDIT:</p>\n<p>I have tried the following:</p>\n<pre><code>count = 0\nfor k in a:\n    if k['Class'] == '1':\n        count += 1\nprint(count)\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using <a href=\"http://docs.python.org/library/functions#sum\" rel=\"noreferrer\"><code>sum</code></a> with <a href=\"https://docs.python.org/3/tutorial/classes.html#generator-expressions\" rel=\"noreferrer\">generator expression</a>:</p>\n<pre><code>&gt;&gt;&gt; xs = [{'Name':'A','Class':'1'},\n          {'Name':'B','Class':'1'},\n          {'Name':'C','Class':'2'}]\n&gt;&gt;&gt; sum(x.get('Class') == '1' for x in xs)\n2\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Using list comprehension as well, to retrieve dictionaries matching your criteria then calculating its <code>len</code>:</p>\n<pre><code>&gt;&gt;&gt; len([d for d in a if d.get('Class')=='1'])\n2\n</code></pre>\n<p>EDIT: Timing Profile under Python3.5.2</p>\n<pre><code>&gt;&gt;&gt;import timeit\n&gt;&gt;&gt;\n&gt;&gt;&gt; timeit.timeit(stmt=\"len([d for d in a if d.get('Class')=='1'])\", globals={'a':a})\n0.8056499021768104\n&gt;&gt;&gt;\n&gt;&gt;&gt; timeit.timeit(stmt=\"sum(x.get('Class') == '1' for x in a)\", globals={'a':a})\n0.9977147589670494\n&gt;&gt;&gt;\n&gt;&gt;&gt; timeit.timeit(stmt=\"len(list(filter(lambda x: x.get('Class')=='1' , a)))\", globals={'a':a})\n1.3259506113099633\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Sure:</p>\n<pre><code>a = [{'Name':'A','Class':'1'},{'Name':'B','Class':'1'},{'Name':'C','Class':'2'}]\nprint(len(filter(lambda x: x.get('Class')=='1' , a)))\n</code></pre>\n<p>Output:</p>\n<pre><code>2\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to use the group_by() and mutate() functions in sparklyr to concatenate rows in a group.</p>\n<p>Here is a simple example that I think should work but doesn't:</p>\n<pre><code>library(sparkylr)\nd &lt;- data.frame(id=c(\"1\", \"1\", \"2\", \"2\", \"1\", \"2\"), \n             x=c(\"200\", \"200\", \"200\", \"201\", \"201\", \"201\"), \n             y=c(\"This\", \"That\", \"The\", \"Other\", \"End\", \"End\"))\nd_sdf &lt;- copy_to(sc, d, \"d\")\nd_sdf %&gt;% group_by(id, x) %&gt;% mutate( y = paste(y, collapse = \" \"))\n</code></pre>\n<p>What I'd like it to produce is:</p>\n<pre><code>Source: local data frame [6 x 3]\nGroups: id, x [4]\n\n# A tibble: 6 x 3\n      id      x         y\n  &lt;fctr&gt; &lt;fctr&gt;     &lt;chr&gt;\n1      1    200 This That\n2      1    200 This That\n3      2    200       The\n4      2    201 Other End\n5      1    201       End\n6      2    201 Other End\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>Error: org.apache.spark.sql.AnalysisException: missing ) at 'AS' near '' '' in selection target; line 1 pos 42\n</code></pre>\n<p>Note that the using the same code on a data.frame works fine:</p>\n<pre><code>d %&gt;% group_by(id, x) %&gt;% mutate( y = paste(y, collapse = \" \"))\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>Spark sql</code> doesn't like it if you use aggregate functions without aggregating, hence the reason why this works in <code>dplyr</code> with an ordinary <code>dataframe</code> but not in a <code>SparkDataFrame</code>- <code>sparklyr</code> translates your commands to an <code>sql</code> statement. You can observe this going wrong if you look at the second bit in the error message:</p>\n<pre><code>== SQL ==\nSELECT `id`, `x`, CONCAT_WS(' ', `y`, ' ' AS \"collapse\") AS `y`\n</code></pre>\n<p><code>paste</code> gets translated to <code>CONCAT_WS</code>. <code>concat</code> however would paste <em>columns</em> together. </p>\n<p>A better equivalent would be <code>collect_list</code> and <code>collect_set</code>, but they produce <code>list</code> outputs. </p>\n<p>But you can build on that:</p>\n<p>If you do <em>not</em> want to have the same row replicated in your result you can use <code>summarise</code>, <code>collect_list</code>, and <code>paste</code>:</p>\n<pre><code>res &lt;- d_sdf %&gt;% \n      group_by(id, x) %&gt;% \n      summarise( yconcat =paste(collect_list(y)))\n</code></pre>\n<p>result:</p>\n<pre><code>Source:     lazy query [?? x 3]\nDatabase:   spark connection master=local[8] app=sparklyr local=TRUE\nGrouped by: id\n\n     id     x         y\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;\n1     1   201       End\n2     2   201 Other End\n3     1   200 This That\n4     2   200       The\n</code></pre>\n<p>you can join this back onto your original data if you <em>do</em> want to have your rows replicated:</p>\n<pre><code>d_sdf %&gt;% left_join(res)\n</code></pre>\n<p>result:</p>\n<pre><code>Source:     lazy query [?? x 4]\nDatabase:   spark connection master=local[8] app=sparklyr local=TRUE\n\n     id     x     y   yconcat\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;\n1     1   200  This This That\n2     1   200  That This That\n3     2   200   The       The\n4     2   201 Other Other End\n5     1   201   End       End\n6     2   201   End Other End\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am installing layout-parser and following this <a href=\"https://layout-parser.readthedocs.io/en/latest/notes/installation.html\" rel=\"nofollow noreferrer\">link</a>. Did not face any issues with the following packages.  </p>\n<pre><code>pip install layoutparser    \npip install \"layoutparser[effdet]\"    \npip install layoutparser torchvision     \npip install \"layoutparser[paddledetection]\"    \npip install \"layoutparser[ocr]\" \n</code></pre>\n<p>But I am not able to install detectron2</p>\n<pre><code>pip install \"git+https://github.com/facebookresearch/<a class=\"__cf_email__\" data-cfemail=\"771312031214030518194559101e033701475942\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>#egg=detectron2\" \n</code></pre>\n<p>while installing this package I am getting this error    </p>\n<blockquote>\n<p>ERROR: Could not find a version that satisfies the requirement detectron2 (unavailable) (from versions: none)</p>\n<p>ERROR: No matching distribution found for detectron2 (unavailable)</p>\n</blockquote>\n<p>I followed the same installation guide in Google collab and it worked but not able to install them in my Azure workspace.  </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Open a terminal or command prompt.\nCreate a new environment called detectron2-env with the following command:</p>\n<pre><code>conda create --name detectron2-env python==3.9 -y\n</code></pre>\n<p>Activate the environment with the following command:</p>\n<h1>Linux</h1>\n<pre><code>conda activate detectron2-env\n</code></pre>\n<h1>Windows</h1>\n<pre><code>activate detectron2-env\n</code></pre>\n<p>Install the dependencies with the following commands:</p>\n<pre><code>pip3 install torch torchvision torchaudio\ngit clone https://github.com/facebookresearch/detectron2.git\npython -m pip install -e detectron2\n</code></pre>\n<p>For more help, please refer to the following documentation:</p>\n<p><a href=\"https://detectron2.readthedocs.io/en/latest/tutorials/install.html\" rel=\"noreferrer\">https://detectron2.readthedocs.io/en/latest/tutorials/install.html</a></p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Check your python version. If you have Python version as 3.10 or higher you will face this issue. This is because detectron2 library has only wheels for &lt;= Python 3.9.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to build a neural network to predict per-capita-income for counties in US based on the education level of their citizens.\nX and y have the same dtype (I have checked this) but I'm getting an error.\nHere is my data:</p>\n<pre><code>   county_FIPS state          county  per_capita_personal_income_2019  \\\n0        51013    VA   Arlington, VA                            97629   \n\n   per_capita_personal_income_2020  per_capita_personal_income_2021  \\\n0                           100687                           107603    \n\n   associate_degree_numbers_2016_2020  bachelor_degree_numbers_2016_2020  \\\n0                               19573                             132394   \n \n</code></pre>\n<p>And here is my network</p>\n<pre><code>import torch\nimport pandas as pd\ndf = pd.read_csv(\"./input/US counties - education vs per capita personal income - results-20221227-213216.csv\")\nX = torch.tensor(df[[\"bachelor_degree_numbers_2016_2020\", \"associate_degree_numbers_2016_2020\"]].values)\ny = torch.tensor(df[\"per_capita_personal_income_2020\"].values)\n\nX.dtype\ntorch.int64\n\ny.dtype\ntorch.int64\n\nimport torch.nn as nn\nclass BaseNet(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim):\n        super(BaseNet, self).__init__()\n        self.classifier = nn.Sequential(\n        nn.Linear(in_dim, hidden_dim, bias=True), \n        nn.ReLU(), \n        nn.Linear(feature_dim, out_dim, bias=True))\n        \n    def forward(self, x): \n        return self.classifier(x)\n\nfrom torch import optim\nimport matplotlib.pyplot as plt\nin_dim, hidden_dim, out_dim = 2, 20, 1\nlr = 1e-3\nepochs = 40\nloss_fn = nn.CrossEntropyLoss()\nclassifier = BaseNet(in_dim, hidden_dim, out_dim)\noptimizer = optim.SGD(classifier.parameters(), lr=lr)\n\ndef train(classifier, optimizer, epochs, loss_fn):\n    classifier.train()\n    losses = []\n    for epoch in range(epochs):\n        out = classifier(X)\n        loss = loss_fn(out, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        losses.append(loss/len(X))\n        print(\"Epoch {} train loss: {}\".format(epoch+1, loss/len(X)))\n    \n    plt.plot([i for i in range(1, epochs + 1)])\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Training Loss\")\n    plt.show()\n\ntrain(classifier, optimizer, epochs, loss_fn)\n</code></pre>\n<p>Here is the full stack trace of the error that I am getting when I try to train the network:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [77], in &lt;cell line: 39&gt;()\n     36     plt.ylabel(\"Training Loss\")\n     37     plt.show()\n---&gt; 39 train(classifier, optimizer, epochs, loss_fn)\n\nInput In [77], in train(classifier, optimizer, epochs, loss_fn)\n     24 losses = []\n     25 for epoch in range(epochs):\n---&gt; 26     out = classifier(X)\n     27     loss = loss_fn(out, y)\n     28     loss.backward()\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [77], in BaseNet.forward(self, x)\n     10 def forward(self, x): \n---&gt; 11     return self.classifier(x)\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204, in Sequential.forward(self, input)\n    202 def forward(self, input):\n    203     for module in self:\n--&gt; 204         input = module(input)\n    205     return input\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 must have the same dtype\n</code></pre>\n<h1>Updates</h1>\n<p>I have tried casting X and y to float tensors but this comes up with the following error: <code>expected scalar type Long but found Float</code>. If someone who knows PyTorch could try running this notebook for themselves that would be a great help. I'm struggling to get off the ground with Kaggle and ML.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The reason for this is because the parameters dtype of <code>nn.Linear</code> doesn't match your input's dtype; the default dtype for <code>nn.Linear</code> is <code>torch.float32</code> which is in your case different from your input data - <code>float64</code>.</p>\n<p>The solution to <a href=\"https://stackoverflow.com/questions/67456368/pytorch-getting-runtimeerror-found-dtype-double-but-expected-float\">this question</a> solves your problem and explains why @Anonymous answer works.</p>\n<p>In short, add <code>self.double()</code> at the end of your constructor and things should run.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I converted the input to np.float32 which solved a similar problem for me</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Convert tensor array object into float32 if it float64</p>\n<p>Before:</p>\n<pre><code>model(X_train) # float64\n</code></pre>\n<p>After:</p>\n<pre><code>model(X_train.to(torch.float32)) # float32\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I would like to use Google Colab to accelerate the process of calculation of my deep learning model. However, I cannot run the model directly from the .ipynb file in the Google Colaboratory, since it has several .py functions written separately and then the main program will call them together in another .py file.</p>\n<p>One of the proposed solutions consisted of following three steps:</p>\n<ol>\n<li>Commit the code on Github</li>\n<li>Clone on collab</li>\n<li>run this command: !python model_Trainer.py on Colab</li>\n</ol>\n<p>I have done steps <code>1</code> and <code>2</code> successfully, however, I still cannot run the third step. And I am getting the following error:  python3: can't open file 'model_trainer.py': [Errno 2] No such file or directory.</p>\n<p>Does anyone have a solution to this problem?</p>\n<p>I look forward to hearing from you.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When you clone a GitHub repo in Colab, it will create a directory for that repo.\nTo run your python script, you must use <code>%cd</code> before using <code>!python</code>.</p>\n<hr/>\n<p><strong>Example</strong>: <a href=\"https://i.sstatic.net/TvyCx.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/TvyCx.png\"/></a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to extract only numbers and only strings in two different dataframes. I am using regular expression to extract numbers and string.</p>\n<pre><code>import pandas as pd\n\ndf_num = pd.DataFrame({\n        'Colors': ['lila1.5', 'rosa2.5', 'gelb3.5', 'grün4', 'rot5', 'schwarz6', 'grau7', 'weiß8', 'braun9', 'hellblau10'],\n        'Animals': ['hu11nd', '12welpe', '13katze', 's14chlange', 'vo15gel', '16papagei', 'ku17h', '18ziege', '19pferd',\n                    'esel20']\n    })\n\nfor column in df_num.columns:\n    df_num[column] = df_num[column].str.extract('(\\d+)').astype(float)\n\nprint(df_num)\n</code></pre>\n<p>I have also tried using <code>'([\\d+][\\d+\\.\\d+])' and '([\\d+\\.\\d+])'</code></p>\n<p>Here I am getting output but not what I am expecting. Though I am expecting float numbers I am not getting 1.5 or 2.5.</p>\n<p>I am getting something like below image:</p>\n<p><a href=\"https://i.sstatic.net/H8ZIb.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/H8ZIb.png\"/></a></p>\n<pre><code>df_str = pd.DataFrame({\n        'Colors': ['lila1.5', 'rosa2.5', 'gelb3', 'grün4', 'rot5', 'schwarz6', 'grau7', 'weiß8', 'braun9', 'hellblau10'],\n        'Animals': ['hu11nd', '12welpe', '13katze', 's14chlange', 'vo15gel', '16papagei', 'ku17h', '18ziege', '19pferd',\n                    'esel20']\n    })\n\nfor column in df_str.columns:\n    df_str[column] = df_str[column].str.extract('([a-zA-Z]+)')\n\nprint(df_str)\n</code></pre>\n<p>In this case when the number is at the end or in the beginning then I am getting the string but if the number placed in the middle or any other place then the result which I expect I am not getting.\nCurrent output is like below image:</p>\n<p><a href=\"https://i.sstatic.net/QggiE.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/QggiE.png\"/></a></p>\n<p>I think my regular expression is not correct. Which will be the right regular expression to solve these problems? Or is there any other way to extract only numbers and only strings in pandas dataframe?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your code is on the right track, you just need to account for the decimals and the possibility of integers : </p>\n<pre><code>df_num['colors_num'] = df_num.Colors.str.extract(r'(\\d+[.\\d]*)')\ndf_num['animals_num'] = df_num.Animals.str.extract(r'(\\d+[.\\d]*)')\ndf_num['colors_str'] = df_num.Colors.str.replace(r'(\\d+[.\\d]*)','')\ndf_num['animals_text'] = df_num.Animals.str.replace(r'(\\d+[.\\d]*)','')\n\n\n    Colors  Animals colors_num  animals_num colors_str  animals_text\n0   lila1.5 hu11nd  1.5 11  lila    hund\n1   rosa2.5 12welpe 2.5 12  rosa    welpe\n2   gelb3.5 13katze 3.5 13  gelb    katze\n3   grün4   s14chlange  4   14  grün    schlange\n4   rot5    vo15gel 5   15  rot vogel\n5   schwarz6    16papagei   6   16  schwarz papagei\n6   grau7   ku17h   7   17  grau    kuh\n7   weiß8   18ziege 8   18  weiß    ziege\n8   braun9  19pferd 9   19  braun   pferd\n9   hellblau10  esel20  10  20  hellblau    esel\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The easiest way to go about this is to define some functions:</p>\n<pre><code>def text(x):\n    return x.str.replace(r'[0-9.]+','')\ndef values(x):\n    return x.str.extract(r'([0-9.]+)', expand = False)\n\ndf_str.transform([text,values])\n\n          Colors          Animals       \n       text values      text values\n0      lila    1.5      hund     11\n1      rosa    2.5     welpe     12\n2      gelb      3     katze     13\n3      grün      4  schlange     14\n4       rot      5     vogel     15\n5   schwarz      6   papagei     16\n6      grau      7       kuh     17\n7      weiß      8     ziege     18\n8     braun      9     pferd     19\n9  hellblau     10      esel     20\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could use <code>(\\d+\\.\\d+|\\d+)</code> to <code>extract</code> your numbers, and <code>replace</code> the results with <code>\"\"</code> to get your string.</p>\n<pre><code>print (df_num.assign(colors_num=df_num[\"Colors\"].str.extract(r\"(\\d+\\.\\d+|\\d+)\"))\n             .assign(colors_col=df_num[\"Colors\"].str.replace(r\"(\\d+\\.\\d+|\\d+)\",\"\")))\n\n       Colors     Animals colors_num colors_col\n0     lila1.5      hu11nd        1.5       lila\n1     rosa2.5     12welpe        2.5       rosa\n2     gelb3.5     13katze        3.5       gelb\n3       grün4  s14chlange          4       grün\n4        rot5     vo15gel          5        rot\n5    schwarz6   16papagei          6    schwarz\n6       grau7       ku17h          7       grau\n7       weiß8     18ziege          8       weiß\n8      braun9     19pferd          9      braun\n9  hellblau10      esel20         10   hellblau\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>Closed</b>. This question is <a href=\"/help/closed-questions\">opinion-based</a>. It is not currently accepting answers.\n                                \n                            </div>\n</div>\n</div>\n</div>\n</div>\n<hr class=\"my12 outline-none baw0 bb bc-blue-400\"/>\n<div class=\"fw-nowrap fc-black-500\">\n<div class=\"d-flex fd-column lh-md\">\n<div class=\"mb0 d-flex\">\n<div class=\"flex--item mr8\">\n<svg aria-hidden=\"true\" class=\"svg-icon iconLightbulb\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\"><path d=\"M15 6.38A6.5 6.5 0 0 0 7.78.04h-.02A6.5 6.5 0 0 0 2.05 5.6a6.3 6.3 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.3 6.3 0 0 0 15 6.37M4.03 5.85A4.5 4.5 0 0 1 8 2.02a4.5 4.5 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.3 4.3 0 0 1-1.64-3.94M10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2z\"></path></svg>\n</div>\n<p><b>Want to improve this question?</b> Update the question so it can be answered with facts and citations by <a href=\"/posts/39653769/edit\">editing this post</a>.</p>\n</div>\n<div class=\"mb0 mt6 d-flex\">\n<p class=\"ml24 pl2\">Closed <span class=\"relativetime\" title=\"2016-09-23 07:29:16Z\">8 years ago</span>.</p>\n</div>\n<div class=\"ml24 pl2\">\n</div>\n</div>\n</div>\n<div class=\"mt24 d-flex gsx gs8\">\n<a class=\"s-btn s-btn__outlined flex--item js-post-notice-edit-post\" href=\"/posts/39653769/edit\">\n                        Improve this question\n                    </a>\n</div>\n</aside>\n</div>\n<p>I have only experience in <b>RDBMS PostgresSQL</b> only But Im new to <b>Apache Spark and MongoDB</b>.<br/> So im having following confusions please me <br/><br/></p>\n<p>1) What is difference  between Apache Spark SQL and MongoDB?<br/>\n2) What kind of places/scenarios/domains i need to use SparkSQL or MongoDB or combined manner?<br/>\n3) Apache Spark is replace of like mondoDB,cassandra... ?<br/>\n4) Im having multiple terabyte of data's  in MongoDB from that I want do data analytics and then need to provide reports.<br/>\n<br/>So please share me your knowledge and give me your inputs \n<br/><br/>\nRegards<br/>\nShankar S</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>1)</strong>\n<strong>Apache Spark</strong>:\n Apache Spark for doing Parallel Computing Operations on Big Data in SQL queries. </p>\n<p><strong>MongoDB</strong>:\n MongoDB is a document Store and essentially is a database so cannot be compared with Spark which is a computing engine and not a store.</p>\n<p><strong>2)</strong>\n SparkSQL can be ideal for processing Structure Data imported in the Spark Cluster where you have millions of data available for big computing. \n Mongodb can be use where you need NoSQL functionalities(It has full NoSQL Capabilities, compare to SparkSQL).</p>\n<p><strong>3)</strong>\n No Apache Spark is use for different purpose , you can not replace it with mondoDB,cassandra.It is like computing engine to give you predict results on  <code>large data sets</code></p>\n<p><strong>4)</strong>\nUse third party service like SLAM DATA <a href=\"http://slamdata.com/\" rel=\"noreferrer\">http://slamdata.com/</a> to apply mongodb analytics also use \nspark data-frame to read in the data from MongoDB </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>These are two different and broad questions but i am making attempt to answer:-</p>\n<p>1) What is difference between Apache Spark SQL and MongoDB?</p>\n<p>Spark SQL is a library provided by Apache Spark for doing Parallel Computing Operations on Big Data in SQL queries.\n   MongoDB is a document Store and essentially is a database so cannot be compared with Spark which is a computing engine and not a store.</p>\n<p>2) What kind of places/scenarios/domains i need to use SparkSQL or MongoDB or combined manner?</p>\n<p>SparkSQL can be ideal for processing Structure Data imported in the Spark Cluster.\n   Mongodb can be ideal where you need NoSQL functionalities(It has full NoSQL Capabilities, compare to SparkSQL) </p>\n<p>3) Apache Spark is replace of like mondoDB,cassandra... ?</p>\n<p>Not Exactly since they are in different scope.\n   Apache Spark is not replacing but can be called out as successor to Map-reduce for parallel computing on big datsets.</p>\n<p>4) Im having multiple terabyte of data's in MongoDB from that I want do data analytics and then need to provide reports.</p>\n<p>Use spark dataframe to read in the data from MongoDB using jdbc driver and then you can run some Spark SQL queries on the dataframe and then you can use other visualization tools like pyplot to generate reports.</p>\n<p>Thanks,</p>\n<p>Charles.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to centre and normalise a data set in python with the following code</p>\n<pre><code>mean = np.mean(train, axis=0)\nstd = np.std(train, axis=0)\nnorm_train = (train - mean) / std\n</code></pre>\n<p>The problem is that I get a devision by zero error. Two of the values in the data set end up having a zero std. The data set if of shape (3750, 55). My stats skills are not so strong so I'm not sure how to overcome this. Any suggestions?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Since the <a href=\"https://en.wikipedia.org/wiki/Standard_deviation\" rel=\"noreferrer\">standard deviation</a> is calculated by taking the sum of the <em>squared</em> deviations from the mean, a zero standard deviation can only be possible when all the values of a variable are the same (all equal to the mean). In this case, those variables have no discriminative power so they can be removed from the analysis. They cannot improve any classification, clustering or regression task. Many implementations will do it for you or throw an error about a matrix calculation.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One standard is to include an epsilon variable that prevents divide by zero. In theory, it is not needed because it doesn't make logical sense to do such calculations. In reality, machines are just calculators and divide by zero becomes either NaN or +/-Inf.</p>\n<p>In short, define your function like this:</p>\n<pre><code>def z_norm(arr, epsilon=1e-100):\n    return (arr-arr.mean())/(arr.std()+epsilon)  \n</code></pre>\n<p>This assumes a 1D array, but it would be easy to change to row-wise or column-wise calculation of a 2D array.</p>\n<p>Epsilon is an intentional error added to calculations to prevent creating NaN or Inf. In the case of Inf, you will still end up with numbers that are really large, but later calculations will not propagate Inf and may still retain some meaning.</p>\n<p>The value of 1/(1 x 10^100) is  incredibly small and will not change your result much. You can go down to 1e-300 or so if you want, but you risk hitting the lowest precision value after further calculation. Be aware of the precision you use and the smallest precision it can handle. I was using float64.</p>\n<p><strong>Update 2021-11-03</strong>: Adding test code. The objective of this epsilon is to minimize damage and remove the chance of random NaNs in your data pipeline. Setting epsilon to a positive value fixes the problem.</p>\n<pre><code>for arr in [\n        np.array([0,0]),\n        np.array([1e-300,1e-300]),\n        np.array([1,1]),\n        np.array([1,2])\n    ]:\n    for epi in [1e-100,0,1e100]:\n        stdev = arr.std()\n        mean = arr.mean()\n        result = z_norm(arr, epsilon=epi)\n        print(f' z_norm(np.array({str(arr):&lt;21}),{epi:&lt;7}) ### stdev={stdev}; mean={mean:&lt;6}; becomes --&gt; {str(result):&lt;19} (float-64) --&gt; Truncate to 32 bits. =', result.astype(np.float32))\n\nz_norm(np.array([0 0]                ),1e-100 ) ### stdev=0.0; mean=0.0   ; becomes --&gt; [0. 0.]             (float-64) --&gt; Truncate to 32 bits. = [0. 0.]\nz_norm(np.array([0 0]                ),0      ) ### stdev=0.0; mean=0.0   ; becomes --&gt; [nan nan]           (float-64) --&gt; Truncate to 32 bits. = [nan nan]\nz_norm(np.array([0 0]                ),1e+100 ) ### stdev=0.0; mean=0.0   ; becomes --&gt; [0. 0.]             (float-64) --&gt; Truncate to 32 bits. = [0. 0.]\nz_norm(np.array([1.e-300 1.e-300]    ),1e-100 ) ### stdev=0.0; mean=1e-300; becomes --&gt; [0. 0.]             (float-64) --&gt; Truncate to 32 bits. = [0. 0.]\nz_norm(np.array([1.e-300 1.e-300]    ),0      ) ### stdev=0.0; mean=1e-300; becomes --&gt; [nan nan]           (float-64) --&gt; Truncate to 32 bits. = [nan nan]\nz_norm(np.array([1.e-300 1.e-300]    ),1e+100 ) ### stdev=0.0; mean=1e-300; becomes --&gt; [0. 0.]             (float-64) --&gt; Truncate to 32 bits. = [0. 0.]\nz_norm(np.array([1 1]                ),1e-100 ) ### stdev=0.0; mean=1.0   ; becomes --&gt; [0. 0.]             (float-64) --&gt; Truncate to 32 bits. = [0. 0.]\nz_norm(np.array([1 1]                ),0      ) ### stdev=0.0; mean=1.0   ; becomes --&gt; [nan nan]           (float-64) --&gt; Truncate to 32 bits. = [nan nan]\nz_norm(np.array([1 1]                ),1e+100 ) ### stdev=0.0; mean=1.0   ; becomes --&gt; [0. 0.]             (float-64) --&gt; Truncate to 32 bits. = [0. 0.]\nz_norm(np.array([1 2]                ),1e-100 ) ### stdev=0.5; mean=1.5   ; becomes --&gt; [-1.  1.]           (float-64) --&gt; Truncate to 32 bits. = [-1.  1.]\nz_norm(np.array([1 2]                ),0      ) ### stdev=0.5; mean=1.5   ; becomes --&gt; [-1.  1.]           (float-64) --&gt; Truncate to 32 bits. = [-1.  1.]\nz_norm(np.array([1 2]                ),1e+100 ) ### stdev=0.5; mean=1.5   ; becomes --&gt; [-5.e-101  5.e-101] (float-64) --&gt; Truncate to 32 bits. = [-0.  0.]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You could just replace the 0 std to 1 for that feature. This would basically mean that the scaled value would be zero for all the data points for that feature. This makes sense as this implies that the feature values do not deviate even a bit form the mean(as the values is constant, the constant is the mean.)</p>\n<p>FYI- This is what sklearn does!\n<a href=\"https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/preprocessing/data.py#L70\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/preprocessing/data.py#L70</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For a particular prediction problem, I observed that a certain variable ranks high in the XGBoost feature importance that gets generated (on the basis of Gain) while it ranks quite low in the SHAP output.</p>\n<p>How to interpret this? As in, is the variable highly important or not that important for our prediction problem?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Impurity-based importances (such as sklearn and xgboost built-in routines) summarize the overall usage of a feature by the tree nodes. This naturally gives more weight to high cardinality features (more feature values yield more possible splits), while gain may be affected by tree structure (node order matters even though predictions may be same). There may be lots of splits with little effect on the prediction or the other way round (many splits diluting the average importance) - see <a href=\"https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\" rel=\"noreferrer\">https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27</a> and <a href=\"https://www.actuaries.digital/2019/06/18/analytics-snippet-feature-importance-and-the-shap-approach-to-machine-learning-models/\" rel=\"noreferrer\">https://www.actuaries.digital/2019/06/18/analytics-snippet-feature-importance-and-the-shap-approach-to-machine-learning-models/</a> for various mismatch examples.</p>\n<p>In an oversimplified way:</p>\n<ul>\n<li>impurity-base importance explains the feature usage for generalizing on the train set;</li>\n<li>permutation importance explains the contribution of a feature to the model accuracy;</li>\n<li>SHAP explains how much would changing a feature value affect the prediction (not necessarily correct).</li>\n</ul>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Looking to get a continuous rolling mean of a dataframe. </p>\n<p>df looks like this </p>\n<pre><code>index price\n\n0      4\n1      6\n2      10\n3      12\n</code></pre>\n<p>looking to get a continuous rolling of price</p>\n<p>the goal is to have it look this a moving mean of all the prices.</p>\n<pre><code>index price  mean\n\n0      4      4\n1      6      5\n2      10     6.67\n3      12     8 \n</code></pre>\n<p>thank you in advance!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can use expanding:</p>\n<pre><code>df['mean'] = df.price.expanding().mean()\n\ndf\nindex   price   mean\n0       4       4.000000\n1       6       5.000000\n2       10      6.666667\n3       12      8.000000\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Welcome to SO: Hopefully people will soon remember you from prior SO posts, such as this one. </p>\n<p>From your example, it seems that @Allen has given you code that produces the answer in your table. That said, this isn't exactly the same as a \"rolling\" mean. The <code>expanding()</code> function Allen uses is taking the mean of the first row divided by n (which is 1), then adding rows 1 and 2 and dividing by n (which is now 2), and so on, so that the last row is (4+6+10+12)/4 = 8. </p>\n<p>This last number could be the answer if the window you want for the rolling mean is 4, since that would indicate that you want a mean of 4 observations. However, if you keep moving forward with a window size 4, and start including rows 5, 6, 7... then the answer from <code>expanding()</code> might differ from what you want. In effect, <code>expanding()</code> is recording the mean of the entire series (<code>price</code> in this case) as though it were receiving a new piece of data at each row. \"Rolling\", on the other hand, gives you a result from an aggregation of some window size. </p>\n<p>Here's another option for doing rolling calculations: the <code>rolling()</code> method in a <code>pandas.dataframe</code>. </p>\n<p>In your case, you would do: </p>\n<pre><code>df['rolling_mean'] = df.price.rolling(4).mean()\n\ndf\nindex   price   rolling_mean\n0       4       nan\n1       6       nan\n2       10      nan\n3       12      8.000000\n</code></pre>\n<p>Those <code>nan</code>s are a result of the windowing: until there are enough rows to calculate the mean, the result is <code>nan</code>. You could set a smaller window: </p>\n<pre><code>df['rolling_mean'] = df.price.rolling(2).mean()\n\ndf\nindex   price   rolling_mean\n0       4       nan\n1       6       5.000000\n2       10      8.000000\n3       12      11.00000\n</code></pre>\n<p>This shows the reduction in the <code>nan</code> entries as well as the rolling function: it 's only averaging within the size-two window you provided. That results in a different <code>df['rolling_mean']</code> value than when using <code>df.price.expanding()</code>.</p>\n<p>Note: you can get rid of the <code>nan</code> by using <code>.rolling(2, min_periods = 1)</code>, which tells the function the minimum number of defined values within a window that have to be present to calculate a result. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want to count the number of unique values in each column and select only those columns which have less than 32 unique values. </p>\n<p>I tried using \n<code>df.filter(nunique&lt;32)</code>\n and </p>\n<pre><code>df[[ c for df.columns in df if c in c.nunique&lt;32]] \n</code></pre>\n<p>but because nunique is a method and not function they don't work. Thought len(set() would work and tried </p>\n<pre><code>df.apply(lambda x : len(set(x))\n</code></pre>\n<p>but doesn't work as well. Any ideas please? thanks in advance!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>nunique</code> can be called on the entire DataFrame (you have to call it). You can then filter out columns using <code>loc</code>:</p>\n<pre><code>df.loc[:, df.nunique() &lt; 32]\n</code></pre>\n<hr/>\n<p><strong>Minimal Verifiable Example</strong></p>\n<pre><code>df = pd.DataFrame({'A': list('abbcde'), 'B': list('ababab')})\ndf\n   A  B\n0  a  a\n1  b  b\n2  b  a\n3  c  b\n4  d  a\n5  e  b\n\ndf.nunique()\nA    5\nB    2\ndtype: int64\n\ndf.loc[:, df.nunique() &lt; 3]\n   B\n0  a\n1  b\n2  a\n3  b\n4  a\n5  b\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The number of packages offered by CRAN changes every day. I am trying to programmatically calculate the number of packages with the following R code :</p>\n<pre><code>nrow(installed.packages())\n</code></pre>\n<p>however this code only returns the number of packages currently installed in the system. How can I calculate the number of all packages?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>&gt; nrow(available.packages())\n[1] 13429\n&gt; Sys.Date()\n[1] \"2018-11-29\"\n</code></pre>\n<p>You can find out about <code>available.packages</code> (and other related functions) in the <em>See Also</em> section of <code>?installed.packages</code>.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can accurately calculate the number of packages offered by CRAN at anytime using the web scraping code :</p>\n<pre><code>library(rvest)\n\npkgs &lt;- read_html(\"https://cran.r-project.org/web/packages/available_packages_by_name.html\")\nmylines &lt;- pkgs %&gt;% \n    html_nodes(\"tr\") %&gt;%\n    xml_text()\n\nnb_pkgs &lt;- length(which(sapply(mylines, nchar)&gt;5))\nprint(paste(\"There are\", nb_pkgs, \"packages available in CRAN as of\", Sys.Date()))\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>What is the difference between fit_transform and transform?\nWhy doesn't transform directly works?</p>\n<pre><code>from sklearn.preprocessing import StandardScaler\n\n X_scaler = StandardScaler()\n X_train = X_scaler.fit_transform(X_train)\n X_test = X_scaler.transform(X_test)\n</code></pre>\n<p>If directly transformed it gives the below error</p>\n<blockquote>\n<p>NotFittedError: This StandardScaler instance is not fitted yet. Call\n  'fit' with appropriate arguments before using this method.</p>\n</blockquote>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>StandardScaler</code>, as <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" rel=\"noreferrer\">per documentation</a>:</p>\n<blockquote>\n<p>Standardize features by removing the mean and scaling to unit variance</p>\n</blockquote>\n<p>So it needs to somehow first know about the mean and variance of your data.\nSo <code>fit()</code> or <code>fit_transform()</code> is needed so that <code>StandardScaler</code> can go through all of your data to find the mean and variance. Those can be accessed \nby attributes:</p>\n<blockquote>\n<p><strong>mean_</strong> : The mean value for each feature (column) in the training set.</p>\n<p><strong>var_</strong> : The variance for each feature in the training set. </p>\n</blockquote>\n<p>Note that those will be calculated separately for each column in the data. </p>\n<p>In <code>transform()</code>, it will just use those <code>mean</code> and <code>variance</code> values to scale the data.</p>\n<p>Now you might say that why just it don't calculate those attributes during <code>transform()</code>. This is done so that the test data is scaled in the same way as a training data is scaled (from <code>fit_transform()</code>). If you calculate mean and variance of data in each call to <code>transform()</code>, then all passed data will have different scale, which is not what you want.</p>\n<p>This is true for all scikit transformers. </p>\n<p>1) <code>fit()</code> - Will only go through the data and save all needed attributes of data</p>\n<p>2) <code>transform()</code> -  Use the saved attributes from <code>fit()</code> to change the data</p>\n<p>3) <code>fit_transform()</code> - Utility function to <code>fit()</code> and then <code>transform()</code> the same data.</p>\n<p>Usually you would call <code>fit_transform()</code> on training data, and only <code>transform()</code> on test data.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You will need to remeber the mean_a nd scale_ as follows:</p>\n<p>You can print the mean_and scale_ after fitting the model.</p>\n<pre><code>scalerModel = StandardScaler()\nX = scalerModel.fit_transform(X)\nprint(\"scalerModel mean: \", **scaler**.mean_)\nprint(\"**Scaler** scale: \", **scaler**.scale_)\n</code></pre>\n<p>Sample values are as follows:</p>\n<pre><code>scalerModel mean: [ 9.978054e+05, 1.904232e+01,5.000918e+01] \nscalerModel scale: [ 5.75234545e+05,1.15379326e+01,2.83283511e+01]\n</code></pre>\n<p>When you need you Scaler again, i.e for predicting (scaler1 is the new scaler to be sure not to use the old one):\nAnd when you need the scaler after loading the model or by any other mean, create a new scaler and set the mean_ and scale_ values as recovered from above.</p>\n<pre><code>scaler1 = StandardScaler()\nscaler1.mean_ = np.array([ 9.978054e+05, 1.904232e+01,5.000918e+01])\nscaler1.scale_ = np.array([ 5.75234545e+05,1.15379326e+01,2.83283511e+01]) \n\n# then use it to transform your data\nX = scaler1.transform(X)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Question:</strong> </p>\n<p>How can you use R to remove all special characters from a dataframe, quickly and efficiently? </p>\n<p><strong>Progress:</strong></p>\n<p>This SO <a href=\"https://stackoverflow.com/questions/21641522/how-to-remove-specific-special-characters-in-r/21641569\">post</a> details how to remove special characters. I can apply the gsub function to single columns (images 1 and 2), but not the entire dataframe.</p>\n<p><strong>Problem:</strong></p>\n<p>My dataframe consists of 100+ columns of integers, string, etc. When I try to run the gsub on the dataframe, it doesn't return the output I desire. Instead, I get what's shown in image 3.</p>\n<pre><code>df &lt;- read.csv(\"C:/test.csv\")\ndfa &lt;- gsub(\"[[:punct:]]\", \"\", df$a) #this works on a single column\ndfb &lt;- gsub(\"[[:punct:]]\", \"\", df$b) #this works on a single column\ndf_all &lt;- gsub(\"[[:punct:]]\", \"\", df) #this does not work on the entire df\nView(df_all)\n</code></pre>\n<p><strong>df</strong> - This is the original dataframe:</p>\n<p><a href=\"https://i.sstatic.net/z3EaQ.png\" rel=\"noreferrer\"><img alt=\"Original dataframe\" src=\"https://i.sstatic.net/z3EaQ.png\"/></a></p>\n<p><strong>dfa</strong> - This is gsub applied to column b. Good!</p>\n<p><a href=\"https://i.sstatic.net/EE1fV.png\" rel=\"noreferrer\"><img alt=\"gsub applied to column b\" src=\"https://i.sstatic.net/EE1fV.png\"/></a></p>\n<p><strong>df_all</strong> - This is gsub applied to the entire dataframe. Bad!</p>\n<p><a href=\"https://i.sstatic.net/Ciec5.png\" rel=\"noreferrer\"><img alt=\"gsub applied to entire dataframe\" src=\"https://i.sstatic.net/Ciec5.png\"/></a></p>\n<p><strong>Summary:</strong></p>\n<p>Is there a way to gsub an entire dataframe? Else, should an apply function be used instead?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a possible solution using dplyr:</p>\n<pre><code># Example data\nbla &lt;- data.frame(a = c(1,2,3), \n              b = c(\"fefa%^%\", \"fes^%#$%\", \"gD%^E%Ewfseges\"), \n              c = c(\"%#%$#^#\", \"%#$#%@\", \",.,gdgd$%,.\"))\n\n# Use mutate_all from dplyr\nbla %&gt;%\n  mutate_all(funs(gsub(\"[[:punct:]]\", \"\", .)))\n\n  a           b    c\n1 1        fefa     \n2 2         fes     \n3 3 gDEEwfseges gdgd\n</code></pre>\n<hr/>\n<h2>Update:</h2>\n<p><code>mutate_all</code> has been <a href=\"https://search.r-project.org/CRAN/refmans/dplyr/html/mutate_all.html\" rel=\"nofollow noreferrer\">superseded</a>, and <code>funs</code> is <a href=\"https://dplyr.tidyverse.org/reference/funs.html#:%7E:text=funs()%20is%20deprecated%3B%20please,other%20packages%20in%20the%20tidyverse.\" rel=\"nofollow noreferrer\">deprecated</a> as of dplyr 0.8.0. Here is an updated solution using <code>mutate</code> and <code>across</code>:</p>\n<pre><code># Example data\ndf &lt;- data.frame(a = c(1,2,3), \n                 b = c(\"fefa%^%\", \"fes^%#$%\", \"gD%^E%Ewfseges\"), \n                 c = c(\"%#%$#^#\", \"%#$#%@\", \",.,gdgd$%,.\"))\n\n# Use mutate_all from dplyr\ndf %&gt;%\n  mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .x)))\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Another solution is to convert the data frame to a matrix first then run the gsub and then convert back to a data frame as follows:</p>\n<pre><code>as.data.frame(gsub(\"[[:punct:]]\", \"\", as.matrix(df))) \n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I like Ryan's answer using dplyr. As <code>mutate_all</code> and <code>funs</code> are now deprecated, here is my suggested updated solution using <code>mutate</code> and <code>across</code>:</p>\n<pre><code># Example data\ndf &lt;- data.frame(a = c(1,2,3), \n                 b = c(\"fefa%^%\", \"fes^%#$%\", \"gD%^E%Ewfseges\"), \n                 c = c(\"%#%$#^#\", \"%#$#%@\", \",.,gdgd$%,.\"))\n\n# Use across() from dplyr\ndf %&gt;%\n  mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .x)))\n\n  a           b    c\n1 1        fefa     \n2 2         fes     \n3 3 gDEEwfseges gdgd\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code>n_level = range(1, steps + 2)\n</code></pre>\n<p><code>steps</code> is user input, using multi-index dataframe</p>\n<pre><code>    df = {'crest': [754, 755, 762, 785], 'trough': [752, 725, 759, 765], 'L1T': [761, 761, 761, 761], 'L2T': [772, 772, 772, 772], 'L3T': [783, 783, 783, 783], 'L4T': [794, 794, 794, 794], 'L1M': [740, 740, 740, 740], 'L2M': [729, 729, 729, 729], 'L3M': [718, 718, 718, 718], 'L4T': [707, 707, 707, 707]}\n\n\n    for i in n_level:\n        if df['Crest'] &gt;= df[f'L{i}T']:\n            df['Marker'] = i\n        elif df['Trough'] &lt;= df[f'L{i}M']:\n            df['Marker'] = -i\n        else:\n            df['Marker'] = 0\n</code></pre>\n<p>I am expecting below df with the col marker</p>\n<pre><code>    df = {'crest': [754, 755, 762, 785], 'trough': [752, 725, 759, 765], 'L1T': [761, 761, 761, 761], 'L2T': [772, 772, 772, 772], 'L3T': [783, 783, 783, 783], 'L4T': [794, 794, 794, 794], 'L1M': [740, 740, 740, 740], 'L2M': [729, 729, 729, 729], 'L3M': [718, 718, 718, 718], 'L4T': [707, 707, 707, 707], 'Marker': [0, -2, 1, 3]}, \n</code></pre>\n<p>The if statement is returning True or False, using that can we convert it to the ith Value (+/-)</p>\n<p>what I need is another col df['Marker'], which will measure if the crest or trough crossed L{i}T or L{i}M, any breach on the upside in case of the crest and breach on the downside in case of the trough</p>\n<p><a href=\"https://i.sstatic.net/0YRDZ.png\" rel=\"noreferrer\"><img alt=\"Sample Dataframe\" src=\"https://i.sstatic.net/0YRDZ.png\"/></a></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>this can be achieved easily using binary search, there are many ways to apply that(NumPy, bisect).\nI would recommend the library bisect.</p>\n<p>Added Buu for the Crest and See for the Trough, so that code and differentiate the segments. You can choose anything</p>\n<pre><code>def marker_up(row):\n    n_row = [row[col] for col in columns if \"Buu\" in col]\n    n_high = row['Crest']\n    if n_high in n_row:\n        q = bs.bisect_left(n_row, n_high)\n        q = max(0, q)\n        return q\n    else:\n        q_mod = bs.bisect_left(n_row, n_high)\n        q_mod -= 1\n        q_mod = max(0, q_mod)\n        return q_mod\n\nMarker_up = df.apply(marker_up, axis=1)\n\ndef marker_down(row):\n    n_row = [row[col] for col in columns if \"See\" in col]\n    n_low = row['Trough']\n    if n_low in n_row:\n        q = bs.bisect_left(n_row, n_low)\n        q = q - len(n_row)\n        return q\n    else:\n        q_mod = bs.bisect_left(n_row, n_low)\n        q_mod = q_mod - len(n_row)\n        return q_mod\n\nMarker_down = df.apply(marker_down, axis=1)\ndf['Marker'] = Marker_up + Marker_down\n\nreturn df\n</code></pre>\n<p>Don't use iloc, dict or iterrrows if you have large df. A vectorized form will help to run code faster.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can you Numpy searchsorted\nthe list should be an array of values for the row.</p>\n<pre><code>df['Marker'] = np.searchsorted(df['Crest'], list)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am working on the titanic kaggle competition, to deal with categorical data I’ve splited the data into 2 sets: one for numerical variables and the other for categorical variables.\nAfter working with sklearn one hot encoding on the set with categorical variables I tried the regroup the two datasets but since the categorical set is an ndarray and the other one is a dataframe I used:</p>\n<pre><code>np.hstack((X_train_num, X_train_cat))\n</code></pre>\n<p>which works perfectly but I no longer have the names of my variables.</p>\n<p>Is there another way to do this while maintaining the names of the variables without using pd.get_dummies()?</p>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try</p>\n<pre><code>X_train = X_train_num.join(\n   pd.DataFrame(X_train_cat, X_train_num.index).add_prefix('cat_')\n)\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Well, as you stated in your question, there's <code>pd.get_dummies</code>, which I think is the best choice here. Having said that, you could use</p>\n<pre><code>pd.concat([X_train_num, pd.DataFrame(X_train_cat, index=X_train_num.index)], axis=1)\n</code></pre>\n<p>If you like, you could give also useful column names with </p>\n<pre><code>pd.concat([X_train_num, pd.DataFrame(X_train_cat, index=X_train_num.index, columns=cols)], axis=1)\n</code></pre>\n<p>and <code>cols</code> can be whatever list of strings you want (of the appropriate length).</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Adding columns in sklearn onehot encoder</p>\n<pre><code>from sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(dev_data[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(test_data[object_cols]))\n\n# Adding column names to the encoded data set.\nOH_cols_train.columns = OH_encoder.get_feature_names(object_cols)\nOH_cols_valid.columns = OH_encoder.get_feature_names(object_cols)\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = dev_data.index\nOH_cols_valid.index = test_data.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = dev_data.drop(object_cols, axis=1)\nnum_X_valid = test_data.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\ndev_data = pd.concat([num_X_train, OH_cols_train], axis=1)\ntest_data = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Suppose I have a pipeline for my data which does preprocessing and has an estimator at the end. Now if I want to just change the estimator/model at the last step of the pipeline, how do I do it without preprocessing the same data all over again. Below is a code example</p>\n<pre><code>pipe = make_pipeline(\n    ColumnSelector(columns),\n    CategoricalEncoder(categories),\n    FunctionTransformer(pd.get_dummies, validate=False),\n    StandardScaler(scale),\n    LogisticRegression(),\n)\n</code></pre>\n<p>Now I want to change the model to use Ridge or some  other model than the LogisticRegression. How do I do this without doing the preprocessing all over again?</p>\n<p><em>EDIT</em>: Can I get my transformed data from a pipeline of the following sort</p>\n<pre><code>pipe = make_pipeline(\n        ColumnSelector(columns),\n        CategoricalEncoder(categories),\n        FunctionTransformer(pd.get_dummies, validate=False),\n        StandardScaler(scale)\n    )\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>For the case that you have computationally expensive transformers, you can use  <a href=\"http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html#caching-transformers-within-a-pipeline\" rel=\"noreferrer\">caching</a>. As you didn't provide your transformer, here an extension of the sklearn example from the link, where two models are grid searched with a cached pipeline:</p>\n<pre><code>from tempfile import mkdtemp\nfrom shutil import rmtree\nfrom sklearn.externals.joblib import Memory\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_digits\n\n# Create a temporary folder to store the transformers of the pipeline\ncachedir = mkdtemp()\nmemory = Memory(cachedir=cachedir, verbose=10)\n\n# the pipeline\npipe = Pipeline([('reduce_dim', PCA()),\n                ('classify', LinearSVC())],\n                memory=memory)\n# models to try\nparam_grid = {\"classify\" : [LinearSVC(), ElasticNet()]}\n\n# do the gridsearch on the models\ngrid = GridSearchCV(pipe, param_grid=param_grid)\ndigits = load_digits()\ngrid.fit(digits.data, digits.target)\n\n# delete the temporary cache before exiting\nrmtree(cachedir)\n</code></pre>\n<h1>Edit:</h1>\n<p>As you focus on models in your question, and <a href=\"https://stackoverflow.com/questions/43366561/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once?rq=1\">this question</a> focusses on parameters, I wouldn't consider it an exact duplicate. However, the proposed solution there in combination with the param_grid set up as here would also be a good, maybe even better solution, depending on your exact problem. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>With the rise of Spark, Scala has gained a tremendous momentum as programming language of choice for data science applications.</p>\n<p>To increase the efficiency when working on data science applications, <strong>specialized IDEs</strong> have been released for </p>\n<ul>\n<li>R (e.g. RStudio) and </li>\n<li>Python (e.g. Spyder or Rodeo, see <a href=\"https://stackoverflow.com/questions/8305809/is-there-something-like-rstudio-for-python\">Is there something like RStudio for Python?</a>). </li>\n</ul>\n<p>Is there something similar for Scala?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Unfortunately there doesn't seem to be any dedicated Data Science IDEs for Scala at this time. I think these would be your best options:</p>\n<h2><a href=\"https://confluence.jetbrains.com/display/IntelliJIDEA/Working+with+Scala+Worksheet\" rel=\"nofollow noreferrer\">IntelliJ Worksheets:</a></h2>\n<p><a href=\"https://i.sstatic.net/JcafD.png\" rel=\"nofollow noreferrer\"><img alt=\"IntelliJ Worksheet\" src=\"https://i.sstatic.net/JcafD.png\"/></a>\nThis is basically a text editor with an output window which gets updated as often as you want. Eclipse has something similar, I just prefer IntelliJ. </p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li>Backed by IntelliJ's fantastic code completion, error checking, and sbt/maven integration.</li>\n<li>You can prototype within the same project setup as your actual development system (if you have one).</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li>I am not aware of any caching/selective evaluation so the entire worksheet is evaluated each time you want an answer, something you may not want if you have some operations which take a long time to complete.</li>\n<li>No workspace variables window or plot integration.</li>\n</ul>\n<h2><a href=\"http://jupyter.org/\" rel=\"nofollow noreferrer\">Jupyter Notebooks</a></h2>\n<p><a href=\"https://i.sstatic.net/RLzOc.png\" rel=\"nofollow noreferrer\"><img alt=\"Jupyter Notebook\" src=\"https://i.sstatic.net/RLzOc.png\"/></a>\nThe Jupyter Notebook is a generalization of the iPython notebook which now supports <a href=\"https://github.com/ipython/ipython/wiki/IPython-kernels-for-other-languages\" rel=\"nofollow noreferrer\">dozens of interpreted languages</a> (new kernels are being added all of the time).</p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li><a href=\"https://github.com/alexarchambault/jupyter-scala\" rel=\"nofollow noreferrer\">Scala</a> and <a href=\"https://github.com/apache/incubator-toree\" rel=\"nofollow noreferrer\">Spark Scala</a> Kernels are fairly easy to install, both have the ability to add maven/sbt dependencies and JARs.</li>\n<li>The cells in the notebook can be run individually (allowing you to train a model once and use it many times, for example).</li>\n<li>The cells support markdown (with LaTeX!) which can be rendered on its own (<a href=\"https://github.com/EvanOman/KaggleCompetitions/blob/master/StackExchange/scala/Review_Aspect_Extraction_Spark.ipynb\" rel=\"nofollow noreferrer\">a github example</a>), allowing you to use your notebooks as a report/demonstration.</li>\n<li>Notebooks are backed by a Notebook Server so you could easily use a more powerful computer as your notebook server and then interact with the notebook from another location.</li>\n<li>Some kernels have autocompletion.</li>\n<li>Looks like there is some plot integration (<a href=\"http://nbviewer.jupyter.org/urls/gist.githubusercontent.com/chilang/b28319c2955d76219001/raw/645d4df74186548752a43f93eec3880e4e0d27d1/Bokeh.ipynb\" rel=\"nofollow noreferrer\">example</a>) but it is not totally polished.</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li>Not all kernels are perfect, some have bugs or limited functionality.</li>\n<li>No workspace variables window.</li>\n<li>You really need to be careful about the ordering of your cells, failure to do so can cause a lot of confusion.</li>\n</ul>\n<hr/>\n<p>For most of the data-sciency stuff I do I use Jupyter but it is far from perfect. In order for Scala to really take over as a Data Science language it really needs more data science libraries (scikit-learn is sooo far ahead here) and it needs a solid plotting library (there are a few options but none I have seen both use idiomatic Scala and are able to run without a server). I think as soon as it has those two elements it will become more popular and hopefully someone will make a nice RStudio-esque IDE.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Your best shot (nothing like rstudio but this would be your best shot for scala) is <a href=\"https://zeppelin.apache.org/\" rel=\"nofollow noreferrer\">apache zeppelin</a></p>\n<p><img alt=\"zeppelin\" src=\"https://zeppelin.apache.org/assets/themes/zeppelin/img/notebook.png\"/></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have dataframe:</p>\n<pre><code>         subject           A_target_word_gd  A_target_word_fd B_target_word_gd  B_target_word_fd  subject_type \n           1                      1             2                3                    4             mild \n           2                      11            12               13                  14             moderate\n</code></pre>\n<p>And I want to melt it to a dataframe that will look:</p>\n<pre><code>     cond    subject    subject_type     value_type   value\n      A         1        mild             gd           1           \n      A         1        mild             fg           2           \n      B         1        mild             gd           3            \n      B         1        mild             fg           4  \n      A         2        moderate         gd           11           \n      A         2        moderate         fg           12           \n      B         2        moderate         gd           13            \n      B         2        moderate         fg           14          \n...\n\n...\n</code></pre>\n<p>Meaning, to melt based on the delimiter of the columns name. </p>\n<p>What is the best way to do that?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is my way using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html\" rel=\"nofollow noreferrer\"><code>melt</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html\" rel=\"nofollow noreferrer\"><code>series.str.split()</code></a>:</p>\n<pre><code>m = df.melt(['subject','subject_type'])\nn = m['variable'].str.split('_',expand=True).iloc[:,[0,-1]]\nn.columns = ['cond','value_type']\nm = m.drop('variable',1).assign(**n).sort_values('subject')\n</code></pre>\n<hr/>\n<pre><code>print(m)\n\n   subject subject_type  value cond value_type\n0        1         mild      1    A         gd\n2        1         mild      2    A         fd\n4        1         mild      3    B         gd\n6        1         mild      4    B         fd\n1        2     moderate     11    A         gd\n3        2     moderate     12    A         fd\n5        2     moderate     13    B         gd\n7        2     moderate     14    B         fd\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Set index to <code>subject</code>, <code>subject_type</code>. Split columns by the string <code>_target_word_</code> to make multiindex columns. Rename axis to proper names and <code>stack</code> and <code>reset_index</code></p>\n<pre><code>df1 = df.set_index(['subject', 'subject_type'])\ndf1.columns = df1.columns.str.split('_target_word_', expand=True)\ndf_final = df1.rename_axis(['cond','value_type'],axis=1).stack([0,1]).reset_index(name='value')\n\nOut[91]:\n   subject subject_type cond value_type  value\n0        1         mild    A         fd      2\n1        1         mild    A         gd      1\n2        1         mild    B         fd      4\n3        1         mild    B         gd      3\n4        2     moderate    A         fd     12\n5        2     moderate    A         gd     11\n6        2     moderate    B         fd     14\n7        2     moderate    B         gd     13\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First reshape <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.set_index</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>DataFrame.stack</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.reset_index</code></a> and then convert column with <code>_</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html\" rel=\"nofollow noreferrer\"><code>Series.str.split</code></a> to new columns:</p>\n<pre><code>df = df.set_index(['subject','subject_type']).stack().reset_index(name='value')\ndf[['cond','value_type']] = df.pop('level_2').str.split('_', expand=True).iloc[:, [0,-1]]\nprint (df)\n   subject subject_type  value cond value_type\n0        1         mild      1    A         gd\n1        1         mild      2    A         fd\n2        1         mild      3    B         gd\n3        1         mild      4    B         fd\n4        2     moderate     11    A         gd\n5        2     moderate     12    A         fd\n6        2     moderate     13    B         gd\n7        2     moderate     14    B         fd\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>My neural network is not giving the expected output after training in Python. Is there any error in the code? Is there any way to reduce the mean squared error (MSE)? </p>\n<p>I tried to train (Run the program) the network repeatedly but it is not learning, instead it is giving the same MSE and output. </p>\n<p>Here is the Data I used:</p>\n<p><a href=\"https://drive.google.com/open?id=1GLm87-5E_6YhUIPZ_CtQLV9F9wcGaTj2\" rel=\"nofollow noreferrer\">https://drive.google.com/open?id=1GLm87-5E_6YhUIPZ_CtQLV9F9wcGaTj2</a></p>\n<p>Here is my code:</p>\n<pre><code>#load and evaluate a saved model\nfrom numpy import loadtxt\nfrom tensorflow.keras.models import load_model\n\n# load model\nmodel = load_model('ANNnew.h5')\n# summarize model.\nmodel.summary()\n#Model starts\nimport numpy as np\nimport pandas as pd \nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# Importing the dataset\nX = pd.read_excel(r\"C:\\filelocation\\Data.xlsx\",\"Sheet1\").values\ny = pd.read_excel(r\"C:\\filelocation\\Data.xlsx\",\"Sheet2\").values\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.08, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(32, activation = 'tanh', input_dim = 4))\n\n# Adding the second hidden layer\nmodel.add(Dense(units = 18, activation = 'tanh'))\n\n# Adding the third hidden layer\nmodel.add(Dense(units = 32, activation = 'tanh'))\n\n#model.add(Dense(1))\nmodel.add(Dense(units = 1))\n\n# Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, batch_size = 100, epochs = 1000)\n\ny_pred = model.predict(X_test)\nfor i in range(5):\n    print('%s =&gt; %d (expected %s)' % (X[i].tolist(), y_pred[i], y[i].tolist()))\n\n\nplt.plot(y_test, color = 'red', label = 'Test data')\nplt.plot(y_pred, color = 'blue', label = 'Predicted data')\nplt.title('Prediction')\nplt.legend()\nplt.show()\n\n# save model and architecture to single file\nmodel.save(\"ANNnew.h5\")\nprint(\"Saved model to disk\")\n</code></pre>\n<p><img alt=\"Output\" src=\"https://i.sstatic.net/sni0S.jpg\"/></p>\n<p><img alt=\"Output\" src=\"https://i.sstatic.net/xrThb.jpg\"/></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have noticed one minor mistake in your reporting through print - instead of:</p>\n<pre><code>for i in range(5):\n    print('%s =&gt; %d (expected %s)' % (X[i].tolist(), y_pred[i], y[i].tolist()))\n</code></pre>\n<p>you should have:</p>\n<pre><code>for i in range(len(y_test)):\n    print('%s =&gt; %d (expected %s)' % (X[i].tolist(), y_pred[i], y_test[i].tolist()))\n</code></pre>\n<p>At this print you will finally compare prediction for test with true for test (previously you were comparing prediction for test with true for first 5 observations in array y), and for all 6 observation in test, not just 5 :-)</p>\n<p>What you should also monitor is model quality on train data. Being extremely simplistic, for clarity of this case:</p>\n<ol>\n<li>you should try over-fitting train data with neural net (NN); if you can't even over-fit train data with NN, it might be the case that NNs are disappointing solution for your problem at current state; in this case you would need to look for additional features (also mentioned below), change model quality metric or just accept limitations of prediction quality attributed to solution being prepared;</li>\n<li>having assured over-fitting train data is possible or accepting limitations of prediction quality, your goal is to find the best model that can be generalized; monitoring both train and test quality of your model is crucial; generalizable model is a model performing similarly on both train data and valid data; in order to find the best generalizable model you can:\n\n<ul>\n<li>look for valuable features (transformations of data you have or additional data sources)</li>\n<li>play with NN architecture</li>\n<li>play with NN estimation process</li>\n</ul></li>\n</ol>\n<p>In general, for achieving the ultimate goal of finding the best NN that can be generalized, it is a good practice to use either validation_split or validation_data in model.fit call.</p>\n<p><strong>Imports</strong></p>\n<pre><code># imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport random\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow import set_random_seed\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom importlib import reload\n</code></pre>\n<p><strong>Useful functions</strong></p>\n<pre><code># useful pandas display settings\npd.options.display.float_format = '{:.3f}'.format\n\n# useful functions\ndef plot_history(history, metrics_to_plot):\n    \"\"\"\n    Function plots history of selected metrics for fitted neural net.\n\n    \"\"\"\n\n    # plot\n    for metric in metrics_to_plot:\n        plt.plot(history.history[metric])\n\n    # name X axis informatively\n    plt.xlabel('epoch')\n\n    # name Y axis informatively\n    plt.ylabel('metric')\n\n    # add informative legend\n    plt.legend(metrics_to_plot)\n\n    # plot\n    plt.show()\n\ndef plot_fit(y_true, y_pred, title='title'):\n    \"\"\"\n    Function plots true values and predicted values, sorted in increase order by true values.\n\n    \"\"\"\n\n    # create one dataframe with true values and predicted values\n    results = y_true.reset_index(drop=True).merge(pd.DataFrame(y_pred), left_index=True, right_index=True)\n\n    # rename columns informartively\n    results.columns = ['true', 'prediction']\n\n    # sort for clarity of visualization\n    results = results.sort_values(by=['true']).reset_index(drop=True)\n\n    # plot true values vs predicted values\n    results.plot()\n\n    # adding scatter on line plots\n    plt.scatter(results.index, results.true, s=5)\n    plt.scatter(results.index, results.prediction, s=5)\n\n    # name X axis informatively\n    plt.xlabel('obs sorted in ascending order with respect to true values')\n\n    # add customizable title\n    plt.title(title)\n\n    # plot\n    plt.show();\n\ndef reset_all_randomness():\n    \"\"\"\n    Function assures reproducibility of NN estimation results.\n\n    \"\"\"\n\n    # reloads\n    reload(tf)\n    reload(np)\n    reload(random)\n\n    # seeds - for reproducibility\n    os.environ['PYTHONHASHSEED']=str(984797)\n    random.seed(984797)\n    set_random_seed(984797)\n    np.random.seed(984797)\n    my_init = glorot_uniform(seed=984797)\n\n    return my_init\n</code></pre>\n<p><strong>Load X and y from file</strong></p>\n<pre><code>X = pd.read_excel(r\"C:\\filelocation\\Data.xlsx\",\"Sheet1\").values\ny = pd.read_excel(r\"C:\\filelocation\\Data.xlsx\",\"Sheet2\").values\n</code></pre>\n<p><strong>Splitting X and y into the Training set and Test set</strong></p>\n<pre><code># reset_all_randomness - for reproducibility\nmy_init = reset_all_randomness()\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.08, random_state = 0)\n</code></pre>\n<p><strong>Feature Scaling</strong></p>\n<pre><code># Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n</code></pre>\n<p><strong>Model0</strong> - try overfitting on train data and verify overfitting</p>\n<pre><code># reset_all_randomness - for reproducibility\nmy_init = reset_all_randomness()\n\n# model0\n\n# Initialising the ANN\nmodel0 = Sequential()\n\n# Adding 1 hidden layer: the input layer and the first hidden layer\nmodel0.add(Dense(units = 128, activation = 'tanh', input_dim = 4, kernel_initializer=my_init))\n\n# Adding 2 hidden layer\nmodel0.add(Dense(units = 64, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding 3 hidden layer\nmodel0.add(Dense(units = 32, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding 4 hidden layer\nmodel0.add(Dense(units = 16, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding output layer\nmodel0.add(Dense(units = 1, kernel_initializer=my_init))\n\n# Set up Optimizer\nOptimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.99)\n\n# Compiling the ANN\nmodel0.compile(optimizer = Optimizer, loss = 'mean_squared_error', metrics=['mse','mae'])\n\n# Fitting the ANN to the Train set, at the same time observing quality on Valid set\nhistory = model0.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100, epochs = 1000)\n\n# Generate prediction for both Train and Valid set\ny_train_pred_model0 = model0.predict(X_train)\ny_test_pred_model0 = model0.predict(X_test)\n\n# check what metrics are in fact available in history\nhistory.history.keys()\n\ndict_keys(['val_loss', 'val_mean_squared_error', 'val_mean_absolute_error', 'loss', 'mean_squared_error', 'mean_absolute_error'])\n\n# look at model fitting history\nplot_history(history, ['mean_squared_error', 'val_mean_squared_error'])\nplot_history(history, ['mean_absolute_error', 'val_mean_absolute_error'])\n\n# look at model fit quality\nfor i in range(len(y_test)):\n    print('%s =&gt; %s (expected %s)' % (X[i].tolist(), y_test_pred_model0[i], y_test[i]))\nplot_fit(pd.DataFrame(y_train), y_train_pred_model0, 'Fit on train data')\nplot_fit(pd.DataFrame(y_test), y_test_pred_model0, 'Fit on test data')\n\nprint('MSE on train data is: {}'.format(history.history['mean_squared_error'][-1]))\nprint('MSE on test data is: {}'.format(history.history['val_mean_squared_error'][-1]))\n</code></pre>\n<p><a href=\"https://i.sstatic.net/Ucdxk.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/Ucdxk.png\"/></a>\n<a href=\"https://i.sstatic.net/H0TXb.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/H0TXb.png\"/></a></p>\n<pre><code>[1000.0, 25.0, 2235.3, 1.0] =&gt; [2.2463024] (expected [3])\n[1000.0, 30.0, 2190.1, 1.0] =&gt; [5.6396966] (expected [3])\n[1000.0, 35.0, 2144.7, 1.0] =&gt; [5.6486473] (expected [5])\n[1000.0, 40.0, 2098.9, 1.0] =&gt; [4.852657] (expected [3])\n[1000.0, 45.0, 2052.9, 1.0] =&gt; [3.9801836] (expected [4])\n[1000.0, 25.0, 2235.3, 1.0] =&gt; [5.761505] (expected [6])\n</code></pre>\n<p><a href=\"https://i.sstatic.net/vUEmo.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/vUEmo.png\"/></a>\n<a href=\"https://i.sstatic.net/h7k7z.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/h7k7z.png\"/></a></p>\n<pre><code>MSE on train data is: 0.1629941761493683\nMSE on test data is: 1.9077353477478027\n</code></pre>\n<p>With this result, let's assume over-fitting succeeded.</p>\n<p><strong>Look for valuable features (transformations of data you have)</strong></p>\n<pre><code># augment features by calculating absolute values and squares of original features\nX_train = np.array([list(x) + list(np.abs(x)) + list(x**2) for x in X_train])\nX_test = np.array([list(x) + list(np.abs(x)) + list(x**2) for x in X_test])\n</code></pre>\n<p><strong>Model1</strong> - with 8 additional features, 12 inputs overall (instead of 4)</p>\n<pre><code># reset_all_randomness - for reproducibility\nmy_init = reset_all_randomness()\n\n# model1\n\n# Initialising the ANN\nmodel1 = Sequential()\n\n# Adding 1 hidden layer: the input layer and the first hidden layer\nmodel1.add(Dense(units = 128, activation = 'tanh', input_dim = 12, kernel_initializer=my_init))\n\n# Adding 2 hidden layer\nmodel1.add(Dense(units = 64, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding 3 hidden layer\nmodel1.add(Dense(units = 32, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding 4 hidden layer\nmodel1.add(Dense(units = 16, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding output layer\nmodel1.add(Dense(units = 1, kernel_initializer=my_init))\n\n# Set up Optimizer\nOptimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.99)\n\n# Compiling the ANN\nmodel1.compile(optimizer = Optimizer, loss = 'mean_squared_error', metrics=['mse','mae'])\n\n# Fitting the ANN to the Train set, at the same time observing quality on Valid set\nhistory = model1.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100, epochs = 1000)\n\n# Generate prediction for both Train and Valid set\ny_train_pred_model1 = model1.predict(X_train)\ny_test_pred_model1 = model1.predict(X_test)\n\n# look at model fitting history\nplot_history(history, ['mean_squared_error', 'val_mean_squared_error'])\nplot_history(history, ['mean_absolute_error', 'val_mean_absolute_error'])\n\n# look at model fit quality\nfor i in range(len(y_test)):\n    print('%s =&gt; %s (expected %s)' % (X[i].tolist(), y_test_pred_model1[i], y_test[i]))\nplot_fit(pd.DataFrame(y_train), y_train_pred_model1, 'Fit on train data')\nplot_fit(pd.DataFrame(y_test), y_test_pred_model1, 'Fit on test data')\n\nprint('MSE on train data is: {}'.format(history.history['mean_squared_error'][-1]))\nprint('MSE on test data is: {}'.format(history.history['val_mean_squared_error'][-1]))\n</code></pre>\n<p><a href=\"https://i.sstatic.net/U0vr8.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/U0vr8.png\"/></a>\n<a href=\"https://i.sstatic.net/RXTOB.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/RXTOB.png\"/></a></p>\n<pre><code>[1000.0, 25.0, 2235.3, 1.0] =&gt; [2.5696845] (expected [3])\n[1000.0, 30.0, 2190.1, 1.0] =&gt; [5.0152197] (expected [3])\n[1000.0, 35.0, 2144.7, 1.0] =&gt; [4.4963903] (expected [5])\n[1000.0, 40.0, 2098.9, 1.0] =&gt; [5.004753] (expected [3])\n[1000.0, 45.0, 2052.9, 1.0] =&gt; [3.982211] (expected [4])\n[1000.0, 25.0, 2235.3, 1.0] =&gt; [6.158882] (expected [6])\n</code></pre>\n<p><a href=\"https://i.sstatic.net/r5Os8.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/r5Os8.png\"/></a>\n<a href=\"https://i.sstatic.net/8I8gT.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/8I8gT.png\"/></a></p>\n<pre><code>MSE on train data is: 0.17548464238643646\nMSE on test data is: 1.4240833520889282\n</code></pre>\n<p><strong>Model2</strong> - grid-search experiments with 2-hidden-layers NNs\nAddressing:</p>\n<p>play with NN architecture (<em>layer1_neurons</em>, <em>layer2_neurons</em>, <em>activation_function</em>)</p>\n<p>play with NN estimation process (<em>learning_rate</em>, <em>beta1</em>, <em>beta2</em>)</p>\n<pre><code># init experiment_results\nexperiment_results = []\n\n# the experiment\nfor layer1_neurons in [4, 8, 16,32 ]:\n    for layer2_neurons in [4, 8, 16, 32]:\n        for activation_function in ['tanh', 'relu']:\n            for learning_rate in [0.01, 0.001]:\n                for beta1 in [0.9]:\n                    for beta2 in [0.99]:\n\n                        # reset_all_randomness - for reproducibility\n                        my_init = reset_all_randomness()\n\n                        # model2\n                        # Initialising the ANN\n                        model2 = Sequential()\n\n                        # Adding 1 hidden layer: the input layer and the first hidden layer\n                        model2.add(Dense(units = layer1_neurons, activation = activation_function, input_dim = 12, kernel_initializer=my_init))\n\n                        # Adding 2 hidden layer\n                        model2.add(Dense(units = layer2_neurons, activation = activation_function, kernel_initializer=my_init))\n\n                        # Adding output layer\n                        model2.add(Dense(units = 1, kernel_initializer=my_init))\n\n                        # Set up Optimizer\n                        Optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2)\n\n                        # Compiling the ANN\n                        model2.compile(optimizer = Optimizer, loss = 'mean_squared_error', metrics=['mse','mae'])\n\n                        # Fitting the ANN to the Train set, at the same time observing quality on Valid set\n                        history = model2.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100, epochs = 1000, verbose=0)\n\n                        # Generate prediction for both Train and Valid set\n                        y_train_pred_model2 = model2.predict(X_train)\n                        y_test_pred_model2 = model2.predict(X_test)\n\n                        print('MSE on train data is: {}'.format(history.history['mean_squared_error'][-1]))\n                        print('MSE on test data is: {}'.format(history.history['val_mean_squared_error'][-1]))\n\n                        # create data you want to save for each processed NN\n                        partial_results = \\\n                        {\n                            'layer1_neurons': layer1_neurons,\n                            'layer2_neurons': layer2_neurons,\n                            'activation_function': activation_function,\n\n                            'learning_rate': learning_rate,\n                            'beta1': beta1,\n                            'beta2': beta2,\n\n                            'final_train_mean_squared_error': history.history['mean_squared_error'][-1],\n                            'final_val_mean_squared_error': history.history['val_mean_squared_error'][-1],\n\n                            'best_train_epoch': history.history['mean_squared_error'].index(min(history.history['mean_squared_error'])),\n                            'best_train_mean_squared_error': np.min(history.history['mean_squared_error']),\n\n                            'best_val_epoch': history.history['val_mean_squared_error'].index(min(history.history['val_mean_squared_error'])),\n                            'best_val_mean_squared_error': np.min(history.history['val_mean_squared_error']),\n\n                        }\n\n                        experiment_results.append(\n                            partial_results\n                        )\n</code></pre>\n<p>Explore experiment results:</p>\n<pre><code># put experiment_results into DataFrame\nexperiment_results_df = pd.DataFrame(experiment_results)\n\n# identifying models hopefully not too much overfitted to valid data at the end of estimation (after 1000 epochs) : \nexperiment_results_df['valid'] = experiment_results_df['final_val_mean_squared_error'] &gt; experiment_results_df['final_train_mean_squared_error']\n\n# display the best combinations of parameters for valid data, which seems not overfitted\nexperiment_results_df[experiment_results_df['valid']].sort_values(by=['final_val_mean_squared_error']).head()\n\n    layer1_neurons  layer2_neurons activation_function  learning_rate  beta1    beta2  final_train_mean_squared_error  final_val_mean_squared_error  best_train_epoch  best_train_mean_squared_error  best_val_epoch  best_val_mean_squared_error  valid\n26               8              16                relu          0.010  0.900    0.990                           0.992                         1.232               998                          0.992             883                        1.117   True\n36              16               8                tanh          0.010  0.900    0.990                           0.178                         1.345               998                          0.176              40                        1.245   True\n14               4              32                relu          0.010  0.900    0.990                           1.320                         1.378               980                          1.300              98                        0.937   True\n2                4               4                relu          0.010  0.900    0.990                           1.132                         1.419               996                          1.131             695                        1.002   True\n57              32              16                tanh          0.001  0.900    0.990                           1.282                         1.432               999                          1.282             999                        1.432   True\n</code></pre>\n<p>You can do slightly better if you take into account whole training history:</p>\n<pre><code># for each NN estimation identify dictionary of epochs for which NN was not overfitted towards valid data \n# for each such epoch I store its number and corresponding mean_squared_error on valid data\nexperiment_results_df['not_overfitted_epochs_on_valid'] = \\\nexperiment_results_df.apply(\n    lambda row:\n    {\n        i: row['val_mean_squared_error_history'][i]\n        for i in range(len(row['train_mean_squared_error_history']))\n        if row['val_mean_squared_error_history'][i] &gt; row['train_mean_squared_error_history'][i]\n    },\n    axis=1\n)\n\n# basing on previosuly prepared dict, for each NN estimation I can identify:\n# best not overfitted mse value on valid data and corresponding best not overfitted epoch on valid data\nexperiment_results_df['best_not_overfitted_mse_on_valid'] = \\\nexperiment_results_df['not_overfitted_epochs_on_valid'].apply(\n    lambda x: np.min(list(x.values())) if len(list(x.values()))&gt;0 else np.NaN\n)\n\nexperiment_results_df['best_not_overfitted_epoch_on_valid'] = \\\nexperiment_results_df['not_overfitted_epochs_on_valid'].apply(\n    lambda x: list(x.keys())[list(x.values()).index(np.min(list(x.values())))] if len(list(x.values()))&gt;0 else np.NaN\n)\n\n# now I can sort all estimations according to best not overfitted mse on valid data overall, not only at the end of estimation\nexperiment_results_df.sort_values(by=['best_not_overfitted_mse_on_valid'])[[\n    'layer1_neurons','layer2_neurons','activation_function','learning_rate','beta1','beta2',\n    'best_not_overfitted_mse_on_valid','best_not_overfitted_epoch_on_valid'\n]].head()\n\n    layer1_neurons  layer2_neurons activation_function  learning_rate  beta1    beta2  best_not_overfitted_mse_on_valid  best_not_overfitted_epoch_on_valid\n26               8              16                relu          0.010  0.900    0.990                             1.117                             883.000\n54              32               8                relu          0.010  0.900    0.990                             1.141                             717.000\n50              32               4                relu          0.010  0.900    0.990                             1.210                             411.000\n36              16               8                tanh          0.010  0.900    0.990                             1.246                             821.000\n56              32              16                tanh          0.010  0.900    0.990                             1.264                             693.000\n</code></pre>\n<p>Now I record top estimation combination for final model estimation:</p>\n<ul>\n<li>layer1_neurons = 8</li>\n<li>layer2_neurons = 16</li>\n<li>activation_function = 'relu'</li>\n<li>learning_rate = 0.010</li>\n<li>beta1 = 0.900</li>\n<li>beta2 = 0.990</li>\n<li>epoch to stop training = 883</li>\n</ul>\n<p><strong>Model3</strong> - final model</p>\n<pre><code># reset_all_randomness - for reproducibility\nmy_init = reset_all_randomness()\n\n# model3\n\n# Initialising the ANN\nmodel3 = Sequential()\n\n# Adding 1 hidden layer: the input layer and the first hidden layer\nmodel3.add(Dense(units = 8, activation = 'relu', input_dim = 12, kernel_initializer=my_init))\n\n# Adding 2 hidden layer\nmodel3.add(Dense(units = 16, activation = 'relu', kernel_initializer=my_init))\n\n# Adding output layer\nmodel3.add(Dense(units = 1, kernel_initializer=my_init))\n\n# Set up Optimizer\nOptimizer = tf.train.AdamOptimizer(learning_rate=0.010, beta1=0.900, beta2=0.990)\n\n# Compiling the ANN\nmodel3.compile(optimizer = Optimizer, loss = 'mean_squared_error', metrics=['mse','mae'])\n\n# Fitting the ANN to the Train set, at the same time observing quality on Valid set\nhistory = model3.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100, epochs = 884)\n\n# Generate prediction for both Train and Valid set\ny_train_pred_model3 = model3.predict(X_train)\ny_test_pred_model3 = model3.predict(X_test)\n\n# look at model fitting history\nplot_history(history, ['mean_squared_error', 'val_mean_squared_error'])\nplot_history(history, ['mean_absolute_error', 'val_mean_absolute_error'])\n\n# look at model fit quality\nfor i in range(len(y_test)):\n    print('%s =&gt; %s (expected %s)' % (X[i].tolist(), y_test_pred_model3[i], y_test[i]))\nplot_fit(pd.DataFrame(y_train), y_train_pred_model3, 'Fit on train data')\nplot_fit(pd.DataFrame(y_test), y_test_pred_model3, 'Fit on test data')\n\nprint('MSE on train data is: {}'.format(history.history['mean_squared_error'][-1]))\nprint('MSE on test data is: {}'.format(history.history['val_mean_squared_error'][-1]))\n</code></pre>\n<p><a href=\"https://i.sstatic.net/x8EjQ.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/x8EjQ.png\"/></a>\n<a href=\"https://i.sstatic.net/v8utQ.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/v8utQ.png\"/></a></p>\n<pre><code>[1000.0, 25.0, 2235.3, 1.0] =&gt; [1.8813248] (expected [3])\n[1000.0, 30.0, 2190.1, 1.0] =&gt; [4.3430963] (expected [3])\n[1000.0, 35.0, 2144.7, 1.0] =&gt; [4.827326] (expected [5])\n[1000.0, 40.0, 2098.9, 1.0] =&gt; [4.6029215] (expected [3])\n[1000.0, 45.0, 2052.9, 1.0] =&gt; [3.8530324] (expected [4])\n[1000.0, 25.0, 2235.3, 1.0] =&gt; [4.9882255] (expected [6])\n</code></pre>\n<p><a href=\"https://i.sstatic.net/S45Ua.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/S45Ua.png\"/></a>\n<a href=\"https://i.sstatic.net/HOqfg.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/HOqfg.png\"/></a></p>\n<pre><code>MSE on train data is: 1.088669776916504\nMSE on test data is: 1.1166337728500366\n</code></pre>\n<p>In no case I claim that <strong>Model3</strong> is the best possible for your data. I just wanted to introduce you to ways of working with NNs. You might be also interested in further exploration of topics:</p>\n<ul>\n<li>exploratory analysis (look for ideas for features),</li>\n<li>feature extraction (calculating features),</li>\n<li>cross-validation (method related to assuring generalization of models - especially because your data is small),</li>\n<li>hyperparameters of neural networks and their estimation process (what to tweak),</li>\n<li>hyperparameters optimization (methods like grid search, random search, bayesian search, genetic algorithms which support tweaking parameters = finding the best model),</li>\n<li>early-stopping of neural network estimation (estimation rule which could save you some estimation time).</li>\n</ul>\n<p>Hope you will find it inspiring for further studies :-)</p>\n<p><br/>\n<strong>EDIT:</strong></p>\n<p>I am sharing exemplary steps, required for redefinition of this problem from approximation to classification, as for <strong>Model0</strong>. I would also like to share valuable literature reference in case you would want to get more acquainted with NNs in Python:</p>\n<p>[2018 Chollet] <em>Deep Learning with Python</em></p>\n<p><strong>Additional useful function</strong></p>\n<pre><code>def give_me_mse(true, prediction):\n    \"\"\"\n    This function returns mse for 2 vectors: true and predicted values.\n\n    \"\"\"\n\n    return np.mean((true-prediction)**2)\n</code></pre>\n<p><strong>Load X and y from file</strong></p>\n<pre><code># as previosly\n</code></pre>\n<p><strong>Encode target - since now you need 7 vectors reflecting target values (due to the fact that your target has 7 levels)</strong></p>\n<pre><code>from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(np.ravel(y))\ny_encoded = encoder.transform(np.ravel(y))\n# convert integers to dummy variables (i.e. one hot encoded)\ny_dummy = np_utils.to_categorical(y_encoded)\n</code></pre>\n<p><strong>Splitting X and y into the Training set and Test set</strong></p>\n<pre><code># reset_all_randomness - for reproducibility\nmy_init = reset_all_randomness()\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test, y_train_dummy, y_test_dummy = train_test_split(X, y, y_dummy, test_size = 0.08, random_state = 0)\n</code></pre>\n<p><strong>Feature Scaling</strong></p>\n<pre><code># as previosly\n</code></pre>\n<p><strong>Model0 - rearranged for classification problem</strong></p>\n<p><strong>Now NN produces 7-element output for single input-data entry</strong></p>\n<p><strong>Output constitutes of 7 probabilities, which are probabilities of belonging to corresponding target level</strong></p>\n<pre><code># model0\n\n# Initialising the ANN\nmodel0 = Sequential()\n\n# Adding 1 hidden layer: the input layer and the first hidden layer\nmodel0.add(Dense(units = 128, activation = 'tanh', input_dim = 4, kernel_initializer=my_init))\n\n# Adding 2 hidden layer\nmodel0.add(Dense(units = 64, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding 3 hidden layer\nmodel0.add(Dense(units = 32, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding 4 hidden layer\nmodel0.add(Dense(units = 16, activation = 'tanh', kernel_initializer=my_init))\n\n# Adding output layer\nmodel0.add(Dense(units = 7, activation = 'softmax', kernel_initializer=my_init))\n\n# Set up Optimizer\nOptimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.99)\n\n# Compiling the ANN\nmodel0.compile(optimizer = Optimizer, loss = 'categorical_crossentropy', metrics=['accuracy','categorical_crossentropy','mse'])\n\n# Fitting the ANN to the Train set, at the same time observing quality on Valid set\nhistory = model0.fit(X_train, y_train_dummy, validation_data=(X_test, y_test_dummy), batch_size = 100, epochs = 1000)\n\n# Generate prediction for both Train and Valid set\ny_train_pred_model0 = model0.predict(X_train)\ny_test_pred_model0 = model0.predict(X_test)\n\n# find final prediction by taking class with highest probability\ny_train_pred_model0 = np.array([[list(x).index(max(list(x))) + 1] for x in y_train_pred_model0])\ny_test_pred_model0 = np.array([[list(x).index(max(list(x))) + 1] for x in y_test_pred_model0])\n\n# check what metrics are in fact available in history\nhistory.history.keys()\n\ndict_keys(['val_loss', 'val_acc', 'val_categorical_crossentropy', 'val_mean_squared_error', 'loss', 'acc', 'categorical_crossentropy', 'mean_squared_error'])\n\n# look at model fitting history\nplot_history(history, ['mean_squared_error', 'val_mean_squared_error'])\nplot_history(history, ['categorical_crossentropy', 'val_categorical_crossentropy'])\nplot_history(history, ['acc', 'val_acc'])\n\n# look at model fit quality\nplot_fit(pd.DataFrame(y_train), y_train_pred_model0, 'Fit on train data')\nplot_fit(pd.DataFrame(y_test), y_test_pred_model0, 'Fit on test data')\n\nprint('MSE on train data is: {}'.format(give_me_mse(y_train, y_train_pred_model0)))\nprint('MSE on test data is: {}'.format(give_me_mse(y_test, y_test_pred_model0)))\n</code></pre>\n<p><a href=\"https://i.sstatic.net/WVdwg.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/WVdwg.png\"/></a>\n<a href=\"https://i.sstatic.net/OoxsS.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/OoxsS.png\"/></a>\n<a href=\"https://i.sstatic.net/oHjde.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/oHjde.png\"/></a>\n<a href=\"https://i.sstatic.net/ukXmY.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ukXmY.png\"/></a>\n<a href=\"https://i.sstatic.net/ob0nk.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/ob0nk.png\"/></a></p>\n<pre><code>MSE on train data is: 0.0\nMSE on test data is: 1.3333333333333333\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Can someone explain me what's happening here? </p>\n<p>Why is there more decimal points for 0.3 and 0.7 values. \nI just want 1 decimal point values.</p>\n<pre class=\"lang-py prettyprint-override\"><code>threshold_range = np.arange(0.1,1,0.1)\nthreshold_range.tolist()\n</code></pre>\n<pre><code>[Output]: [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <code>np.round</code> </p>\n<p><strong>Ex.</strong></p>\n<pre><code>import numpy as np\n\nthreshold_range = np.arange(0.1,1,0.1)\nprint(threshold_range.tolist())\nprint(np.round(threshold_range, 2).tolist())\n</code></pre>\n<p><strong>O/P:</strong></p>\n<pre><code>[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]\n[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Solution: You can simply use <em>round</em> function:</p>\n<pre><code>threshold_range = np.arange(0.1,1,0.1).round(1)\nthreshold_range.tolist() # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n</code></pre>\n<p>Reason of error: I think it has to do with floating point precision ;) </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I need to obtain the type for each column to properly preprocess it. </p>\n<p>Currently I do this via the following method:</p>\n<pre><code>import pandas as pd\n\n# input is of type List[List[any]]\n# but has one type (int, float, str, bool) per column\n\ndf = pd.DataFrame(input, columns=key_labels)\ncolumn_types = dict(df.dtypes)\nmatrix = df.values\n</code></pre>\n<p>Since I only use pandas for obtaining the dtypes (per column) and use numpy for everything else I want to cut pandas from my project. </p>\n<p>In summary: Is there a way to obtain (specific) dtypes per column from numpy</p>\n<p>!Or: Is there a fast way to recompute the dtype of ndarray (after splicing the matrix)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It would help if you gave a concrete example, but I'll demonstrate with <code>@jpp's</code> list:</p>\n<pre><code>In [509]: L = [[0.5, True, 'hello'], [1.25, False, 'test']]\nIn [510]: df = pd.DataFrame(L)\nIn [511]: df\nOut[511]: \n      0      1      2\n0  0.50   True  hello\n1  1.25  False   test\nIn [512]: df.dtypes\nOut[512]: \n0    float64\n1       bool\n2     object\ndtype: object\n</code></pre>\n<p><code>pandas</code> doesn't like to use string dtypes, so the last column is <code>object</code>.</p>\n<pre><code>In [513]: arr = df.values\nIn [514]: arr\nOut[514]: \narray([[0.5, True, 'hello'],\n       [1.25, False, 'test']], dtype=object)\n</code></pre>\n<p>So because of the mix in column dtypes, <code>pandas</code> is making the whole thing <code>object</code>.  I don't know pandas well enough to know if you can control the dtype better.</p>\n<p>To make a <code>numpy</code> structured array from <code>L</code>, the obvious thing to do is:</p>\n<pre><code>In [515]: np.array([tuple(row) for row in L], dtype='f,bool,U10')\nOut[515]: \narray([(0.5 ,  True, 'hello'), (1.25, False, 'test')],\n      dtype=[('f0', '&lt;f4'), ('f1', '?'), ('f2', '&lt;U10')])\n</code></pre>\n<p>That answers the question of how to specify a different dtype per 'column'.  But keep in mind that this array is 1d, and has <code>fields</code> not <code>columns</code>.</p>\n<p>But whether it's possible to deduce or set the dtype automatically, that's trickier.  It might be possible to build a <code>recarray</code> from the columns, or use one of the functions in <code>np.lib.recfunctions</code>.</p>\n<p>If I use a list 'transpose' I can format each column as a separate numpy array.</p>\n<pre><code>In [537]: [np.array(col) for col in zip(*L)]\nOut[537]: \n[array([0.5 , 1.25]),\n array([ True, False]),\n array(['hello', 'test'], dtype='&lt;U5')]\n</code></pre>\n<p>Then join them into one array with <code>rec.fromarrays</code>:</p>\n<pre><code>In [538]: np.rec.fromarrays([np.array(col) for col in zip(*L)])\nOut[538]: \nrec.array([(0.5 ,  True, 'hello'), (1.25, False, 'test')],\n          dtype=[('f0', '&lt;f8'), ('f1', '?'), ('f2', '&lt;U5')])\n</code></pre>\n<hr/>\n<p>Or I could use <code>genfromtxt</code> to deduce fields from a <code>csv</code> format.</p>\n<pre><code>In [526]: np.savetxt('test.txt', np.array(L,object),delimiter=',',fmt='%s')\nIn [527]: cat test.txt\n0.5,True,hello\n1.25,False,test\n\nIn [529]: data = np.genfromtxt('test.txt',dtype=None,delimiter=',',encoding=None)\nIn [530]: data\nOut[530]: \narray([(0.5 ,  True, 'hello'), (1.25, False, 'test')],\n      dtype=[('f0', '&lt;f8'), ('f1', '?'), ('f2', '&lt;U5')])\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In numpy, an array has the same dtypes for all its entries. So no, it's not possible to have the dedicated/fast float in one column and another one in another column.</p>\n<p>That's the point of pandas to allow you to jump from one column with one type to another.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>In order to obtain each column type and use it in your program, you can use <a href=\"https://numpy.org/doc/stable/user/basics.rec.html\" rel=\"nofollow noreferrer\">Numpy Structured Arrays</a>.</p>\n<p>Structured Arrays are a composition of simpler data types organized as a sequence of named fields.</p>\n<p>They have a property called <code>dtype</code> which you can use to answer your question.</p>\n<p>Note that Numpy also has a “Record Array” or “recarray” data type, that is quite similar to Structured Arrays.\nBut according to <a href=\"https://stackoverflow.com/a/52443038/3057377\">this post</a>, Record Arrays are much slower than Structured Arrays and are probably kept for convenience and backward compatibility.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\n\n# Initialize structured array.\ndf = np.array([(10, 3.14, 'Hello', True),\n                 (20, 2.71, 'World', False)],\n                dtype=[\n                    (\"ci\", \"i4\"),\n                    (\"cf\", \"f4\"),\n                    (\"cs\", \"U16\"),\n                    (\"cb\", \"?\")])\n\n# Basic usage.\nprint(df)\nprint(np.size(df))\nprint(df.shape)\nprint(df[\"cs\"])\nprint(df[\"cs\"][0])\nprint(type(df))\nprint(df.dtype)\nprint(df.dtype.names)\n\n# Check exact data type.\nprint(df.dtype[\"ci\"] == \"i4\")\nprint(df.dtype[\"cf\"] == \"f4\")\nprint(df.dtype[\"cs\"] == \"U16\")\nprint(df.dtype[\"cb\"] == \"?\")\n\n# Check general data type kind.\nprint(df.dtype[\"ci\"].kind == \"i\")\nprint(df.dtype[\"cf\"].kind == \"f\")\nprint(df.dtype[\"cs\"].kind == \"U\")\nprint(df.dtype[\"cb\"].kind == \"b\")\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe with over 280 features.\nI ran correlation map to detect groups of features that are highly correlated:\n<a href=\"https://i.sstatic.net/deDPD.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/deDPD.png\"/></a>\nNow, I want to divide the features to groups, such that each group will be a \"red zone\", meaning each group will have features that are all have correlation &gt;0.5 with each other.</p>\n<p>How can it be done?</p>\n<p>Thanks</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><strong>Disclaimer</strong>:</p>\n<ol>\n<li>Visualization is not addressed in this solution. Only groups were found.</li>\n<li>The solution is known to be NP-hard, so mind efficiency problems.</li>\n</ol>\n<h2>Theory</h2>\n<p>The problem is essentially a <a href=\"https://en.wikipedia.org/wiki/Clique_problem\" rel=\"noreferrer\">clique problem</a> in graph theory, which means finding all the complete subgraphs in a given graph (with nodes &gt; 2).</p>\n<p>Imagine a graph that all the features are nodes and pairs of features satisfying <code>corr &gt; 0.5</code> are edges. Then the task of finding all \"groups\" requested can simply translates into \"finding all complete subgraphs in the graph\".</p>\n<h2>Code</h2>\n<p>The code uses <a href=\"https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.clique.find_cliques.html\" rel=\"noreferrer\">networkx.algorithms.find_cliques</a> for the search task, which implements <a href=\"https://en.wikipedia.org/wiki/Bron%E2%80%93Kerbosch_algorithm\" rel=\"noreferrer\">Bron–Kerbosch algorithm</a> according to the docs.</p>\n<p>The code conprises of two parts. The first part extract the edges using <code>np.triu</code> (modified from <a href=\"https://stackoverflow.com/questions/34417685\">this post</a>) and the second part feeds the edge list into <code>networkx</code>.</p>\n<p><strong>The Coorelation Matrix</strong></p>\n<p>Feature [A,B,C] and [C,D,E] are closely correlated respectively, but not between [A,B] and [D,E].</p>\n<pre><code>np.random.seed(111)  # reproducibility\nx = np.random.normal(0, 1, 100)\ny = np.random.normal(0, 1, 100)\na = x\nb = x + np.random.normal(0, .5, 100)\nc = x + y\nd = y + np.random.normal(0, .5, 100)\ne = y + np.random.normal(0, .5, 100)\n\ndf = pd.DataFrame({\"A\":a, \"B\":b, \"C\":c, \"D\":d, \"E\":e})\ncorr = df.corr()\n\ncorr\nOut[24]: \n          A         B         C         D         E\nA  1.000000  0.893366  0.677333 -0.078369 -0.090510\nB  0.893366  1.000000  0.577459 -0.072025 -0.079855\nC  0.677333  0.577459  1.000000  0.587695  0.579891\nD -0.078369 -0.072025  0.587695  1.000000  0.777803\nE -0.090510 -0.079855  0.579891  0.777803  1.000000\n</code></pre>\n<p><strong>Part 1</strong></p>\n<pre><code># keep only upper triangle elements (excluding diagonal elements)\nmask_keep = np.triu(np.ones(corr.shape), k=1).astype('bool').reshape(corr.size)\n# melt (unpivot) the dataframe and apply mask\nsr = corr.stack()[mask_keep]\n# filter and get names\nedges = sr[sr &gt; 0.5].reset_index().values[:, :2]\n\nedges\nOut[25]: \narray([['A', 'B'],\n       ['A', 'C'],\n       ['B', 'C'],\n       ['C', 'D'],\n       ['C', 'E'],\n       ['D', 'E']], dtype=object)\n</code></pre>\n<p><strong>Part 2</strong></p>\n<pre><code>import networkx as nx\ng = nx.from_edgelist(edges)\nls_cliques = []\nfor clique in nx.algorithms.find_cliques(g):\n    ls_cliques.append(clique)\n\n# result\nls_cliques\nOut[26]: [['C', 'A', 'B'], ['C', 'D', 'E']]\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I had the same issue here: the length of the stacked correlation matrix differed from that of the mask.\nWhat worked for me was to keep the NaNs while stacking as follows:</p>\n<pre><code>sr = corr.stack([dropna=False][1])[mask_keep]\n</code></pre>\n<p>@<a href=\"https://stackoverflow.com/users/3218693/bill-huang\">billhuang</a> correctly states reasons why this could happen.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I was trying to make a variable inaccessible for a project I'm doing, and I ran across an SO post on <a href=\"https://stackoverflow.com/q/1641219/6505499\">Does Python have “private” variables in classes?</a>. For me, it raised some interesting questions that, to try and make this answerable, I'll label with <sup>Q1</sup> , <sup>Q2</sup> , etc. I've looked around, but I didn't find answers to the questions I'm asking, especially to those about sensitive data. </p>\n<p>I found useful stuff in <a href=\"https://stackoverflow.com/q/1641219/6505499\">that post</a>, but it seems that the general consensus was something like <em>if you see a variable with a <code>_</code> before it, act like an adult and realize you shouldn't be messing with it</em>. The same kind of idea was put forward for variables preceded by <code>__</code>. There, I got the general idea that you trust people not to use tricks like those described <a href=\"https://stackoverflow.com/questions/1641219/does-python-have-private-variables-in-classes#comment41361223_25614597\">here</a> and (in more detail) <a href=\"https://stackoverflow.com/a/32802486/6505499\">here</a>. I also found some good information at <a href=\"https://stackoverflow.com/q/28138734/6505499\">this SO post</a>.</p>\n<p>This is all very good advice when you're talking about good coding practices. </p>\n<p>I posted some thoughts in comments to the posts I've shared. My main question <a href=\"https://stackoverflow.com/questions/1641219/does-python-have-private-variables-in-classes#comment101593673_1641236\">was posted</a> as a comment.</p>\n<blockquote>\n<p>I'm surprised there hasn't been more discussion of those who want to introduce malicious code. This is a real question: <strong>Is there no way in Python to prevent a black-hat hacker from accessing your variables and methods and inserting code/data that could deny service, reveal personal (or proprietary company) information<sup>Q1</sup></strong>? <strong>If Python doesn't allow this type of security, should it ever be used for sensitive data<sup>Q2</sup></strong>?</p>\n</blockquote>\n<p>Am I totally missing something: <strong>Could a malicious coder even access variables and methods to insert code/data that could deny service or reveal sensitive data<sup>Q3</sup></strong>?</p>\n<p>I imagine I could be misunderstanding a concept, missing something, putting a problem in a place where it doesn't belong, or just being completely ignorant on what computer security is. However, I want to understand what's going on here. If I'm totally off the mark, I want an answer that tells me so, but I would also like to know how I'm totally off the mark and how to get back on it.</p>\n<p>Another part of the question I'm asking here is from another comment I made on those posts/answers. @SLott <a href=\"https://stackoverflow.com/a/1641305/6505499\">said</a> (somewhat paraphrased) </p>\n<blockquote>\n<p>... I've found that <code>private</code> and <code>protected</code> are very, very important design concepts. But as a practical matter, in tens of thousands of lines of Java and Python, I've never <em>actually</em> used <code>private</code> or <code>protected</code>. ... Here's my question \"protected [or private] from whom?\"</p>\n</blockquote>\n<p>To try and find out whether my concerns are anything to be concerned about, I <a href=\"https://stackoverflow.com/questions/1641219/does-python-have-private-variables-in-classes#comment101594236_1641305\">commented</a> on that post. Here it is, edited.</p>\n<blockquote>\n<p>Q: \"protected from whom?\" A: \"From malicious, black-hat hackers who would want to access variables and functions so as to be able to deny service, to access sensitive info, ...\" It seems the <code>A._no_touch = 5</code> approach would cause such a malicious coder to laugh at my \"please don't touch this\". My <code>A.__get_SSN(self)</code> seems to be just wishful hoping that B.H. (Black Hat) doesn't know the <code>x = A(); x._A__get_SSN()</code> trick (<a href=\"https://stackoverflow.com/questions/1641219/does-python-have-private-variables-in-classes#comment41361223_25614597\">trick by @Zorf</a>).</p>\n</blockquote>\n<p>I could be putting the problem in the wrong place, and if so, I'd like someone to tell me I'm putting the problem in the wrong place, but also to explain. <strong>Are there ways of being secure with a class-based approach<sup>Q4</sup></strong>? <strong>What other non-class-and-variable solutions are there for handling sensitive data in Python<sup>Q5</sup></strong>?</p>\n<p>Here's some code that shows why I see the answers to these questions as a reason for wondering <strong>if Python should ever be used for sensitive data <sup>Q2</sup></strong>. It's not complete code (why would I put these private values and methods down without using them anywhere?), but I hope it shows the type of thing I'm trying to ask about. I typed and ran all this at the Python interactive console.</p>\n<pre class=\"lang-py prettyprint-override\"><code>## Type this into the interpreter to define the class.\nclass A():\n  def __init__(self):\n    self.name = \"Nice guy.\"\n    self.just_a_4 = 4\n    self.my_number = 4\n    self._this_needs_to_be_pi = 3.14\n    self.__SSN = \"I hope you do not hack this...\"\n    self.__bank_acct_num = 123\n  def get_info():\n    print(\"Name, SSN, bank account.\")\n  def change_my_number(self, another_num):\n    self.my_number = another_num\n  def _get_more_info(self):\n    print(\"Address, health problems.\")\n  def send_private_info(self):\n    print(self.name, self.__SSN, self.__bank_acct_num)\n  def __give_20_bucks_to(self, ssn):\n    self.__SSN += \" has $20\"\n  def say_my_name(self):\n    print(\"my name\")\n  def say_my_real_name(self):\n    print(self.name)\n  def __say_my_bank(self):\n    print(str(self.__bank_acct_num))\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; my_a = A()\n&gt;&gt;&gt; my_a._this_needs_to_be_pi\n3.14\n&gt;&gt;&gt; my_a._this_needs_to_be_pi=4 # I just ignored begins-with-`_` 'rule'.\n&gt;&gt;&gt; my_a._this_needs_to_be_pi\n4\n\n## This next method could actually be setting up some kind of secure connection,  \n## I guess, which could send the private data. I just print it, here.\n&gt;&gt;&gt; my_a.send_private_info()\nNice guy. I hope you do not hack this... 123\n\n## Easy access and change a \"private\" variable\n&gt;&gt;&gt; my_a.__SSN\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nAttributeError: 'A' object has no attribute '__SSN'\n&gt;&gt;&gt; my_a.__dict__\n{'name': 'Nice guy.', 'just_a_4': 4, 'my_number': 4, '_this_needs_to_be_pi': 4, \n'_A__SSN': 'I hope you do not hack this...', '_A__bank_acct_num': 123}\n&gt;&gt;&gt; my_a._A__SSN\n'I hope you do not hack this...'\n\n# (maybe) potentially more dangerous\n&gt;&gt;&gt; def give_me_your_money(self, bank_num):\n      print(\"I don't know how to inject code, but I can\")\n      print(\"access your bank account number:\")\n      print(my_a._A__bank_acct_num)\n      print(\"and use my bank account number:\")\n      print(bank_num)\n&gt;&gt;&gt; give_me_your_money(my_a,345)\nI don't know how to inject code, but I can\naccess your bank account number:\n123\nand use my account number:\n345\n</code></pre>\n<p>At this point, I re-entered in the class definition, which probably wasn't necessary.</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; this_a = A()\n&gt;&gt;&gt; this_a.__give_20_bucks_to('unnecessary param')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nAttributeError: 'A' object has no attribute '__give_20_bucks_to'\n&gt;&gt;&gt; this_a._A__give_20_bucks_to('unnecessary param')\n&gt;&gt;&gt; this_a._A__SSN\n'I hope you do not hack this... has $20'\n\n## Adding a fake \"private\" variable, `this_a.__SSN`\n&gt;&gt;&gt; this_a.__SSN = \"B.H.'s SSN\"\n&gt;&gt;&gt; this_a.__dict__\n{'name': 'Nice guy.', 'just_a_4': 4, 'my_number': 4, '_this_needs_to_be_pi': 3.14, \n'_A__SSN': 'I hope you do not hack this... has $20', '_A__bank_acct_num': 123, \n'__SSN': \"B.H.'s SSN\"}\n&gt;&gt;&gt; this_a.__SSN\n\"B.H.'s SSN\"\n\n## Now, changing the real one and \"sending/stealing the money\"\n&gt;&gt;&gt; this_a._A__SSN = \"B.H.'s SSN\"\n&gt;&gt;&gt; this_a._A__give_20_bucks_to('unnecessary param')\n&gt;&gt;&gt; this_a._A__SSN\n\"B.H.'s SSN has $20\"\n</code></pre>\n<p>I've actually done some work at a previous contracting job with sensitive data - not SSNs and bank account numbers, but things like people's ages, addresses, phone numbers, personal history, marital and other relationship history, criminal records, etc. I wasn't involved in the programming to secure this data; I helped with trying to extract useful information by helping to ground-truth the data as preparation for machine learning. We had permission and legal go-aheads to work with such data. Another main question is this: <strong>How, in Python, could one collect, manage, analyze, and draw useful conclusions with this sensitive data<sup>Q6</sup></strong>? From what I've discussed here, it doesn't seem that classes (or any of the other data structures, which I didn't go into here, but which seem to have the same problems) would allow this to be done securely (privately or in a protected manner. I imagine that a class-based solution probably has something to do with compilation. <strong>Is this true<sup>Q7</sup></strong>?</p>\n<p>Finally, since it wasn't security, but code reliability that brought me here, I'll post another post I found and comment I made to complete my questions.</p>\n<p>@Marcin <a href=\"https://stackoverflow.com/a/28138772/6505499\">posted</a>,</p>\n<blockquote>\n<p>[In response to the OP's words,] \"The problem is simple. I want private variables to be accessed and changed only inside the class.\" [Marcin responded] So, don't write code outside the class that accesses variables starting with <code>__</code>. Use <code>pylint</code> or the like to catch style mistakes like that.</p>\n</blockquote>\n<p>My goal with <a href=\"https://stackoverflow.com/questions/28138734/how-do-i-make-private-variables-inaccessable-in-python?noredirect=1&amp;lq=1#comment101594321_28138772\">my following reply comment</a> was to see if my thoughts represent actual coding concerns. I hope it did't come across as rude</p>\n<blockquote>\n<p>It seems this answer would be nice if you wrote code only for your own personal enjoyment and never had to hand it on to someone else to maintain it. Any time you're in a collaborative coding environment (any post-secondary education and/or work experience), the code will be used by many. Someone down the line will want to use an easy way to change your <code>__you_really_should_not_touch_this</code> variable. They may have a good reason for doing so, but it's possible you set up your code such that their \"easy way\" is going to break things.</p>\n</blockquote>\n<p><strong>Is mine a valid point, or do most coders respect the double underscore<sup>Q8</sup></strong>? <strong>Is there a better way, using Python, to protect the integrity of the code - better than the <code>__</code> strategy<sup>Q9</sup></strong>?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p><code>private</code> and <code>protected</code> do not exist for <em>security</em>. They exist to enforce <em>contracts</em> within your code, namely logical <a href=\"https://en.wikipedia.org/wiki/Encapsulation_(computer_programming)\" rel=\"noreferrer\"><em>encapsulation</em></a>. If you mark a piece as <code>protected</code> or <code>private</code>, it means that it is a <strong><em>logical</em></strong> implementation detail of the implementing class, and no other code should touch it directly, since other code may not [be able to] use it correctly and may mess up state.</p>\n<p>E.g., if your logical rule is that whenever you change <code>self._a</code> you must also update <code>self._b</code> with a certain value, then you don't want external code to modify those variables, as your internal state may get messed up if the external code does not follow this rule. You want only your one class to handle this internally since that localises the potential points of failure.</p>\n<p>In the end all this gets compiled into a big ball of bytes anyway, and all the data is stored in memory at runtime. At that point there is no protection of individual memory offsets within the application's scope anyway, it's all just byte soup. <code>protected</code> and <code>private</code> are constraints the programmer imposes on their own code to keep their own logic straight. For this purpose, more or less informal conventions like <code>_</code> are perfectly adequate.</p>\n<p>An attacker cannot attack at the level of individual properties. The running software is a black box to them, whatever goes on internally doesn't matter. <em>If</em> an attacker is in a position to actually access individual memory offsets, or actually <strong><em>inject code</em></strong>, then it's pretty much game over either way. <code>protected</code> and <code>private</code> doesn't matter at that point.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ol>\n<li><p>What are the differences between the sklearnAPI(LGBMModel, LGBMClassifier etc) and default API(lgb.Dataset, lgb.cv, lgb.train) of lightgbm? Which one should I prefer using?</p></li>\n<li><p>Is it better to use lgb.cv or gridsearchcv/randomisedsearchcv of sklearn when using lightgbm?</p></li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<ol>\n<li><p>This answer has been well-covered <a href=\"https://stackoverflow.com/a/51077977/4300257\">here</a></p>\n</li>\n<li><p>Based on this <a href=\"https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb\" rel=\"nofollow noreferrer\">notebook</a> by Will Koehrsen, the sklearn cross validation API does not include the option for early stopping. Therefore, if you wish to use early stopping rounds(which can be very useful if you want to stop training when the validation score has not improved for a given number of estimators), it is better to use LightGBM cross validation (lgb.cv) function.</p>\n<p>Furthermore, an excerpt from Mikhail Lisyovi's <a href=\"https://stackoverflow.com/a/50316411/4300257\">answer</a> -  \"Technically, lightbgm.cv() allows you only to evaluate performance on a k-fold split with fixed model parameters. For hyper-parameter tuning you will need to run it in a loop providing different parameters and recoding averaged performance to choose the best parameter set. after the loop is complete. This interface is different from sklearn, which provides you with complete functionality to do hyperparameter optimisation in a CV loop. Personally, I would recommend to use the sklearn-API of lightgbm. It is just a wrapper around the native lightgbm.train() functionality, thus it is not slower. But it allows you to use the full stack of sklearn toolkit, thich makes your life MUCH easier.\"</p>\n</li>\n</ol>\n<p>Thus, which method you end up using depends on the context of the problem as well as what factors matter more to you - early_stopping_rounds or ease of hyperparameter optimisation over varying parameters.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Iam getting the error as </p>\n<blockquote>\n<p>\"ValueError: Expected 2D array, got 1D array instead: array=[  45000. \n  50000.   60000.   80000.  110000.  150000.  200000.  300000.\n    500000. 1000000.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it\n  contains a single sample.\"</p>\n</blockquote>\n<p>while executing the following code: </p>\n<pre><code># SVR\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Position_S.csv')\nX = dataset.iloc[:, 1:2].values\ny = dataset.iloc[:, 2].values\n\n # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX = sc_X.fit_transform(X)\ny = sc_y.fit_transform(y)\n\n# Fitting SVR to the dataset\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X, y)\n\n# Visualising the SVR results\nplt.scatter(X, y, color = 'red')\nplt.plot(X, regressor.predict(X), color = 'blue')\nplt.title('Truth or Bluff (SVR)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()\n\n# Visualising the SVR results (for higher resolution and smoother curve)\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X, y, color = 'red')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\nplt.title('Truth or Bluff (SVR)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Seems, expected dimension is wrong. Could you try:</p>\n<pre><code>regressor = SVR(kernel='rbf')\nregressor.fit(X.reshape(-1, 1), y)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a dataframe called \"dataframe\" that contains a bunch of information of sales on a certain date.  Each date entry is in the format of YYYY-MM-DD, and data ranges from 2012 to 2017.  I would like to split this data frame into 6 separate dataframes, one for each year.  So for example, the first split dataframe will have all the entries from 2012.</p>\n<p>I think I was able to do this in the code below.  I split the dataframe into one for each year and put them in the list \"years\".  However, when I try to run auto_arima on each dataframe I get the error \"Found input variables with inconsistent numbers of samples.\"</p>\n<p>I think this is because I'm not properly splitting my original dataframe correctly.  How do I properly split my dataframe based on year?</p>\n<pre><code>#Partition data into years\nyears = [g for n, g in dataframe.set_index('Date').groupby(pd.Grouper(freq='Y'))]\n\n#Create a list that will hold all auto_arima results for every dataframe\nstepwise_models = []\n\n#Call auto_arima on every dataframe\nfor x in range(len(years)-1):\n    currentDf = years[x]\n    model = auto_arima(currentDf['price'], exogenous=xreg, start_p=1, start_q=1,\n        max_p=3, max_q=3, m=12,\n        start_P=0, seasonal=True,\n        d=1, D=1, trace=True,\n        error_action='ignore',  \n        suppress_warnings=True, \n        stepwise=True)\n    stepwise_models.append(model) #Store current auto_arima result in our stepwise_models[] list\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If you want to split a dataframe by all available years you can do this by finding the unique years in your dataframe, then loop through these unique years and then use boolean indexing for filtering out in a loop each single year.</p>\n<p>So this idea could be implemented in a function like:</p>\n<pre><code>def split_years(dt):\n    dt['year'] = dt['Date'].dt.year\n    return [dt[dt['year'] == y] for y in dt['year'].unique()]\n</code></pre>\n<p>The result of the function above will be a list of dataframes, each with a single year.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use datetime accesor to filter the rows by year and create a new dataframe by year</p>\n<pre><code>import datetime as dt\ndataframe1=dataframe[dataframe['Date'].dt.year == 2012]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have multiple files with bits to analyse. First i read them into a list of BitString.Bits. Then i split each file bits into the specific parts i want to see and save them into a list of Pandas.DataFrames. One DF for each file.</p>\n<p>Now for further plotting and analysis purposes i want to store all data in one Xarray.Dataset, where i have the DataFrames stacked along the third axis with the name \"dataset\".</p>\n<p>I have tried to concat each DataFrame together to an DataSet:</p>\n<pre><code>xr.concat(data_df[:], dim=\"dataset\")\n</code></pre>\n<p>but i got an error saying that i cant concatenate other than DataArray or DataSets. Can i convert the DataFrames on the fly to DataArrays?</p>\n<p>Thanks for your help!</p>\n<p>Greetings from Germany</p>\n<p>Jan</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>you can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_xarray.html\" rel=\"noreferrer\">DataFrame.to_xarray()</a> method:</p>\n<pre><code>xr.concat([df.to_xarray() for df in data_df], dim=\"dataset\")\n</code></pre>\n<p>where <code>data_df</code> is a list of DataFrames</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I find it hard to explain with words what I want to achieve, so please don't judge me for showing a simple example instead. I have a table that looks like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>main_col</th>\n<th>some_metadata</th>\n<th>value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>this</td>\n<td>True</td>\n<td>10</td>\n</tr>\n<tr>\n<td>this</td>\n<td>False</td>\n<td>3</td>\n</tr>\n<tr>\n<td>that</td>\n<td>True</td>\n<td>50</td>\n</tr>\n<tr>\n<td>that</td>\n<td>False</td>\n<td>10</td>\n</tr>\n<tr>\n<td>other</td>\n<td>True</td>\n<td>20</td>\n</tr>\n<tr>\n<td>other</td>\n<td>False</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I want to normalize this data separately for each case of <code>main_col</code>. For example, if we're to choose min-max normalization and scale it to range [0; 100], I want the output to look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>main_col</th>\n<th>some_metadata</th>\n<th>value (normalized)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>this</td>\n<td>True</td>\n<td>100</td>\n</tr>\n<tr>\n<td>this</td>\n<td>False</td>\n<td>30</td>\n</tr>\n<tr>\n<td>that</td>\n<td>True</td>\n<td>100</td>\n</tr>\n<tr>\n<td>that</td>\n<td>False</td>\n<td>20</td>\n</tr>\n<tr>\n<td>other</td>\n<td>True</td>\n<td>100</td>\n</tr>\n<tr>\n<td>other</td>\n<td>False</td>\n<td>25</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Where for each case of <code>main_col</code>, the highest value is scaled to 100 and another value is scaled in respective proportion.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You can use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html\" rel=\"noreferrer\"><code>groupby.transform('max')</code></a> to get the max per group, then normalize in place:</p>\n<pre><code>df['value'] /= df.groupby('main_col')['value'].transform('max').div(100)\n</code></pre>\n<p>or:</p>\n<pre><code>df['value'] *= df.groupby('main_col')['value'].transform('max').rdiv(100)\n</code></pre>\n<p>output:</p>\n<pre><code>  main_col  some_metadata  value\n0     this           True  100.0\n1     this          False   30.0\n2     that           True  100.0\n3     that          False   20.0\n4    other           True  100.0\n5    other          False   25.0\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The normalization formula you are looking for is <code>100 * (x / x.max())</code>:</p>\n<pre><code>df.groupby(['main_col'])['value'].transform(lambda x: 100 * (x / x.max()))\n</code></pre>\n<p>Result:</p>\n<pre><code>0    100.0\n1     30.0\n2    100.0\n3     20.0\n4    100.0\n5     25.0\nName: value, dtype: float64\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have a temp DF that has the following data in it</p>\n<pre><code>Quarter\n2016Q3    146660510.0\n2016Q4    123641451.0\n2017Q1    125905843.0\n2017Q2    129656327.0\n2017Q3    126586708.0\n2017Q4    116804168.0\n2018Q1    118167263.0\n2018Q2    121633740.0\n2018Q3    125314447.0\n2018Q4    120994896.0\n2019Q1    126124709.0\n2019Q2    134753318.0\n</code></pre>\n<p>I'm passing this into <a href=\"https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html?highlight=seasonal_decompose#statsmodels.tsa.seasonal.seasonal_decompose\" rel=\"noreferrer\">seasonal_decompose</a> as quarterly data as per below but I get an error messsage saying \"Axis must have <code>freq</code> set to convert to Periods\". Frequency has been set in the DF. I think the issue is related to the fact you have to specify to matplotlib that the x axis is periods but I don't know how to do that and can't find any other examples where a <a href=\"https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.DecomposeResult.html#statsmodels.tsa.seasonal.DecomposeResult\" rel=\"noreferrer\">DecomposeResult</a> object with quarters is used</p>\n<pre><code>result = seasonal_decompose(temp, model='additive',period=4)  \nresult.plot()\nplt.show()\n</code></pre>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Just convert your PeriodIndex to a DatetimeIndex, that will solve the issue:</p>\n<pre><code>    df.index = df.index.to_timestamp()\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<pre><code> date=['2016Q3'\n ,'2016Q4'\n ,'2017Q1'\n ,'2017Q1'\n ,'2017Q3'\n ,'2017Q4'\n ,'2018Q1'\n ,'2018Q3'\n ,'2018Q3'\n ,'2018Q4'\n ,'2019Q1'\n ,'2019Q2']\n\n data=[146660510.0\n ,123641451.0\n ,125905843.0\n ,129656327.0\n ,126586708.0\n ,116804168.0\n ,118167263.0\n ,121633740.0\n ,125314447.0\n ,120994896.0\n ,126124709.0\n ,134753318.0]\n\n df=pd.DataFrame({'date':date,'data':data})\n df['date']=pd.to_datetime(df['date'])\n df=df.set_index('date')\n ax=df.plot(figsize=(14,2))\n plt.show()\n\n decomposition=sm.tsa.seasonal_decompose(x=df['data'],model='additive',      extrapolate_trend='freq', period=3)\n\n decomposition_trend=decomposition.trend\n ax= decomposition_trend.plot(figsize=(14,2))\n ax.set_xlabel('Date')\n ax.set_ylabel('Trend of time series')\n ax.set_title('Trend values of the time series')\n plt.show()\n\n decomposition_residual=decomposition.resid\n ax= decomposition_residual.plot(figsize=(14,2))\n ax.set_xlabel('Date')\n ax.set_ylabel('Residual of time series')\n ax.set_title('Residual values of the time series')\n plt.show()\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<div>\n<aside class=\"s-notice s-notice__info post-notice js-post-notice mb16\" role=\"status\">\n<div class=\"d-flex fd-column fw-nowrap\">\n<div class=\"d-flex fw-nowrap\">\n<div class=\"flex--item wmn0 fl1 lh-lg\">\n<div class=\"flex--item fl1 lh-lg\">\n<div>\n<b>This question already has answers here</b>:\n                                \n                            </div>\n</div>\n</div>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/38982807/are-a-wsgi-server-and-http-server-required-to-serve-a-flask-app\">Are a WSGI server and HTTP server required to serve a Flask app?</a>\n<span class=\"question-originals-answer-count\">\n                                (3 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/62492978/how-to-install-ssl-certificate-text-on-ubuntu-nginx\">How to install SSL certificate text on Ubuntu nginx</a>\n<span class=\"question-originals-answer-count\">\n                                (1 answer)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt4\">\n<a dir=\"ltr\" href=\"/questions/29458548/can-you-add-https-functionality-to-a-python-flask-web-server\">can you add HTTPS functionality to a python flask web server?</a>\n<span class=\"question-originals-answer-count\">\n                                (9 answers)\n                            </span>\n</div>\n<div class=\"flex--item mb0 mt8\">Closed <span class=\"relativetime\" title=\"2020-11-15 00:16:11Z\">3 years ago</span>.</div>\n</div>\n</aside>\n</div>\n<p>How can I enable HTTPS for a  DASH aplication running on a webserver with Python?</p>\n<p>I already have a SSL certificate (.key and .crt)</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If dash is the web server handling the routing (instead of Apache or Nginx),\nin your index.py file, on the part where you initiate the server, put the following code (replace local.crt and local.key with the absolute or relative path of your certificates):</p>\n<pre><code>if __name__ == \"__main__\":\n    context = ('local.crt','local.key')\n    app.run_server(host=\"192.168.200.172\", port=\"8050\", debug=True, ssl_context=context)\n</code></pre>\n<p>The address and the port is whatever you have on your server</p>\n<p>or with the run method</p>\n<pre><code>app.run(debug=True, ssl_context=context)\n</code></pre>\n<p>If, Nginx or apache is handling the reverse proxy, meaning, it receives the request from the client and then directs it to different apps, Dash for example, then you need to configure the SSL certificate in that server, and then it will redirect a http petition to the Dash, but it will be shown to the user as a Https.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am a bit confused on how Keras fits the models. In general, Keras models are fitted by simply using <code>model.fit(...)</code> something like the following:</p>\n<pre><code>model.fit(X_train, y_train, epochs=300, batch_size=64, validation_data=(X_test, y_test))\n</code></pre>\n<p>My question is: Because I stated the testing data by the argument <code>validation_data=(X_test, y_test)</code>, does it mean that each epoch is independent? In other words, I understand that at each epoch, Keras train the model using the training data (after getting shuffled) followed by testing the trained model using the provided validation_data. If that's the case, then no matter how many epochs I choose, I only take the results of the last epoch!!</p>\n<p>If this scenario is correct, so we do we need multiple epoches? Unless these epoches are dependent somwhow where each epoch uses the same NN weights from the previous epoch, correct?</p>\n<p>Thank you</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>When Keras fit your model it pass throught all the dataset at each epoch by a step corresponding to your batch_size.<br/>\nFor exemple if you have a dataset of 1000 items and a batch_size of 8, the weight of your model will be updated by using 8 items and this until it have seen all your data set.</p>\n<p>At the end of that epoch, the model will try to do a prediction on your validation set.<br/>\nIf we have made only one epoch, it would mean that the weight of the model is updated only once per element (because it only \"saw\" one time the complete dataset).<br/>\nBut in order to minimize the loss function and by backpropagation, we need to update those weights multiple times in order to reach the optimum loss, so pass throught all the dataset multiple times, in other word, multiple epochs.</p>\n<p>I hope i'm clear, ask if you need more informations.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe.</p>\n<p>I have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger):</p>\n<pre><code>In [116]: df0\nOut[116]:     \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n\n\nIn [118]: df1\nOut[118]: \n   A  D  E\n0  2  7  2\n1  6  5  5\n2  4  3  2\n3  0  1  0\n4  5  4  6\n5  0  1  0\n</code></pre>\n<p>And I want to have a new dataframe (or added to df0, whatever), as:</p>\n<pre><code>df2: \n   A  B  C  D  E\n0  0  1  0  1  0\n1  2  3  2  7  2\n2  4  5  4  3  2\n3  5  5  5  4  6\n</code></pre>\n<p>As you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2.</p>\n<p>Actually, I'm having trouble with the selection method. I can do this:</p>\n<pre><code>df1.loc[df1['A'].isin(df0['A'])]\n</code></pre>\n<p>But I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0).\nI've search here and I don't know see how to apply something like groupby, or even map.</p>\n<p>Any idea?</p>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is a basic application of <code>merge</code> (<a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\" rel=\"noreferrer\">docs</a>):</p>\n<pre><code>import pandas as pd\ndf2 = pd.merge(df0,df1, left_index=True, right_index=True)\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Lets say i have 10gb of csv file and i want to get the summary statistics of the file using DataFrame describe method.</p>\n<p>In this case first i need to create a DataFrame for all the 10gb csv data. </p>\n<pre><code>text_csv=Pandas.read_csv(\"target.csv\")\ndf=Pandas.DataFrame(text_csv)\ndf.describe()\n</code></pre>\n<p>Does this mean all the 10gb will get loaded in to memory and calculate the statistics?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Yes, I think you are right. And you can omit <code>df=Pandas.DataFrame(text_csv)</code>, because output from <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> is <code>DataFrame</code>:</p>\n<pre><code>import pandas as pd\n\ndf = pd.read_csv(\"target.csv\")\nprint df.describe()\n</code></pre>\n<p>Or you can use <a href=\"http://dask.pydata.org/en/latest/dataframe-api.html?highlight=describe#dask.dataframe.core.DataFrame.describe\" rel=\"noreferrer\">dask</a>:</p>\n<pre><code>import dask.dataframe as dd\n\ndf = dd.read_csv('target.csv.csv')\n\nprint df.describe()\n</code></pre>\n<p>You can use parameter <code>chunksize</code> of <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a>, but you get output <code>TextParser</code> <strong>not</strong> <code>DataFrame</code>, so then you need <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\" rel=\"noreferrer\"><code>concat</code></a>:</p>\n<pre><code>import pandas as pd\nimport io\n\ntemp=u\"\"\"a;b\n1;525\n1;526\n1;533\n2;527\n2;528\n2;532\n3;519\n3;534\n3;535\n4;530\n5;529\n5;531\n6;520\n6;521\n6;524\"\"\"\n#after testing replace io.StringIO(temp) to filename\n#chunksize = 2 for testing\ntp = pd.read_csv(io.StringIO(temp), sep=\";\", chunksize=2)\nprint tp\n&lt;pandas.io.parsers.TextFileReader object at 0x000000001995ADA0&gt;\ndf = pd.concat(tp, ignore_index=True)\nprint df.describe()\n               a           b\ncount  15.000000   15.000000\nmean    3.333333  527.600000\nstd     1.877181    5.082182\nmin     1.000000  519.000000\n25%     2.000000  524.500000\n50%     3.000000  528.000000\n75%     5.000000  531.500000\nmax     6.000000  535.000000\n</code></pre>\n<p>You can convert <code>TextFileReader</code> to <code>DataFrame</code>, but aggregate this output can be difficult:</p>\n<pre><code>import pandas as pd\n\nimport io\ntemp=u\"\"\"a;b\n1;525\n1;526\n1;533\n2;527\n2;528\n2;532\n3;519\n3;534\n3;535\n4;530\n5;529\n5;531\n6;520\n6;521\n6;524\"\"\"\n\n#after testing replace io.StringIO(temp) to filename\ntp = pd.read_csv(io.StringIO(temp), sep=\";\", chunksize=2)\nprint tp\n\ndfs = []\nfor t in tp:\n    df = pd.DataFrame(t)\n    df1 = df.describe()\n    dfs.append(df1.T)\n\ndf2 = pd.concat(dfs)\n</code></pre>\n<pre><code>print df2\n   count   mean        std  min     25%    50%     75%  max\na      2    1.0   0.000000    1    1.00    1.0    1.00    1\nb      2  525.5   0.707107  525  525.25  525.5  525.75  526\na      2    1.5   0.707107    1    1.25    1.5    1.75    2\nb      2  530.0   4.242641  527  528.50  530.0  531.50  533\na      2    2.0   0.000000    2    2.00    2.0    2.00    2\nb      2  530.0   2.828427  528  529.00  530.0  531.00  532\na      2    3.0   0.000000    3    3.00    3.0    3.00    3\nb      2  526.5  10.606602  519  522.75  526.5  530.25  534\na      2    3.5   0.707107    3    3.25    3.5    3.75    4\nb      2  532.5   3.535534  530  531.25  532.5  533.75  535\na      2    5.0   0.000000    5    5.00    5.0    5.00    5\nb      2  530.0   1.414214  529  529.50  530.0  530.50  531\na      2    6.0   0.000000    6    6.00    6.0    6.00    6\nb      2  520.5   0.707107  520  520.25  520.5  520.75  521\na      1    6.0        NaN    6    6.00    6.0    6.00    6\nb      1  524.0        NaN  524  524.00  524.0  524.00  524\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Seems there is no limitation of file size for <code>pandas.read_csv</code> method.</p>\n<p>According to @fickludd's and @Sebastian Raschka's answer in <a href=\"https://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas\">Large, persistent DataFrame in pandas</a>, you can use <code>iterator=True</code> and <code>chunksize=xxx</code> to load the giant csv file and calculate the statistics you want:</p>\n<pre><code>import pandas as pd\n\ndf = pd.read_csv('some_data.csv', iterator=True, chunksize=1000) # gives TextFileReader, which is iteratable with chunks of 1000 rows.\npartial_desc = df.describe()\n</code></pre>\n<p>And aggregate all the partial describe info all yourself.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to figure out the differences between PCA using Singular Value Decomposition as oppossed to PCA using Eigenvector-Decomposition.</p>\n<p>Picture the following matrix:</p>\n<pre><code> B = np.array([          [1, 2],\n                         [3, 4],\n                         [5, 6] ])\n</code></pre>\n<p>When computing the PCA of this matrix B using eigenvector-Decomposition, we follow these steps:</p>\n<ol>\n<li>Center the data (entries of B) by substracting the column-mean from each column</li>\n<li>Compute the covariance matrix <code>C = Cov(B) = B^T * B / (m -1)</code>, where m = # rows of B</li>\n<li>Find eigenvectors of C</li>\n<li><code>PCs = X * eigen_vecs</code></li>\n</ol>\n<p>When computing the PCA of matrix B using SVD, we follow these steps: </p>\n<ol>\n<li>Compute SVD of B: <code>B = U * Sigma * V.T</code></li>\n<li><code>PCs = U * Sigma</code></li>\n</ol>\n<p>I have done both for the given matrix. </p>\n<p>With eigenvector-Decomposition I obtain this result: </p>\n<pre><code>[[-2.82842712  0.        ]\n [ 0.          0.        ]\n [ 2.82842712  0.        ]]\n</code></pre>\n<p>With SVD I obtain this result:</p>\n<pre><code>[[-2.18941839  0.45436451]\n [-4.99846626  0.12383458]\n [-7.80751414 -0.20669536]]\n</code></pre>\n<p>The result obtained with eigenvector-Decomposition is the result given as solution. So, why is the result obtained with the SVD different? </p>\n<p>I know that: <code>C = Cov(B) = V * (Sigma^2)/(m-1)) * V.T</code> and I have a feeling this might be related to why the two results are different. Still. Can anyone help me understand better?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Please see below a comparision for your matrix with sklearn.decomposition.PCA and numpy.linalg.svd. Can you compare or post how you derived SVD results.</p>\n<p>Code for sklearn.decomposition.PCA:</p>\n<pre><code>from sklearn.decomposition import PCA\nimport numpy as np \nnp.set_printoptions(precision=3)\n\nB = np.array([[1.0,2], [3,4], [5,6]])\n\nB1 = B.copy() \nB1 -= np.mean(B1, axis=0) \nn_samples = B1.shape[0]\nprint(\"B1 is B after centering:\")\nprint(B1)\n\ncov_mat = np.cov(B1.T)\npca = PCA(n_components=2) \nX = pca.fit_transform(B1)\nprint(\"X\")\nprint(X)\n\neigenvecmat =   []\nprint(\"Eigenvectors:\")\nfor eigenvector in pca.components_:\n   if eigenvecmat == []:\n        eigenvecmat = eigenvector\n   else:\n        eigenvecmat = np.vstack((eigenvecmat, eigenvector))\n   print(eigenvector)\nprint(\"eigenvector-matrix\")\nprint(eigenvecmat)\n\nprint(\"CHECK FOR PCA:\")\nprint(\"X * eigenvector-matrix (=B1)\")\nprint(np.dot(PCs, eigenvecmat))\n</code></pre>\n<p>Output for PCA:</p>\n<pre><code>B1 is B after centering:\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\nX\n[[-2.828  0.   ]\n [ 0.     0.   ]\n [ 2.828  0.   ]]\nEigenvectors:\n[0.707 0.707]\n[-0.707  0.707]\neigenvector-matrix\n[[ 0.707  0.707]\n [-0.707  0.707]]\nCHECK FOR PCA:\nX * eigenvector-matrix (=B1)\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\n</code></pre>\n<p>numpy.linalg.svd:</p>\n<pre><code>print(\"B1 is B after centering:\")\nprint(B1)\n\nfrom numpy.linalg import svd \nU, S, Vt = svd(X1, full_matrices=True)\n\nprint(\"U:\")\nprint(U)\nprint(\"S used for building Sigma:\")\nprint(S)\nSigma = np.zeros((3, 2), dtype=float)\nSigma[:2, :2] = np.diag(S)\nprint(\"Sigma:\")\nprint(Sigma)\nprint(\"V already transposed:\")\nprint(Vt)\nprint(\"CHECK FOR SVD:\")\nprint(\"U * Sigma * Vt (=B1)\")\nprint(np.dot(U, np.dot(Sigma, Vt)))\n</code></pre>\n<p>Output for SVD:</p>\n<pre><code>B1 is B after centering:\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\nU:\n[[-0.707  0.     0.707]\n [ 0.     1.     0.   ]\n [ 0.707  0.     0.707]]\nS used for building Sigma:\n[4. 0.]\nSigma:\n[[4. 0.]\n [0. 0.]\n [0. 0.]]\nV already transposed:\n[[ 0.707  0.707]\n [-0.707  0.707]]\nCHECK FOR SVD:\nU * Sigma * Vt (=B1)\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am trying to filter data from a dataframe which are less than a certain value. If there is no NaN then its working fine. But when there is a nan then it is ignoring the NaN value. I want to include all the time its doesn't matter its less than or bigger than the comparing value. </p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        'index': [1, 2, 3,  4,  5,  6,   7,  8, 9],\n        'value': [5, 6, 7, np.nan, 9, 3, 11, 34, 78]\n    }\n)\n\ndf_chunked = df[(df['index'] &gt;= 1) &amp; (df['index'] &lt;= 5)]\n\nprint('df_chunked')\nprint(df_chunked)\n\ndf_result = df_chunked[(df_chunked['value'] &lt; 10)]\n# df_result = df_chunked[(df_chunked['value'] &lt; 10) | (df_chunked['value'] == np.isnan(df_chunked['value']))]\n\nprint('df_result')\nprint(df_result)\n</code></pre>\n<p><a href=\"https://i.sstatic.net/4MPxk.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.sstatic.net/4MPxk.png\"/></a></p>\n<p>In the above result 5,6,7,9 is showing. but i want also the nan there. I tried with </p>\n<pre><code>df_result = df_chunked[(df_chunked['value'] &lt; 10) | (df_chunked['value'] == np.isnan(df_chunked['value']))]\n</code></pre>\n<p>But it is not working. </p>\n<p>How can I do this?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Use <strong>not</strong> operator: <code>~</code></p>\n<pre><code>df_chunked[~(df_chunked['value'].ge(10))]\n#df_chunked[~(df_chunked['value']&gt;=10)] #greater or equal(the same)\n\n   index  value\n0      1    5.0\n1      2    6.0\n2      3    7.0\n3      4    NaN\n4      5    9.0\n</code></pre>\n<p><strong>why?</strong></p>\n<p>Because the logical operations simply <strong>ignore <code>NaN</code> values and take it as <code>False</code></strong>, always as you can see in the following data frame, then if you want to avoid using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isna.html\" rel=\"nofollow noreferrer\"><code>series.isna</code></a> <strong>(\navoid unnecessary additional code)</strong> and simplify your code simply use the inverse logic with <code>~</code></p>\n<pre><code>print(df.assign(greater_than_5 = df['value'].gt(5),\n          not_greater_than_5 = df['value'].le(5)))\n\n\n   index  value  greater_than_5  not_greater_than_5\n0      1    5.0           False                True\n1      2    6.0            True               False\n2      3    7.0            True               False\n3      4    NaN           False               False\n4      5    9.0            True               False\n5      6    3.0           False                True\n6      7   11.0            True               False\n7      8   34.0            True               False\n8      9   78.0            True               False\n</code></pre>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Try:</p>\n<pre><code>df_result = df_chunked[(df_chunked['value'] &lt; 10) | (df_chunked['value'].isna())]\ndf_result \n   index  value\n0      1    5.0\n1      2    6.0\n2      3    7.0\n3      4    NaN\n4      5    9.0\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have been using the PyTorch implementation of Google's <a href=\"https://github.com/google-research/bert#fine-tuning-with-bert\" rel=\"nofollow noreferrer\">BERT</a> by <a href=\"https://github.com/huggingface/pytorch-pretrained-BERT\" rel=\"nofollow noreferrer\">HuggingFace</a> for the MADE 1.0 dataset for quite some time now. Up until last time (11-Feb), I had been using the library and getting an <strong>F-Score</strong> of <strong>0.81</strong> for my Named Entity Recognition task by Fine Tuning the model. But this week when I ran the exact same code which had compiled and run earlier, it threw an error when executing this statement:</p>\n<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n</code></pre>\n<blockquote>\n<p>ValueError: Token indices sequence length is longer than the specified\n  maximum  sequence length for this BERT model (632 &gt; 512). Running this\n  sequence through BERT will result in indexing errors</p>\n</blockquote>\n<p>The full code is available in this <a href=\"https://colab.research.google.com/drive/1JxWdw1BjXZCFC2a8IwtZxvvq4rFGcxas\" rel=\"nofollow noreferrer\">colab notebook</a>.</p>\n<p>To get around this error I modified the above statement to the one below by taking the first 512 tokens of any sequence and made the necessary changes to add the index of [SEP] to the end of the truncated/padded sequence as required by BERT.</p>\n<pre><code>input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt[:512]) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n</code></pre>\n<p>The result shouldn't have changed because I am only considering the first 512 tokens in the sequence and later truncating to 75 as my (MAX_LEN=75) but my <strong>F-Score</strong> has dropped to <strong>0.40</strong> and my <strong>precision</strong> to <strong>0.27</strong> while the <strong>Recall</strong> remains the same <strong>(0.85)</strong>. I am unable to share the dataset as I have signed a confidentiality clause but I can assure all the preprocessing as required by BERT has been done and all extended tokens like (Johanson --&gt; Johan ##son) have been tagged with X and replaced later after the prediction as said in the <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow noreferrer\">BERT Paper</a>.</p>\n<p>Has anyone else faced a similar issue or can elaborate on what might be the issue or what changes the PyTorch (Huggingface) people have done on their end recently?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I've found a fix to get around this.\nRunning the same code with pytorch-pretrained-bert==0.4.0 solves the issue and the performance is restored to normal.\nThere's something messing with the model performance in BERT Tokenizer or BERTForTokenClassification in the new update which is affecting the model performance.\nHoping that HuggingFace clears this up soon. :)</p>\n<p>pytorch-pretrained-bert==0.4.0, Test F1-Score: 0.82</p>\n<p>pytorch-pretrained-bert==0.6.1, Test F1-Score: 0.41</p>\n<p>Thanks.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I think you should use <code>batch_encode_plus</code> and mask output as well as the encoding.</p>\n<p>Please see batch_encode_plus in <a href=\"https://huggingface.co/transformers/main_classes/tokenizer.html\" rel=\"nofollow noreferrer\">https://huggingface.co/transformers/main_classes/tokenizer.html</a></p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Is there any GAS libraries for data analysis and matrix operations? Something like Python <code>numpy</code> &amp; <code>pandas</code> or JavaScript <code>numjs</code> &amp;<code>undescorejs</code>. I want to do some statistical operations in GAS. </p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You might want to consider <strong>d3.js</strong></p>\n<pre><code>  var d3Url = \"https://d3js.org/d3.v5.min.js\";\n  eval(UrlFetchApp.fetch(d3Url).getContentText());\n  var array = [4,5,5,9];\n  Logger.log(d3.max(array));\n  Logger.log(d3.mean(array));\n  Logger.log(d3.median(array));\n  Logger.log(d3.sum(array));\n</code></pre>\n<p>More detail <a href=\"https://github.com/d3/d3-array/blob/master/README.md#statistics\" rel=\"noreferrer\">here</a>.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have the following dataset represented like <code>numpy</code> array</p>\n<pre><code>direccion_viento_pos\n\n    Out[32]:\n\n    array([['S'],\n           ['S'],\n           ['S'],\n           ...,\n           ['SO'],\n           ['NO'],\n           ['SO']], dtype=object)\n</code></pre>\n<p>The dimension of this array is:</p>\n<pre><code>direccion_viento_pos.shape\n(17249, 8)\n</code></pre>\n<p>I am using python and scikit learn to encode these categorical variables in this way:</p>\n<pre><code>from __future__ import unicode_literals\nimport pandas as pd\nimport numpy as np\n# from sklearn import preprocessing\n# from matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n</code></pre>\n<p>Then I create a label encoder object:</p>\n<pre><code>labelencoder_direccion_viento_pos = LabelEncoder() \n</code></pre>\n<p>I take the column position 0 (the unique column) of the <code>direccion_viento_pos</code> and apply the <code>fit_transform()</code> method addressing all their rows:</p>\n<pre><code> direccion_viento_pos[:, 0] = labelencoder_direccion_viento_pos.fit_transform(direccion_viento_pos[:, 0]) \n</code></pre>\n<p>My <code>direccion_viento_pos</code> is of this way:</p>\n<pre><code>direccion_viento_pos[:, 0]\narray([5, 5, 5, ..., 7, 3, 7], dtype=object)\n</code></pre>\n<p>Until this moment, each row/observation of <code>direccion_viento_pos</code> have a numeric value, but I want solve the inconvenient of weight in the sense that there are rows with a value more higher than others.</p>\n<p>Due to this, I create the dummy variables, <a href=\"https://discuss.analyticsvidhya.com/t/what-is-a-dummy-variable/18960\" rel=\"noreferrer\">which according to this reference</a> are:</p>\n<blockquote>\n<p>A Dummy variable or Indicator Variable is an artificial variable created to represent an attribute with two or more distinct categories/levels</p>\n</blockquote>\n<p>Then, in my <code>direccion_viento_pos</code> context, I have 8 values</p>\n<ul>\n<li><code>SO</code> - Sur oeste</li>\n<li><code>SE</code> - Sur este</li>\n<li><code>S</code> - Sur </li>\n<li><code>N</code> - Norte</li>\n<li><code>NO</code> - Nor oeste</li>\n<li><code>NE</code> - Nor este</li>\n<li><code>O</code> - Oeste</li>\n<li><code>E</code> - Este</li>\n</ul>\n<p>This mean, 8 categories.\nNext, I create a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\" rel=\"noreferrer\" title=\"sklearn.preprocessing.OneHotEncoder\">OneHotEncoder</a> object with the <code>categorical_features</code> attribute which specifies what features will be treated like categorical variables.</p>\n<p><code>onehotencoder = OneHotEncoder(categorical_features = [0])</code></p>\n<p>And apply this <code>onehotencoder</code> to our <code>direccion_viento_pos</code> matrix.</p>\n<p><code>direccion_viento_pos = onehotencoder.fit_transform(direccion_viento_pos).toarray()</code></p>\n<p>My <code>direccion_viento_pos</code> with their categorized variables has stayed so:</p>\n<pre><code>direccion_viento_pos\n\narray([[0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n</code></pre>\n<p>Then, until here, I've created dummy variables to each category.</p>\n<p><img alt=\"Dirección del viento categorizada\" src=\"https://cldup.com/W-GUXDLAXD-3000x3000.png\" title=\"Dirección del viento categorizada\"/></p>\n<p>I wanted to narrate this process, to arrive at my question.</p>\n<p>If these dummy encoder variables already in a 0-1 range, is necessary apply the MinMaxScaler feature scaling?</p>\n<p>Some say that it is not necessary to scale these fictitious variables. Others say that if necessary because we want accuracy in predictions</p>\n<p>I ask this question due to when I apply the <code>MinMaxScaler</code> with the <code>feature_range=(0, 1)</code>\nmy values have been changed in some positions ... despite to still keep this scale.</p>\n<p>What is the best option which can I have to choose with respect to my dataset <code>direccion_viento_pos</code></p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't think scaling them will change the answer at all. They're all on the same scale already. Min 0, max 1, range 1. If some continuous variables were present, you'd want to normalize the continuous variables only, leaving the dummy variables alone. You could use the min-max scaler to give those continuous variables the same minimum of zero, max of one, range of 1. Then your regression slopes would be very easy to interpret. Your dummy variables are already normalized. </p>\n<p>Here's a <a href=\"https://stats.stackexchange.com/questions/59392/should-you-ever-standardise-binary-variables\">related question</a> asking if one should ever standardize binary variables. </p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>So I have the following code-</p>\n<pre><code>params = {'n_estimators': [1000, 2000], 'max_depth': [10, 20], 'min_samples_split': [2, 3],\n          'learning_rate': [0.1, 0.05, 0.01], 'loss': ('ls', 'huber', 'lad', 'quantile'), 'verbose': [1]}\ngbr = ensemble.GradientBoostingRegressor()\nclf = GridSearchCV(gbr, params)\n</code></pre>\n<p>And, although I don't think I've given it much to contemplate, it's taking FOREVER to determine the best parameters. After watching for a couple hours I've seen great instances (<code>verbose:[1]</code>) and wish to stop it from continuing. I don't want to have to come back to this in the morning and hope that it's finished. I want to finish it now without losing the hours of tuning in the process.</p>\n<p>Is there a safe way I can stop the tuning and still have the best result?</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I use the following approach to get optimal hyperparameters for DNN (which you can use for yourself):</p>\n<ul>\n<li>for loop for parameter #1</li>\n<li>inside for loop for parameter #2</li>\n<li>...</li>\n<li>inside you fit your model and save/print results, pictures etc.</li>\n</ul>\n<p>Sure, if you ctrl-c your running code your pictures, logs will no be removed.</p>\n<hr/>\n<p>But as I see here, you do not have much possible parameter combinations inside. So why it runs forever:</p>\n<ol>\n<li>Data is extremely big (take subsample)</li>\n<li>Number of cv-s is very big (take less than 10)</li>\n<li>Reduce # of estimators. 1000-2000 is too much</li>\n<li>Take small max_features</li>\n<li>Reduce max_depth. 10-20 is too much I believe. For most cases it should be &lt;10 (try with 3-5 for example)</li>\n<li>Use XGBoost (it's a lot faster). In general, XGBoost and GBR are the same (and both tend not to overfit with a growing number of estimators) but XGBoost has some additions that helps in this better.</li>\n<li>Run on some server or upgrade your PC :)</li>\n</ol>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>If this question is anything to go by, <a href=\"https://stackoverflow.com/questions/39377052/if-i-interrupt-sklearn-grid-search-fit-before-completion-can-i-access-the-curr\">then the answer appears to be no.</a>\nI know when I did this a few weeks back, I too got impatient. I just ctrl+f'ed through the logs for numbers I was hoping it would achieve. Starting at 95,94,93,... etc.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm trying to calculate the <a href=\"https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index\" rel=\"nofollow noreferrer\">Davies-Bouldin Index</a> in Python.</p>\n<p>Here are the steps the code below tries to reproduce.</p>\n<p><strong>5 Steps</strong>:</p>\n<ol>\n<li>For each cluster, compute euclidean distances between each point to the centroid</li>\n<li>For each cluster, compute the average of these distances</li>\n<li>For each pair of clusters, compute the euclidean distance between their centroids</li>\n</ol>\n<p>Then,</p>\n<ol start=\"4\">\n<li>For each pair of clusters, make the sum of the average distances to their respective centroid (computed at step 2) and divide it by the distance separating them (computed at step 3).</li>\n</ol>\n<p>Finally,</p>\n<ol start=\"5\">\n<li>Compute the mean of all these divisions (= all indexes) to get the Davies-Bouldin index of the whole clustering</li>\n</ol>\n<p><strong>Code</strong></p>\n<pre><code>def daviesbouldin(X, labels, centroids):\n\n    import numpy as np\n    from scipy.spatial.distance import pdist, euclidean\n    \n    nbre_of_clusters = len(centroids) #Get the number of clusters\n    distances = [[] for e in range(nbre_of_clusters)] #Store intra-cluster distances by cluster\n    distances_means = [] #Store the mean of these distances\n    DB_indexes = [] #Store Davies_Boulin index of each pair of cluster\n    second_cluster_idx = [] #Store index of the second cluster of each pair\n    first_cluster_idx = 0 #Set index of first cluster of each pair to 0\n    \n    # Step 1: Compute euclidean distances between each point of a cluster to their centroid\n    for cluster in range(nbre_of_clusters):\n        for point in range(X[labels == cluster].shape[0]):\n            distances[cluster].append(euclidean(X[labels == cluster][point], centroids[cluster]))\n    \n    # Step 2: Compute the mean of these distances\n    for e in distances:\n        distances_means.append(np.mean(e))\n  \n    # Step 3: Compute euclidean distances between each pair of centroid\n    ctrds_distance = pdist(centroids) \n     \n    # Tricky step 4: Compute Davies-Bouldin index of each pair of cluster   \n    for i, e in enumerate(e for start in range(1, nbre_of_clusters) for e in range(start, nbre_of_clusters)):\n        second_cluster_idx.append(e)\n        if second_cluster_idx[i-1] == nbre_of_clusters - 1:\n            first_cluster_idx += 1\n        DB_indexes.append((distances_means[first_cluster_idx] + distances_means[e]) / ctrds_distance[i])\n     \n    # Step 5: Compute the mean of all DB_indexes   \n    print(\"DAVIES-BOULDIN Index: %.5f\" % np.mean(DB_indexes)) \n</code></pre>\n<p>In arguments:</p>\n<ul>\n<li><code>X</code> is the data</li>\n<li><code>labels</code>, are the labels computed by a clustering algorithm (i.e: kmeans)</li>\n<li><code>centroids</code> are the coordinates of each cluster's centroid (i.e: <code>cluster_centers_</code>)</li>\n</ul>\n<p>Also, note that I'm using Python 3</p>\n<p><strong>QUESTION1</strong>: Is the computation of euclidean distances between each pair of centroid correct (step 3)?</p>\n<p><strong>QUESTION2</strong>: Is my implementation of step 4 correct?</p>\n<p><strong>QUESTION3</strong>: Do I need to normalise intra and inter cluster distances ?</p>\n<hr/>\n<p><em><strong>Further explanations on Step 4</strong></em></p>\n<p>Let's say we have 10 clusters.\nThe loop <em>should</em> compute the DB index of each pair of cluster.</p>\n<p>At the first iteration:</p>\n<ul>\n<li>sums intra-distances mean of cluster 1 (index 0 of <code>distances_means</code>) and intra-distances mean of cluster 2 (index 1 of <code>distances_means</code>)</li>\n<li>divides this sum by the distance between the 2 clusters (index 0 of <code>ctrds_distance</code>)</li>\n</ul>\n<p>At the second iteration:</p>\n<ul>\n<li>sums intra-distances mean of cluster 1 (index 0 of <code>distances_means</code>) and intra-distances mean of cluster 3 (index 2 of <code>distances_means</code>)</li>\n<li>divides this sum by the distance between the 2 clusters (index 1 of <code>ctrds_distance</code>)</li>\n</ul>\n<p>and so on...</p>\n<p>With the example of 10 clusters, the full iteration process should look like this:</p>\n<pre><code>intra-cluster distance intra-cluster distance       distance between their\n      of cluster:             of cluster:           centroids(storage num):\n         0           +             1            /             0\n         0           +             2            /             1\n         0           +             3            /             2\n         0           +             4            /             3\n         0           +             5            /             4\n         0           +             6            /             5\n         0           +             7            /             6\n         0           +             8            /             7\n         0           +             9            /             8\n         1           +             2            /             9\n         1           +             3            /             10\n         1           +             4            /             11\n         1           +             5            /             12\n         1           +             6            /             13\n         1           +             7            /             14\n         1           +             8            /             15\n         1           +             9            /             16\n         2           +             3            /             17\n         2           +             4            /             18\n         2           +             5            /             19\n         2           +             6            /             20\n         2           +             7            /             21\n         2           +             8            /             22\n         2           +             9            /             23\n         3           +             4            /             24\n         3           +             5            /             25\n         3           +             6            /             26\n         3           +             7            /             27\n         3           +             8            /             28\n         3           +             9            /             29\n         4           +             5            /             30\n         4           +             6            /             31\n         4           +             7            /             32\n         4           +             8            /             33\n         4           +             9            /             34\n         5           +             6            /             35\n         5           +             7            /             36\n         5           +             8            /             37\n         5           +             9            /             38\n         6           +             7            /             39\n         6           +             8            /             40\n         6           +             9            /             41\n         7           +             8            /             42\n         7           +             9            /             43\n         8           +             9            /             44\n</code></pre>\n<p>The problem here is I'm not quite sure that the index of <code>distances_means</code> matches the index of <code>ctrds_distance</code>.</p>\n<p>In other words, I'm not sure that the first inter-cluster distance computed corresponds to the distance between cluster 1 and cluster 2. And that the second inter-cluster distance computed corresponds to the distance between cluster 3 and cluster 1... and so on, following the pattern above.</p>\n<p>In short: I'm afraid I'm dividing pairs of intra-cluster distances by an inter-cluster distance that is not corresponding.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Here is a shorter, faster corrected version of the Davies-Bouldin index naive implementation above.</p>\n<pre><code>def DaviesBouldin(X, labels):\n    n_cluster = len(np.bincount(labels))\n    cluster_k = [X[labels == k] for k in range(n_cluster)]\n    centroids = [np.mean(k, axis = 0) for k in cluster_k]\n    variances = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)]\n    db = []\n\n    for i in range(n_cluster):\n        for j in range(n_cluster):\n            if j != i:\n                db.append((variances[i] + variances[j]) / euclidean(centroids[i], centroids[j]))\n\n    return(np.max(db) / n_cluster)\n</code></pre>\n<p>Answering my own questions:</p>\n<ul>\n<li>the counter on the first draft (step 4) was correct BUT irrelevant </li>\n<li>there's no need to normalise intra and inter cluster distances</li>\n<li>there was a mistake when calculating Euclidean distances</li>\n</ul>\n<p>Note you can find innovative approaches that try to improve this index, notably the \"<a href=\"https://www.researchgate.net/publication/312485976_New_Version_of_Davies-Bouldin_Index_for_Clustering_Validation_Based_on_Cylindrical_Distance\" rel=\"nofollow noreferrer\">New Version of Davies-Bouldin Index</a>\" that replaces Euclidean distance by Cylindrical distance. </p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>thank you for your implementation. I just have one question: Is there missing a division in the last row. In the last step the value of max(db) should be divided by the implemented number of clusters.</p>\n<pre><code>def DaviesBouldin(Daten, DatenLabels):\nn_cluster = len(np.bincount(DatenLabels)) \ncluster_k = [Daten[DatenLabels == k] for k in range(n_cluster)] \ncentroids = [np.mean(k, axis = 0) for k in cluster_k] \nvariances = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)] # mittlere Entfernung zum jeweiligen Clusterzentrum\ndb = []\n\nfor i in range(n_cluster):\n    for j in range(n_cluster):\n        if j != i:\n            db.append((variances[i] + variances[j]) / euclidean(centroids[i], centroids[j]) / n_cluster)\nreturn(np.max(db))\n</code></pre>\n<p>Maybe I oversee that division because I'm new to Python. But in my graphics (I'm iterating over a range of clusters) the value of DB.max is very low at the beginning and increases afterwards. After the Scaling by the number of clusters the graph looks better (high DB.max value at the beginning and constantly falling with increasing number of clusters).</p>\n<p>Best regards</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Thanks for the code and revision - really helped me to get started. The shorter, faster version is not entirely correct. I amended it to correctly average the dispersion scores of the most similar cluster for each cluster.     </p>\n<p>See <a href=\"https://www.researchgate.net/publication/224377470_A_Cluster_Separation_Measure\" rel=\"nofollow noreferrer\">https://www.researchgate.net/publication/224377470_A_Cluster_Separation_Measure</a> for original algorithm and explanation:</p>\n<blockquote>\n<p>The DBI is the average of the similarity measures of each cluster with\n  its most similar cluster. </p>\n</blockquote>\n<pre><code>def DaviesBouldin(X, labels):\n    n_cluster = len(np.bincount(labels))\n    cluster_k = [X[labels == k] for k in range(n_cluster)]\n    centroids = [np.mean(k, axis = 0) for k in cluster_k]\n\n    # calculate cluster dispersion\n    S = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)]\n    Ri = []\n\n    for i in range(n_cluster):\n        Rij = []\n        # establish similarity between each cluster and all other clusters\n        for j in range(n_cluster):\n            if j != i:\n                r = (S[i] + S[j]) / euclidean(centroids[i], centroids[j])\n                Rij.append(r)\n         # select Ri value of most similar cluster\n         Ri.append(max(Rij)) \n\n    # get mean of all Ri values    \n    dbi = np.mean(Ri)\n\n    return dbi\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am doing a Kaggle tutorial for Titanic using the Datacamp platform.</p>\n<p>I understand the use of .loc within Pandas - to select values by row using column labels... </p>\n<p>My confusion comes from the fact that in the Datacamp tutorial, we want to locate all the \"Male\" inputs within the \"Sex\" column, and replace it with the value of 0. They use the following piece of code to do it:</p>\n<pre><code>titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n</code></pre>\n<p>Can someone please explain how this works? I thought .loc took inputs of row and column, so what is the == for?</p>\n<p>Shouldn't it be:</p>\n<pre><code>titanic.loc[\"male\", \"Sex\"] = 0\n</code></pre>\n<p>Thanks!</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>It set column <code>Sex</code> to <code>1</code> if condition is <code>True</code> only, another values are untouched:</p>\n<pre><code>titanic[\"Sex\"] == \"male\"\n</code></pre>\n<p>Sample:</p>\n<pre><code>titanic = pd.DataFrame({'Sex':['male','female', 'male']})\nprint (titanic)\n      Sex\n0    male\n1  female\n2    male\n\nprint (titanic[\"Sex\"] == \"male\")\n0     True\n1    False\n2     True\nName: Sex, dtype: bool\n\ntitanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\nprint (titanic)\n\n0       0\n1  female\n2       0\n</code></pre>\n<p>It is very similar by <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"noreferrer\"><code>boolean indexing</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.loc.html\" rel=\"noreferrer\"><code>loc</code></a> - it select only values of column <code>Sex</code> by condition:</p>\n<pre><code>print (titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"])\n0    male\n2    male\nName: Sex, dtype: object\n</code></pre>\n<p>But I think here better is use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\" rel=\"noreferrer\"><code>map</code></a> if only <code>male</code> and <code>female</code> values need convert to some another values:</p>\n<pre><code>titanic = pd.DataFrame({'Sex':['male','female', 'male']})\ntitanic[\"Sex\"] = titanic[\"Sex\"].map({'male':0, 'female':1})\nprint (titanic)\n   Sex\n0    0\n1    1\n2    0\n</code></pre>\n<p>EDIT:</p>\n<p>Primary <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\" rel=\"noreferrer\"><code>loc</code></a> is used for set new value by index and columns:</p>\n<pre><code>titanic = pd.DataFrame({'Sex':['male','female', 'male']}, index=['a','b','c'])\nprint (titanic)\n      Sex\na    male\nb  female\nc    male\n\ntitanic.loc[\"a\", \"Sex\"] = 0\nprint (titanic)\n      Sex\na       0\nb  female\nc    male\n\ntitanic.loc[[\"a\", \"b\"], \"Sex\"] = 0\nprint (titanic)\n    Sex\na     0\nb     0\nc  male\n</code></pre>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I looked around online but couldn't find anything, but I may well have missed a piece of literature on this.  I am running a basic neural net on a 289 component vector to produce a 285 component vector.  In my input, the last 4 pieces of data are critical to change the rest of the input into the resultant 285 for the output.  That is to say, the input is 285 + 4, such that the 4 morph the rest of the input into the output.</p>\n<p>But when running a neural network on this, I am not sure how to reflect this.  Would I need to use convolution on the rest of the input? I want my system to emphasize the 4 data points that critically affect the other 285.  I am still new to all of this, so a few pointers would be great!</p>\n<p>Again, if there is something already written on this, then that would be awesome too.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I don't think you have any reason doing this since the network will infer that on its own. The weights will be reduced or enhanced for each input according to their importance considering the output.</p>\n<p>What you could do though, is to have a preliminary network that is going to have the 285 component as an input, and then a new network that is going to have the 4 critical components and the output of the preliminary network as an input.</p>\n<pre><code>[285 compo.]---[neural network]---+---[neural network]---[output 285 compo.]\n                                  |\n                       [4 compo.]-+\n</code></pre>\n<p>For instance, you could treat a picture with convolution networks and then add some meta information later in a fully connected network to process everything.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>The neural network should more or less learn this thing by itself. Especially with newer approaches like deep learning &amp; friends, where the amount of hand-tuning is almost zero. However, this does assume that the function which you're trying to learn is learnable and that the system you use has enough power to learn it. That's a function of the complexity of the network involved (number of layers, nodes, types of activations etc.), the learning algorithms involved, as well as the data you supply.</p>\n<p>It's really hard to tell without knowing more about the domain you're addressing? What sort of signals are we talking about (I assume they're signals since you speak of convolution)? What are the four inputs about? I assume they have a different modality than the other 285.</p>\n<p>Perhaps this <a href=\"http://webcache.googleusercontent.com/search?q=cache%3ayhYe72_20OoJ%3aftp://ftp.sas.com/pub/neural/importance.html%20&amp;cd=1&amp;hl=en&amp;ct=clnk\" rel=\"nofollow\">doc</a> will help a little bit though.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Theoretically, you can let the network try to learn this relationship. However, there are good reasons to try to rethink the way you're formulating the problem. Also, the difficulty a neural network will have learning this function is going to depend strongly on your specific problem (and the best way to figure it out is probably just to try it and find out).</p>\n<p>Let me try to help by making an analogy to a simpler problem: let's take your 289-element vector and assume that 285 elements take values from -1 to 1 and the remaining four take values from -1000 to 1000. This maintains your original premise: that the four variables are somehow far more important in determining the output than the 285. (I understand that this loses the coupled relationship between the variables, but let's run with the example anyways.)</p>\n<p>This is a simpler example for two reasons:</p>\n<ol>\n<li><p>it's easier to see why it's harder to learn</p></li>\n<li><p>there are a bag of well-understood tricks to solve it</p></li>\n</ol>\n<p>Compared to a scenario where all 289 inputs have the same input range, a gradient descent algorithm will be slower to converge on the heterogeneous case. (Extra credit: try this!) Geoff Hinton has a rather famous set of slides which describes this effect fairly well: <a href=\"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\" rel=\"nofollow\">Lecture 6</a>. I believe this is also part of a Coursera course now.</p>\n<p>Hinton's slides also touch on two ways to attack this simpler version of the problem. The first is just to pre-process your inputs. If you scale down the inputs to have the same mean and variance, your gradient descent optimizer will converge more quickly. The other is to use a more powerful optimization method, specifically one with per-parameter adaptive learning rates, which handles this case as well as trickier scenarios. Andrej Karpathy's <a href=\"http://cs231n.github.io/neural-networks-3/#ada\" rel=\"nofollow\">fantastic notes</a> from Stanford's CS231n class are a good intro.</p>\n<p>But let's tie this back to <em>your</em> problem: that there are four \"special\" variables which transform the entire input. Given enough time and input, it's possible that a network can learn this function. But understand that if this transformation is complex and makes the optimization landscape rough, your network will likely have some trouble dealing with it.</p>\n<p>If there's a way to transform your representation of the problem to avoid this link, I'd say try to pursue that. If not, then be prepared to resort to some bigger guns to solve the problem.</p>\n<p>Without knowing the specifics of your problem, it's hard to give more concrete advice. Plus, ultimately, you're the one that will be solving it, so you're going to be the expert eventually!</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I want all the articles related to specific tag example sustainability from documents. But it is only returing me Four articles. There are total 7 articles related to sustainability in vectorstore out of 20 articles.</p>\n<p>Here is my code:</p>\n<pre><code>import pinecone\nfrom langchain.schema import Document\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Pinecone\nimport os\nfrom langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\n\npinecone_api_key = \"xxxxxxxx\"\npinecone_env = \"xxxxxxxxxx\"\n# pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"])\npinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n\nembeddings = OpenAIEmbeddings()\n\nindex_name=\"langchain-self-retriever-ppo\"\nindex = pinecone.Index(index_name)\ntext_field = \"text\"\n\n# vectorstore = Pinecone.from_documents(\n#     docs, embeddings, index_name=\"langchain-self-retriever-demo\"\n# )\n\nvectorstore = Pinecone(\n    index, embeddings.embed_query, text_field\n)\n\nmetadata_field_info=[\n    AttributeInfo(\n        name=\"headline\",\n        description=\"The headline of the news article\", \n        type=\"string or list[string]\", \n    ),\n    AttributeInfo(\n        name=\"date\",\n        description=\"The date, news article was published\", \n        type=\"integer\", \n    ),\n    AttributeInfo(\n        name=\"publication\",\n        description=\"The name of the publication which published this news article\", \n        type=\"string\", \n    ),\n    AttributeInfo(\n        name=\"domain\",\n        description=\"The domain of the news article\",\n        type=\"float\"\n    ),\n]\ndocument_content_description = \"Brief summary of a news article\"\nllm = OpenAI(temperature=0)\n# retriever = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description, metadata_field_info, verbose=True)\n\nretriever = SelfQueryRetriever.from_llm(\n    llm, \n    vectorstore, \n    document_content_description, \n    metadata_field_info, \n    enable_limit=True,\n    verbose=True\n)\n\n# This example only specifies a relevant query\nretrieved_docs = retriever.get_relevant_documents(\"Articles which are related to sustainability\")\nprint(retrieved_docs)\nprint(len(retrieved_docs))\n</code></pre>\n<p>I have gone inside <code>get_relevant_documents</code> method here it uses <code>self.vectorstore.search</code> which calls <code>self.similarity_search</code> method which by defaults sets limit to 4 if not given.</p>\n<p>I tried setting limit to 7 it returned 7 <code>sustainability</code> articles.\nBut I wouldn't know how much articles will be related to <code>sustainability</code> so I can't by default set the limit.</p>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Add <code>search_kwargs={\"k\": k}</code> as an argument to the retriever class and replace k with the desired number of results.</p>\n</div>",
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>One option is to change the retriever method to \"similarity_score_threshold\" as described on the <a href=\"https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore#similarity-score-threshold-retrieval\" rel=\"nofollow noreferrer\">Langchain site</a>, e.g.:</p>\n<pre><code>retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5})\n</code></pre>\n<p>You still need to adjust the \"k\" argument if you do this. Otherwise, I found that it still defaults to k=4 max documents returned. As a workaround, I set \"k\" to the length of the full set of documents so that it returns every document above the \"score_threshold\", e.g.:</p>\n<pre><code>retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.7, \"k\": len(docs)})\n</code></pre>\n<p>The downside is that this requires experimenting with score thresholds, which is an inexact science.</p>\n</div>"
        ]
    },
    {
        "question": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Most of the libraries like requests or matplotlib don't include proper documentation of kwargs/args. There are sometimes examples but mostly the specific use case is missing.</p>\n<p>My Questions:</p>\n<ol>\n<li>Where can I find that sorta information.</li>\n<li>Why developers don't document the kwargs/args properly</li>\n</ol>\n</div>",
        "answers": [
            "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I just try to find the source in many instances like that. Usually, if it's not documented, the args are being passed to some lower level function. Once you know what low-level function the higher-level function is deferring to, the purpose will make more sense.</p>\n<p>For example, take a look at the docs for <a href=\"https://requests.readthedocs.io/en/master/api/#requests.request\" rel=\"nofollow noreferrer\"><code>requests.request</code></a>. As you mention, it shows that that method takes a <code>kwargs</code>, but doesn't mention its use. It <em>does</em> however give us a handy <a href=\"https://requests.readthedocs.io/en/master/_modules/requests/api/#request\" rel=\"nofollow noreferrer\">link to the source</a>, which shows:</p>\n<pre><code>def request(method, url, **kwargs):\n    . . .\n    with sessions.Session() as session:\n        return session.request(method=method, url=url, **kwargs)\n</code></pre>\n<p>So, we can see that it's a fairly thin wrapper over <code>sessions</code>' instance method <code>request</code>, where it just passes the <code>kwargs</code> down.</p>\n<p>What if we check the <a href=\"https://requests.readthedocs.io/en/master/_modules/requests/sessions/#Session\" rel=\"nofollow noreferrer\">source for that method?</a>:</p>\n<pre><code>def request(self, method, url,\n            params=None, data=None, headers=None, cookies=None, files=None,\n            auth=None, timeout=None, allow_redirects=True, proxies=None,\n            hooks=None, stream=None, verify=None, cert=None, json=None):\n    . . .\n</code></pre>\n<p>We can see that the <code>kwargs</code> get expanded, and would be expected to be one of these parameters. At that point, we can check <a href=\"https://2.python-requests.org/en/master/api/#requests.Session.request\" rel=\"nofollow noreferrer\">the documentation for that method</a> to get a better idea about what each parameter does.</p>\n<p>I'll note that if you're using Pycharm, you can <kbd>ctrl</kbd>+<kbd>b</kbd> over top of a symbol to jump to its source, so you don't even need to track down the source to do any sleuthing.</p>\n<hr/>\n<p>Why not document them? People are lazy and/or miss important details when writing things. They may have expected that it's \"intuitive enough\" that documenting every detail is unnecessary. Who knows. Sometimes, you learn more reading the source than you do the documentation for certain details.</p>\n</div>"
        ]
    }
]