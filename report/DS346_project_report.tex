\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{titlesec}

\geometry{margin=1in}

% Modify section formatting to prevent page breaks
\titleformat{\section}{\normalfont\Large\bfseries}{}{0em}{}
\titleformat{\subsection}[runin]{\normalfont\bfseries}{}{1em}{}
\titlespacing{\section}{0pt}{*3}{*1}
\titlespacing{\subsection}{0pt}{*2}{*1}

\title{Your Paper Title}

\author{
    \fbox{%
        \begin{minipage}{0.3\textwidth}
            \centering
            Arlo Steyn\\
            Department of Computer Science\\
            University of Stellenbosch\\
            24713848\\
            \texttt{24713848@sun.ac.za}
        \end{minipage}%  
    }%
    \hfill
    \fbox{%
        \begin{minipage}{0.3\textwidth}
            \centering
            Andre van der Merwe\\
            Department of Computer Science\\
            University of Stellenbosch\\
            24923273\\
            \texttt{24923273@sun.ac.za}
        \end{minipage}%
    }%
    \hfill
    \fbox{%
        \begin{minipage}{0.3\textwidth}
            \centering
            Stephan Delport\\
            Department of Computer Science\\
            University of Stellenbosch\\
            242710083\\
            \texttt{242710083@sun.ac.za}
        \end{minipage}%
    }
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract of the paper. Summarise the main points and contributions here.
\end{abstract}

\section{Introduction}
This is where you introduce the topic of your paper.

\section{Web Scraping Cross Validated}

This section of the report provides an overview of the Cross Validated website,
details about the scraped data, and the tools used in the scraping process.

\subsection{Overview of Cross Validated}

\subsection{Data Overview}

\subsection{Tools and Techniques for Data Scraping}

\section{Wrangling The Scraped Data}

\subsection{Data Cleaning}

\subsection{Text Normalisation}

\subsection{Structuring And Saving The Data}

\section{LLaMA Model Overview and Fine-tuning Process}

We used the Large Language Model Meta AI (LLaMA) model, specifically the 3-billion parameter version,
which is part of a family of transformer-based models. Llama is an open source LLM provided by Meta
and we specifically used the LLaMA-3B-Instruct model for question-and-answering. LLaMA is designed for natural
language processing tasks, and its architecture follows a standard decoder-only transformer,
making it suitable for autoregressive text generation. This model is ideal for tasks like question answering,
summarisation, and dialogue generation, but also has many other abilities and applications.

\subsection{Model Architecture}
The LLaMA model is built on the transformer architecture, which uses self attention mechanisms to process input
sequences and generate output. It is also autoregressive which means that it generates text one token at a time,
using previously generated tokens to predict the next one.

Key components of the LLaMA model architecture include:
\begin{itemize}
    \item Multi-Head Self-Attention: This allows the model to focus on different parts of the input sequence at the same time, which allows it to capture relationships between words across long sequences.
    \item Attention Heads: 32 heads per layer
    \item Attention Head Dimension: 128 dimensions per head
    \item Feed-Forward Networks (FFN): These are fully connected layers that transform the output of the self-attention mechanism.
    \item Feed-Forward Network Dimension: 11008 units (intermediate layer)
    \item Model Size: 3 billion parameters
    \item Number of Layers: 32 transformer layers
    \item Hidden Size: 4096 hidden units per layer
    \item Positional Encoding: Rotary positional encodings (RoPE)
    \item Layer Normalisation: Applied before self-attention and feed-forward layers
\end{itemize}

\subsection{Pre-Trained Weights and Dataset}

The pre-trained weights for the LLaMA model were obtained from the Unsloth repository, which
provided a specialised model version fine-tuned for question-and-answer and conversational tasks.
The initial pre-training of LLaMA was done on a large corpus of diverse text, including books,
research papers, and web data. This gives the model a strong foundation in understanding complex
language structures and domain specific terminology.

We fine-tuned the model using a custom dataset of 39,668 question-and-answer pairs, which were scraped
from `Cross Validated' which focused on data science, artificial intelligence (AI), and machine learning (ML) topics.
These questions ranged from simple definitions to more complex conceptual explanations. Fine-tuning allowed the
model to better capture the complexity of these technical subjects, which makes it more accurate in responding and
understanding to domain specific applications.

\subsection{Fine-Tuning Process}

The version of LLaMA we used was fine-tuned for question-and-answering tasks, making it ideal for providing answers
to questions related to data science, AI, and ML.
Fine-tuning was performed using the Low-Rank Adaptation (LoRA) technique, which allows for efficient fine-tuning
of large models by freezing most of the pre-trained model parameters and only updating a small subset of parameters.
This drastically reduces the computational resources required, and also still achives high accuracy. We used
a combination of gradient accumulation and batch processing to handle the relatively large dataset size during
the fine-tuning phase.

The fine-tuning process involved adjusting several hyperparameters, including:
\begin{itemize}
    \item Learning rate: Set to 2e-4 for optimal convergence without overfitting.
    \item Batch size: A batch size of 4 per GPU was used to maximise the use of available memory.
    \item Gradient accumulation: This was set to 4, allowing larger effective batch sizes without exceeding memory limits.
\end{itemize}
We also used 4-bit quantization during training to optimise memory usage, enabling us to fine-tune the model on hardware with limited GPU resources.

\section{UnSloth Optimisation}

\subsection{What is UnSloth Optimisation}

\subsection{How UnSloth Optimisation Works}

\subsection{Benefits of UnSloth in Our Llama-3.2 Model}

\section{Results}

\section{Ethics}

\section{Conclusion}

\section{References}
\begin{thebibliography}{9}

\bibitem{llama3herd}
Meta AI Research. \textit{The LLaMA-3: Herd of Models}. Available at: \url{https://ai.meta.com/research/publications/the-llama-3-herd-of-models/}. Accessed: 2024-10-13.

\bibitem{unslothdocs}
Unsloth Documentation Team. \textit{Unsloth AI Documentation}. Available at: \url{https://docs.unsloth.ai/}. Accessed: 2024-10-13.

\bibitem{unslothfine-tune}
Unsloth AI Tutorials. \textit{How to Fine-Tune Llama 3 and Export to Ollama}. Available at: \url{https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama}. Accessed: 2024-10-13.

\bibitem{stackexchange}
Stack Exchange Inc. \textit{Cross Validated - Statistical Analysis, Machine Learning, Data Mining, and Data Visualization}. Available at: \url{https://stats.stackexchange.com/}. Accessed: 2024-10-12.

\bibitem{googlesearch}
Google Search. \textit{Search Results: Cross Validated robots.txt}. Available at: \url{https://www.google.com/search?q=cross+validated+robots.txt&rlz=1C1GCEU_en-gbZA1068ZA1068&oq=cross+val}. Accessed: 2024-10-12.

\end{thebibliography}


\end{document}
